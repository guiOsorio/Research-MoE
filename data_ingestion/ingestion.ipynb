{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read .docx files\n",
    "def read_docx(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "# Function to read .pdf files\n",
    "def read_pdf(file_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"../research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store file content\n",
    "file_contents = {}\n",
    "\n",
    "# Traverse the directory and read files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file.endswith('.pdf'):\n",
    "            content = read_pdf(file_path)\n",
    "            file_contents[file] = {}\n",
    "            file_contents[file][\"content\"] = content\n",
    "        # elif file in ('MoE Notes FINAL.docx', 'MoE Notes.docx'):\n",
    "        #     content = read_docx(file_path)\n",
    "        #     file_contents[file] = {}\n",
    "        #     file_contents[file][\"content\"] = content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_names = {'MoE Notes.docx': (\"MoE NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE%20Notes.docx\"),\n",
    " 'MoE Notes FINAL.docx': (\"MoE NOTES FINAL\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE%20Notes%20FINAL.docx\"),\n",
    " 'Adaptive_Mixture_of_Local_Experts.pdf': (\"Adaptive Mixture of Local Experts\", \"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\"),\n",
    " 'Unified_Scaling_Laws_NOTES.pdf': (\"Unified Scaling Laws for Routes Language Models NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Unified_Scaling_Laws_NOTES.pdf\"),\n",
    " 'Switch_Transformers.pdf': (\"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\", \"https://arxiv.org/abs/2101.03961\"),\n",
    " 'ST_MoE.docx': (\"ST-MoE: Designing Stable and Transferable Sparse Expert Models\", \"https://arxiv.org/abs/2202.08906\"),\n",
    " 'GLaM.pdf': (\"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\", \"https://arxiv.org/abs/2112.06905\"),\n",
    " 'GShard.pdf': (\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"https://arxiv.org/abs/2006.16668\"),\n",
    " 'Unified_Scaling_Laws.pdf': (\"Unified Scaling Laws for Routes Language Models\", \"https://arxiv.org/abs/2202.01169\"),\n",
    " 'ST_MoE.pdf': (\"ST-MoE: Designing Stable and Transferable Sparse Expert Models\", \"https://arxiv.org/abs/2202.08906\"),\n",
    " 'GLaM_NOTES.pdf': (\"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GLaM_NOTES.pdf\"),\n",
    " 'Efficient_Large_Scale_LM.docx': (\"Efficient Large Scale Language Modeling with Mixtures of Experts\", \"https://arxiv.org/abs/2112.10684\"),\n",
    " 'Unified_Scaling_Laws.docx': (\"Unified Scaling Laws for Routed Language Models\", \"https://arxiv.org/abs/2202.01169\"),\n",
    " 'GShard.docx': (\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\", \"https://arxiv.org/abs/2006.16668\"),\n",
    " 'ST_MoE_NOTES.pdf': (\"ST-MoE: Designing Stable and Transferable Sparse Expert Models NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/ST_MoE_NOTES.pdf\"),\n",
    " 'Switch_Transformers.docx': (\"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\", \"https://arxiv.org/abs/2101.03961\"),\n",
    " 'GLaM.docx': (\"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\", \"https://arxiv.org/abs/2112.06905\"),\n",
    " 'Switch_Transformers_NOTES.pdf': (\"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Switch_Transformers_NOTES.pdf\"),\n",
    " 'GShard_NOTES.pdf': (\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GShard_NOTES.pdf\"),\n",
    " 'Efficient_Large_Scale_LM.pdf': (\"Efficient Large Scale Language Modeling with Mixtures of Experts\", \"https://arxiv.org/abs/2112.10684\"),\n",
    " 'Efficient_Large_Scale_LM_NOTES.pdf': (\"Efficient Large Scale Language Modeling with Mixtures of Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Efficient_Large_Scale_LM_NOTES.pdf\"),\n",
    " 'Benefits_of_ELMs_NOTES.docx': (\"Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Benefits_of_ELMs_NOTES.pdf\"),\n",
    " 'BTM_NOTES.docx': (\"Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BTM_NOTES.pdf\"),\n",
    " 'BTM.pdf': (\"Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models\", \"https://arxiv.org/abs/2208.03306\"),\n",
    " 'Benefits_of_ELMs.pdf': (\"Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES\", \"https://arxiv.org/abs/2302.03202\"),\n",
    " 'Expert_Gate_NOTES.pdf': (\"Expert Gate: Lifelong Learning with a Network of Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Expert_Gate_NOTES.pdf\"),\n",
    " 'BeyondDistillation_Task_Level_MoE.pdf': (\"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference\", \"https://arxiv.org/abs/2110.03742\"),\n",
    " 'cBTM_NOTES.docx': (\"Scaling Expert Language Models with Unsupervised Domain Discovery NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/cBTM_NOTES.pdf\"),\n",
    " 'Expert_Gate_NOTES.docx': (\"Expert Gate: Lifelong Learning with a Network of Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Expert_Gate_NOTES.pdf\"),\n",
    " 'BeyondDistillation_Task_Level_MoE_NOTES.docx': (\"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BeyondDistillation_Task_Level_MoE_NOTES.pdf\"),\n",
    " 'DEMix_NOTES.pdf': (\"DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/DEMix_NOTES.pdf\"),\n",
    " 'BTM_NOTES.pdf': (\"Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BTM_NOTES.pdf\"),\n",
    " 'DEMix.pdf': (\"DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES\", \"https://arxiv.org/abs/2108.05036\"),\n",
    " 'cBTM_NOTES.pdf': (\"Scaling Expert Language Models with Unsupervised Domain Discovery NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/cBTM_NOTES.pdf\"),\n",
    " 'BeyondDistillation_Task_Level_MoE_NOTES.pdf': (\"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BeyondDistillation_Task_Level_MoE_NOTES.pdf\"),\n",
    " 'Expert_Gate.pdf': (\"Expert Gate: Lifelong Learning with a Network of Experts\", \"https://arxiv.org/abs/1611.06194\"),\n",
    " 'cBTM.pdf': (\"Scaling Expert Language Models with Unsupervised Domain Discovery\", \"https://arxiv.org/abs/2303.14177\"),\n",
    " 'DEMix_NOTES.docx': (\"DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/DEMix_NOTES.pdf\"),\n",
    " 'Benefits_of_ELMs_NOTES.pdf': (\"Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Benefits_of_ELMs_NOTES.pdf\"),\n",
    " 'MoE_Mamba.pdf': (\"MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\", \"https://arxiv.org/abs/2401.04081\"),\n",
    " 'MoE_meets_instruction_tuning_NOTES.docx': (\"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf\"),\n",
    " 'BlackMamba.pdf': (\"BlackMamba: Mixture of Experts for State-Space Models\", \"https://arxiv.org/abs/2402.01771\"),\n",
    " 'Soft_Merging_of_Experts_NOTES.docx': (\"Soft Merging of Experts with Adaptive Routing NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Soft_Merging_of_Experts_NOTES.pdf\"),\n",
    " 'MoE_meets_instruction_tuning.pdf': (\"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\", \"https://arxiv.org/abs/2305.14705\"),\n",
    " 'EvoMoE_NOTES.docx': (\"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/EvoMoE_NOTES.docx\"),\n",
    " 'MoE_Mamba_NOTES.pdf': (\"MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_Mamba_NOTES.pdf\"),\n",
    " 'Sparse_Upcycling.pdf': (\"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints\", \"https://arxiv.org/abs/2212.05055\"),\n",
    " 'Soft_Merging_of_Experts.pdf': (\"Soft Merging of Experts with Adaptive Routing\", \"https://arxiv.org/abs/2306.03745\"),\n",
    " 'Sparse_Upcycling_NOTES.pdf': (\"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Sparse_Upcycling_NOTES.pdf\"),\n",
    " 'MoE_meets_instruction_tuning_NOTES.pdf': (\"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf\"),\n",
    " 'Soft_Merging_of_Experts_NOTES.pdf': (\"Soft Merging of Experts with Adaptive Routing NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Soft_Merging_of_Experts_NOTES.pdf\"),\n",
    " 'EvoMoE.pdf': (\"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate\", \"https://arxiv.org/abs/2112.14397\"),\n",
    " 'BlackMamba_NOTES.pdf': (\"BlackMamba: Mixture of Experts for State-Space Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/BlackMamba_NOTES.pdf\"),\n",
    " 'MoE_Mamba_NOTES.docx': (\"MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_Mamba_NOTES.pdf\"),\n",
    " 'Sparse_Upcycling_NOTES.docx': (\"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Sparse_Upcycling_NOTES.pdf\"),\n",
    " 'EvoMoE_NOTES.pdf': (\"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate NOTES\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/EvoMoE_NOTES.docx\"),\n",
    " 'BlackMamba_NOTES.docx': (\"BlackMamba: Mixture of Experts for State-Space Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/BlackMamba_NOTES.pdf\"),\n",
    " 'PE_SparsityCrafting_NOTES.pdf': (\"Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_SparsityCrafting_NOTES.pdf\"),\n",
    " 'MegaBlocks.pdf': (\"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\", \"https://arxiv.org/abs/2211.15841\"),\n",
    " 'QMoE.pdf': (\"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\", \"https://arxiv.org/abs/2310.16795\"),\n",
    " 'QMoE_NOTES.pdf': (\"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/QMoE_NOTES.pdf\"),\n",
    " 'PE_SparsityCrafting_NOTES.docx': (\"Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_SparsityCrafting_NOTES.pdf\"),\n",
    " 'PE_MoE_for_LMs_NOTES.docx': (\"Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_MoE_for_LMs_NOTES.pdf\"),\n",
    " 'FastInferenceMoE.pdf': (\"Fast-Inference of Mixture-of-Experts Language Models with Offloading\", \"https://arxiv.org/abs/2312.17238\"),\n",
    " 'ExtremelyPE_MoE_for_InstructionTuning_NOTES.docx': (\"Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/ExtremelyPE_MoE_for_InstructionTuning_NOTES.docx\"),\n",
    " 'QMoE_NOTES.docx': (\"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/QMoE_NOTES.pdf\"),\n",
    " 'ExtremelyPE_MoE_for_InstructionTuning.pdf': (\"Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning\", \"https://arxiv.org/abs/2309.05444\"),\n",
    " 'FastInferenceMoE_NOTES.docx': (\"Fast-Inference of Mixture-of-Experts Language Models with Offloading\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FastInferenceMoE_NOTES.pdf\"),\n",
    " 'MegaBlocks_NOTES.pdf': (\"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/MegaBlocks_NOTES.pdf\"),\n",
    " 'PE_SparsityCrafting.pdf': (\"Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\", \"https://arxiv.org/abs/2401.02731\"),\n",
    " 'FastInferenceMoE_NOTES.pdf': (\"Fast-Inference of Mixture-of-Experts Language Models with Offloading\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FastInferenceMoE_NOTES.pdf\"),\n",
    " 'MegaBlocks_NOTES.docx': (\"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/MegaBlocks_NOTES.pdf\"),\n",
    " 'PE_MoE_for_LMs.pdf': (\"Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models\", \"https://arxiv.org/abs/2203.01104\"),\n",
    " 'PE_MoE_for_LMs_NOTES.pdf': (\"Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_MoE_for_LMs_NOTES.pdf\"),\n",
    " 'FFFs_NOTES.docx': (\"Fast Feedforward Networks\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FFFs/FFFs_NOTES.pdf\"),\n",
    " 'FFFs_NOTES.pdf': (\"Fast Feedforward Networks\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FFFs/FFFs_NOTES.pdf\"),\n",
    " 'FFF.pdf': (\"Fast Feedforward Networks\", \"https://arxiv.org/abs/2308.14711\"),\n",
    " 'FFF_to_language.pdf': (\"Exponentially Faster Language Modeling\", \"https://arxiv.org/abs/2311.10770\"),\n",
    " 'MixtureOfTokens_NOTES.docx': (\"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf\"),\n",
    " 'BASE_layers_NOTES.docx': (\"BASE Layers: Simplifying Training of Large, Sparse Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/BASE_layers_NOTES.pdf\"),\n",
    " 'DSelect_k_NOTES.pdf': (\"DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DSelect_k_NOTES.pdf\"),\n",
    " 'StableMoE_NOTES.docx': (\"StableMoE: Stable Routing Strategy for Mixture of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/StableMoE_NOTES.pdf\"),\n",
    " 'Hash_Layers_NOTES.pdf': (\"Hash Layers for Large Sparse Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Hash_Layers_NOTES.pdf\"),\n",
    " 'Expert_Choice_NOTES.docx': (\"Mixture-of-Experts with Expert Choice Routing\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Expert_Choice_NOTES.pdf\"),\n",
    " 'Soft_MoE_NOTES.pdf': (\"From Sparse to Soft Mixture of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Soft_MoE_NOTES.pdf\"),\n",
    " 'Soft_MoE_NOTES.docx': (\"From Sparse to Soft Mixture of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Soft_MoE_NOTES.pdf\"),\n",
    " 'Expert_Choice_NOTES.pdf': (\"Mixture-of-Experts with Expert Choice Routing\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Expert_Choice_NOTES.pdf\"),\n",
    " 'Expert_Choice.pdf': (\"Mixture-of-Experts with Expert Choice Routing\", \"https://arxiv.org/abs/2202.09368\"),\n",
    " 'Soft_MoE.pdf': (\"From Sparse to Soft Mixture of Experts\", \"https://arxiv.org/abs/2308.00951\"),\n",
    " 'BASE_layers.pdf': (\"BASE Layers: Simplifying Training of Large, Sparse Models\", \"https://arxiv.org/abs/2103.16716\"),\n",
    " 'MixtureOfTokens.pdf': (\"Mixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation\", \"https://arxiv.org/abs/2310.15961\"),\n",
    " 'DeepSeekMoE.pdf': (\"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\", \"https://arxiv.org/abs/2401.06066\"),\n",
    " 'StableMoE.pdf': (\"StableMoE: Stable Routing Strategy for Mixture of Experts\", \"https://arxiv.org/abs/2204.08396\"),\n",
    " 'DeepSeekMoE_NOTES.pdf': (\"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DeepSeekMoE_NOTES.pdf\"),\n",
    " 'DSelect_k_NOTES.docx': (\"DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DSelect_k_NOTES.pdf\"),\n",
    " 'MixtureOfTokens_NOTES.pdf': (\"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf\"),\n",
    " 'Hash_Layers_NOTES.docx': (\"Hash Layers for Large Sparse Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Hash_Layers_NOTES.pdf\"),\n",
    " 'StableMoE_NOTES.pdf': (\"StableMoE: Stable Routing Strategy for Mixture of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/StableMoE_NOTES.pdf\"),\n",
    " 'BASE_layers_NOTES.pdf': (\"BASE Layers: Simplifying Training of Large, Sparse Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/BASE_layers_NOTES.pdf\"),\n",
    " 'DeepSeekMoE_NOTES.docx': (\"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DeepSeekMoE_NOTES.pdf\"),\n",
    " 'DSelect_k.pdf': (\"DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning\", \"https://arxiv.org/abs/2106.03760\"),\n",
    " 'Hash_Layers.pdf': (\"Hash Layers for Large Sparse Models\", \"https://arxiv.org/abs/2106.04426\"),\n",
    " 'Mistral.pdf': (\"Mistral 7B\", \"https://arxiv.org/abs/2310.06825\"),\n",
    " 'Mixtral.pdf': (\"Mixtral of Experts\", \"https://arxiv.org/abs/2401.04088\"),\n",
    " 'Mixtral_NOTES.docx': (\"Mixtral of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Mixtral/Mixtral_NOTES.pdf\"),\n",
    " 'Mixtral_NOTES.pdf': (\"Mixtral of Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Mixtral/Mixtral_NOTES.pdf\"),\n",
    " 'Towards_Understanding_MoE_NOTES.pdf': (\"Towards Understanding MoE\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Towards_Understanding_MoE_NOTES.pdf\"),\n",
    " 'Towards_Understanding_MoE.pdf': (\"Towards Understanding MoE\", \"https://arxiv.org/abs/2208.02813\"),\n",
    " 'Mixture of Experts Explained_HF_NOTES.docx': (\"HuggingFace MoE Article\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Mixture%20of%20Experts%20Explained_HF_NOTES.pdf\"),\n",
    " 'Mixture of Experts Explained_HF_NOTES.pdf': (\"HuggingFace MoE Article\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Mixture%20of%20Experts%20Explained_HF_NOTES.pdf\"),\n",
    " 'Sparsely_Gated_MoE_NOTES.pdf': (\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Sparsely_Gated_MoE_NOTES.pdf\"),\n",
    " 'Sparsely_Gated_MoE.pdf': (\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\", \"https://arxiv.org/abs/1701.06538\"),\n",
    " 'Towards_Understanding_MoE_NOTES.docx': (\"Towards Understanding MoE\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Towards_Understanding_MoE_NOTES.pdf\"),\n",
    " 'Learning_Factored_Representations_NOTES.docx': (\"Learning Factorized Representations in a Deep Mixture-of-Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf\"),\n",
    " 'Original_MoE.pdf': (\"Adaptive Mixture of Local Experts\", \"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\"),\n",
    " 'Learning_Factored_Representations_NOTES.pdf': (\"Learning Factorized Representations in a Deep Mixture-of-Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf\"),\n",
    " 'Mixture of Experts Explained_HF.pdf': (\"HuggingFace MoE Article\", \"https://huggingface.co/blog/moe\"),\n",
    " 'Sparsely_Gated_MoE_NOTES.docx': (\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Sparsely_Gated_MoE_NOTES.pdf\"),\n",
    " 'OpenMoE.pdf': (\"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\", \"https://arxiv.org/abs/2402.01739\"),\n",
    " 'OpenMoE_NOTES.docx': (\"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/OpenMoE_NOTES.pdf\"),\n",
    " 'MoESurvey.pdf': (\"A Review of Sparse Expert Models in Deep Learning\", \"https://arxiv.org/abs/2209.01667\"),\n",
    " 'OpenMoE_NOTES.pdf': (\"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/OpenMoE_NOTES.pdf\"),\n",
    " 'Learning_Factored_Representations.pdf': (\"Learning Factorized Representations in a Deep Mixture-of-Experts\", \"https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf\"),\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (file, content) in paper_names.items():\n",
    "    # if (file in ('MoE Notes FINAL.docx', 'MoE Notes.docx')) or (file.endswith('.pdf')):\n",
    "    if file.endswith('.pdf'):\n",
    "        file_contents[file]['source_name'] = content[0]\n",
    "        file_contents[file]['source_url'] = content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Scaling_Laws_NOTES.pdf #1\n",
      "{'content': 'Unified Scaling Laws For Routed Language Models \\nMain idea: this paper investigates the scaling behaviors of routing networks, more \\nspecifically in the axis of parameter count (in terms of total number of parameters) and \\ncomputational requirements (total number of active parameters).  \\nRouting: \\nIt experiments with 3 different routing techniques: \\n- \\nAn approach based on BASE (linear programming). \\no This represents a more traditional learned algorithm for routing. BASE in \\nspecific approaches routing as a linear programming problem, which \\nnaturally distributes tokens evenly through experts (no load balancing \\nissues). The algorithm experimented with has slight modifications to BASE \\nto be more efficient in accelerated hardware (they call it Sinkhorn-BASE). \\n- \\nA non-parametric approach (hash layer). \\no HASH layers approaches routing as a fixed function of the input, meaning \\nit does not have learnable parameters. \\n- \\nA Reinforcement Learning approach. \\nResults: \\n- \\nAlthough routing (sparse) performs better than no routing (dense) on all sizes \\nexperimented with (up to 1.3B active parameters, up to 512 experts – biggest \\nmodel has around 200B parameters), the sparse gains over dense are diminishing \\nwith scale (BASE is more robust than other routing techniques). \\n- \\nScaling the number of experts when the number of active? parameters is fixed \\nimproves the validation loss during pre-training. \\n- \\nEffective Parameter Count (EPC) is created to compare the performance of dense \\nagainst sparse models based on an equation that considers the total number of \\nparameters and the active parameters of a model. \\nMain takeaways (as listed in the paper): \\n- \\nRouting improves performance across all model sizes and routing strategies \\n(compared to dense aka no routing). \\n- \\nRL routing is more effective than expected, although BASE is the best performer. \\n- \\nPerformance can be described by scaling the number of experts and dense model \\nsize. \\n- \\nDevelopment of an effective parameter count mapping for performance vs scaling. \\nRecommendations: \\n- \\nUse routing when training any model with N (parameter count of base model) <= \\n1.3B. \\n- \\nSinkhorn-BASE is a good default routing algorithm. \\n- \\nAlthough more experts lead to improved performance, it is recommended to use \\nbetween 64 and 128 experts due to diminishing returns above that range. \\n- \\nIt is recommended to use k=1 experts. \\n \\n \\nMy takeaways: \\n- \\nShows that learned routing (represented through BASE) is the best routing \\nstrategy.  \\no Non-parametric routing can be used in cases where there might not be \\nenough data to train specific experts (for example, on task/domain-level \\nMoE like DEMix where we are not certain if the training load for each expert \\nwill be similar, which will lead to load balancing issues that cannot be solved \\nthrough traditional auxiliary loss or adding noise – this might not happen at \\ntoken-level routing) \\no RL routing performs worse than BASE but looks to not be too far off \\n- \\nTo describe performance, the number of experts and dense model size (number \\nof active parameters for each forward pass) are the most relevant features. This is \\nlogic as the number of experts represents the horizontal scale of the model while \\nthe dense model size represents the vertical scale of the model. (dense model size \\ncorresponds to vertical scaling, does number of experts as mentioned here \\ncorrespond to an increase in the number of experts with the same total parameter \\ncount or is this accounting for an increase in the total parameter count coming from \\nthe added experts). \\n- \\nSparse models seem to be the most useful at small scales, with diminishing returns \\nover dense with an increase in the scale of active parameters, but this can be \\nprevented to a certain extent by choosing a robust routing strategy. \\n- \\nThe result arrived at that scaling the number of experts when the number of active \\nparameters is fixed is logical as this scales the model horizontally. However, my \\nintuition in this is that scaling the number of experts might make things difficult for \\nfine-tuning (more data will be needed to update all experts while not overfitting on \\nothers). Therefore, a balance is needed. (authors recommend between 64 and 128 \\nexperts due to diminishing returns in increasing the number of experts). -> how \\ndoes fine-tuning performance change with differing number of experts and in \\nrespect to more training data to use for fine-tuning (explore how scaling the number \\nof experts while keeping the active parameter count constant impacts fine-tuning \\nperformance)? \\n- \\nThe EPC equation seems to be useful for practitioners looking to train a MoE model \\nfrom scratch. This would help with design choices in number of active parameters \\nand number of total parameters. \\n- \\nInteresting how the authors recommend MoE in scenarios of training smallish \\nmodels (up to 1.3B). I believe that this is because that was the bigger dense model \\nstudied, so it is not saying that dense models perform better when scaled above \\n1.3B, but just that a bigger dense model was not used in the experiments. It is \\nimportant to note that the experiments showed diminishing returns for routing \\nmodels -> did any other papers dive into this question? \\n- \\nIt is also interesting to note that the authors concluded that k=1 experts is the ideal \\nnumber for k. \\n \\n', 'source_name': 'Unified Scaling Laws for Routes Language Models NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Unified_Scaling_Laws_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Switch_Transformers.pdf #2\n",
      "{'content': 'Journal of Machine Learning Research 23 (2022) 1-40\\nSubmitted 8/21; Revised 3/22; Published 4/22\\nSwitch Transformers: Scaling to Trillion Parameter Models\\nwith Simple and Eﬃcient Sparsity\\nWilliam Fedus∗\\nliamfedus@google.com\\nBarret Zoph∗\\nbarretzoph@google.com\\nNoam Shazeer\\nnoam@google.com\\nGoogle, Mountain View, CA 94043, USA\\nEditor: Alexander Clark\\nAbstract\\nIn deep learning, models typically reuse the same parameters for all inputs.\\nMixture\\nof Experts (MoE) models defy this and instead select diﬀerent parameters for each in-\\ncoming example. The result is a sparsely-activated model—with an outrageous number\\nof parameters—but a constant computational cost. However, despite several notable suc-\\ncesses of MoE, widespread adoption has been hindered by complexity, communication costs,\\nand training instability. We address these with the introduction of the Switch Transformer.\\nWe simplify the MoE routing algorithm and design intuitive improved models with reduced\\ncommunication and computational costs. Our proposed training techniques mitigate the\\ninstabilities, and we show large sparse models may be trained, for the ﬁrst time, with lower\\nprecision (bﬂoat16) formats. We design models based oﬀT5-Base and T5-Large (Raﬀel\\net al., 2019) to obtain up to 7x increases in pre-training speed with the same computational\\nresources. These improvements extend into multilingual settings where we measure gains\\nover the mT5-Base version across all 101 languages. Finally, we advance the current scale\\nof language models by pre-training up to trillion parameter models on the “Colossal Clean\\nCrawled Corpus”, and achieve a 4x speedup over the T5-XXL model.12\\nKeywords: mixture-of-experts, natural language processing, sparsity, large-scale machine\\nlearning, distributed computing\\n∗. Equal contribution.\\n1. JAX code for Switch Transformer and all model checkpoints are available at https://github.com/\\ngoogle-research/t5x\\n2. Tensorﬂow code for Switch Transformer is available at https://github.com/tensorflow/mesh/blob/\\nmaster/mesh_tensorflow/transformer/moe.py\\n©2022 William Fedus, Barret Zoph and Noam Shazeer.\\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided\\nat http://jmlr.org/papers/v23/21-0998.html.\\narXiv:2101.03961v3  [cs.LG]  16 Jun 2022\\nFedus, Zoph and Shazeer\\nContents\\n1\\nIntroduction\\n3\\n2\\nSwitch Transformer\\n4\\n2.1\\nSimplifying Sparse Routing\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\nEﬃcient Sparse Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3\\nPutting It All Together: The Switch Transformer . . . . . . . . . . . . . . .\\n8\\n2.4\\nImproved Training and Fine-Tuning Techniques . . . . . . . . . . . . . . . .\\n8\\n3\\nScaling Properties\\n11\\n3.1\\nScaling Results on a Step-Basis . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n3.2\\nScaling Results on a Time-Basis . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n3.3\\nScaling Versus a Larger Dense Model . . . . . . . . . . . . . . . . . . . . . .\\n13\\n4\\nDownstream Results\\n14\\n4.1\\nFine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4.2\\nDistillation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n4.3\\nMultilingual Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n17\\n5\\nDesigning Models with Data, Model, and Expert-Parallelism\\n18\\n5.1\\nData Parallelism\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n5.2\\nModel Parallelism\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n5.3\\nModel and Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n5.4\\nExpert and Data Parallelism\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n5.5\\nExpert, Model and Data Parallelism . . . . . . . . . . . . . . . . . . . . . .\\n22\\n5.6\\nTowards Trillion Parameter Models . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n6\\nRelated Work\\n24\\n7\\nDiscussion\\n25\\n8\\nFuture Work\\n26\\n9\\nConclusion\\n27\\nA Switch for Attention\\n27\\nB Preventing Token Dropping with No-Token-Left-Behind\\n29\\nC Encouraging Exploration Across Experts\\n29\\nD Switch Transformers in Lower Compute Regimes\\n29\\nE Relation of Upstream to Downstream Model Performance\\n32\\nF Pseudo Code for Switch Transformers\\n33\\n2\\nSwitch Transformers\\n1. Introduction\\nLarge scale training has been an eﬀective path towards ﬂexible and powerful neural language\\nmodels (Radford et al., 2018; Kaplan et al., 2020; Brown et al., 2020). Simple architectures—\\nbacked by a generous computational budget, data set size and parameter count—surpass\\nmore complicated algorithms (Sutton, 2019). An approach followed in Radford et al. (2018);\\nRaﬀel et al. (2019); Brown et al. (2020) expands the model size of a densely-activated\\nTransformer (Vaswani et al., 2017). While eﬀective, it is also extremely computationally\\nintensive (Strubell et al., 2019). Inspired by the success of model scale, but seeking greater\\ncomputational eﬃciency, we instead propose a sparsely-activated expert model: the Switch\\nTransformer. In our case the sparsity comes from activating a subset of the neural network\\nweights for each incoming example.\\n109\\n1010\\nSparse Model Parameters\\n4.8\\n5.0\\n5.2\\n5.4\\n5.6\\n5.8\\n6.0\\nTest Loss\\n1e\\n2e\\n4e\\n8e\\n16e\\n32e\\n64e\\n128e\\n256e\\n0\\n1\\n2\\n3\\n4\\nTraining Step\\n1e5\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nNeg Log Perplexity\\nSwitch-Base: 128e\\nSwitch-Base: 64e\\nSwitch-Base: 32e\\nSwitch-Base: 16e\\nT5-Base\\nFigure 1: Scaling and sample eﬃciency of Switch Transformers. Left Plot: Scaling prop-\\nerties for increasingly sparse (more experts) Switch Transformers. Right Plot:\\nNegative log perplexity comparing Switch Transformers to T5 (Raﬀel et al., 2019)\\nmodels using the same compute budget.\\nSparse training is an active area of research and engineering (Gray et al., 2017; Gale\\net al., 2020), but as of today, machine learning libraries and hardware accelerators still cater\\nto dense matrix multiplications. To have an eﬃcient sparse algorithm, we start with the\\nMixture-of-Expert (MoE) paradigm (Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer\\net al., 2017), and simplify it to yield training stability and computational beneﬁts. MoE\\nmodels have had notable successes in machine translation (Shazeer et al., 2017, 2018; Lep-\\nikhin et al., 2020), however, widespread adoption is hindered by complexity, communication\\ncosts, and training instabilities.\\nWe address these issues, and then go beyond translation, to ﬁnd that these class of\\nalgorithms are broadly valuable in natural language. We measure superior scaling on a\\ndiverse set of natural language tasks and across three regimes in NLP: pre-training, ﬁne-\\ntuning and multi-task training. While this work focuses on scale, we also show that the\\nSwitch Transformer architecture not only excels in the domain of supercomputers, but is\\n3\\nFedus, Zoph and Shazeer\\nbeneﬁcial even with only a few computational cores. Further, our large sparse models can\\nbe distilled (Hinton et al., 2015) into small dense versions while preserving 30% of the sparse\\nmodel quality gain. Our contributions are the following:\\n• The Switch Transformer architecture, which simpliﬁes and improves over Mixture of\\nExperts.\\n• Scaling properties and a benchmark against the strongly tuned T5 model (Raﬀel et al.,\\n2019) where we measure 7x+ pre-training speedups while still using the same FLOPS\\nper token. We further show the improvements hold even with limited computational\\nresources, using as few as two experts.\\n• Successful distillation of sparse pre-trained and specialized ﬁne-tuned models into\\nsmall dense models. We reduce the model size by up to 99% while preserving 30% of\\nthe quality gains of the large sparse teacher.\\n• Improved pre-training and ﬁne-tuning techniques: (1) selective precision training that\\nenables training with lower bﬂoat16 precision (2) an initialization scheme that allows\\nfor scaling to a larger number of experts and (3) increased expert regularization that\\nimproves sparse model ﬁne-tuning and multi-task training.\\n• A measurement of the pre-training beneﬁts on multilingual data where we ﬁnd a\\nuniversal improvement across all 101 languages and with 91% of languages beneﬁting\\nfrom 4x+ speedups over the mT5 baseline (Xue et al., 2020).\\n• An increase in the scale of neural language models achieved by eﬃciently combining\\ndata, model, and expert-parallelism to create models with up to a trillion parameters.\\nThese models improve the pre-training speed of a strongly tuned T5-XXL baseline by\\n4x.\\n2. Switch Transformer\\nThe guiding design principle for Switch Transformers is to maximize the parameter count of\\na Transformer model (Vaswani et al., 2017) in a simple and computationally eﬃcient way.\\nThe beneﬁt of scale was exhaustively studied in Kaplan et al. (2020) which uncovered power-\\nlaw scaling with model size, data set size and computational budget. Importantly, this work\\nadvocates training large models on relatively small amounts of data as the computationally\\noptimal approach.\\nHeeding these results, we investigate a fourth axis: increase the parameter count while\\nkeeping the ﬂoating point operations (FLOPs) per example constant. Our hypothesis is\\nthat the parameter count, independent of total computation performed, is a separately\\nimportant axis on which to scale. We achieve this by designing a sparsely activated model\\nthat eﬃciently uses hardware designed for dense matrix multiplications such as GPUs and\\nTPUs. Our work here focuses on TPU architectures, but these class of models may be\\nsimilarly trained on GPU clusters. In our distributed training setup, our sparsely activated\\nlayers split unique weights on diﬀerent devices. Therefore, the weights of the model increase\\nwith the number of devices, all while maintaining a manageable memory and computational\\nfootprint on each device.\\n4\\nSwitch Transformers\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nRouter\\nSelf-Attention\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\np = 0.65\\np = 0.8\\nPositional\\nembedding\\nPositional\\nembedding\\nAdd + Normalize\\nSelf-Attention\\nAdd + Normalize\\nSwitching FFN Layer\\ny\\nx\\nMore\\nParameters\\nFigure 2: Illustration of a Switch Transformer encoder block. We replace the dense feed\\nforward network (FFN) layer present in the Transformer with a sparse Switch\\nFFN layer (light blue). The layer operates independently on the tokens in the\\nsequence. We diagram two tokens (x1 = “More” and x2 = “Parameters” below)\\nbeing routed (solid lines) across four FFN experts, where the router independently\\nroutes each token. The switch FFN layer returns the output of the selected FFN\\nmultiplied by the router gate value (dotted-line).\\n2.1 Simplifying Sparse Routing\\nMixture of Expert Routing. Shazeer et al. (2017) proposed a natural language Mixture-\\nof-Experts (MoE) layer which takes as an input a token representation x and then routes\\nthis to the best determined top-k experts, selected from a set {Ei(x)}N\\ni=1 of N experts.\\nThe router variable Wr produces logits h(x) = Wr · x which are normalized via a softmax\\ndistribution over the available N experts at that layer. The gate-value for expert i is given\\nby,\\npi(x) =\\neh(x)i\\nPN\\nj eh(x)j .\\n(1)\\nThe top-k gate values are selected for routing the token x. If T is the set of selected top-k\\nindices then the output computation of the layer is the linearly weighted combination of\\neach expert’s computation on the token by the gate value,\\ny =\\nX\\ni∈T\\npi(x)Ei(x).\\n(2)\\nSwitch Routing: Rethinking Mixture-of-Experts. Shazeer et al. (2017) conjec-\\ntured that routing to k > 1 experts was necessary in order to have non-trivial gradients to\\nthe routing functions. The authors intuited that learning to route would not work without\\nthe ability to compare at least two experts. Ramachandran and Le (2018) went further to\\n5\\nFedus, Zoph and Shazeer\\nstudy the top-k decision and found that higher k-values in lower layers in the model were\\nimportant for models with many routing layers. Contrary to these ideas, we instead use\\na simpliﬁed strategy where we route to only a single expert. We show this simpliﬁcation\\npreserves model quality, reduces routing computation and performs better. This k = 1\\nrouting strategy is later referred to as a Switch layer. Note that for both MoE and Switch\\nRouting, the gate value pi(x) in Equation 2 permits diﬀerentiability of the router.\\nThe beneﬁts for the Switch layer are three-fold: (1) The router computation is reduced\\nas we are only routing a token to a single expert. (2) The batch size (expert capacity) of\\neach expert can be at least halved since each token is only being routed to a single expert.3\\n(3) The routing implementation is simpliﬁed and communication costs are reduced. Figure\\n3 shows an example of routing with diﬀerent expert capacity factors.\\nDevice 0\\nDevice 0\\nDevice 1\\nDevice 2\\nTokens\\nExpert 1\\nExpert 2\\nExpert 3\\n(Capacity Factor: 1.0)\\nTokens\\nExpert 1\\nExpert 2\\nExpert 3\\n(Capacity Factor: 1.5)\\nAcross Device\\nCommunication\\nDevice 0\\nDevice 1\\nDevice 2\\nDevice 1\\nDevice 0\\nTerminology\\nExperts: Split across devices,\\neach having their own unique\\nparameters. Perform standard feed-\\nforward computation.\\nExpert Capacity: Batch size of\\neach expert. Calculated as\\xa0\\n(tokens_per_batch / num_experts) *\\ncapacity_factor\\nCapacity Factor: Used when\\ncalculating expert capacity. Expert\\ncapacity allows more buffer to help\\nmitigate token overﬂow during\\nrouting.\\xa0\\nFigure 3: Illustration of token routing dynamics. Each expert processes a ﬁxed batch-size\\nof tokens modulated by the capacity factor. Each token is routed to the expert\\nwith the highest router probability, but each expert has a ﬁxed batch size of\\n(total tokens / num experts) × capacity factor. If the tokens are unevenly dis-\\npatched then certain experts will overﬂow (denoted by dotted red lines), resulting\\nin these tokens not being processed by this layer. A larger capacity factor allevi-\\nates this overﬂow issue, but also increases computation and communication costs\\n(depicted by padded white/empty slots).\\n2.2 Eﬃcient Sparse Routing\\nWe use Mesh-Tensorﬂow (MTF) (Shazeer et al., 2018) which is a library, with similar seman-\\ntics and API to Tensorﬂow (Abadi et al., 2016) that facilitates eﬃcient distributed data and\\nmodel parallel architectures. It does so by abstracting the physical set of cores to a logical\\nmesh of processors. Tensors and computations may then be sharded per named dimensions,\\nfacilitating easy partitioning of models across dimensions. We design our model with TPUs\\nin mind, which require statically declared sizes. Below we describe our distributed Switch\\nTransformer implementation.\\n3. See Section 2.2 for a technical description.\\n6\\nSwitch Transformers\\nDistributed Switch Implementation. All of our tensor shapes are statically deter-\\nmined at compilation time, but our computation is dynamic due to the routing decisions at\\ntraining and inference. Because of this, one important technical consideration is how to set\\nthe expert capacity. The expert capacity—the number of tokens each expert computes—is\\nset by evenly dividing the number of tokens in the batch across the number of experts, and\\nthen further expanding by a capacity factor,\\nexpert capacity =\\n\\x12 tokens per batch\\nnumber of experts\\n\\x13\\n× capacity factor.\\n(3)\\nA capacity factor greater than 1.0 creates additional buﬀer to accommodate for when to-\\nkens are not perfectly balanced across experts. If too many tokens are routed to an expert\\n(referred to later as dropped tokens), computation is skipped and the token representa-\\ntion is passed directly to the next layer through the residual connection. Increasing the\\nexpert capacity is not without drawbacks, however, since high values will result in wasted\\ncomputation and memory. This trade-oﬀis explained in Figure 3. Empirically we ﬁnd en-\\nsuring lower rates of dropped tokens are important for the scaling of sparse expert-models.\\nThroughout our experiments we didn’t notice any dependency on the number of experts\\nfor the number of tokens dropped (typically < 1%). Using the auxiliary load balancing loss\\n(next section) with a high enough coeﬃcient ensured good load balancing. We study the\\nimpact that these design decisions have on model quality and speed in Table 1.\\nA Diﬀerentiable Load Balancing Loss. To encourage a balanced load across experts\\nwe add an auxiliary loss (Shazeer et al., 2017, 2018; Lepikhin et al., 2020). As in Shazeer\\net al. (2018); Lepikhin et al. (2020), Switch Transformers simpliﬁes the original design in\\nShazeer et al. (2017) which had separate load-balancing and importance-weighting losses.\\nFor each Switch layer, this auxiliary loss is added to the total model loss during training.\\nGiven N experts indexed by i = 1 to N and a batch B with T tokens, the auxiliary loss is\\ncomputed as the scaled dot-product between vectors f and P,\\nloss = α · N ·\\nN\\nX\\ni=1\\nfi · Pi\\n(4)\\nwhere fi is the fraction of tokens dispatched to expert i,\\nfi = 1\\nT\\nX\\nx∈B\\n1{argmax p(x) = i}\\n(5)\\nand Pi is the fraction of the router probability allocated for expert i, 2\\nPi = 1\\nT\\nX\\nx∈B\\npi(x).\\n(6)\\nSince we seek uniform routing of the batch of tokens across the N experts, we desire both\\nvectors to have values of 1/N. The auxiliary loss of Equation 4 encourages uniform routing\\nsince it is minimized under a uniform distribution. The objective can also be diﬀerentiated as\\n2. A potential source of confusion: pi(x) is the probability of routing token x to expert i.\\nPi is the\\nprobability fraction to expert i across all tokens in the batch B.\\n7\\nFedus, Zoph and Shazeer\\nthe P-vector is diﬀerentiable, but the f-vector is not. The ﬁnal loss is multiplied by expert\\ncount N to keep the loss constant as the number of experts varies since under uniform\\nrouting PN\\ni=1(fi · Pi) = PN\\ni=1( 1\\nN · 1\\nN ) = 1\\nN . Finally, a hyper-parameter α is a multiplicative\\ncoeﬃcient for these auxiliary losses; throughout this work we use an α = 10−2 which was\\nsuﬃciently large to ensure load balancing while small enough to not to overwhelm the\\nprimary cross-entropy objective. We swept hyper-parameter ranges of α from 10−1 to 10−5\\nin powers of 10 and found 10−2 balanced load quickly without interfering with training loss.\\n2.3 Putting It All Together: The Switch Transformer\\nOur ﬁrst test of the Switch Transformer starts with pre-training on the “Colossal Clean\\nCrawled Corpus” (C4), introduced in (Raﬀel et al., 2019). For our pre-training objective,\\nwe use a masked language modeling task (Taylor, 1953; Fedus et al., 2018; Devlin et al.,\\n2018) where the model is trained to predict missing tokens. In our pre-training setting, as\\ndetermined in Raﬀel et al. (2019) to be optimal, we drop out 15% of tokens and then replace\\nthe masked sequence with a single sentinel token. To compare our models, we record the\\nnegative log perplexity.4 Throughout all tables in the paper, ↑indicates that a higher value\\nfor that metric is better and vice-versa for ↓. A comparison of all the models studied in\\nthis work are in Table 9.\\nA head-to-head comparison of the Switch Transformer and the MoE Transformer is\\npresented in Table 1. Our Switch Transformer model is FLOP-matched to ‘T5-Base’ (Raﬀel\\net al., 2019) (same amount of computation per token is applied). The MoE Transformer,\\nusing top-2 routing, has two experts which each apply a separate FFN to each token and\\nthus its FLOPS are larger. All models were trained for the same number of steps on identical\\nhardware. Note that the MoE model going from capacity factor 2.0 to 1.25 actually slows\\ndown (840 to 790) in the above experiment setup, which is unexpected.5\\nWe highlight three key ﬁndings from Table 1: (1) Switch Transformers outperform\\nboth carefully tuned dense models and MoE Transformers on a speed-quality basis. For\\na ﬁxed amount of computation and wall-clock time, Switch Transformers achieve the best\\nresult. (2) The Switch Transformer has a smaller computational footprint than the MoE\\ncounterpart. If we increase its size to match the training speed of the MoE Transformer,\\nwe ﬁnd this outperforms all MoE and Dense models on a per step basis as well. (3) Switch\\nTransformers perform better at lower capacity factors (1.0, 1.25). Smaller expert capacities\\nare indicative of the scenario in the large model regime where model memory is very scarce\\nand the capacity factor will want to be made as small as possible.\\n2.4 Improved Training and Fine-Tuning Techniques\\nSparse expert models may introduce training diﬃculties over a vanilla Transformer. Insta-\\nbility can result because of the hard-switching (routing) decisions at each of these layers.\\nFurther, low precision formats like bﬂoat16 (Wang and Kanwar, 2019) can exacerbate issues\\n4. We use log base-e for this metric so the units are nats.\\n5. Note that speed measurements are both a function of the algorithm and the implementation details.\\nSwitch Transformer reduces the necessary computation relative to MoE (algorithm), but the ﬁnal speed\\ndiﬀerences are impacted by low-level optimizations (implementation).\\n8\\nSwitch Transformers\\nModel\\nCapacity\\nQuality after\\nTime to Quality\\nSpeed (↑)\\nFactor\\n100k steps (↑)\\nThreshold (↓)\\n(examples/sec)\\n(Neg. Log Perp.)\\n(hours)\\nT5-Base\\n—\\n-1.731\\nNot achieved†\\n1600\\nT5-Large\\n—\\n-1.550\\n131.1\\n470\\nMoE-Base\\n2.0\\n-1.547\\n68.7\\n840\\nSwitch-Base\\n2.0\\n-1.554\\n72.8\\n860\\nMoE-Base\\n1.25\\n-1.559\\n80.7\\n790\\nSwitch-Base\\n1.25\\n-1.553\\n65.0\\n910\\nMoE-Base\\n1.0\\n-1.572\\n80.1\\n860\\nSwitch-Base\\n1.0\\n-1.561\\n62.8\\n1000\\nSwitch-Base+\\n1.0\\n-1.534\\n67.6\\n780\\nTable 1: Benchmarking Switch versus MoE. Head-to-head comparison measuring per step\\nand per time beneﬁts of the Switch Transformer over the MoE Transformer and\\nT5 dense baselines. We measure quality by the negative log perplexity and the\\ntime to reach an arbitrary chosen quality threshold of Neg. Log Perp.=-1.50. All\\nMoE and Switch Transformer models use 128 experts, with experts at every other\\nfeed-forward layer. For Switch-Base+, we increase the model size until it matches\\nthe speed of the MoE model by increasing the model hidden-size from 768 to 896\\nand the number of heads from 14 to 16. All models are trained with the same\\namount of computation (32 cores) and on the same hardware (TPUv3). Further\\nnote that all our models required pre-training beyond 100k steps to achieve our\\nlevel threshold of -1.50. † T5-Base did not achieve this negative log perplexity in\\nthe 100k steps the models were trained.\\nin the softmax computation for our router. We describe training diﬃculties here and the\\nmethods we use to overcome them to achieve stable and scalable training.\\nSelective precision with large sparse models. Model instability hinders the ability\\nto train using eﬃcient bﬂoat16 precision, and as a result, Lepikhin et al. (2020) trains with\\nﬂoat32 precision throughout their MoE Transformer. However, we show that by instead\\nselectively casting to ﬂoat32 precision within a localized part of the model, stability may be\\nachieved, without incurring expensive communication cost of ﬂoat32 tensors. This technique\\nis inline with modern mixed precision training strategies where certain parts of the model\\nand gradient updates are done in higher precision Micikevicius et al. (2017). Table 2 shows\\nthat our approach permits nearly equal speed to bﬂoat16 training while conferring the\\ntraining stability of ﬂoat32.\\nTo achieve this, we cast the router input to ﬂoat32 precision. The router function takes\\nthe tokens as input and produces the dispatch and combine tensors used for the selection and\\nrecombination of expert computation (refer to Code Block 15 in the Appendix for details).\\nImportantly, the ﬂoat32 precision is only used within the body of the router function—on\\ncomputations local to that device.\\nBecause the resulting dispatch and combine tensors\\nare recast to bﬂoat16 precision at the end of the function, no expensive ﬂoat32 tensors\\n9\\nFedus, Zoph and Shazeer\\nModel\\nQuality\\nSpeed\\n(precision)\\n(Neg. Log Perp.) (↑)\\n(Examples/sec) (↑)\\nSwitch-Base (ﬂoat32)\\n-1.718\\n1160\\nSwitch-Base (bﬂoat16)\\n-3.780 [diverged]\\n1390\\nSwitch-Base (Selective precision)\\n-1.716\\n1390\\nTable 2: Selective precision. We cast the local routing operations to ﬂoat32 while preserving\\nbﬂoat16 precision elsewhere to stabilize our model while achieving nearly equal\\nspeed to (unstable) bﬂoat16-precision training. We measure the quality of a 32\\nexpert model after a ﬁxed step count early in training its speed performance. For\\nboth Switch-Base in ﬂoat32 and with Selective prevision we notice similar learning\\ndynamics.\\nare broadcast through all-to-all communication operations, but we still beneﬁt from the\\nincreased stability of ﬂoat32.\\nSmaller parameter initialization for stability. Appropriate initialization is critical\\nto successful training in deep learning and we especially observe this to be true for Switch\\nTransformer.\\nWe initialize our weight matrices by drawing elements from a truncated\\nnormal distribution with mean µ = 0 and standard deviation σ =\\np\\ns/n where s is a scale\\nhyper-parameter and n is the number of input units in the weight tensor (e.g. fan-in).6\\nAs an additional remedy to the instability, we recommend reducing the default Trans-\\nformer initialization scale s = 1.0 by a factor of 10. This both improves quality and reduces\\nthe likelihood of destabilized training in our experiments. Table 3 measures the improve-\\nment of the model quality and reduction of the variance early in training. We ﬁnd that\\nModel (Initialization scale)\\nAverage Quality\\nStd. Dev. of Quality\\n(Neg. Log Perp.)\\n(Neg. Log Perp.)\\nSwitch-Base (0.1x-init)\\n-2.72\\n0.01\\nSwitch-Base (1.0x-init)\\n-3.60\\n0.68\\nTable 3: Reduced initialization scale improves stability. Reducing the initialization scale\\nresults in better model quality and more stable training of Switch Transformer.\\nHere we record the average and standard deviation of model quality, measured by\\nthe negative log perplexity, of a 32 expert model after 3.5k steps (3 random seeds\\neach).\\nthe average model quality, as measured by the Neg. Log Perp., is dramatically improved\\nand there is a far reduced variance across runs. Further, this same initialization scheme is\\nbroadly eﬀective for models spanning several orders of magnitude. We use the same ap-\\nproach to stably train models as small as our 223M parameter baseline to enormous models\\nin excess of one trillion parameters.\\n6. Values greater than two standard deviations from the mean are resampled.\\n10\\nSwitch Transformers\\nRegularizing large sparse models. Our paper considers the common NLP approach\\nof pre-training on a large corpus followed by ﬁne-tuning on smaller downstream tasks such\\nas summarization or question answering. One issue that naturally arises is overﬁtting since\\nmany ﬁne-tuning tasks have very few examples.\\nDuring ﬁne-tuning of standard Trans-\\nformers, Raﬀel et al. (2019) use dropout (Srivastava et al., 2014) at each layer to prevent\\noverﬁtting. Our Switch Transformers have signiﬁcantly more parameters than the FLOP\\nmatched dense baseline, which can lead to more severe overﬁtting on these smaller down-\\nstream tasks.\\nModel (dropout)\\nGLUE\\nCNNDM\\nSQuAD\\nSuperGLUE\\nT5-Base (d=0.1)\\n82.9\\n19.6\\n83.5\\n72.4\\nSwitch-Base (d=0.1)\\n84.7\\n19.1\\n83.7\\n73.0\\nSwitch-Base (d=0.2)\\n84.4\\n19.2\\n83.9\\n73.2\\nSwitch-Base (d=0.3)\\n83.9\\n19.6\\n83.4\\n70.7\\nSwitch-Base (d=0.1, ed=0.4)\\n85.2\\n19.6\\n83.7\\n73.0\\nTable 4: Fine-tuning regularization results.\\nA sweep of dropout rates while ﬁne-tuning\\nSwitch Transformer models pre-trained on 34B tokens of the C4 data set (higher\\nnumbers are better).\\nWe observe that using a lower standard dropout rate at\\nall non-expert layer, with a much larger dropout rate on the expert feed-forward\\nlayers, to perform the best.\\nWe thus propose a simple way to alleviate this issue during ﬁne-tuning: increase the\\ndropout inside the experts, which we name as expert dropout. During ﬁne-tuning we simply\\nincrease the dropout rate by a signiﬁcant amount only at the interim feed-forward com-\\nputation at each expert layer. Table 4 has the results for our expert dropout protocol.\\nWe observe that simply increasing the dropout across all layers leads to worse performance.\\nHowever, setting a smaller dropout rate (0.1) at non-expert layers and a much larger dropout\\nrate (0.4) at expert layers leads to performance improvements on four smaller downstream\\ntasks.\\n3. Scaling Properties\\nWe present a study of the scaling properties of the Switch Transformer architecture dur-\\ning pre-training. Per Kaplan et al. (2020), we consider a regime where the model is not\\nbottlenecked by either the computational budget or amount of data. To avoid the data\\nbottleneck, we use the large C4 corpus with over 180B target tokens (Raﬀel et al., 2019)\\nand we train until diminishing returns are observed.\\nThe number of experts is the most eﬃcient dimension for scaling our model. Increasing\\nthe experts keeps the computational cost approximately ﬁxed since the model only selects\\none expert per token, regardless of the number of experts to choose from.\\nThe router\\nmust compute a probability distribution over more experts, however, this is a lightweight\\ncomputation of cost O(dmodel × num experts) where dmodel is the embedding dimension of\\n11\\nFedus, Zoph and Shazeer\\ntokens passed between the layers. In this section, we consider the scaling properties on a\\nstep-basis and a time-basis with a ﬁxed computational budget.\\n3.1 Scaling Results on a Step-Basis\\nFigure 4 demonstrates consistent scaling beneﬁts with the number of experts when training\\nall models for a ﬁxed number of steps. We observe a clear trend: when keeping the FLOPS\\nper token ﬁxed, having more parameters (experts) speeds up training.\\nThe left Figure\\ndemonstrates consistent scaling properties (with ﬁxed FLOPS per token) between sparse\\nmodel parameters and test loss. This reveals the advantage of scaling along this additional\\naxis of sparse model parameters. Our right Figure measures sample eﬃciency of a dense\\nmodel variant and four FLOP-matched sparse variants. We ﬁnd that increasing the number\\nof experts leads to more sample eﬃcient models. Our Switch-Base 64 expert model achieves\\nthe same performance of the T5-Base model at step 60k at step 450k, which is a\\n7.5x\\nspeedup in terms of step time. In addition, consistent with the ﬁndings of Kaplan et al.\\n(2020), we ﬁnd that larger models are also more sample eﬃcient—learning more quickly\\nfor a ﬁxed number of observed tokens.\\n109\\n1010\\nSparse Model Parameters\\n4.8\\n5.0\\n5.2\\n5.4\\n5.6\\n5.8\\n6.0\\nTest Loss\\n1e\\n2e\\n4e\\n8e\\n16e\\n32e\\n64e\\n128e\\n256e\\n0\\n1\\n2\\n3\\n4\\nTraining Step\\n1e5\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nNeg Log Perplexity\\nSwitch-Base: 128e\\nSwitch-Base: 64e\\nSwitch-Base: 32e\\nSwitch-Base: 16e\\nT5-Base\\nFigure 4: Scaling properties of the Switch Transformer. Left Plot: We measure the quality\\nimprovement, as measured by perplexity, as the parameters increase by scaling\\nthe number of experts. The top-left point corresponds to the T5-Base model with\\n223M parameters. Moving from top-left to bottom-right, we double the number of\\nexperts from 2, 4, 8 and so on until the bottom-right point of a 256 expert model\\nwith 14.7B parameters. Despite all models using an equal computational budget,\\nwe observe consistent improvements scaling the number of experts. Right Plot:\\nNegative log perplexity per step sweeping over the number of experts. The dense\\nbaseline is shown with the purple line and we note improved sample eﬃciency of\\nour Switch-Base models.\\n12\\nSwitch Transformers\\n3.2 Scaling Results on a Time-Basis\\nFigure 4 demonstrates that on a step basis, as we increase the number of experts, the\\nperformance consistently improves. While our models have roughly the same amount of\\nFLOPS per token as the baseline, our Switch Transformers incurs additional communication\\ncosts across devices as well as the extra computation of the routing mechanism. Therefore,\\nthe increased sample eﬃciency observed on a step-basis doesn’t necessarily translate to a\\nbetter model quality as measured by wall-clock. This raises the question:\\nFor a ﬁxed training duration and computational budget, should one train a dense or a\\nsparse model?\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nTraining Time\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nNeg Log Perplexity\\n7x Speedup\\nSwitch-Base: 128e\\nSwitch-Base: 64e\\nSwitch-Base: 32e\\nT5-Base\\nFigure 5: Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores\\nwith equal FLOPs per example. For a ﬁxed amount of computation and training\\ntime, Switch Transformers signiﬁcantly outperform the dense Transformer base-\\nline. Our 64 expert Switch-Base model achieves the same quality in one-seventh\\nthe time of the T5-Base and continues to improve.\\nFigures 5 and 6 address this question. Figure 5 measures the pre-training model quality\\nas a function of time.\\nFor a ﬁxed training duration and computational budget, Switch\\nTransformers yield a substantial speed-up. In this setting, our Switch-Base 64 expert model\\ntrains in one-seventh the time that it would take the T5-Base to get similar perplexity.\\n3.3 Scaling Versus a Larger Dense Model\\nThe above analysis shows that a computationally-matched dense model is outpaced by its\\nSwitch counterpart. Figure 6 considers a diﬀerent scenario: what if we instead had allocated\\nour resources to a larger dense model? We do so now, measuring Switch-Base against the\\nnext strong baseline, T5-Large. But despite T5-Large applying 3.5x more FLOPs per token,\\n13\\nFedus, Zoph and Shazeer\\nSwitch-Base is still more sample eﬃcient and yields a 2.5x speedup. Furthermore, more\\ngains can be had simply by designing a new, larger sparse version, Switch-Large, which is\\nFLOP-matched to T5-Large. We do this and demonstrate superior scaling and ﬁne-tuning\\nin the following section.\\n0\\n1\\n2\\n3\\n4\\nTraining Step\\n1e5\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nNeg Log Perplexity\\nSwitch-Base: 64e\\nT5-Large\\nT5-Base\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nTraining Time\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nNeg Log Perplexity\\n7.0x Speedup\\n2.5x Speedup\\nSwitch-Base: 64e\\nT5-Large\\nT5-Base\\nFigure 6: Scaling Transformer models with Switch layers or with standard dense model\\nscaling. Left Plot: Switch-Base is more sample eﬃcient than both the T5-Base,\\nand T5-Large variant, which applies 3.5x more FLOPS per token. Right Plot: As\\nbefore, on a wall-clock basis, we ﬁnd that Switch-Base is still faster, and yields a\\n2.5x speedup over T5-Large.\\n4. Downstream Results\\nSection 3 demonstrated the superior scaling properties while pre-training, but we now val-\\nidate that these gains translate to improved language learning abilities on downstream\\ntasks. We begin by ﬁne-tuning on a diverse set of NLP tasks. Next we study reducing\\nthe memory footprint of our sparse models by over 90% by distilling into small—and easily\\ndeployed—dense baselines. Finally, we conclude this section measuring the improvements\\nin a multi-task, multilingual setting, where we show that Switch Transformers are strong\\nmulti-task learners, improving over the multilingual T5-base model across all 101 languages.\\n4.1 Fine-Tuning\\nBaseline and Switch models used for ﬁne-tuning. Our baselines are the highly-tuned\\n223M parameter T5-Base model and the 739M parameter T5-Large model (Raﬀel et al.,\\n2019). For both versions, we design a FLOP-matched Switch Transformer, with many more\\nparameters, which is summarized in Table 9.7 Our baselines diﬀer slightly from those in\\nRaﬀel et al. (2019) because we pre-train on an improved C4 corpus which removes intra-\\nexample text duplication and thus increases the eﬃcacy as a pre-training task Lee et al.\\n7. FLOPS are calculated for the forward pass as done in Kaplan et al. (2020).\\n14\\nSwitch Transformers\\n(2021). In our protocol we pre-train with 220 (1,048,576) tokens per batch for 550k steps\\namounting to 576B total tokens. We then ﬁne-tune across a diverse set of tasks using a\\ndropout rate of 0.1 for all layers except the Switch layers, which use a dropout rate of 0.4\\n(see Table 4). We ﬁne-tune using a batch-size of 1M for 16k steps and for each task, we\\nevaluate model quality every 200-steps and report the peak performance as computed on\\nthe validation set.\\nFine-tuning tasks and data sets. We select tasks probing language capabilities in-\\ncluding question answering, summarization and knowledge about the world. The language\\nbenchmarks GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are handled\\nas composite mixtures with all the tasks blended in proportion to the amount of tokens\\npresent in each.\\nThese benchmarks consist of tasks requiring sentiment analysis (SST-\\n2), word sense disambiguation (WIC), sentence similarty (MRPC, STS-B, QQP), natural\\nlanguage inference (MNLI, QNLI, RTE, CB), question answering (MultiRC, RECORD,\\nBoolQ), coreference resolution (WNLI, WSC) and sentence completion (COPA) and sen-\\ntence acceptability (CoLA). The CNNDM (Hermann et al., 2015) and BBC XSum (Narayan\\net al., 2018) data sets are used to measure the ability to summarize articles. Question an-\\nswering is probed with the SQuAD data set (Rajpurkar et al., 2016) and the ARC Reasoning\\nChallenge (Clark et al., 2018). And as in Roberts et al. (2020), we evaluate the knowledge\\nof our models by ﬁne-tuning on three closed-book question answering data sets: Natural\\nQuestions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA\\n(Joshi et al., 2017). Closed-book refers to questions posed with no supplemental reference\\nor context material. To gauge the model’s common sense reasoning we evaluate it on the\\nWinogrande Schema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s\\nnatural language inference capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\\nFine-tuning metrics. The following evaluation metrics are used throughout the paper:\\nWe report the average scores across all subtasks for GLUE and SuperGLUE. The Rouge-2\\nmetric is used both the CNNDM and XSum. In SQuAD and the closed book tasks (Web,\\nNatural, and Trivia Questions) we report the percentage of answers exactly matching the\\ntarget (refer to Roberts et al. (2020) for further details and deﬁciency of this measure).\\nFinally, in ARC Easy, ARC Challenge, ANLI, and Winogrande we report the accuracy of\\nthe generated responses.\\nFine-tuning results. We observe signiﬁcant downstream improvements across many\\nnatural language tasks.\\nNotable improvements come from SuperGLUE, where we ﬁnd\\nFLOP-matched Switch variants improve by 4.4 and 2 percentage points over the T5-Base\\nand T5-Large baselines, respectively as well as large improvements in Winogrande, closed\\nbook Trivia QA, and XSum.8 In our ﬁne-tuning study, the only tasks where we do not\\nobserve gains are on the AI2 Reasoning Challenge (ARC) data sets where the T5-Base\\noutperforms Switch-Base on the challenge data set and T5-Large outperforms Switch-Large\\non the easy data set. Taken as a whole, we observe signiﬁcant improvements spanning both\\nreasoning and knowledge-heavy tasks. This validates our architecture, not just as one that\\npre-trains well, but can translate quality improvements to downstream tasks via ﬁne-tuning.\\n8. Our T5 and Switch models were pre-trained with 220 tokens per batch for 550k steps on a revised C4\\ndata set for fair comparisons.\\n15\\nFedus, Zoph and Shazeer\\nModel\\nGLUE\\nSQuAD\\nSuperGLUE\\nWinogrande (XL)\\nT5-Base\\n84.3\\n85.5\\n75.1\\n66.6\\nSwitch-Base\\n86.7\\n87.2\\n79.5\\n73.3\\nT5-Large\\n87.8\\n88.1\\n82.7\\n79.1\\nSwitch-Large\\n88.5\\n88.6\\n84.7\\n83.0\\nModel\\nXSum\\nANLI (R3)\\nARC Easy\\nARC Chal.\\nT5-Base\\n18.7\\n51.8\\n56.7\\n35.5\\nSwitch-Base\\n20.3\\n54.0\\n61.3\\n32.8\\nT5-Large\\n20.9\\n56.6\\n68.8\\n35.5\\nSwitch-Large\\n22.3\\n58.6\\n66.0\\n35.5\\nModel\\nCB Web QA\\nCB Natural QA\\nCB Trivia QA\\nT5-Base\\n26.6\\n25.8\\n24.5\\nSwitch-Base\\n27.4\\n26.8\\n30.7\\nT5-Large\\n27.7\\n27.6\\n29.5\\nSwitch-Large\\n31.3\\n29.5\\n36.9\\nTable 5: Fine-tuning results. Fine-tuning results of T5 baselines and Switch models across\\na diverse set of natural language tests (validation sets; higher numbers are better).\\nWe compare FLOP-matched Switch models to the T5-Base and T5-Large base-\\nlines. For most tasks considered, we ﬁnd signiﬁcant improvements of the Switch-\\nvariants. We observe gains across both model sizes and across both reasoning and\\nknowledge-heavy language tasks.\\n4.2 Distillation\\nDeploying massive neural networks with billions, or trillions, of parameters is inconvenient.\\nTo alleviate this, we study distilling (Hinton et al., 2015) large sparse models into small\\ndense models. Future work could additionally study distilling large models into smaller\\nsparse models.\\nDistillation techniques.\\nIn Table 6 we study a variety of distillation techniques.\\nThese techniques are built oﬀof\\nSanh et al. (2019), who study distillation methods for\\nBERT models. We ﬁnd that initializing the dense model with the non-expert weights yields\\na modest improvement. This is possible since all models are FLOP matched, so non-expert\\nlayers will have the same dimensions. Since expert layers are usually only added at every\\nor every other FFN layer in a Transformer, this allows for many of the weights to be\\ninitialized with trained parameters. Furthermore, we observe a distillation improvement\\nusing a mixture of 0.25 for the teacher probabilities and 0.75 for the ground truth label. By\\ncombining both techniques we preserve ≈30% of the quality gains from the larger sparse\\nmodels with only ≈1/20th of the parameters. The quality gain refers to the percent of\\n16\\nSwitch Transformers\\nthe quality diﬀerence between Switch-Base (Teacher) and T5-Base (Student). Therefore, a\\nquality gain of 100% implies the Student equals the performance of the Teacher.\\nTechnique\\nParameters\\nQuality (↑)\\nT5-Base\\n223M\\n-1.636\\nSwitch-Base\\n3,800M\\n-1.444\\nDistillation\\n223M\\n(3%) -1.631\\n+ Init. non-expert weights from teacher\\n223M\\n(20%) -1.598\\n+ 0.75 mix of hard and soft loss\\n223M\\n(29%) -1.580\\nInitialization Baseline (no distillation)\\nInit. non-expert weights from teacher\\n223M\\n-1.639\\nTable 6: Distilling Switch Transformers for Language Modeling. Initializing T5-Base with\\nthe non-expert weights from Switch-Base and using a loss from a mixture of teacher\\nand ground-truth labels obtains the best performance. We can distill 30% of the\\nperformance improvement of a large sparse model with 100x more parameters back\\ninto a small dense model. For a ﬁnal baseline, we ﬁnd no improvement of T5-Base\\ninitialized with the expert weights, but trained normally without distillation.\\nAchievable compression rates. Using our best distillation technique described in\\nTable 6, we distill a wide variety of sparse models into dense models. We distill Switch-\\nBase versions, sweeping over an increasing number of experts, which corresponds to varying\\nbetween 1.1B to 14.7B parameters. Through distillation, we can preserve 37% of the quality\\ngain of the 1.1B parameter model while compressing 82%.\\nAt the extreme, where we\\ncompress the model 99%, we are still able to maintain 28% of the teacher’s model quality\\nimprovement.\\nDistilling a ﬁne-tuned model. We conclude this with a study of distilling a ﬁne-\\ntuned sparse model into a dense model. Table 8 shows results of distilling a 7.4B parameter\\nSwitch-Base model, ﬁne-tuned on the SuperGLUE task, into the 223M T5-Base. Similar\\nto our pre-training results, we ﬁnd we are able to preserve 30% of the gains of the sparse\\nmodel when distilling into a FLOP matched dense variant. One potential future avenue,\\nnot considered here, may examine the speciﬁc experts being used for ﬁne-tuning tasks and\\nextracting them to achieve better model compression.\\n4.3 Multilingual Learning\\nIn our ﬁnal set of downstream experiments, we measure the model quality and speed trade-\\noﬀs while pre-training on a mixture of 101 diﬀerent languages. We build and benchmark oﬀ\\nthe recent work of mT5 (Xue et al., 2020), a multilingual extension to T5. We pre-train on\\nthe multilingual variant of the Common Crawl data set (mC4) spanning 101 languages in-\\ntroduced in mT5, but due to script variants within certain languages, the mixture contains\\n107 tasks.\\nIn Figure 7 we plot the quality improvement in negative log perplexity for all languages\\nof a FLOP-matched Switch model, mSwitch-Base to the T5 base variant, mT5-Base. After\\n17\\nFedus, Zoph and Shazeer\\nDense\\nSparse\\nParameters\\n223M\\n1.1B\\n2.0B\\n3.8B\\n7.4B\\n14.7B\\nPre-trained Neg. Log Perp. (↑)\\n-1.636\\n-1.505\\n-1.474\\n-1.444\\n-1.432\\n-1.427\\nDistilled Neg. Log Perp. (↑)\\n—\\n-1.587\\n-1.585\\n-1.579\\n-1.582\\n-1.578\\nPercent of Teacher Performance\\n—\\n37%\\n32%\\n30 %\\n27 %\\n28 %\\nCompression Percent\\n—\\n82 %\\n90 %\\n95 %\\n97 %\\n99 %\\nTable 7: Distillation compression rates. We measure the quality when distilling large sparse\\nmodels into a dense baseline. Our baseline, T5-Base, has a -1.636 Neg. Log Perp.\\nquality.\\nIn the right columns, we then distill increasingly large sparse models\\ninto this same architecture. Through a combination of weight-initialization and\\na mixture of hard and soft losses, we can shrink our sparse teachers by 95%+\\nwhile preserving 30% of the quality gain. However, for signiﬁcantly better and\\nlarger pre-trained teachers, we expect larger student models would be necessary\\nto achieve these compression rates.\\nModel\\nParameters\\nFLOPS\\nSuperGLUE (↑)\\nT5-Base\\n223M\\n124B\\n74.6\\nSwitch-Base\\n7410M\\n124B\\n81.3\\nDistilled T5-Base\\n223M\\n124B\\n(30%) 76.6\\nTable 8: Distilling a ﬁne-tuned SuperGLUE model. We distill a Switch-Base model ﬁne-\\ntuned on the SuperGLUE tasks into a T5-Base model. We observe that on smaller\\ndata sets our large sparse model can be an eﬀective teacher for distillation. We\\nﬁnd that we again achieve 30% of the teacher’s performance on a 97% compressed\\nmodel.\\npre-training both versions for 1M steps, we ﬁnd that on all 101 languages considered,\\nSwitch Transformer increases the ﬁnal negative log perplexity over the baseline. In Figure\\n8, we present a diﬀerent view and now histogram the per step speed-up of using Switch\\nTransformer over the mT5-Base.9\\nWe ﬁnd a mean speed-up over mT5-Base of 5x and\\nthat 91% of languages achieve at least a 4x speedup. This presents evidence that Switch\\nTransformers are eﬀective multi-task and multi-lingual learners.\\n5. Designing Models with Data, Model, and Expert-Parallelism\\nArbitrarily increasing the number of experts is subject to diminishing returns (Figure 4).\\nHere we describe complementary scaling strategies. The common way to scale a Transformer\\nis to increase dimensions in tandem, like dmodel or dff. This increases both the parameters\\n9. The speedup on a step basis is computed as the ratio of the number of steps for the baseline divided by\\nthe number of steps required by our model to reach that same quality.\\n18\\nSwitch Transformers\\nja\\nny\\nxh\\nmr\\nsu\\nzh\\neo\\nbg-latn\\nms\\nzu\\nso\\net\\nta\\nla\\nhi-latn\\nha\\nsn\\nht\\nmy\\njv\\naf\\nmi\\nfi\\nfil\\nsw\\nno\\neu\\nlo\\nsv\\nyo\\nde\\nen\\nth\\nco\\ntr\\nml\\nfy\\nes\\nar\\nsl\\nmn\\nky\\nsi\\niw\\nhu\\nis\\nko\\nja-latn\\nro\\nuz\\nzh-latn\\nte\\nam\\nkm\\nkk\\nku\\nnl\\nst\\nit\\nda\\nmg\\nlt\\nsr\\nsq\\nmt\\ngl\\nhi\\ncs\\nhmn\\nig\\nhaw\\nlv\\nfr\\nsm\\nru\\nga\\nsk\\nur\\ntg\\nyi\\npt\\naz\\nps\\nbg\\nru-latn\\nca\\npl\\nmk\\nid\\nkn\\nne\\nfa\\nbe\\nka\\nel-latn\\nuk\\ngu\\nbn\\ncy\\nhy\\nlb\\npa\\nel\\nceb\\nvi\\nsd\\ngd\\nLanguage\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\n0.6\\n0.4\\nNeg. Log Perplexity\\nSwitch\\nDense\\nFigure 7: Multilingual pre-training on 101 languages.\\nImprovements of Switch T5 Base\\nmodel over dense baseline when multi-task training on 101 languages. We observe\\nSwitch Transformers to do quite well in the multi-task training setup and yield\\nimprovements on all 101 languages.\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nSwitch Speedup over Dense Baseline\\n0\\n10\\n20\\n30\\n40\\n50\\nNumber of Languages\\nFigure 8: Multilingual pre-training on 101 languages. We histogram for each language, the\\nstep speedup of Switch Transformers over the FLOP matched T5 dense baseline\\nto reach the same quality. Over all 101 languages, we achieve a mean step speed-\\nup over mT5-Base of 5x and, for 91% of languages, we record a 4x, or greater,\\nspeedup to reach the ﬁnal perplexity of mT5-Base.\\nand computation performed and is ultimately limited by the memory per accelerator. Once\\nit exceeds the size of the accelerator’s memory, single program multiple data (SPMD) model-\\nparallelism can be employed. This section studies the trade-oﬀs of combining data, model,\\nand expert-parallelism.\\nReviewing the Feed-Forward Network (FFN) Layer. We use the FFN layer as\\nan example of how data, model and expert-parallelism works in Mesh TensorFlow (Shazeer\\net al., 2018) and review it brieﬂy here. We assume B tokens in the batch, each of dimension\\n19\\nFedus, Zoph and Shazeer\\ndmodel. Both the input (x) and output (y) of the FFN are of size [B, dmodel] and the inter-\\nmediate (h) is of size [B, dff] where dff is typically several times larger than dmodel. In the\\nFFN, the intermediate is h = xWin and then the output of the layer is y = ReLU(h)Wout.\\nThus Win and Wout are applied independently to each token and have sizes [dmodel, dff]\\nand [dff, dmodel].\\nWe describe two aspects of partitioning: how the weights and batches of data divide\\nover cores, depicted in Figure 9. We denote all cores available as N which Mesh Tensorﬂow\\nmay then remap into a logical multidimensional mesh of processors.\\nHere we create a\\ntwo-dimensional logical mesh, with one dimension representing the number of ways for\\ndata-parallel sharding (n) and the other, the model-parallel sharding (m). The total cores\\nmust equal the ways to shard across both data and model-parallelism, e.g. N = n × m.\\nTo shard the layer across cores, the tensors containing that batch of B tokens are sharded\\nacross n data-parallel cores, so each core contains B/n tokens. Tensors and variables with\\ndff are then sharded across m model-parallel cores. For the variants with experts-layers,\\nwe consider E experts, each of which can process up to C tokens.\\nTerm\\nDescription\\nB\\nNumber of tokens in the batch.\\nN\\nNumber of total cores.\\nn\\nNumber of ways for data-parallelism sharding.\\nm\\nNumber of ways for model-parallelism sharding.\\nE\\nNumber of experts in Switch layers.\\nC\\nExpert capacity, the batch size of each expert.\\n5.1 Data Parallelism\\nWhen training data parallel models, which is the standard for distributed training, then all\\ncores are allocated to the data-parallel dimension or n = N, m = 1. This has the advantage\\nthat no communication is needed until the entire forward and backward pass is ﬁnished and\\nthe gradients need to be then aggregated across all cores. This corresponds to the left-most\\ncolumn of Figure 9.\\n5.2 Model Parallelism\\nWe now consider a scenario where all cores are allocated exclusively to the model-parallel\\ndimension and so n = 1, m = N. Now all cores must keep the full B tokens and each\\ncore will contain a unique slice of the weights. For each forward and backward pass, a\\ncommunication cost is now incurred. Each core sends a tensor of [B, dmodel] to compute the\\nsecond matrix multiplication ReLU(h)Wout because the dff dimension is partitioned and\\nmust be summed over. As a general rule, whenever a dimension that is partitioned across\\ncores must be summed, then an all-reduce operation is added for both the forward and\\nbackward pass. This contrasts with pure data parallelism where an all-reduce only occurs\\nat the end of the entire forward and backward pass.\\n20\\nSwitch Transformers\\nExpert and Data\\nParallelism\\nModel \\nParallelism\\nExpert, Model and Data\\nParallelism\\nHow the model weights\\xa0are split over cores\\nHow the data\\xa0is split over cores\\nModel and Data \\nParallelism\\nData\\xa0\\nParallelism\\nExpert and Data\\nParallelism\\nModel \\nParallelism\\nExpert, Model and Data\\nParallelism\\nModel and Data \\nParallelism\\nData\\xa0\\nParallelism\\nFigure 9: Data and weight partitioning strategies. Each 4×4 dotted-line grid represents 16\\ncores and the shaded squares are the data contained on that core (either model\\nweights or batch of tokens). We illustrate both how the model weights and the\\ndata tensors are split for each strategy. First Row: illustration of how model\\nweights are split across the cores. Shapes of diﬀerent sizes in this row represent\\nlarger weight matrices in the Feed Forward Network (FFN) layers (e.g larger dff\\nsizes). Each color of the shaded squares identiﬁes a unique weight matrix. The\\nnumber of parameters per core is ﬁxed, but larger weight matrices will apply\\nmore computation to each token. Second Row: illustration of how the data\\nbatch is split across cores. Each core holds the same number of tokens which\\nmaintains a ﬁxed memory usage across all strategies. The partitioning strategies\\nhave diﬀerent properties of allowing each core to either have the same tokens or\\ndiﬀerent tokens across cores, which is what the diﬀerent colors symbolize.\\n5.3 Model and Data Parallelism\\nIt is common to mix both model and data parallelism for large scale models, which was done\\nin the largest T5 models (Raﬀel et al., 2019; Xue et al., 2020) and in GPT-3 (Brown et al.,\\n2020). With a total of N = n × m cores, now each core will be responsible for B/n tokens\\nand dff/m of both the weights and intermediate activation. In the forward and backward\\npass each core communicates a tensor of size [B/n, dmodel] in an all-reduce operation.\\n21\\nFedus, Zoph and Shazeer\\n5.4 Expert and Data Parallelism\\nNext we describe the partitioning strategy for expert and data parallelism. Switch Trans-\\nformers will allocate all of their cores to the data partitioning dimension n, which will also\\ncorrespond to the number of experts in the model. For each token per core a router locally\\ncomputes assignments to the experts. The output is a binary matrix of size [n, B/n, E,\\nC] which is partitioned across the ﬁrst dimension and determines expert assignment. This\\nbinary matrix is then used to do a gather via matrix multiplication with the input tensor\\nof [n, B/n, dmodel].\\neinsum([n, B/n, dmodel], [n, B/n, E, C], dimension = [B/n])\\n(7)\\nresulting in the ﬁnal tensor of shape [n, E, C, dmodel], which is sharded across the ﬁrst\\ndimension. Because each core has its own expert, we do an all-to-all communication of\\nsize [E, C, dmodel] to now shard the E dimension instead of the n-dimension. There are\\nadditional communication costs of bﬂoat16 tensors of size E×C ×dmodel in the forward pass\\nto analogusly receive the tokens from each expert located on diﬀerent cores. See Appendix F\\nfor a detailed analysis of the expert partitioning code.\\n5.5 Expert, Model and Data Parallelism\\nIn the design of our best model, we seek to balance the FLOPS per token and the parameter\\ncount. When we scale the number of experts, we increase the number of parameters, but do\\nnot change the FLOPs per token. In order to increase FLOPs, we must also increase the dff\\ndimension (which also increases parameters, but at a slower rate). This presents a trade-oﬀ:\\nas we increase dff we will run out of memory per core, which then necessitates increasing\\nm. But since we have a ﬁxed number of cores N, and N = n × m, we must decrease n,\\nwhich forces use of a smaller batch-size (in order to hold tokens per core constant).\\nWhen combining both model and expert-parallelism, we will have all-to-all communica-\\ntion costs from routing the tokens to the correct experts along with the internal all-reduce\\ncommunications from the model parallelism. Balancing the FLOPS, communication costs\\nand memory per core becomes quite complex when combining all three methods where the\\nbest mapping is empirically determined. See our further analysis in section 5.6 for how the\\nnumber of experts eﬀects the downstream performance as well.\\n5.6 Towards Trillion Parameter Models\\nCombining expert, model and data parallelism, we design two large Switch Transformer\\nmodels, one with 395 billion and 1.6 trillion parameters, respectively. We study how these\\nmodels perform on both up-stream pre-training as language models and their downstream\\nﬁne-tuning performance. The parameters, FLOPs per sequence and hyper-parameters of\\nthe two diﬀerent models are listed below in Table 9. Standard hyper-parameters of the\\nTransformer, including dmodel, dff, dkv, number of heads and number of layers are described,\\nas well as a less common feature, FFNGEGLU, which refers to a variation of the FFN layer\\nwhere the expansion matrix is substituted with two sets of weights which are non-linearly\\ncombined (Shazeer, 2020).\\nThe Switch-C model is designed using only expert-parallelism, and no model-parallelism,\\nas described earlier in Section 5.4. As a result, the hyper-parameters controlling the width,\\n22\\nSwitch Transformers\\nModel\\nParameters\\nFLOPs/seq\\ndmodel\\nFFNGEGLU\\ndff\\ndkv\\nNum. Heads\\nT5-Base\\n0.2B\\n124B\\n768\\n✓\\n2048\\n64\\n12\\nT5-Large\\n0.7B\\n425B\\n1024\\n✓\\n2816\\n64\\n16\\nT5-XXL\\n11B\\n6.3T\\n4096\\n✓\\n10240\\n64\\n64\\nSwitch-Base\\n7B\\n124B\\n768\\n✓\\n2048\\n64\\n12\\nSwitch-Large\\n26B\\n425B\\n1024\\n✓\\n2816\\n64\\n16\\nSwitch-XXL\\n395B\\n6.3T\\n4096\\n✓\\n10240\\n64\\n64\\nSwitch-C\\n1571B\\n890B\\n2080\\n6144\\n64\\n32\\nModel\\nExpert Freq.\\nNum. Layers\\nNum Experts\\nNeg. Log Perp. @250k\\nNeg. Log Perp. @ 500k\\nT5-Base\\n–\\n12\\n–\\n-1.599\\n-1.556\\nT5-Large\\n–\\n24\\n–\\n-1.402\\n-1.350\\nT5-XXL\\n–\\n24\\n–\\n-1.147\\n-1.095\\nSwitch-Base\\n1/2\\n12\\n128\\n-1.370\\n-1.306\\nSwitch-Large\\n1/2\\n24\\n128\\n-1.248\\n-1.177\\nSwitch-XXL\\n1/2\\n24\\n64\\n-1.086\\n-1.008\\nSwitch-C\\n1\\n15\\n2048\\n-1.096\\n-1.043\\nTable 9: Switch model design and pre-training performance.\\nWe compare the hyper-\\nparameters and pre-training performance of the T5 models to our Switch Trans-\\nformer variants. The last two columns record the pre-training model quality on the\\nC4 data set after 250k and 500k steps, respectively. We observe that the Switch-\\nC Transformer variant is 4x faster to a ﬁxed perplexity (with the same compute\\nbudget) than the T5-XXL model, with the gap increasing as training progresses.\\ndepth, number of heads, and so on, are all much smaller than the T5-XXL model.\\nIn\\ncontrast, the Switch-XXL is FLOP-matched to the T5-XXL model, which allows for larger\\ndimensions of the hyper-parameters, but at the expense of additional communication costs\\ninduced by model-parallelism (see Section 5.5 for more details).\\nSample eﬃciency versus T5-XXL. In the ﬁnal two columns of Table 9 we record\\nthe negative log perplexity on the C4 corpus after 250k and 500k steps, respectively. After\\n250k steps, we ﬁnd both Switch Transformer variants to improve over the T5-XXL version’s\\nnegative log perplexity by over 0.061.10 To contextualize the signiﬁcance of a gap of 0.061,\\nwe note that the T5-XXL model had to train for an additional 250k steps to increase\\n0.052. The gap continues to increase with additional training, with the Switch-XXL model\\nout-performing the T5-XXL by 0.087 by 500k steps.\\nTraining instability. However, as described in the introduction, large sparse models\\ncan be unstable, and as we increase the scale, we encounter some sporadic issues.\\nWe\\nﬁnd that the larger Switch-C model, with 1.6T parameters and 2048 experts, exhibits no\\ntraining instability at all. Instead, the Switch XXL version, with nearly 10x larger FLOPs\\nper sequence, is sometimes unstable. As a result, though this is our better model on a\\nstep-basis, we do not pre-train for a full 1M steps, in-line with the ﬁnal reported results of\\nT5 (Raﬀel et al., 2019).\\n10. This reported quality diﬀerence is a lower bound, and may actually be larger. The T5-XXL was pre-\\ntrained on an easier C4 data set which included duplicated, and thus easily copied, snippets within\\nexamples.\\n23\\nFedus, Zoph and Shazeer\\nReasoning ﬁne-tuning performance.\\nAs a preliminary assessment of the model\\nquality, we use a Switch-XXL model partially pre-trained on 503B tokens, or approximately\\nhalf the text used by the T5-XXL model. Using this checkpoint, we conduct multi-task\\ntraining for eﬃciency, where all tasks are learned jointly, rather than individually ﬁne-tuned.\\nWe ﬁnd that SQuAD accuracy on the validation set increases to 89.7 versus state-of-the-art\\nof 91.3. Next, the average SuperGLUE test score is recorded at 87.5 versus the T5 version\\nobtaining a score of 89.3 compared to the state-of-the-art of 90.0 (Wang et al., 2019). On\\nANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7\\naccuracy versus the prior best of 49.4 (Yang et al., 2020). We note that while the Switch-\\nXXL has state-of-the-art Neg. Log Perp. on the upstream pre-training task, its gains have\\nnot yet fully translated to SOTA downstream performance. We study this issue more in\\nAppendix E.\\nKnowledge-based ﬁne-tuning performance. Finally, we also conduct an early ex-\\namination of the model’s knowledge with three closed-book knowledge-based tasks: Natural\\nQuestions, WebQuestions and TriviaQA, without additional pre-training using Salient Span\\nMasking (Guu et al., 2020). In all three cases, we observe improvements over the prior state-\\nof-the-art T5-XXL model (without SSM). Natural Questions exact match increases to 34.4\\nversus the prior best of 32.8, Web Questions increases to 41.0 over 37.2, and TriviaQA\\nincreases to 47.5 versus 42.9.\\nSumming up, despite training on less than half the data of other models, we already\\nﬁnd comparable, and sometimes state-of-the-art, model quality.\\nCurrently, the Switch\\nTransformer translates substantial upstream gains better to knowledge-based tasks, than\\nreasoning-tasks (see Appendix E). Extracting stronger ﬁne-tuning performance from large\\nexpert models is an active research question, and the pre-training perplexity indicates future\\nimprovements should be possible.\\n6. Related Work\\nThe importance of scale in neural networks is widely recognized and several approaches have\\nbeen proposed. Recent works have scaled models to billions of parameters through using\\nmodel parallelism (e.g. splitting weights and tensors across multiple cores) (Shazeer et al.,\\n2018; Rajbhandari et al., 2019; Raﬀel et al., 2019; Brown et al., 2020; Shoeybi et al., 2019).\\nAlternatively, Harlap et al. (2018); Huang et al. (2019) propose using pipeline based model\\nparallelism, where diﬀerent layers are split across devices and micro-batches are pipelined to\\nthe diﬀerent layers. Finally, Product Key networks (Lample et al., 2019) were proposed to\\nscale up the capacity of neural networks by doing a lookup for learnable embeddings based\\non the incoming token representations to a given layer.\\nOur work studies a speciﬁc model in a class of methods that do conditional computation,\\nwhere computation decisions are made dynamically based on the input. Cho and Bengio\\n(2014) proposed adaptively selecting weights based on certain bit patterns occuring in the\\nmodel hidden-states.\\nEigen et al. (2013) built stacked expert layers with dense matrix\\nmultiplications and ReLU activations and showed promising results on jittered MNIST and\\nmonotone speech. In computer vision Puigcerver et al. (2020) manually route tokens based\\non semantic classes during upstream pre-training and then select the relevant experts to be\\nused according to the downstream task.\\n24\\nSwitch Transformers\\nMixture of Experts (MoE), in the context of modern deep learning architectures, was\\nproven eﬀective in Shazeer et al. (2017). That work added an MoE layer which was stacked\\nbetween LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately\\nrouted to combinations of experts.\\nThis resulted in state-of-the-art results in language\\nmodeling and machine translation benchmarks. The MoE layer was reintroduced into the\\nTransformer architecture by the Mesh Tensorﬂow library (Shazeer et al., 2018) where MoE\\nlayers were introduced as a substitute of the FFN layers, however, there were no accom-\\npanying NLP results. More recently, through advances in machine learning infrastructure,\\nGShard (Lepikhin et al., 2020), which extended the XLA compiler, used the MoE Trans-\\nformer to dramatically improve machine translation across 100 languages. Finally Fan et al.\\n(2021) chooses a diﬀerent deterministic MoE strategy to split the model parameters into\\nnon-overlapping groups of languages.\\nSparsity along the sequence length dimension (L) in the Transformer attention patterns\\nhas been a successful technique to reduce the attention complexity from O(L2) (Child et al.,\\n2019; Correia et al., 2019; Sukhbaatar et al., 2019; Kitaev et al., 2020; Zaheer et al., 2020;\\nBeltagy et al., 2020). This has enabled learning longer sequences than previously possi-\\nble. This version of the Switch Transformer does not employ attention sparsity, but these\\ntechniques are complimentary, and, as future work, these could be combined to potentially\\nimprove learning on tasks requiring long contexts.\\n7. Discussion\\nWe pose and discuss questions about the Switch Transformer, and sparse expert models\\ngenerally, where sparsity refers to weights, not on attention patterns.\\nIsn’t Switch Transformer better due to sheer parameter count? Yes, and by\\ndesign! Parameters, independent of the total FLOPs used, are a useful axis to scale neural\\nlanguage models. Large models have been exhaustively shown to perform better (Kaplan\\net al., 2020). But in this case, our model is more sample eﬃcient and faster while using the\\nsame computational resources.\\nI don’t have access to a supercomputer—is this still useful for me? Though\\nthis work has focused on extremely large models, we also ﬁnd that models with as few as two\\nexperts improves performance while easily ﬁtting within memory constraints of commonly\\navailable GPUs or TPUs (details in Appendix D). We therefore believe our techniques are\\nuseful in small-scale settings.\\nDo sparse models outperform dense models on the speed-accuracy Pareto\\ncurve? Yes. Across a wide variety of diﬀerent models sizes, sparse models outperform\\ndense models per step and on wall clock time. Our controlled experiments show for a ﬁxed\\namount of computation and time, sparse models outperform dense models.\\nI can’t deploy a trillion parameter model—can we shrink these models? We\\ncannot fully preserve the model quality, but compression rates of 10 to 100x are achievable\\nby distilling our sparse models into dense models while achieving ≈30% of the quality gain\\nof the expert model.\\nWhy use Switch Transformer instead of a model-parallel dense model? On a\\ntime basis, Switch Transformers can be far more eﬃcient than dense-models with sharded\\nparameters (Figure 6). Also, we point out that this decision is not mutually exclusive—we\\n25\\nFedus, Zoph and Shazeer\\ncan, and do, use model-parallelism in Switch Transformers, increasing the FLOPs per token,\\nbut incurring the slowdown of conventional model-parallelism.\\nWhy aren’t sparse models widely used already? The motivation to try sparse\\nmodels has been stymied by the massive success of scaling dense models (the success of\\nwhich is partially driven by co-adaptation with deep learning hardware as argued in Hooker\\n(2020)). Further, sparse models have been subject to multiple issues including (1) model\\ncomplexity, (2) training diﬃculties, and (3) communication costs.\\nSwitch Transformer\\nmakes strides to alleviate these issues.\\n8. Future Work\\nThis paper lays out a simpliﬁed architecture, improved training procedures, and a study\\nof how sparse models scale. However, there remain many open future directions which we\\nbrieﬂy describe here:\\n1. A signiﬁcant challenge is further improving training stability for the largest models.\\nWhile our stability techniques were eﬀective for our Switch-Base, Switch-Large and\\nSwitch-C models (no observed instability), they were not suﬃcient for Switch-XXL.\\nWe have taken early steps towards stabilizing these models, which we think may be\\ngenerally useful for large models, including using regularizers for improving stability\\nand adapted forms of gradient clipping, but this remains unsolved.\\n2. Generally we ﬁnd that improved pre-training quality leads to better downstream re-\\nsults (Appendix E), though we sometimes encounter striking anomalies. For instance,\\ndespite similar perplexities modeling the C4 data set, the 1.6T parameter Switch-C\\nachieves only an 87.7 exact match score in SQuAD, which compares unfavorably to\\n89.6 for the smaller Switch-XXL model. One notable diﬀerence is that the Switch-\\nXXL model applies ≈10x the FLOPS per token than the Switch-C model, even though\\nit has ≈4x less unique parameters (395B vs 1.6T). This suggests a poorly understood\\ndependence between ﬁne-tuning quality, FLOPS per token and number of parameters.\\n3. Perform a comprehensive study of scaling relationships to guide the design of ar-\\nchitectures blending data, model and expert-parallelism. Ideally, given the specs of\\na hardware conﬁguration (computation, memory, communication) one could more\\nrapidly design an optimal model. And, vice versa, this may also help in the design of\\nfuture hardware.\\n4. Our work falls within the family of adaptive computation algorithms. Our approach\\nalways used identical, homogeneous experts, but future designs (facilitated by more\\nﬂexible infrastructure) could support heterogeneous experts. This would enable more\\nﬂexible adaptation by routing to larger experts when more computation is desired—\\nperhaps for harder examples.\\n5. Investigating expert layers outside the FFN layer of the Transformer. We ﬁnd pre-\\nliminary evidence that this similarly can improve model quality.\\nIn Appendix A,\\nwe report quality improvement adding these inside Self-Attention layers, where our\\n26\\nSwitch Transformers\\nlayer replaces the weight matrices which produce Q, K, V. However, due to training\\ninstabilities with the bﬂoat16 format, we instead leave this as an area for future work.\\n6. Examining Switch Transformer in new and across diﬀerent modalities. We have thus\\nfar only considered language, but we believe that model sparsity can similarly provide\\nadvantages in new modalities, as well as multi-modal networks.\\nThis list could easily be extended, but we hope this gives a ﬂavor for the types of\\nchallenges that we are thinking about and what we suspect are promising future directions.\\n9. Conclusion\\nSwitch Transformers are scalable and eﬀective natural language learners. We simplify Mix-\\nture of Experts to produce an architecture that is easy to understand, stable to train and\\nvastly more sample eﬃcient than equivalently-sized dense models. We ﬁnd that these models\\nexcel across a diverse set of natural language tasks and in diﬀerent training regimes, includ-\\ning pre-training, ﬁne-tuning and multi-task training. These advances make it possible to\\ntrain models with hundreds of billion to trillion parameters and which achieve substantial\\nspeedups relative to dense T5 baselines. We hope our work motivates sparse models as\\nan eﬀective architecture and that this encourages researchers and practitioners to consider\\nthese ﬂexible models in natural language tasks, and beyond.\\nAcknowledgments\\nThe authors would like to thank Margaret Li who provided months of key insights into\\nalgorithmic improvements and suggestions for empirical studies. Hugo Larochelle for sage\\nadvising and clarifying comments on the draft, Irwan Bello for detailed comments and\\ncareful revisions, Colin Raﬀel and Adam Roberts for timely advice on neural language\\nmodels and the T5 code-base, Yoshua Bengio for advising and encouragement on research\\nin adaptive computation, Jascha Sohl-dickstein for interesting new directions for stabilizing\\nnew large scale models and paper revisions, and the Google Brain Team for useful discussions\\non the paper. Blake Hechtman who provided invaluable help in proﬁling and improving the\\ntraining performance of our models.\\nA. Switch for Attention\\nShazeer et al. (2018); Lepikhin et al. (2020) designed MoE Transformers (Shazeer et al.,\\n2017) by adding MoE layers into the dense feedfoward network (FFN) computations of\\nthe Transformer. Similarly, our work also replaced the FFN layer in the Transformer, but\\nwe brieﬂy explore here an alternate design. We add Switch layers into the Transformer\\nSelf-Attention layers. To do so, we replace the trainable weight matrices that produce the\\nqueries, keys and values with Switch layers as seen in Figure 10.\\nTable 10 records the quality after a ﬁxed number of steps as well as training time\\nfor several variants. Though we ﬁnd improvements, we also found these layers to be more\\nunstable when using bﬂoat16 precision and thus we did not include them in the ﬁnal variant.\\n27\\nFedus, Zoph and Shazeer\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\np = 0.5\\np = 0.7\\nPositional\\nembedding\\nPositional\\nembedding\\nAdd + Normalize\\nSwitching Self-Attention\\nAdd + Normalize\\nFeed Forward Layer\\ny\\nx\\nMore\\nParameters\\nSelf-Attention\\nQ\\xa0 \\xa0 \\xa0 \\xa0 K\\xa0 \\xa0 \\xa0 \\xa0 V\\nRouter\\nFeed-Forward Layer\\nSelf-Attention\\nQ\\xa0 \\xa0 \\xa0 \\xa0 K\\xa0 \\xa0 \\xa0 \\xa0 V\\nFigure 10: Switch layers in attention. We diagram how to incorporate the Switch layer into\\nthe Self-Attention transformer block. For each token (here we show two tokens,\\nx1 = “More” and x2 = “Parameters”), one set of weights produces the query\\nand the other set of unique weights produces the shared keys and values. We\\nexperimented with each expert being a linear operation, as well as a FFN, as\\nwas the case throughout this work. While we found quality improvements using\\nthis, we found this to be more unstable when used with low precision number\\nformats, and thus leave it for future work.\\nHowever, when these layers do train stably, we believe the preliminary positive results\\nsuggests a future promising direction.\\nModel\\nPrecision\\nQuality\\nQuality\\nSpeed\\n@100k Steps (↑)\\n@16H (↑)\\n(ex/sec) (↑)\\nExperts FF\\nﬂoat32\\n-1.548\\n-1.614\\n1480\\nExpert Attention\\nﬂoat32\\n-1.524\\n-1.606\\n1330\\nExpert Attention\\nbﬂoat16\\n[diverges]\\n[diverges]\\n–\\nExperts FF + Attention\\nﬂoat32\\n-1.513\\n-1.607\\n1240\\nExpert FF + Attention\\nbﬂoat16\\n[diverges]\\n[diverges]\\n–\\nTable 10: Switch attention layer results. All models have 32 experts and train with 524k to-\\nkens per batch. Experts FF is when experts replace the FFN in the Transformer,\\nwhich is our standard setup throughout the paper. Experts FF + Attention is\\nwhen experts are used to replace both the FFN and the Self-Attention layers.\\nWhen training with bﬂoat16 precision the models that have experts attention\\ndiverge.\\n28\\nSwitch Transformers\\nB. Preventing Token Dropping with No-Token-Left-Behind\\nDue to software constraints on TPU accelerators, the shapes of our Tensors must be stat-\\nically sized.\\nAs a result, each expert has a ﬁnite and ﬁxed capacity to process token\\nrepresentations. This, however, presents an issue for our model which dynamically routes\\ntokens at run-time that may result in an uneven distribution over experts. If the number of\\ntokens sent to an expert is less than the expert capacity, then the computation may simply\\nbe padded – an ineﬃcient use of the hardware, but mathematically correct. However, when\\nthe number of tokens sent to an expert is larger than its capacity (expert overﬂow), a proto-\\ncol is needed to handle this. Lepikhin et al. (2020) adapts a Mixture-of-Expert model and\\naddresses expert overﬂow by passing its representation to the next layer without processing\\nthrough a residual connection which we also follow.\\nWe suspected that having no computation applied to tokens could be very wasteful,\\nespecially since if there is overﬂow on one expert, that means another expert will have extra\\ncapacity. With this intuition we create No-Token-Left-Behind, which iteratively reroutes\\nany tokens that are at ﬁrst routed to an expert that is overﬂowing. Figure\\n11 shows a\\ngraphical description of this method, which will allow us to guarantee almost no tokens\\nwill be dropped during training and inference. We hypothesised that this could improve\\nperformance and further stabilize training, but we found no empirical beneﬁts. We suspect\\nthat once the network learns associations between diﬀerent tokens and experts, if this as-\\nsociation is changed (e.g. sending a token to its second highest expert) then performance\\ncould be degraded.\\nC. Encouraging Exploration Across Experts\\nAt each expert-layer, the router determines to which expert to send the token. This is a\\ndiscrete decision over the available experts, conditioned on information about the token’s\\nrepresentation.\\nBased on the incoming token representation, the router determines the\\nbest expert, however, it receives no counterfactual information about how well it would\\nhave done selecting an alternate expert. As in reinforcement learning, a classic exploration-\\nexploitation dilemma arises (Sutton and Barto, 2018). These issues have been similarly\\nnoted and addressed diﬀerently by Rosenbaum et al. (2017) which demonstrated success\\nin multi-task learning. This particular setting most closely matches that of a contextual\\nbandit (Robbins, 1952). Deterministically selecting the top expert always amounts to an\\nexploitative strategy – we consider balancing exploration to seek better expert assignment.\\nTo introduce exploration, we consider several approaches: 1) deterministic or argmax 2)\\nsampling from the softmax distribution 3) input dropout on the incoming representation 4)\\nmultiplicative jitter noise on the incoming representation. The resulting impact on model\\nquality is reported in Table 11. Throughout this work, we use input jitter to inject noise as\\nwe have found it to empirically perform the best.\\nD. Switch Transformers in Lower Compute Regimes\\nSwitch Transformer is also an eﬀective architecture at small scales as well as in regimes\\nwith thousands of cores and trillions of parameters. Many of our prior experiments were\\n29\\nFedus, Zoph and Shazeer\\nTokens\\nExpert 1\\nExpert 2\\nExpert 3\\nRouter\\nProbabilities\\n0.1\\n0.7\\n0.2\\n0.7\\n0.2\\n0.1\\n0.5\\n0.3\\n0.2\\n0.8\\n0.1\\n0.1\\n0.3\\n0.1\\n0.6\\n0.7\\n0.1\\n0.2\\nRoute token to\\nhighest probability\\nStage-1\\nRoute token to\\nsecond highest\\nprobability if not\\nrouted\\nStage-2\\nFigure 11: Diagram of the No-Token-Left-Behind Routing. Stage 1 is equivalent to Switch\\nrouting where tokens are routed to the expert with the highest probability from\\nthe router. In Stage 2 we look at all tokens that have overﬂowed and route them\\nto the expert with which has the second highest probability. Tokens can still be\\noverﬂowed if their second highest expert has too many tokens, but this allows\\nmost of the tokens to be routed.\\nThis process can be iterated to guarantee\\nvirtually no tokens are dropped at all.\\nModel\\nQuality (Neg. Log Perp.) (↑)\\nArgmax\\n-1.471\\nSample softmax\\n-1.570\\nInput dropout\\n-1.480\\nInput jitter\\n-1.468\\nTable 11: Router Exploration Strategies. Quality of the Switch Transformer, measured by\\nthe negative log perplexity, under diﬀerent randomness-strategies for selecting\\nthe expert (lower is better). There is no material speed performance diﬀerence\\nbetween the variants.\\nat the scale of 10B+ parameter models, but we show in Figure 12 as few as 2 experts\\nproduce compelling gains over a FLOP-matched counterpart. Even if a super computer is\\nnot readily available, training Switch Transformers with 2, 4, or 8 experts (as we typically\\nrecommend one expert per core) results in solid improvements over T5 dense baselines.\\n30\\nSwitch Transformers\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nTraining Step\\n1e5\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\n1.5\\nNeg Log Perplexity\\nSwitch-Base: 8e\\nSwitch-Base: 4e\\nSwitch-Base: 2e\\nT5-Base\\nFigure 12: Switch Transformer with few experts. Switch Transformer improves over the\\nbaseline even with very few experts. Here we show scaling properties at very\\nsmall scales, where we improve over the T5-Base model using 2, 4, and 8 experts.\\n31\\nFedus, Zoph and Shazeer\\nE. Relation of Upstream to Downstream Model Performance\\nThere is no guarantee that a model’s quality on a pre-training objective will translate to\\ndownstream task results. Figure 13 presents the correlation of the upstream model quality,\\nfor both dense and Switch models, on the C4 pre-training task with two downstream task\\nmeasures: average SuperGLUE performance and TriviaQA score.\\nWe choose these two\\ntasks as one probes the model’s reasoning and the other factual knowledge.\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\n1.1\\n1.0\\nC4 Neg. Log Perplexity\\n65\\n70\\n75\\n80\\n85\\n90\\nSuperGLUE Score\\nSOTA\\nDense\\nSwitch\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\n1.2\\n1.1\\n1.0\\nC4 Neg. Log Perplexity\\n10\\n20\\n30\\n40\\n50\\nTriviaQA Score\\nSOTA\\nDense\\nSwitch\\nFigure 13: Upstream pre-trained quality to downstream model quality. We correlate the\\nupstream performance with downstream quality on both SuperGLUE and Triv-\\niaQA (SOTA recorded without SSM), reasoning and knowledge-heavy bench-\\nmarks, respectively (validation sets). We ﬁnd that, as with the baseline, the\\nSwitch model scales with improvements in the upstream pre-training task. For\\nSuperGLUE, we ﬁnd a loosely linear relation between negative log perplexity\\nand the average SuperGLUE score. However, the dense model often performs\\nbetter for a ﬁxed perplexity, particularly in the large-scale regime. Conversely,\\non the knowledge-heavy task, TriviaQA, we ﬁnd that the Switch Transformer\\nmay follow an improved scaling relationship – for a given upstream perplexity,\\nit does better than a dense counterpart. Further statistics (expensive to collect\\nand left to future work) would be necessary to conﬁrm these observations.\\nWe ﬁnd a consistent correlation, indicating that for both baseline and Switch models,\\nimproved pre-training leads to better downstream results.\\nAdditionally, for a ﬁxed up-\\nstream perplexity we ﬁnd that both Switch and dense models perform similarly in the small\\nto medium model size regime. However, in the largest model regime (T5-11B/T5-XXL)\\nour largest Switch models, as mentioned in Section 5.6, do not always translate their up-\\nstream perplexity well to downstream ﬁne-tuning on the SuperGLUE task. This warrants\\nfuture investigation and study to fully realize the potential of sparse models. Understand-\\ning the ﬁne-tuning dynamics with expert-models is very complicated and is dependent on\\nregularization, load-balancing, and ﬁne-tuning hyper-parameters.\\n32\\nSwitch Transformers\\nF. Pseudo Code for Switch Transformers\\nPseudocode for Switch Transformers in Mesh Tensorﬂow (Shazeer et al., 2018). No model\\nparallelism is being used for the below code (see 5.4 for more details).\\nimport mesh tensorflow as mtf\\ndef load balance loss(router probs, expert mask):\\n\"\"\"Calculate load−balancing loss to ensure diverse expert routing.\"\"\"\\n# router probs is the probability assigned for each expert per token.\\n# router probs shape: [num cores, tokens per core, num experts]\\n# expert index contains the expert with the highest router probability in one−hot format.\\n# expert mask shape: [num cores, tokens per core, num experts]\\n# For each core, get the fraction of tokens routed to each expert.\\n# density 1 shape: [num cores, num experts]\\ndensity 1 = mtf.reduce mean(expert mask, reduced dim=tokens per core)\\n# For each core, get fraction of probability mass assigned to each expert\\n# from the router across all tokens.\\n# density 1 proxy shape: [num cores, num experts]\\ndensity 1 proxy = mtf.reduce mean(router probs, reduced dim=tokens per core)\\n# density l for a single core: vector of length num experts that sums to 1.\\n# density l proxy for a single core: vector of length num experts that sums to 1.\\n# Want both vectors to have uniform allocation (1/num experts) across all num expert elements.\\n# The two vectors will be pushed towards uniform allocation when the dot product is minimized.\\nloss = mtf.reduce mean(density 1 proxy ∗density 1) ∗(num experts ˆ 2)\\nreturn loss\\nFigure 14: Pseudo code for the load balance loss for Switch Transformers in Mesh Tensor-\\nﬂow.\\n33\\nFedus, Zoph and Shazeer\\nimport mesh tensorflow as mtf\\ndef router(inputs, capacity factor):\\n\"\"\"Produce the combine and dispatch tensors used for sending and\\nreceiving tokens from their highest probability expert. \"\"\"\\n# Core layout is split across num cores for all tensors and operations.\\n# inputs shape: [num cores, tokens per core, d model]\\nrouter weights = mtf.Variable(shape=[d model, num experts])\\n# router logits shape: [num cores, tokens per core, num experts]\\nrouter logits = mtf.einsum([inputs, router weights], reduced dim=d model)\\nif is training:\\n# Add noise for exploration across experts.\\nrouter logits += mtf.random uniform(shape=router logits.shape, minval=1−eps, maxval=1+eps)\\n# Convert input to softmax operation from bfloat16 to float32 for stability.\\nrouter logits = mtf.to float32(router logits)\\n# Probabilities for each token of what expert it should be sent to.\\nrouter probs = mtf.softmax(router logits, axis=−1)\\n# Get the top−1 expert for each token. expert gate is the top−1 probability\\n# from the router for each token. expert index is what expert each token\\n# is going to be routed to.\\n# expert gate shape: [num cores, tokens per core]\\n# expert index shape: [num cores, tokens per core]\\nexpert gate, expert index = mtf.top 1(router probs, reduced dim=num experts)\\n# expert mask shape: [num cores, tokens per core, num experts]\\nexpert mask = mtf.one hot(expert index, dimension=num experts)\\n# Compute load balancing loss.\\naux loss = load balance loss(router probs, expert mask)\\n# Experts have a fixed capacity, ensure we do not exceed it. Construct\\n# the batch indices, to each expert, with position in expert\\n# make sure that not more that expert capacity examples can be routed to\\n# each expert.\\nposition in expert = mtf.cumsum(expert mask, dimension=tokens per core) ∗expert mask\\n# Keep only tokens that fit within expert capacity.\\nexpert mask ∗= mtf.less(position in expert, expert capacity)\\nexpert mask flat = mtf.reduce sum(expert mask, reduced dim=experts dim)\\n# Mask out the experts that have overflowed the expert capacity.\\nexpert gate ∗= expert mask flat\\n# combine tensor used for combining expert outputs and scaling with router probability.\\n# combine tensor shape: [num cores, tokens per core, num experts, expert capacity]\\ncombine tensor = (\\nexpert gate ∗expert mask flat ∗\\nmtf.one hot(expert index, dimension=num experts) ∗\\nmtf.one hot(position in expert, dimension=expert capacity))\\n# Cast back outputs to bfloat16 for the rest of the layer.\\ncombine tensor = mtf.to bfloat16(combine tensor)\\n# Create binary dispatch tensor that is 1 if the token gets routed to the corresponding expert.\\n# dispatch tensor shape: [num cores, tokens per core, num experts, expert capacity]\\ndispatch tensor = mtf.cast(combine tensor, tf.bool)\\nreturn dispatch tensor, combine tensor, aux loss\\nFigure 15: Pseudo code for the router for Switch Transformers in Mesh Tensorﬂow.\\n34\\nSwitch Transformers\\nimport mesh tensorflow as mtf\\ndef switch layer(inputs, n, capacity factor, num experts):\\n\"\"\"Distributed switch transformer feed−forward layer.\"\"\"\\n# num cores (n) = total cores for training the model (scalar).\\n# d model = model hidden size (scalar).\\n# num experts = total number of experts.\\n# capacity factor = extra buffer for each expert.\\n# inputs shape: [batch, seq len, d model]\\nbatch, seq len, d model = inputs.get shape()\\n# Each core will route tokens per core tokens to the correct experts.\\ntokens per core = batch ∗seq len / num cores\\n# Each expert will have shape [num cores, expert capacity, d model].\\n# Each core is responsible for sending expert capacity tokens\\n# to each expert.\\nexpert capacity = tokens per core ∗capacity factor / num experts\\n# Reshape to setup per core expert dispatching.\\n# shape: [batch, seq len, d model] −> [num cores, tokens per core, d model]\\n# Core layout: [n, 1, 1] −> [n, 1, 1]\\ninputs = mtf.reshape(inputs, [num cores, tokens per core, d model])\\n# Core Layout: [n, 1, 1] −> [n, 1, 1, 1], [n, 1, 1, 1]\\n# dispatch tensor (boolean) shape: [num cores, tokens per core, num experts, expert capacity]\\n# dispatch tensor is used for routing tokens to the correct expert.\\n# combine tensor (float) shape: [num cores, tokens per core, num experts, expert capacity]\\n# combine tensor used for combining expert outputs and scaling with router\\n# probability.\\ndispatch tensor, combine tensor, aux loss = router(inputs, expert capacity)\\n# Matmul with large boolean tensor to assign tokens to the correct expert.\\n# Core Layout: [n, 1, 1], −> [1, n, 1, 1]\\n# expert inputs shape: [num experts, num cores, expert capacity, d model]\\nexpert inputs = mtf.einsum([inputs, dispatch tensor], reduce dims=[tokens per core])\\n# All−to−All communication. Cores split across num cores and now we want to split\\n# across num experts. This sends tokens, routed locally, to the correct expert now\\n# split across different cores.\\n# Core layout: [1, n, 1, 1] −> [n, 1, 1, 1]\\nexpert inputs = mtf.reshape(expert inputs, [num experts, num cores, expert capacity, d model])\\n# Standard feed forward computation, where each expert will have its own\\n# unique set of parameters.\\n# Total unique parameters created: num experts ∗(d model ∗d ff ∗2).\\n# expert outputs shape: [num experts, num cores, expert capacity, d model]\\nexpert outputs = feed forward(expert inputs)\\n# All−to−All communication. Cores are currently split across the experts\\n# dimension, which needs to be switched back to being split across num cores.\\n# Core Layout: [n, 1, 1, 1] −> [1, n, 1, 1]\\nexpert outputs = mtf.reshape(expert outputs, [num experts, num cores, expert capacity, d model])\\n# Convert back to input shape and multiply outputs of experts by the routing probability.\\n# expert outputs shape: [num experts, num cores, tokens per core, d model]\\n# expert outputs combined shape: [num cores, tokens per core, d model]\\n# Core Layout: [1, n, 1, 1] −> [n, 1, 1]\\nexpert outputs combined = mtf.einsum([expert outputs, combine tensor], reduce dims=[tokens per core])\\n# Remove tokens per core shapes used for local routing dispatching to match input shape.\\n# Core Layout: [n, 1, 1] −> [n, 1, 1]\\noutputs = mtf.reshape(expert outputs combined, [batch, seq len, d model])\\nreturn outputs, aux loss\\nFigure 16: Pseudo code of the Switch Transformer layer in Mesh Tensorﬂow.\\n35\\nFedus, Zoph and Shazeer\\nReferences\\nMart´\\nın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al. Tensorﬂow:\\nA system for large-scale machine learning. In 12th {USENIX} symposium on operating\\nsystems design and implementation ({OSDI} 16), pages 265–283, 2016.\\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\\nformer. arXiv preprint arXiv:2004.05150, 2020.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on free-\\nbase from question-answer pairs.\\nIn Proceedings of the 2013 conference on empirical\\nmethods in natural language processing, pages 1533–1544, 2013.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\\nLanguage models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\\nwith sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\nKyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation\\nratio for conditional computation in deep learning. arXiv preprint arXiv:1406.7362, 2014.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\\nGon¸\\ncalo M Correia, Vlad Niculae, and Andr´\\ne FT Martins. Adaptively sparse transformers.\\narXiv preprint arXiv:1909.00015, 2019.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBert:\\nPre-\\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805, 2018.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations\\nin a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,\\nMandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond\\nenglish-centric multilingual machine translation. Journal of Machine Learning Research,\\n22(107):1–48, 2021.\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via\\nﬁlling in the . arXiv preprint arXiv:1801.07736, 2018.\\nTrevor Gale, Matei Zaharia, CliﬀYoung, and Erich Elsen. Sparse gpu kernels for deep\\nlearning. arXiv preprint arXiv:2006.10901, 2020.\\nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights.\\nhttps://openai.com/blog/block-sparse-gpu-kernels/, 2017.\\n36\\nSwitch Transformers\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\\nRetrieval-augmented language model pre-training.\\narXiv preprint arXiv:2002.08909,\\n2020.\\nAaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur,\\nGreg Ganger, and Phil Gibbons. Pipedream: Fast and eﬃcient pipeline parallel dnn\\ntraining. arXiv preprint arXiv:1806.03377, 2018.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom.\\nTeaching machines to read and comprehend.\\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Ad-\\nvances in Neural Information Processing Systems, volume 28, pages 1693–1701. Cur-\\nran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf.\\nGeoﬀrey Hinton, Oriol Vinyals, and JeﬀDean. Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531, 2015.\\nSepp Hochreiter and J¨\\nurgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\nSara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,\\nHyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Eﬃcient training\\nof giant neural networks using pipeline parallelism. In Advances in neural information\\nprocessing systems, pages 103–112, 2019.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoﬀrey E Hinton. Adaptive\\nmixtures of local experts. Neural computation, 3(1):79–87, 1991.\\nMichael I Jordan and Robert A Jacobs.\\nHierarchical mixtures of experts and the em\\nalgorithm. Neural computation, 6(2):181–214, 1994.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\\nTriviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\\narXiv:1705.03551, 2017.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\\nChild, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural\\nlanguage models. arXiv preprint arXiv:2001.08361, 2020.\\nNikita Kitaev,  \\nLukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer.\\narXiv preprint arXiv:2001.04451, 2020.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural\\nquestions: a benchmark for question answering research. Transactions of the Association\\nfor Computational Linguistics, 7:453–466, 2019.\\n37\\nFedus, Zoph and Shazeer\\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and\\nHerv´\\ne J´\\negou. Large memory layers with product keys. In Advances in Neural Information\\nProcessing Systems, pages 8548–8559, 2019.\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\\nbetter. arXiv preprint arXiv:2107.06499, 2021.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\\nHuang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models\\nwith conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668,\\n2020.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David\\nGarcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al.\\nMixed precision training. arXiv preprint arXiv:1710.03740, 2017.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the\\nsummary! topic-aware convolutional neural networks for extreme summarization. arXiv\\npreprint arXiv:1808.08745, 2018.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\\nAdversarial nli: A new benchmark for natural language understanding. arXiv preprint\\narXiv:1910.14599, 2019.\\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´\\ne Susano Pinto,\\nSylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert\\nmodels. arXiv preprint arXiv:2009.13239, 2020.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training, 2018.\\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning\\nwith a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\\nSamyam Rajbhandari, JeﬀRasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory opti-\\nmization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054,\\n2019.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\\nquestions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\nPrajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models.\\nIn International Conference on Learning Representations, 2018.\\nHerbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the\\nAmerican Mathematical Society, 58(5):527–535, 1952.\\n38\\nSwitch Transformers\\nAdam Roberts, Colin Raﬀel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\\nClemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive\\nselection of non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239,\\n2017.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence, volume 34, pages 8732–8740, 2020.\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled\\nversion of bert: smaller, faster, cheaper and lighter, 2019.\\nNoam Shazeer. Glu variants improve transformer, 2020.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoﬀrey Hin-\\nton, and JeﬀDean. Outrageously large neural networks: The sparsely-gated mixture-of-\\nexperts layer. arXiv preprint arXiv:1701.06538, 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, CliﬀYoung, et al.\\nMesh-tensorﬂow: Deep learning for supercomputers. In Advances in Neural Information\\nProcessing Systems, pages 10414–10423, 2018.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\\ngpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.\\nNitish Srivastava, Geoﬀrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\\nSalakhutdinov.\\nDropout: a simple way to prevent neural networks from overﬁtting.\\nJournal of Machine Learning Research, 15(1):1929–1958, 2014. URL http://www.cs.\\ntoronto.edu/~rsalakhu/papers/srivastava14a.pdf.\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations\\nfor deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.\\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive\\nattention span in transformers. arXiv preprint arXiv:1905.07799, 2019.\\nRich Sutton. The Bitter Lesson. http://www.incompleteideas.net/IncIdeas/BitterLesson.html,\\n2019.\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Stanford\\nUniversity, 2018.\\nWilson L Taylor. “cloze procedure”: A new tool for measuring readability. Journalism\\nquarterly, 30(4):415–433, 1953.\\n39\\nFedus, Zoph and Shazeer\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\\nGomez,  \\nLukasz Kaiser, and Illia Polosukhin.\\nAttention is all you need.\\nIn Advances\\nin neural information processing systems, pages 5998–6008, 2017.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bow-\\nman. Glue: A multi-task benchmark and analysis platform for natural language under-\\nstanding. arXiv preprint arXiv:1804.07461, 2018.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-\\npurpose language understanding systems. In Advances in Neural Information Processing\\nSystems, pages 3266–3280, 2019.\\nShibo Wang and Pankaj Kanwar. Bﬂoat16: The secret to high performance on cloud tpus.\\nGoogle Cloud Blog, 2019.\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\\nAditya Barua, and Colin Raﬀel. mt5: A massively multilingual pre-trained text-to-text\\ntransformer. arXiv preprint arXiv:2010.11934, 2020.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\\nQuoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding,\\n2020.\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\\nago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.\\nBig bird:\\nTransformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020.\\n40\\n', 'source_name': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity', 'source_url': 'https://arxiv.org/abs/2101.03961'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "GLaM.pdf #3\n",
      "{'content': 'GLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nNan Du * 1 Yanping Huang * 1 Andrew M. Dai * 1 Simon Tong 1 Dmitry Lepikhin 1 Yuanzhong Xu 1\\nMaxim Krikun 1 Yanqi Zhou 1 Adams Wei Yu 1 Orhan Firat 1 Barret Zoph 1 Liam Fedus 1 Maarten Bosma 1\\nZongwei Zhou 1 Tao Wang 1 Yu Emma Wang 1 Kellie Webster 1 Marie Pellat 1 Kevin Robinson 1\\nKathleen Meier-Hellstern 1 Toju Duke 1 Lucas Dixon 1 Kun Zhang 1 Quoc V Le 1 Yonghui Wu 1\\nZhifeng Chen 1 Claire Cui 1\\nAbstract\\nScaling language models with more data, compute\\nand parameters has driven signiﬁcant progress in\\nnatural language processing. For example, thanks\\nto scaling, GPT-3 was able to achieve strong re-\\nsults on in-context learning tasks. However, train-\\ning these large dense models requires signiﬁcant\\namounts of computing resources. In this paper,\\nwe propose and develop a family of language mod-\\nels named GLaM (Generalist Language Model),\\nwhich uses a sparsely activated mixture-of-experts\\narchitecture to scale the model capacity while also\\nincurring substantially less training cost compared\\nto dense variants. The largest GLaM has 1.2 tril-\\nlion parameters, which is approximately 7x larger\\nthan GPT-3. It consumes only 1/3 of the energy\\nused to train GPT-3 and requires half of the com-\\nputation ﬂops for inference, while still achieving\\nbetter overall zero, one and few-shot performance\\nacross 29 NLP tasks.\\n1. Introduction\\nLanguage models have played an important role in the\\nprogress of natural language processing (NLP) in the past\\ndecade. Variants of language models have been used to pro-\\nduce pretrained word vectors (Mikolov et al., 2013; Penning-\\nton et al., 2014), and contextualized word vectors (Peters\\net al., 2018; Devlin et al., 2019) for many NLP applications.\\nThe shift towards scaling with more data and larger mod-\\nels (Shazeer et al., 2017; Huang et al., 2019; Kaplan et al.,\\n2020) has enabled complex natural language tasks to be per-\\nformed with less labeled data. For example, GPT-3 (Brown\\net al., 2020) and FLAN (Wei et al., 2021) demonstrated the\\n*Equal contribution\\n1Google. Correspondence to: Nan Du,\\nYanping Huang, and Andrew M. Dai <dunan@google.com,\\nhuangyp@google.com, adai@google.com>.\\nProceedings of the 39 th International Conference on Machine\\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\\nright 2022 by the author(s).\\nTable 1. Comparison between GPT-3 and GLaM. In a nutshell,\\nGLaM outperforms GPT-3 across 21 natural language understand-\\ning (NLU) benchmarks and 8 natural language generative (NLG)\\nbenchmarks in average while using about half the FLOPs per token\\nduring inference and consuming about one third the energy for\\ntraining.\\nGPT-3 GLaM relative\\ncost\\nFLOPs / token (G)\\n350\\n180\\n−48.6%\\nTrain energy (MWh)\\n1287\\n456\\n−64.6%\\naccuracy\\non average\\nZero-shot\\n56.9\\n62.7\\n+10.2%\\nOne-shot\\n61.6\\n65.5\\n+6.3%\\nFew-shot\\n65.2\\n68.1\\n+4.4%\\nfeasibility of in-context learning for few-shot or even zero-\\nshot generalization, meaning very few labeled examples are\\nneeded to achieve good performance on NLP applications.\\nWhile being effective and performant, scaling further is be-\\ncoming prohibitively expensive and consumes signiﬁcant\\namounts of energy (Patterson et al., 2021).\\nIn this work, we show that a large sparsely activated network\\ncan achieve competitive results compared to state-of-the-art\\ndense models on few-shot tasks while being more compu-\\ntationally efﬁcient. We present a family of generalist lan-\\nguage models called GLaM, that strike a balance between\\ndense and conditional computation. The largest version\\nof GLaM has 1.2T parameters in total with 64 experts per\\nMoE layer (Shazeer et al., 2017; Lepikhin et al., 2021; Fe-\\ndus et al., 2021) where each token in the input batch only\\nactivates a subnetwork of 96.6B (8% of 1.2T) parameters.\\nOn zero, one and few-shot learning, this model compares\\nfavorably to GPT-3 (175B), with signiﬁcantly improved\\nlearning efﬁciency across 29 public NLP benchmarks, rang-\\ning from language completion tasks, open-domain QA tasks,\\nto natural language inference tasks. Thanks to the sparsely\\nactivated architecture and the efﬁcient implementation of the\\nmodel parallelism algorithm, the total energy consumption\\nduring training is only one third of GPT-3’s. We highlight\\nthe comparison between the largest version of GLaM and\\nGPT-3 in Table 1 and Figure 1.\\narXiv:2112.06905v2  [cs.CL]  1 Aug 2022\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nOpen-Domain Question Answering\\nCloze and Completion Tasks\\nWinograd-Style Tasks\\nCommon Sense Reasoning\\nIn-context Reading Comprehension\\nSuperGLUE\\nNatural Language Inference\\n0%\\n10%\\n20%\\n(a) Zero-shot\\nOpen-Domain Question Answering\\nCloze and Completion Tasks\\nWinograd-Style Tasks\\nCommon Sense Reasoning\\nIn-context Reading Comprehension\\nSuperGLUE\\nNatural Language Inference\\n0.0%\\n5.0%\\n10.0%\\n15.0%\\n20.0%\\n(b) One-shot\\nOpen-Domain Question Answering\\nCloze and Completion Tasks\\nWinograd-Style Tasks\\nCommon Sense Reasoning\\nIn-context Reading Comprehension\\nSuperGLUE\\nNatural Language Inference\\n0.0%\\n5.0%\\n10.0%\\n15.0%\\n(c) Few-shot\\n180\\n350\\n0\\n100\\n200\\n300\\nGFLOPS / token\\n456\\n1287\\n0\\n500\\n1000\\nTrain Energy (MWh)\\nGLaM\\nGPT-3\\n(d) Train and inference cost\\nFigure 1. An overview of the percentage change in predictive performance (higher is better) of GLaM (64B/64E) versus GPT-3 (175B) in\\nthe (a) zero-shot, (b) one-shot, and (c) few-shot setting across 7 benchmark categories with 29 public tasks in total. Each bar in panel\\n(a), (b) and (c) represents one benchmark category. Panel (d) compares the FLOPs needed per token prediction and training energy\\nconsumption.\\nWe use GLaM to study the importance of data. Our analysis\\nshows that even for these large models, data quality should\\nnot be sacriﬁced for quantity if the goal is to produce a high-\\nquality auto-regressive language model. More importantly,\\non social dimensions, our results are also the ﬁrst, to our\\nknowledge, to close the performance gap between stereo-\\ntypical and anti-stereotypical examples on the WinoGender\\nbenchmark, suggesting that large, sparsely activated models\\nmay rely less on superﬁcial statistical correlations.\\nFinally, although MoE-based sparse models are not yet com-\\nmon in the NLP community, our work shows that sparse\\ndecoder-only language models can be more performant than\\nthe dense architectures of similar compute FLOPs for the\\nﬁrst time within the few-shot in-context learning setting at\\nscale, suggesting that sparsity is one of the most promising\\ndirections to achieve high-quality NLP models while saving\\nenergy costs (Patterson et al., 2021). MoE should therefore\\nbe considered as a strong candidate for future scaling.\\n2. Related Work\\nLanguage models.\\nNeural language models (Mikolov\\net al., 2010; Sutskever et al., 2011) have been shown to be\\nuseful for many natural language processing tasks. Word em-\\nbedding models and extensions such as word2vec (Mikolov\\net al., 2013), GloVe (Pennington et al., 2014) and paragraph\\nvectors (Le & Mikolov, 2014) have shown good generaliza-\\ntion to many tasks simply by transferring the embeddings.\\nPre-training and Fine-tuning.\\nThe abundance of com-\\npute and data enables training increasingly large models via\\nunsupervised pre-training. This is a natural ﬁt for training\\nneural networks as they exhibit remarkable scalability. Work\\non using recurrent models such as RNNs and LSTMs for\\nlanguage representation (Dai & Le, 2015; Kiros et al., 2015)\\nshowed that general language models could be ﬁne-tuned\\nto improve various language understanding tasks. More re-\\ncently, models that used Transformers (Vaswani et al., 2017)\\nshowed that larger models with self-supervision on unla-\\nbeled data could yield signiﬁcant improvements on NLP\\ntasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019;\\nClark et al., 2020). Transfer learning based on pre-training\\nand ﬁnetuning (Raffel et al., 2020; Houlsby et al., 2019)\\nhas been extensively studied and demonstrated good perfor-\\nmance on downstream tasks. However, a major limitation\\nto this method is that it requires a task-speciﬁc ﬁne-tuning.\\nIn-Context Few-shot Learning.\\nGPT-3 (Brown et al.,\\n2020) and related work (Shoeybi et al., 2019; Lieber et al.,\\n2021; Wei et al., 2021) demonstrated that scaling up lan-\\nguage models greatly improves task-agnostic, few-shot per-\\nformance. These language models are applied without any\\ngradient updates, and only few-shot demonstrations speci-\\nﬁed purely via text interactions with the model are needed.\\nSparsely Gated Networks.\\nMixture-of-Experts based\\nmodels have also shown signiﬁcant advantages. For lan-\\nguage modeling and machine translation, Shazeer et al.\\n(2017) showed that they could effectively use a very large\\nnumber of weights while only needing to compute a small\\nsubset of the computation graph at inference time. There\\nhas also been work on scaling sparsely activated MoE ar-\\nchitectures (Hestness et al., 2017; Shazeer et al., 2018; Lep-\\nikhin et al., 2021; Kudugunta et al., 2021). Recently, Fedus\\net al. (2021) showed results with even larger 1 trillion pa-\\nrameter sparsely activated models (Switch-C). Although\\nboth Switch-C and the largest GLaM model have one tril-\\nlion number of trainable parameters, GLaM is a family of\\ndecoder-only language models, and Switch-C is an encoder-\\ndecoder based sequence to sequence model. Furthermore,\\nSwitch-C is mainly evaluated on ﬁne-tuning benchmarks,\\ne.g., SuperGlue, while GLaM performs well without any\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 2. A sample of related models (Devlin et al., 2019; Raffel\\net al., 2020; Brown et al., 2020; Lieber et al., 2021; Rae et al.,\\n2021; Shoeybi et al., 2019; Lepikhin et al., 2021; Fedus et al.,\\n2021) pre-trained on text corpora. nparams is the total number of\\ntrainable model parameters, nact-params is the number of activated\\nmodel parameters per input token.\\nModel Name\\nModel Type\\nnparams\\nnact-params\\nBERT\\nDense Encoder-only\\n340M\\n340M\\nT5\\nDense Encoder-decoder\\n13B\\n13B\\nGPT-3\\nDense Decoder-only\\n175B\\n175B\\nJurassic-1\\nDense Decoder-only\\n178B\\n178B\\nGopher\\nDense Decoder-only\\n280B\\n280B\\nMegatron-530B\\nDense Decoder-only\\n530B\\n530B\\nGShard-M4\\nMoE Encoder-decoder\\n600B\\n1.5B\\nSwitch-C\\nMoE Encoder-decoder\\n1.5T\\n1.5B\\nGLaM (64B/64E)\\nMoE Decoder-only\\n1.2T\\n96.6B\\nneed for ﬁne-tuning in the few-shot setting shared by GPT-3\\nwhere SuperGlue is a subset. Table 2 summarizes the key\\ndifferences between GLaM and related models pre-trained\\non text corpora.\\n3. Training Dataset\\nTo train our model, we build a high-quality dataset of 1.6\\ntrillion tokens that are representative of a wide range of\\nnatural language use cases. Web pages constitute the vast\\nquantity of data in our unlabeled dataset. However, their\\nquality ranges from professional writing to low-quality com-\\nment and forum pages. Similarly to Brown et al. (2020), we\\ndevelop our own text quality classiﬁer to produce a high-\\nquality web corpus out of an original larger raw corpus. We\\nuse a feature hash based linear classiﬁer for inference speed.\\nThis classiﬁer is trained to classify between a collection\\nof curated text (Wikipedia, books and a few selected web-\\nsites) and other webpages. We use this classiﬁer to estimate\\nthe content quality of a webpage. We then apply this clas-\\nsiﬁer by using a Pareto distribution to sample webpages\\naccording to their score. This allows some lower-quality\\nwebpages to be included to prevent systematic biases in the\\nclassiﬁer (Brown et al., 2020).\\nTable 3. Data and mixture weights in GLaM training set.\\nDataset\\nTokens (B)\\nWeight in mixture\\nFiltered Webpages\\n143\\n0.42\\nWikipedia\\n3\\n0.06\\nConversations\\n174\\n0.28\\nForums\\n247\\n0.02\\nBooks\\n390\\n0.20\\nNews\\n650\\n0.02\\nWe use this process to generate a high-quality ﬁltered subset\\nFigure 2. GLaM model architecture. Each MoE layer (the bottom\\nblock) is interleaved with a Transformer layer (the upper block).\\nFor each input token, e.g., ‘roses’, the Gating module dynamically\\nselects two most relevant experts out of 64, which is represented\\nby the blue grid in the MoE layer. The weighted average of the\\noutputs from these two experts will then be passed to the upper\\nTransformer layer. For the next token in the input sequence, two\\ndifferent experts will be selected.\\nof webpages and combine this with books, Wikipedia pages,\\nforums and news pages and other data sources to create the\\nﬁnal GLaM dataset. We also incorporate the data from pub-\\nlic domain social media conversations used by Adiwardana\\net al. (2020). We set the mixture weights based on the perfor-\\nmance of each component in a smaller model and to prevent\\nsmall sources such as Wikipedia from being over-sampled.\\nTable 3 shows the details of our data component sizes and\\nmixture weights. The mixture weights were chosen based\\non the performance of the component in a small model and\\nto prevent small datasets such as Wikipedia from being over-\\nsampled. To check data contamination, in Section D we\\nconduct an overlap analysis between our training set and\\nthe evaluation data and ﬁnd that it roughly matches that of\\nprevious work (Brown et al., 2020).\\n4. Model Architecture\\nWe\\nleverage\\nsparsely\\nactivated\\nMixture-of-Experts\\n(MoE) (Shazeer et al., 2017; Fedus et al., 2021) in GLaM\\nmodels. Similar to the GShard MoE Transformer (Lepikhin\\net al., 2021), we replace the feed-forward component of\\nevery other Transformer layer with an MoE layer, as shown\\nin Figure 2. Each MoE layer consists of a collection of\\nindependent feed-forward networks as the ‘experts’. A\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\ngating function then uses a softmax activation function\\nto model a probability distribution over these experts.\\nThis distribution indicates how well each expert is able to\\nprocess the incoming input.\\nEven though each MoE layer has many more parameters,\\nthe experts are sparsely activated. This means that for a\\ngiven input token, only a limited subset of experts is used,\\ngiving the model more capacity while limiting computa-\\ntion. In our architecture, the subset size is two1. Each MoE\\nlayer’s learnable gating network is trained to use its input\\nto activate the best two experts for each token of an input\\nsequence. During inference, the learned gating network\\ndynamically picks the two best experts for each token. For\\nan MoE layer with E experts, this essentially provides a\\ncollection of O(E2) different combinations of feed-forward\\nnetworks instead of one in the classic Transformer architec-\\nture, leading to much more computational ﬂexibility. The\\nﬁnal learned representation of a token will be the weighted\\ncombination of the outputs from the selected experts.\\nWe also make additional modiﬁcations to the original Trans-\\nformer architecture. We replace the standard positional\\nembedding with per-layer relative positional bias from Dai\\net al. (2019). In the non-MoE Transformer feed-forward\\nsub-layers, we replace the ﬁrst linear projection and the ac-\\ntivation function with the Gated Linear Unit (Dauphin et al.,\\n2017; Shazeer, 2020), which computes the component-wise\\nproduct of two linear transformation of the input, followed\\nby a Gaussian Error Linear Unit (Hendrycks & Gimpel,\\n2016) activation function. We partition the weights and\\ncomputation of large GLaM models using the 2D shard-\\ning algorithm as described in Xu et al. (2021), which is\\ndescribed in more details in the Section C of the appendix.\\n5. Experiment Setup\\nGLaM is a family of dense and sparse decoder-only lan-\\nguage models, so we ﬁrst elaborate our training settings,\\nhyperparameters, and evaluation protocol in this section.\\n5.1. Training Setting\\nWe train several variants of GLaM to study the behavior of\\nMoE and dense models on the same training data. Table 4\\nshows the hyperparameter settings of different scale GLaM\\nmodels ranging from 130 million parameters to 1.2 trillion\\nparameters. Here, E is the number of experts in the MoE\\nlayer, B is the mini-batch size, S is the input sequence\\nlength, M is the model and embedding dimension, H is\\n1Using more experts will cost more compute FLOPs per pre-\\ndiction, pushing the network to be ‘denser’. Setting the number\\nof selected experts to be two is based on the trade-off between\\npredictive performance and the training/serving efﬁciency of the\\nmodel.\\nthe hidden dimension of the feed-forward network, L is\\nthe number of layers and N is the number of total devices.\\nAdditionally, nparams is the total number of trainable model\\nparameters, nact-params is the number of activated model\\nparameters per input token, nheads is the number of self-\\nattention heads, and dhead is the hidden dimension of each\\nattention head. We also include the respective dense models\\nwith comparable numbers of activated parameters per-token\\nduring inference (and thus similar numbers of per-token\\nFLOPs) as references. We adopt the notation of\\nGLaM (Base Dense Size/E)\\ne.g., GLaM (8B/64E)\\nto describe different variants in the GLaM models. For\\nexample, GLaM (8B/64E) represents the architecture of an\\napproximate 8B parameter dense model with every other\\nlayer replaced by a 64 expert MoE layer. GLaM reduces to a\\ndense Transformer-based language model architecture when\\neach MoE layer only has one expert. We use the notation\\nGLaM (Dense Size)\\ne.g., GLaM (137B)\\nrefers to a dense 137B parameter model trained with the\\nsame dataset.\\n5.2. Hyperparameters and Training Procedure\\nWe use the same learning hyperparameters for all GLaM\\nmodels. More speciﬁcally, We use a maximum sequence\\nlength of 1024 tokens, and pack each input example to have\\nup to 1 million tokens per batch. The dropout rate is set to 0\\nsince the number of available tokens in the training corpus\\nis much greater than the number of processed tokens dur-\\ning training. Our optimizer is Adafactor (Shazeer & Stern,\\n2018) with ﬁrst-moment decay β1 = 0, second-moment\\ndecay β2 = 0.99 with a 1 −t−0.8 decay schedule, update\\nclipping threshold of 1.0, and factored second-moment esti-\\nmation. We keep the initial learning rate of 0.01 for the ﬁrst\\n10K training steps, and then decay it with inverse square\\nroot schedule lr⟨t⟩∝\\n1\\n√t. On top of the standard cross-\\nentropy loss, we add the MoE auxiliary loss as described\\nin GShard (Lepikhin et al., 2021) with a 0.01 coefﬁcient to\\nencourage expert load balancing so that the gating function\\nwill distribute tokens more evenly across all experts. We use\\nthe SentencePiece (Kudo & Richardson, 2018) subword to-\\nkenizer with a vocabulary of size of 256K. During training,\\nwe use ﬂoat32 for model weights and bﬂoat16 for activa-\\ntions. The largest GLaM 64B/64E model was trained on\\n1,024 Cloud TPU-V4 chips.\\nTraining models at the trillion parameter scale is extremely\\nexpensive even for sparsely activated models. There is\\nlittle room for hyperparameter tuning. Here we share our\\ntraining recipes and some implementation tricks for the\\nGLaM models.\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 4. Sizes and architectures of both MoE and dense models that we have trained in our experiments. Models are grouped by the\\nnumber of activated parameters per token. All trained models share the same learning hyperparameters described in Session 5.1.\\nGLaM Model\\nType\\nnparams\\nnact-params\\nL\\nM\\nH\\nnheads\\ndhead\\nE\\n0.1B\\nDense\\n130M\\n130M\\n12\\n768\\n3,072\\n12\\n64\\n–\\n0.1B/64E\\nMoE\\n1.9B\\n145M\\n64\\n1.7B\\nDense\\n1.7B\\n1.700B\\n24\\n2,048\\n8,192\\n16\\n128\\n–\\n1.7B/32E\\nMoE\\n20B\\n1.878B\\n32\\n1.7B/64E\\nMoE\\n27B\\n1.879B\\n64\\n1.7B/128E\\nMoE\\n53B\\n1.881B\\n128\\n1.7B/256E\\nMoE\\n105B\\n1.886B\\n256\\n8B\\nDense\\n8.7B\\n8.7B\\n32\\n4,096\\n16,384\\n32\\n128\\n–\\n8B/64E\\nMoE\\n143B\\n9.8B\\n64\\n137B\\nDense\\n137B\\n137B\\n64\\n8,192\\n65,536\\n128\\n128\\n–\\n64B/64E\\nMoE\\n1.2T\\n96.6B\\n64\\n8,192\\n32,768\\n128\\n128\\n64\\n• We train smaller-scale models to convergence ﬁrst.\\nThis allows us to expose potential issues in the dataset\\nand infrastructure as early as possible.\\n• We skip weight updates for a batch if there are any\\nNaNs or Infs in the gradients (Shen et al., 2019). Note\\nNaN/Inf could still occur during the applying gradient\\nstep, in which case we restart from an earlier check-\\npoint as described below. For example, even if there\\nis no Inf in the existing variable or the gradient, the\\nupdated variable could still lead to Inf.\\n• We restart from an early healthy checkpoint when en-\\ncountering rare large ﬂuctuations or even NaN/Inf dur-\\ning training. Randomness of the sequentially loaded\\nbatches might help escape from previous failed states\\nin the training after restart.\\n5.3. Evaluation Setting\\nProtocol.\\nTo clearly demonstrate the effectiveness of\\nGLaM models, we mainly focus on evaluating the zero,\\none and few-shot learning protocols suggested by Radford\\net al. (2018); Brown et al. (2020). For the zero-shot learn-\\ning setting, in most cases, we evaluate each example in the\\ndevelopment set directly. For one/few-shot learning, we\\nmainly draw random one/few examples from that task’s\\ntraining set as the only demonstration and context. Such a\\ndemonstration is concatenated with the evaluation example\\nwith two newlines in between, and then fed into the model.\\nBenchmarks.\\nTo allow for an apples-to-apples compari-\\nson between GPT-3 and GLaM, we choose the same suite\\nof evaluation tasks as Brown et al. (2020). But for sim-\\nplicity, we exclude 7 synthetic tasks (arithmetic and word\\nunscramble) and 6 machine translation datasets. With this\\nexclusion, we end up with 29 datasets, which includes 8\\nnatural language generative (NLG) tasks and 21 natural lan-\\nguage understanding (NLU) tasks. These datasets can be\\nfurther grouped into 7 categories and are listed in section A.\\nNatural Language Generative tasks.\\nWe compare the\\nlanguage sequences decoded by the models to the ground\\ntruth in generative tasks. These tasks are TriviaQA, NQS,\\nWebQS, SQuADv2, LAMBADA, DROP, QuAC and CoQA.\\nThe performance is measured by the accuracy of exact match\\n(EM) and F1 score, following the standard for each task\\nin Brown et al. (2020). We use beam search with a width of\\n4 to generate the sequences.\\nNatural Language Understanding tasks.\\nMost lan-\\nguage understanding tasks require the model to select one\\ncorrect answer from multiple options. All binary classiﬁca-\\ntion tasks are formulated into the form of selecting among\\ntwo options (‘Yes’ or ‘No’). The prediction is based on the\\nmaximum log-likelihood of each option given the context\\nlog P(option|context) normalized by the token length of\\neach option. On a few tasks, such as ReCoRD (Zhang et al.,\\n2018) and COPA (Gordon et al., 2012), the non-normalized\\nloss can yield better results and thus is adopted. Except for\\nMultiRC (Khashabi et al., 2018) where the F1 metric over\\nthe set of answer options (referred to as F1a) is reported,\\nthe prediction accuracy metric is used for all the other tasks.\\nWe use the average of the scores reported in all datasets to\\nreport the overall few-shot performance of models on both\\nNLG and NLU tasks. Both Accuracy (EM) and F1 scores\\nhave been normalized to lie between 0 and 100. On Trivi-\\naQA, we also report the testing server score of our one-shot\\nsubmission.\\n6. Results\\nWe conduct extensive evaluation on the whole family of\\nGLaM models, to show the advantages of sparsely activated\\nmodels in language modeling and their scaling trends. We\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nalso quantitatively inspect the effectiveness of data quality\\nfor language model training.\\n6.1. Comparison between MoE and Dense Models\\nAs previously presented in Table 1, GLaM (64B/64E) has\\ncompetitive performance compared to GPT-3 (175B) for\\nzero, one and few-shot learning. Figure 1 compares the\\nperformance for each category of tasks. In total, GLaM\\n(64B/64E) outperforms GPT-3 in 6 out of 7 categories on\\naverage, indicating the performance gain is consistent. For\\nmore details on each individual task, see Table 11. We\\ninclude results on the much larger and computationally de-\\nmanding Megatron-NLG and Gopher for reference. More\\nimportantly, as shown in Table 4, GLaM (64B/64E) acti-\\nvates roughly 96.6B parameters per token during inference,\\nwhich requires only half of the compute FLOPs needed by\\nGPT-3 given the same input.\\nWe highlight one particular challenging open-domain ques-\\ntion answer task: TriviaQA. In open-domain question an-\\nswer tasks, the model is required to directly answer a given\\nquery without access to any additional context. Brown\\net al. (2020) show that the few-shot performance of Trivi-\\naQA is able to grow smoothly with model size, indicating\\na language model is able to absorb knowledge using its\\nmodel capacity. As shown in Table 5, GLaM (64B/64E) is\\nbetter than the dense model and outperforms the previous\\nﬁnetuned state-of-the-art (SOTA) on this dataset in the open-\\ndomain setting. Our one-shot result exceeds the previous\\nﬁnetuned SOTA (Yu et al., 2022) where additional knowl-\\nedge graph information is infused by 8.6%, and outperforms\\nthe few-shot GPT-3 on the testing server by 5.3%. This sug-\\ngests that the additional capacity of GLaM plays a crucial\\nrole in the performance gain even though the nact-params of\\nGLaM (64B/64E) is only half of that in GPT-3. Comparing\\nto Switch-C, even though both models have similar total\\nnumber of parameters, GLaM (64B/64E) uses much larger\\nexperts (beyond one TPU core) than Switch-C. Therefore,\\nGLaM’s one-shot performance on TriviaQA is also better\\nthan the ﬁne-tuned results of Switch-C in the open-domain\\nsetting. Finally, we report zero, one and few-shot evaluation\\nmainly on the development set for all tasks in Tables 11, 12,\\n13 and 14 of the appendix.\\n6.2. Effect of Data Quality\\nWe study the impact of data quality on the few-shot perfor-\\nmance of downstream tasks. We use a modest-size GLaM\\nmodel (1.7B/64E) to show the effectiveness of ﬁltering text\\non model quality. We train models with the same hyper-\\nparameters on two datasets. One is the original dataset\\ndescribed in Section 3 and the second consists of the dataset\\nwith the ﬁltered webpages replaced with the unﬁltered web-\\npages. The mixing proportions are ﬁxed as given in Table 3.\\nTable 5. GLaM (64B/64E) one-shot performance signiﬁcantly out-\\nperforms prior SOTAs for open domain settings in the wiki split.\\nModel\\nTriviaQA\\n(Open-Domain)\\nKG-FiD (large) (Yu et al., 2022)\\n(ﬁnetuned, test)\\n69.8\\nSwitch-C (ﬁnetuned, dev)\\n47.5\\nGPT-3 One-shot (dev)\\n68.0\\nGPT-3 64-shot (test)\\n71.2\\nGLaM One-shot (test)\\n75.0\\nGLaM One-shot (dev)\\n75.8\\nThe ﬁltered webpages consist of 143B tokens whereas the\\nunﬁltered webpages consist of around 7T tokens.\\nFigure 3 (c) and (d) show that the model trained on ﬁl-\\ntered data performs consistently better on both NLG and\\nNLU tasks. In particular, the effect of ﬁltering is bigger\\non NLG than that on NLU. Perhaps this is because NLG\\noften requires generating high-quality language and ﬁltered\\npretraining corpora is crucial to the generation capability\\nof language models. Our study highlights the fact that the\\nquality of the pretrained data also plays a critical role, specif-\\nically, in the performance of downstream tasks.\\n6.3. Scaling Studies\\nScaling up dense language models generally involves mak-\\ning the models deeper by adding more layers, and wider by\\nincreasing the embedding dimension of token representa-\\ntions. This process increases the total number of parameters\\nnparams of the model. For each prediction on a given input\\nexample, these models are ‘dense’ in that all nparams param-\\neters will be activated, i.e., nparams = nact-params in Table 4.\\nTherefore, the effective FLOPs per prediction increases\\nlinearly with the model size nparams. While the increased\\nFLOPs may lead to boosted predictive performance, it also\\nraises the overall cost per prediction.\\nIn contrast, GLaM MoE models are sparsely activated in\\nthat only a small fraction of the total nparams parameters will\\nbe activated for each prediction where nparams ≫nact-params.\\nTherefore, GLaM MoE models can scale by also growing\\nthe size or number of experts in the MoE layer.\\nAs shown in Figure 3(a), the average zero, one and few-shot\\nperformance across the generative tasks scales well with the\\neffective FLOPs per prediction which is in turn determined\\nby nact-params. We also ﬁnd that GLaM MoE models perform\\nconsistently better than GLaM dense models for similar ef-\\nfective FLOPs per token. For language understanding tasks\\nshown in Figure 3(b), the performance gain of GLaM MoE\\nmodels has a similar scaling trend to that of the generative\\ntasks. We observe that both MoE and dense models perform\\nsimilarly at smaller scales but MoE models outperform at\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nGPT3 (zero-shot)\\nGPT3 (one-shot)\\nGPT3 (few-shot)\\n0.1B/64E\\n0.1B\\n1.7B/64E\\n1.7B\\n8B/64E\\n8B\\n64B/64E\\n137B\\n20\\n40\\n60\\n0.1\\n1\\n10\\n100\\n1 000\\nGFlops per token prediction\\nScore\\n Dense (Few-shot)\\n Dense (One-shot)\\n Dense (Zero-shot)\\n MoE (Few-shot)\\n MoE (One-shot)\\n MoE (Zero-shot)\\n(a) Scaling (NLG)\\nGPT3 (zero-shot)\\nGPT3 (one-shot)\\nGPT3 (few-shot)\\n0.1B/64E\\n0.1B\\n1.7B/64E\\n1.7B\\n8B/64E\\n8B\\n64B/64E\\n137B\\n50\\n60\\n70\\n0.1\\n1\\n10\\n100\\n1 000\\nGFlops per token prediction\\nScore\\n Dense (Few-shot)\\n Dense (One-shot)\\n Dense (Zero-shot)\\n MoE (Few-shot)\\n MoE (One-shot)\\n MoE (Zero-shot)\\n(b) Scaling (NLU)\\nfiltered\\n(few-shot)\\nfiltered\\n(one-shot)\\nfiltered\\n(zero-shot)\\nunfiltered\\n(few-shot)\\nunfiltered\\n(one-shot)\\nunfiltered\\n(zero-shot)\\n25\\n30\\n35\\n40\\n45\\n200\\n400\\n600\\n800\\nTraining Tokens x109\\nScore\\n(c) Data ﬁltering (NLG)\\nfiltered\\n(few-shot)\\nfiltered\\n(one-shot)\\nfiltered\\n(zero-shot)\\nunfiltered\\n(few-shot)\\nunfiltered\\n(one-shot)\\nunfiltered\\n(zero-shot)\\n54\\n55\\n56\\n57\\n58\\n59\\n200\\n400\\n600\\n800\\nTraining Tokens x109\\nScore\\n(d) Data ﬁltering (NLU)\\nFigure 3. Average zero, one and few-shot performance of GLaM MoE models versus GLaM dense models for similar effective FLOPs per\\ntoken over the 8 NLG tasks (a) and 21 NLU tasks (b). Comparison of model performance with ﬁltered and unﬁltered training data using\\nGLaM (1.7B/64E). Filtered data improves results signiﬁcantly over unﬁltered data for both (c) NLG and (d) NLU tasks across zero, one\\nand few-shot settings.\\nlarger scales. We also show experiments with scaling the\\nnumber of experts in Section B where we observe that, for\\na ﬁxed budget of computation per prediction, adding more\\nexperts generally leads to better predictive performance.\\n6.4. Efﬁciency of GLaM\\nExisting large dense language models usually require\\ntremendous amounts of computation resources for train-\\ning and serving (Patterson et al., 2021). They also need to\\nconsume massive amounts of pretraining data. We investi-\\ngate the data and compute efﬁciency of the proposed GLaM\\nmodels.\\nData Efﬁciency. Figure 4 (a-c) and Figure 4(e-g) show\\nthe learning curves of our models compared to the dense\\nbaselines of similar effective FLOPs in both NLG and NLU\\ntasks. The x-axis is the number of tokens used in train-\\ning where we explicitly include GPT-3’s results when it\\nis around 300B tokens. We ﬁrst observe that GLaM MoE\\nmodels require signiﬁcantly less data than dense models of\\ncomparable FLOPs to achieve similar zero, one, and few-\\nshot performance. In other words, when the same amount\\nof data is used for training, MoE models perform much bet-\\nter, and the difference in performance becomes larger when\\ntraining up to 630B. Moreover, GLaM (64B/64E) model\\ntrained with 280B tokens outperforms GPT-3 trained with\\n300B tokens by large margins on 4 out of the 6 learning set-\\ntings (zero-shot/one-shot NLU and one-shot/few-shot NLG),\\nand matches GPT-3 scores for the remaining setting, i.e.,\\nzero-shot NLG tasks.\\nComputation Efﬁciency & Energy Consumption. Fig-\\nure 4 (d) and Figure 4 (h) show how the average zero, one\\nand few-shot performance scales with the number of TPU\\nyears spent training MoE and dense models. We ﬁnd that to\\nachieve similar performance on downstream tasks, training\\nsparsely activated models takes much less computational\\nresources than training dense models.\\nAs previously presented in Table 1, the GLaM (64B/64E)\\ntraining after 600B tokens consumes 456 MWh, about 1/3\\nof the energy cost of 1287 MWh used by GPT-3. Moreover,\\nto reach similar (and slightly exceeded) scores as GPT-3, we\\ntrain using 1,024 TPU-v4 chips for 574 hours (with 280B\\ntokens). This consumes 213 MWh or 1/6 of the GPT-3\\nenergy cost. The reduced energy consumption of GLaM\\nis due to the MoE architecture and computation efﬁciency\\noptimizations from TPU-v4 hardware and GSPMD software.\\nEnergy calculations can be found in Section F.\\n7. Ethics and Unintended Biases\\nLarge language models’ zero-and few-shot inference is an\\nexciting capability: being able to control model behaviour\\nintuitively with natural language and small datasets signiﬁ-\\ncantly lowers the barrier to prototyping and the development\\nof new applications; it has the potential to help democratise\\nusing AI by dramatically decreasing the need for special-\\nist knowledge. However, such opportunities also serve to\\nhighlight the importance of the many ethical challenges\\n(Leidner & Plachouras, 2017; Bender et al., 2021; Bom-\\nmasani et al., 2021) including representation bias (Blodgett\\net al., 2020), proper selection and handling of training data\\n(Rogers, 2021) and its documentation (Bender & Friedman,\\n2018), privacy (Abadi et al., 2016b; Carlini et al., 2020),\\nand environmental concerns (Strubell et al., 2019; Patterson\\net al., 2021). An important strand of this research focuses\\non unintended biases learnt by language models, includ-\\ning correlations between gender and profession (Bolukbasi\\net al., 2016; Rudinger et al., 2018; Zhao et al., 2018), neg-\\native sentiment about racial and religious groups (Li et al.,\\n2020; Nadeem et al., 2021), and about people with disabili-\\nties (Hutchinson et al., 2020), as well as other social biases\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n10\\n20\\n30\\n40\\n50\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(a) Zero-shot (NLG)\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n20\\n40\\n60\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(b) One-shot (NLG)\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n20\\n40\\n60\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(c) Few-shot (NLG)\\n64B/64E (few-shot)\\n64B/64E (one-shot)\\n64B/64E (zero-shot)\\n137B (few-shot)\\n137B (one-shot)\\n137B (zero-shot)\\n30\\n40\\n50\\n60\\n10\\n30\\n100\\n300\\nTpu x Years\\nScore\\n(d) Scaling in TPU years (NLG)\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n45\\n50\\n55\\n60\\n65\\n70\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(e) Zero-shot (NLU)\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n50\\n60\\n70\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(f) One-shot (NLU)\\n64B/64E\\n137B\\n1.7B/64E\\n1.7B\\n0.1B/64E\\n0.1B\\nGPT3\\n45\\n50\\n55\\n60\\n65\\n70\\n100\\n158.5 251.2 398.1\\n631\\nTraining Tokens x109\\nScore\\n(g) Few-shot (NLU)\\n64B/64E (few-shot)\\n64B/64E (one-shot)\\n64B/64E (zero-shot)\\n137B (few-shot)\\n137B (one-shot)\\n137B (zero-shot)\\n60\\n63\\n66\\n69\\n30\\n100\\n300\\nTpu x Years\\nScore\\n(h) Scaling in TPU years (NLU)\\nFigure 4. Learning efﬁciency comparison. Average zero-shot , one-shot and few-shot performance of GLaM MoE models versus GLaM\\ndense models as more tokens are processed during training for 9 NLG tasks (a-c) and 21 NLU tasks (e-g). Panel (d) and (h) also display\\nthe learning curves against the number of TPU years, respectively.\\n(Caliskan et al., 2017; Rudinger et al., 2017; Sap et al., 2020;\\nSotnikova et al., 2021). While measuring and mitigating\\nthe potential harm of language models is a very active area\\nof research, as recognized by Blodgett et al. (2021); Jacobs\\n& Wallach (2021) there is still a signiﬁcant need for more\\nrigorous evaluation methods to assess the degree to which\\nlanguage models encode harmful stereotypes (May et al.,\\n2019; Webster et al., 2021).\\nWhile there is not yet consensus on measurement methods or\\ncriteria for such general purpose large language models, the\\nversatility and power of these models make it important to\\nassess them on a range of metrics. We take inspiration from\\nGPT-3 (Brown et al., 2020) and examine the co-occurrence\\nin generated text referencing identity terms as well as report\\non the WinoGender benchmark (Rudinger et al., 2018). We\\nalso analyse toxicity degeneration similarly to Gopher (Rae\\net al., 2021), and extend the analysis to consider the human-\\nbehavioral baseline.\\n7.1. Co-occurrence prompts\\nFollowing the procedure described in Brown et al. (2020),\\nwe analyze commonly co-occurring words in the continua-\\ntions when given prompts like “{term} was very...” where\\nthe substituted term references either gender, religions,\\nracial and ethnic identity. For each prompt (Table 7 of\\nthe appendix), 800 outputs are generated using top-k sam-\\npling (k = 40) with a temperature of 1. An off-the-shelf\\nPOS tagger (Bird & Loper, 2004) is used to remove stop\\nwords and select only descriptive words (i.e., adjectives and\\nadverbs). Adverbs are included because we noticed a com-\\nmon pattern of errors where adjectives are misclassiﬁed as\\nadverbs; for example “pretty” in the phrase “She was very\\npretty and very accomplished”. Like Brown et al. (2020), to\\nmake the analysis transparent and easily reproducible, we\\nomit any manual human labeling.\\nLike the analysis of other large language models that we\\nbuild on, we note associative biases for all dimensions are\\nobvious, for example “pretty” is the most associated descrip-\\ntion for the term “She”, while it is not in the top-10 for the\\nterm “He”. Table 8 shows the most frequently occurring\\ndescriptive words in response to prompt-templates for gen-\\ndered pronouns, and Tables 9 and 10 of the appendix show\\nthe same for race and religion prompts.\\n7.2. WinoGender\\nCoreference resolution is a capability that many applica-\\ntions require to perform well, including machine translation\\n(Stanovsky et al., 2019; Webster & Pitler, 2020) and ques-\\ntion answering (Lamm et al., 2020). To assess whether\\ngendered correlations in GLaM cause it to make corefer-\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\n1/27/22, 6:51 PM\\nvisualization (25).svg\\nfile:///Users/kevinrobinson/Downloads/visualization (25).svg\\n1/1\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nPrompt toxicity probability (binned)\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nContinuation toxicity probability\\nDense\\nMoE\\nHuman\\nModel type\\n<1B ····\\n1-10B - - -\\n>100B —\\nModel size\\nFigure 5. The relationship between the Toxicity Probability of the\\nPrompt (TPP), and the Toxicity Probability of the Continuation\\n(TPC). Human refers to the continuation of the original human-\\nwritten sentence.\\nence errors in the one-shot setting, we measure WinoGender\\n(Rudinger et al., 2018). GLaM (64B/64E) achieves a new\\nstate-of-the-art of 71.7% on the full dataset (compared to\\n64.2% for GPT-3 (Brown et al., 2020)). Promisingly, ac-\\ncuracy is remarkably close between ‘he’ examples (70.8%)\\nand ‘she’ examples (72.5%), as well as between stereotyp-\\nical examples (where the intended distribution is assumed\\nto be close to the US occupation statistics, (Rudinger et al.,\\n2018)) and anti-stereotypical (or ‘gotcha’) examples (both\\n71.7%).\\n7.3. Toxicity Degeneration\\nToxicity degeneration is when a language model produces\\ntext that is unintentionally toxic. To evaluate toxicity de-\\ngeneration, we adapt the methodology used in (Welbl et al.,\\n2021; Rae et al., 2021). We use the RealToxicityPrompts\\ndataset (Gehman et al., 2020) which consists of sentences\\nthat have been split into two parts: a prompt preﬁx, and a\\ncontinuation postﬁx. Like the previous studies, we also use\\nthe Perspective API which assigns a probability that the text\\nwould be considered to be rude, disrespectful or otherwise\\nlikely to make people want to leave a conversation. We then\\nasses how likely a continuation is to be toxic given various\\nlikelihoods that the prompt was toxic.\\nFor each of 10K randomly sampled prompts, we generate\\n25 continuations, with up to 100 tokens per continuations\\nusing top-k sampling (k = 40) with a temperature of 1. The\\nPerspective API requires an non-empty string therefore we\\nassign a score of toxicity 0.0 when the continuation is the\\nempty string; this could represent, for example, a chat bot\\nsimply refusing to respond.\\nFigure 5 shows the relationship between the Toxicity Proba-\\nbility of the Prompt (TPP), and the Toxicity Probability of\\nthe Continuation (TPC). Note that, for low TPP, the rela-\\ntively high human TPC is due to the sampling strategy used\\nto create the underlying dataset: sentences were selected\\nacross the toxicity spectrum. Moreover, toxicity can often\\nbe identiﬁed locally within a sentence, and toxicity in this\\ndataset tends to occur later the sentences. This causes the\\nhuman-TPC to slightly drop as the TPP increases. In con-\\ntrast, it is noteworthy that the model’s TPC closely follows\\nTPP, reﬂecting the frequent observation that large language\\nmodels are sometimes overly-strongly inﬂuenced by their\\nprompt, e.g. repeating phrases from the prompt.\\nWe also analysed the distribution of toxicity probabilities\\nfrom the API for batches of 25 continuations. This high-\\nlighted that, even for low toxicity prompts, it is very likely\\nthat some generated continuation will be judged as toxic by\\nmost people reviewing it, according to the Perspective API’s\\npredicted probability; further details can be found in Figure\\n8. We also note that this dataset’s sampling strategy, and the\\nsource it is taken from (Reddit) are likely not reﬂective of\\nother domains. Moreover, even for very low TPP, applica-\\ntions are likely to want a much lower TPC: even generating\\n1 in 100 toxic suggestions is likely to be very problematic\\nfor applications.\\n8. Discussion\\nAs observed in previous work on sparsely-activated mod-\\nels (Fedus et al., 2021), MoE models are more performant in\\nknowledge-oriented tasks. Open-domain tasks are one way\\nof measuring the amount of knowledge stored in a model.\\nThe performance of the MoE model in open-domain QA\\nbenchmarks such as TriviaQA demonstrate the signiﬁcantly\\nincreased information capacity of these models compared\\nto dense models of similar effective FLOPs. Despite the\\nin-context learning and training efﬁciency advantages, the\\nsparsely activated models consist of a higher number of pa-\\nrameters and thus require a larger number of devices. This\\nlimits the resource accessibility and increases the serving\\ncost especially when the serving trafﬁc is low.\\n9. Conclusions\\nWe propose and develop a family of generalist language\\nmodels called GLaM, which use a sparsely activated\\nmixture-of-experts architecture to achieve better average\\nscores than not only their dense counterparts of similar effec-\\ntive FLOPs, but also the GPT-3 models on 29 representative\\nNLP tasks in zero, one and few-shot learning. In partic-\\nular, GLaM (64B/64E), our largest 1.2 trillion parameter\\nMoE language model, achieves better average performance\\nwith only one third of energy consumption compared to\\ntraining GPT-3. We hope that our work will encourage\\nmore research into methods for obtaining high-quality data,\\nand using MoE for more efﬁcient scaling of giant language\\nmodels.\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nReferences\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A.,\\nDean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,\\nKudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,\\nD. G., Steiner, B., Tucker, P., Vasudevan, V., Warden,\\nP., Wicke, M., Yu, Y., and Zheng, X.\\nTensorFlow:\\nA system for Large-Scale machine learning. In 12th\\nUSENIX Symposium on Operating Systems Design and\\nImplementation (OSDI 16), pp. 265–283, Savannah, GA,\\nNovember 2016a. USENIX Association. ISBN 978-1-\\n931971-33-1.\\nURL https://www.usenix.org/\\nconference/osdi16/technical-sessions/\\npresentation/abadi.\\nAbadi, M., Chu, A., Goodfellow, I., McMahan, H. B.,\\nMironov, I., Talwar, K., and Zhang, L. Deep learning\\nwith differential privacy. Proceedings of the 2016 ACM\\nSIGSAC Conference on Computer and Communications\\nSecurity, Oct 2016b. doi: 10.1145/2976749.2978318.\\nURL http://dx.doi.org/10.1145/2976749.\\n2978318.\\nAdiwardana, D., Luong, M., So, D. R., Hall, J., Fiedel,\\nN., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,\\nG., Lu, Y., and Le, Q. V. Towards a human-like open-\\ndomain chatbot. CoRR, abs/2001.09977, 2020. URL\\nhttps://arxiv.org/abs/2001.09977.\\nBender, E. M. and Friedman, B. Data statements for natural\\nlanguage processing: Toward mitigating system bias and\\nenabling better science. Transactions of the Association\\nfor Computational Linguistics, 6:587–604, 2018. doi: 10.\\n1162/tacl a 00041. URL https://aclanthology.\\norg/Q18-1041.\\nBender, E. M., Gebru, T., McMillan-Major, A., and\\nShmitchell, S.\\nOn the dangers of stochastic parrots:\\nCan language models be too big?\\nIn Proceedings\\nof the 2021 ACM Conference on Fairness, Account-\\nability, and Transparency, FAccT ’21, pp. 610–623,\\nNew York, NY, USA, 2021. Association for Comput-\\ning Machinery. ISBN 9781450383097. doi: 10.1145/\\n3442188.3445922.\\nURL https://doi.org/10.\\n1145/3442188.3445922.\\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\\nparsing on Freebase from question-answer pairs. In Pro-\\nceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pp. 1533–1544, Seattle,\\nWashington, USA, October 2013. Association for Compu-\\ntational Linguistics. URL https://aclanthology.\\norg/D13-1160.\\nBird, S. and Loper, E. NLTK: The natural language toolkit.\\nIn Proceedings of the ACL Interactive Poster and Demon-\\nstration Sessions, pp. 214–217, Barcelona, Spain, July\\n2004. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/P04-3031.\\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\\nPiqa: Reasoning about physical commonsense in natural\\nlanguage. In Thirty-Fourth AAAI Conference on Artiﬁcial\\nIntelligence, 2020.\\nBlodgett, S. L., Barocas, S., Daum´\\ne III, H., and Wallach,\\nH. Language (technology) is power: A critical survey\\nof “bias” in NLP. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics,\\npp. 5454–5476, Online, July 2020. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/2020.acl-main.\\n485. URL https://aclanthology.org/2020.\\nacl-main.485.\\nBlodgett, S. L., Lopez, G., Olteanu, A., Sim, R., and\\nWallach, H. Stereotyping Norwegian salmon: An in-\\nventory of pitfalls in fairness benchmark datasets. In\\nProceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Process-\\ning (Volume 1: Long Papers), pp. 1004–1015, Online,\\nAugust 2021. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2021.acl-long.81. URL https:\\n//aclanthology.org/2021.acl-long.81.\\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V.,\\nand Kalai, A. T.\\nMan is to computer programmer\\nas woman is to homemaker?\\ndebiasing word em-\\nbeddings.\\nIn Lee, D., Sugiyama, M., Luxburg, U.,\\nGuyon, I., and Garnett, R. (eds.), Advances in Neural\\nInformation Processing Systems, volume 29. Curran As-\\nsociates, Inc., 2016. URL https://proceedings.\\nneurips.cc/paper/2016/file/\\na486cd07e4ac3d270571622f4f316ec5-Paper.\\npdf.\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\\nlut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\\nD., Castellon, R., Chatterji, N. S., Chen, A. S., Creel,\\nK., Davis, J. Q., Demszky, D., Donahue, C., Doum-\\nbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Etha-\\nyarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L.,\\nGoel, K., Goodman, N. D., Grossman, S., Guha, N.,\\nHashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong,\\nJ., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D.,\\nKalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat-\\ntab, O., Koh, P. W., Krass, M. S., Krishna, R., Kudi-\\ntipudi, R., and et al. On the opportunities and risks of\\nfoundation models. CoRR, abs/2108.07258, 2021. URL\\nhttps://arxiv.org/abs/2108.07258.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,\\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\\nMcCandlish, S., Radford, A., Sutskever, I., and Amodei,\\nD. Language models are few-shot learners. In Larochelle,\\nH., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,\\nH. (eds.), Advances in Neural Information Processing\\nSystems, volume 33, pp. 1877–1901. Curran Asso-\\nciates, Inc., 2020.\\nURL https://proceedings.\\nneurips.cc/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\\npdf.\\nCaliskan, A., Bryson, J. J., and Narayanan, A. Seman-\\ntics derived automatically from language corpora contain\\nhuman-like biases. Science, 356(6334):183–186, Apr\\n2017. ISSN 1095-9203. doi: 10.1126/science.aal4230.\\nURL http://dx.doi.org/10.1126/science.\\naal4230.\\nCarlini, N., Tram`\\ner, F., Wallace, E., Jagielski, M., Herbert-\\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D., Er-\\nlingsson, ´\\nU., Oprea, A., and Raffel, C. Extracting training\\ndata from large language models. CoRR, abs/2012.07805,\\n2020.\\nChoi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi,\\nY., Liang, P., and Zettlemoyer, L. QuAC: Question an-\\nswering in context. In Proceedings of the 2018 Confer-\\nence on Empirical Methods in Natural Language Pro-\\ncessing, pp. 2174–2184, Brussels, Belgium, October-\\nNovember 2018. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/D18-1241. URL https:\\n//aclanthology.org/D18-1241.\\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\\nM., and Toutanova, K. BoolQ: Exploring the surpris-\\ning difﬁculty of natural yes/no questions. In Proceed-\\nings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, Volume 1 (Long\\nand Short Papers), pp. 2924–2936, Minneapolis, Min-\\nnesota, June 2019. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/N19-1300. URL https:\\n//aclanthology.org/N19-1300.\\nClark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. Elec-\\ntra: Pre-training text encoders as discriminators rather\\nthan generators. arXiv preprint arXiv:2003.10555, 2020.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv:1803.05457v1, 2018.\\nDagan, I., Glickman, O., and Magnini, B.\\nThe pascal\\nrecognising textual entailment challenge. In Qui˜\\nnonero-\\nCandela, J., Dagan, I., Magnini, B., and d’Alch´\\ne Buc,\\nF. (eds.), Machine Learning Challenges. Evaluating Pre-\\ndictive Uncertainty, Visual Object Classiﬁcation, and\\nRecognising Tectual Entailment, pp. 177–190, Berlin,\\nHeidelberg, 2006. Springer Berlin Heidelberg. ISBN\\n978-3-540-33428-6.\\nDai, A. M. and Le, Q. V.\\nSemi-supervised sequence\\nlearning.\\nIn Cortes, C., Lawrence, N., Lee, D.,\\nSugiyama, M., and Garnett, R. (eds.), Advances in Neural\\nInformation Processing Systems, volume 28. Curran As-\\nsociates, Inc., 2015. URL https://proceedings.\\nneurips.cc/paper/2015/file/\\n7137debd45ae4d0ab9aa953017286b20-Paper.\\npdf.\\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and\\nSalakhutdinov, R. Transformer-XL: Attentive language\\nmodels beyond a ﬁxed-length context.\\nIn Proceed-\\nings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, pp. 2978–2988, Florence,\\nItaly, July 2019. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/P19-1285. URL https:\\n//aclanthology.org/P19-1285.\\nDauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Lan-\\nguage modeling with gated convolutional networks. In\\nInternational conference on machine learning, pp. 933–\\n941. PMLR, 2017.\\nde Marneffe, M.-C., Simons, M., and Tonhauser, J. The\\ncommitmentbank: Investigating projection in naturally\\noccurring discourse. Proceedings of Sinn und Bedeutung,\\n23(2):107–124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.\\n601.\\nURL https://ojs.ub.uni-konstanz.\\nde/sub/index.php/sub/article/view/601.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), 2019.\\nDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S.,\\nand Gardner, M.\\nDROP: A reading comprehension\\nbenchmark requiring discrete reasoning over paragraphs.\\nIn Burstein, J., Doran, C., and Solorio, T. (eds.), Pro-\\nceedings of the 2019 Conference of the North Ameri-\\ncan Chapter of the Association for Computational Lin-\\nguistics: Human Language Technologies, NAACL-HLT\\n2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\\nume 1 (Long and Short Papers), pp. 2368–2378. As-\\nsociation for Computational Linguistics, 2019.\\ndoi:\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\n10.18653/v1/n19-1246.\\nURL https://doi.org/\\n10.18653/v1/n19-1246.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\\ners: Scaling to trillion parameter models with simple and\\nefﬁcient sparsity. CoRR, abs/2101.03961, 2021. URL\\nhttps://arxiv.org/abs/2101.03961.\\nFyodorov, Y., Winter, Y., and Francez, N. A natural logic in-\\nference system. In Inference in Computational Semantics,\\n2000.\\nGehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith,\\nN. A. Realtoxicityprompts: Evaluating neural toxic de-\\ngeneration in language models, 2020.\\nGordon, A., Kozareva, Z., and Roemmele, M. SemEval-\\n2012 task 7: Choice of plausible alternatives: An evalua-\\ntion of commonsense causal reasoning. In *SEM 2012:\\nThe First Joint Conference on Lexical and Computational\\nSemantics – Volume 1: Proceedings of the main confer-\\nence and the shared task, and Volume 2: Proceedings of\\nthe Sixth International Workshop on Semantic Evaluation\\n(SemEval 2012), pp. 394–398, Montr´\\neal, Canada, 7-8\\nJune 2012. Association for Computational Linguistics.\\nURL https://aclanthology.org/S12-1052.\\nHendrycks, D. and Gimpel, K. Bridging nonlinearities and\\nstochastic regularizers with gaussian error linear units.\\nCoRR, abs/1606.08415, 2016. URL http://arxiv.\\norg/abs/1606.08415.\\nHestness, J., Narang, S., Ardalani, N., Diamos, G. F., Jun,\\nH., Kianinejad, H., Patwary, M. M. A., Yang, Y., and\\nZhou, Y. Deep learning scaling is predictable, empirically.\\nCoRR, abs/1712.00409, 2017. URL http://arxiv.\\norg/abs/1712.00409.\\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\\nGelly, S. Parameter-efﬁcient transfer learning for NLP.\\nIn Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-\\nings of the 36th International Conference on Machine\\nLearning, volume 97 of Proceedings of Machine Learn-\\ning Research, pp. 2790–2799. PMLR, 09–15 Jun 2019.\\nURL https://proceedings.mlr.press/v97/\\nhoulsby19a.html.\\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,\\nM. X., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., and Chen, Z.\\nGpipe: Efﬁcient training of giant neural networks using\\npipeline parallelism. In Wallach, H. M., Larochelle, H.,\\nBeygelzimer, A., d’Alch´\\ne-Buc, F., Fox, E. B., and Garnett,\\nR. (eds.), Advances in Neural Information Processing\\nSystems 32: Annual Conference on Neural Information\\nProcessing Systems 2019, NeurIPS 2019, December 8-14,\\n2019, Vancouver, BC, Canada, pp. 103–112, 2019.\\nHutchinson, B., Prabhakaran, V., Denton, E., Webster, K.,\\nZhong, Y., and Denuyl, S. Social biases in NLP mod-\\nels as barriers for persons with disabilities.\\nIn Pro-\\nceedings of the 58th Annual Meeting of the Associa-\\ntion for Computational Linguistics, pp. 5491–5501, On-\\nline, July 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.acl-main.487. URL https:\\n//aclanthology.org/2020.acl-main.487.\\nJacobs, A. Z. and Wallach, H.\\nMeasurement and fair-\\nness. Proceedings of the 2021 ACM Conference on Fair-\\nness, Accountability, and Transparency, Mar 2021. doi:\\n10.1145/3442188.3445901.\\nURL http://dx.doi.\\norg/10.1145/3442188.3445901.\\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Trivi-\\naqa: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Proceedings of the 55th\\nAnnual Meeting of the Association for Computational\\nLinguistics, Vancouver, Canada, July 2017. Association\\nfor Computational Linguistics.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361, 2020.\\nKhashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,\\nand Roth, D.\\nLooking beyond the surface: A chal-\\nlenge set for reading comprehension over multiple sen-\\ntences. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pp. 252–262, New Orleans,\\nLouisiana, June 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/N18-1023. URL https:\\n//aclanthology.org/N18-1023.\\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R.,\\nUrtasun, R., Torralba, A., and Fidler, S. Skip-thought\\nvectors. In Cortes, C., Lawrence, N., Lee, D., Sugiyama,\\nM.,\\nand Garnett,\\nR. (eds.),\\nAdvances in Neural\\nInformation Processing Systems, volume 28. Curran As-\\nsociates, Inc., 2015. URL https://proceedings.\\nneurips.cc/paper/2015/file/\\nf442d33fa06832082290ad8544a8da27-Paper.\\npdf.\\nKudo, T. and Richardson, J. Sentencepiece: A simple and\\nlanguage independent subword tokenizer and detokenizer\\nfor neural text processing. In EMNLP, 2018.\\nKudugunta, S., Huang, Y., Bapna, A., Krikun, M., Lep-\\nikhin, D., Luong, M.-T., and Firat, O. Beyond distillation:\\nTask-level mixture-of-experts for efﬁcient inference. In\\nFindings of the Association for Computational Linguis-\\ntics: EMNLP 2021, pp. 3577–3599, 2021.\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nKwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M.,\\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\\ncey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones,\\nL., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and\\nPetrov, S. Natural questions: a benchmark for question\\nanswering research. Transactions of the Association of\\nComputational Linguistics, 2019.\\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. RACE:\\nLarge-scale ReAding comprehension dataset from ex-\\naminations. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing, pp.\\n785–794, Copenhagen, Denmark, September 2017. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/\\nD17-1082. URL https://aclanthology.org/\\nD17-1082.\\nLamm, M., Palomaki, J., Alberti, C., Andor, D., Choi, E.,\\nSoares, L. B., and Collins, M. QED: A framework and\\ndataset for explanations in question answering. CoRR,\\nabs/2009.06354, 2020. URL https://arxiv.org/\\nabs/2009.06354.\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents. In International conference on\\nmachine learning, 2014.\\nLeidner, J. L. and Plachouras, V.\\nEthical by design:\\nEthics best practices for natural language processing.\\nIn Proceedings of the First ACL Workshop on Ethics\\nin Natural Language Processing, pp. 30–40, Valencia,\\nSpain, April 2017. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/W17-1604. URL https:\\n//aclanthology.org/W17-1604.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang,\\nY., Krikun, M., Shazeer, N., and Chen, Z. GShard: Scal-\\ning giant models with conditional computation and auto-\\nmatic sharding. In International Conference on Learning\\nRepresentations, 2021. URL https://openreview.\\nnet/forum?id=qrwe7XHTmYb.\\nLevesque, H., Davis, E., and Morgenstern, L. The wino-\\ngrad schema challenge. In 13th International Confer-\\nence on the Principles of Knowledge Representation and\\nReasoning, KR 2012, Proceedings of the International\\nConference on Knowledge Representation and Reason-\\ning, pp. 552–561. Institute of Electrical and Electronics\\nEngineers Inc., 2012. ISBN 9781577355601. 13th In-\\nternational Conference on the Principles of Knowledge\\nRepresentation and Reasoning, KR 2012 ; Conference\\ndate: 10-06-2012 Through 14-06-2012.\\nLi, T., Khashabi, D., Khot, T., Sabharwal, A., and Srikumar,\\nV. UNQOVERing stereotyping biases via underspeci-\\nﬁed questions. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2020, pp. 3475–3489,\\nOnline, November 2020. Association for Computational\\nLinguistics.\\ndoi:\\n10.18653/v1/2020.ﬁndings-emnlp.\\n311. URL https://aclanthology.org/2020.\\nfindings-emnlp.311.\\nLieber, O., Sharir, O., Lenz, B., and Shoham, Y. Jurassic-1:\\nTechnical details and evaluation. White Paper. AI21 Labs,\\n2021.\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\\nRoberta: A robustly optimized bert pretraining approach.\\narXiv preprint arXiv:1907.11692, 2019.\\nMay, C., Wang, A., Bordia, S., Bowman, S. R., and\\nRudinger, R. On measuring social biases in sentence\\nencoders. In Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers), pp. 622–628, Min-\\nneapolis, Minnesota, June 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/N19-1063. URL\\nhttps://aclanthology.org/N19-1063.\\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\\nsuit of armor conduct electricity? a new dataset for open\\nbook question answering. In EMNLP, 2018.\\nMikolov, T., Karaﬁ´\\nat, M., Burget, L., Cernock´\\ny, J. H., and\\nKhudanpur, S. Recurrent neural network based language\\nmodel. In INTERSPEECH, 2010.\\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient\\nestimation of word representations in vector space. In\\nBengio, Y. and LeCun, Y. (eds.), 1st International Confer-\\nence on Learning Representations, ICLR 2013, Scottsdale,\\nArizona, USA, May 2-4, 2013, Workshop Track Proceed-\\nings, 2013. URL http://arxiv.org/abs/1301.\\n3781.\\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Ba-\\ntra, D., Vanderwende, L., Kohli, P., and Allen, J. A\\ncorpus and cloze evaluation for deeper understanding\\nof commonsense stories. In Proceedings of the 2016\\nConference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Lan-\\nguage Technologies, pp. 839–849, San Diego, Cali-\\nfornia, June 2016. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/N16-1098. URL https:\\n//aclanthology.org/N16-1098.\\nNadeem, M., Bethke, A., and Reddy, S. StereoSet: Mea-\\nsuring stereotypical bias in pretrained language models.\\nIn Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language Pro-\\ncessing (Volume 1: Long Papers), pp. 5356–5371, Online,\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nAugust 2021. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2021.acl-long.416. URL https:\\n//aclanthology.org/2021.acl-long.416.\\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q.,\\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\\nFern´\\nandez, R. The LAMBADA dataset: Word prediction\\nrequiring a broad discourse context. In Proceedings of\\nthe 54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pp. 1525–\\n1534, Berlin, Germany, August 2016. Association for\\nComputational Linguistics. doi: 10.18653/v1/P16-1144.\\nURL https://aclanthology.org/P16-1144.\\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-\\nM., Rothchild, D., So, D., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. arXiv\\npreprint arXiv:2104.10350, 2021.\\nPennington, J., Socher, R., and Manning, C.\\nGloVe:\\nGlobal vectors for word representation. In Proceedings\\nof the 2014 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP), pp. 1532–1543,\\nDoha, Qatar, October 2014. Association for Computa-\\ntional Linguistics. doi: 10.3115/v1/D14-1162. URL\\nhttps://aclanthology.org/D14-1162.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. arXiv preprint arXiv:1802.05365,\\n2018.\\nPilehvar, M. T. and Camacho-Collados, J. Wic: 10, 000\\nexample pairs for evaluating context-sensitive representa-\\ntions. ArXiv, abs/1808.09121, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei,\\nD.,\\nand\\nSutskever,\\nI.\\nLanguage\\nmodels\\nare\\nunsupervised multitask learners.\\n2018.\\nURL\\nhttps://d4mucfpksywv.cloudfront.\\nnet/better-language-models/\\nlanguage-models.pdf.\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\\nSong, H. F., Aslanides, J., Henderson, S., Ring, R., Young,\\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\\nPowell, R., van den Driessche, G., Hendricks, L. A.,\\nRauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S.,\\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M.,\\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\\nLazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli,\\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\\nPohlen, T., Gong, Z., Toyama, D., de Masson d’Autume,\\nC., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark,\\nA., de Las Casas, D., Guy, A., Jones, C., Bradbury, J.,\\nJohnson, M., Hechtman, B. A., Weidinger, L., Gabriel,\\nI., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L.,\\nDyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett,\\nL., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scal-\\ning language models: Methods, analysis & insights from\\ntraining gopher. CoRR, abs/2112.11446, 2021.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the\\nlimits of transfer learning with a uniﬁed text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\\nURL http://jmlr.org/papers/v21/20-074.\\nhtml.\\nRajpurkar, P., Jia, R., and Liang, P. Know what you don’t\\nknow: Unanswerable questions for squad. In ACL, 2018.\\nReddy, S., Chen, D., and Manning, C. D. CoQA: A con-\\nversational question answering challenge. Transactions\\nof the Association for Computational Linguistics, 7:249–\\n266, March 2019.\\ndoi: 10.1162/tacl a 00266.\\nURL\\nhttps://aclanthology.org/Q19-1016.\\nRogers, A. Changing the world by changing the data. In\\nProceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Process-\\ning (Volume 1: Long Papers), pp. 2182–2194, Online,\\nAugust 2021. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2021.acl-long.170. URL https:\\n//aclanthology.org/2021.acl-long.170.\\nRudinger, R., May, C., and Van Durme, B. Social bias in\\nelicited natural language inferences. In Proceedings of\\nthe First ACL Workshop on Ethics in Natural Language\\nProcessing, pp. 74–79, Valencia, Spain, April 2017. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/\\nW17-1609. URL https://aclanthology.org/\\nW17-1609.\\nRudinger, R., Naradowsky, J., Leonard, B., and Van Durme,\\nB. Gender bias in coreference resolution. In Proceedings\\nof the 2018 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 2 (Short Papers), pp. 8–\\n14, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/N18-2002.\\nURL https://aclanthology.org/N18-2002.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\\nWinogrande: An adversarial winograd schema challenge\\nat scale. In AAAI, pp. 8732–8740. AAAI Press, 2020.\\nSap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A.,\\nand Choi, Y.\\nSocial bias frames: Reasoning about\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nsocial and power implications of language.\\nIn Pro-\\nceedings of the 58th Annual Meeting of the Associa-\\ntion for Computational Linguistics, pp. 5477–5490, On-\\nline, July 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.acl-main.486. URL https:\\n//aclanthology.org/2020.acl-main.486.\\nShazeer, N. Glu variants improve transformer, 2020.\\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\\nrates with sublinear memory cost. ArXiv, abs/1804.04235,\\n2018.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\\nQ. V., Hinton, G. E., and Dean, J. Outrageously large\\nneural networks: The sparsely-gated mixture-of-experts\\nlayer. In 5th International Conference on Learning Rep-\\nresentations, ICLR 2017, Toulon, France, April 24-26,\\n2017, Conference Track Proceedings. OpenReview.net,\\n2017. URL https://openreview.net/forum?\\nid=B1ckMDqlg.\\nShazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A.,\\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\\nC., Sepassi, R., and Hechtman, B.\\nMesh-tensorﬂow:\\nDeep learning for supercomputers. In Proceedings of the\\n32nd International Conference on Neural Information\\nProcessing Systems, NIPS’18, pp. 10435–10444, Red\\nHook, NY, USA, 2018. Curran Associates Inc.\\nShen, J., Nguyen, P., Wu, Y., Chen, Z., Chen, M. X., Jia,\\nY., Kannan, A., Sainath, T. N., Cao, Y., Chiu, C., He, Y.,\\nChorowski, J., Hinsu, S., Laurenzo, S., Qin, J., Firat, O.,\\nMacherey, W., Gupta, S., Bapna, A., Zhang, S., Pang,\\nR., Weiss, R. J., Prabhavalkar, R., Liang, Q., Jacob, B.,\\nLiang, B., Lee, H., Chelba, C., Jean, S., Li, B., Johnson,\\nM., Anil, R., Tibrewal, R., Liu, X., Eriguchi, A., Jaitly,\\nN., Ari, N., Cherry, C., Haghani, P., Good, O., Cheng,\\nY., Alvarez, R., Caswell, I., Hsu, W., Yang, Z., Wang,\\nK., Gonina, E., Tomanek, K., Vanik, B., Wu, Z., Jones,\\nL., Schuster, M., Huang, Y., Chen, D., Irie, K., Foster,\\nG. F., Richardson, J., Macherey, K., Bruguier, A., Zen,\\nH., Raffel, C., Kumar, S., Rao, K., Rybach, D., Murray,\\nM., Peddinti, V., Krikun, M., Bacchiani, M., Jablin, T. B.,\\nSuderman, R., Williams, I., Lee, B., Bhatia, D., Carlson,\\nJ., Yavuz, S., Zhang, Y., McGraw, I., Galkin, M., Ge, Q.,\\nPundak, G., Whipkey, C., Wang, T., Alon, U., Lepikhin,\\nD., Tian, Y., Sabour, S., Chan, W., Toshniwal, S., Liao,\\nB., Nirschl, M., and Rondon, P. Lingvo: a modular and\\nscalable framework for sequence-to-sequence modeling.\\nCoRR, abs/1902.08295, 2019.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\\nand Catanzaro, B. Megatron-lm: Training multi-billion\\nparameter language models using gpu model parallelism.\\narXiv preprint arXiv:1909.08053, 2019.\\nSotnikova, A., Cao, Y. T., Daum´\\ne III, H., and Rudinger,\\nR.\\nAnalyzing stereotypes in generative text infer-\\nence tasks.\\nIn Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021, pp. 4052–\\n4065, Online, August 2021. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2021.ﬁndings-acl.\\n355. URL https://aclanthology.org/2021.\\nfindings-acl.355.\\nStanovsky, G., Smith, N. A., and Zettlemoyer, L. Eval-\\nuating gender bias in machine translation. In Proceed-\\nings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, pp. 1679–1684, Florence,\\nItaly, July 2019. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/P19-1164. URL https:\\n//aclanthology.org/P19-1164.\\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\\npolicy considerations for deep learning in NLP.\\nIn\\nProceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pp. 3645–3650,\\nFlorence, Italy, July 2019. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P19-1355. URL\\nhttps://aclanthology.org/P19-1355.\\nSutskever, I., Martens, J., and Hinton, G. Generating text\\nwith recurrent neural networks. In Proceedings of the 28th\\nInternational Conference on International Conference on\\nMachine Learning, ICML’11, pp. 1017–1024, Madison,\\nWI, USA, 2011. Omnipress. ISBN 9781450306195.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\\nJones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin,\\nI. Attention is all you need. In Guyon, I., Luxburg,\\nU. V., Bengio, S., Wallach, H., Fergus, R., Vish-\\nwanathan, S., and Garnett, R. (eds.), Advances in Neural\\nInformation Processing Systems, volume 30. Curran As-\\nsociates, Inc., 2017. URL https://proceedings.\\nneurips.cc/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\\npdf.\\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,\\nMichael, J., Hill, F., Levy, O., and Bowman, S.\\nSu-\\nperglue:\\nA stickier benchmark for general-purpose\\nlanguage understanding systems.\\nIn Wallach, H.,\\nLarochelle, H., Beygelzimer, A., d’Alch´\\ne Buc, F.,\\nFox, E., and Garnett, R. (eds.), Advances in Neural\\nInformation Processing Systems, volume 32. Curran As-\\nsociates, Inc., 2019. URL https://proceedings.\\nneurips.cc/paper/2019/file/\\n4496bf24afe7fab6f046bf4923da8de6-Paper.\\npdf.\\nWebster, K. and Pitler, E.\\nScalable cross lingual piv-\\nots to model pronoun gender for translation.\\nCoRR,\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nabs/2006.08881, 2020. URL https://arxiv.org/\\nabs/2006.08881.\\nWebster, K., Wang, X., Tenney, I., Beutel, A., Pitler, E.,\\nPavlick, E., Chen, J., Chi, E., and Petrov, S. Measuring\\nand reducing gendered correlations in pre-trained models,\\n2021.\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\\nmodels are zero-shot learners, 2021.\\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mel-\\nlor, J., Hendricks, L. A., Anderson, K., Kohli, P.,\\nCoppin, B., and Huang, P.-S.\\nChallenges in detox-\\nifying language models.\\nIn Findings of the As-\\nsociation for Computational Linguistics:\\nEMNLP\\n2021, pp. 2447–2469, Punta Cana, Dominican Repub-\\nlic, November 2021. Association for Computational\\nLinguistics.\\ndoi:\\n10.18653/v1/2021.ﬁndings-emnlp.\\n210. URL https://aclanthology.org/2021.\\nfindings-emnlp.210.\\nXu, Y., Lee, H., Chen, D., Hechtman, B. A., Huang, Y.,\\nJoshi, R., Krikun, M., Lepikhin, D., Ly, A., Maggioni, M.,\\nPang, R., Shazeer, N., Wang, S., Wang, T., Wu, Y., and\\nChen, Z. GSPMD: general and scalable parallelization for\\nML computation graphs. CoRR, abs/2105.04663, 2021.\\nURL https://arxiv.org/abs/2105.04663.\\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov,\\nR. R., and Le, Q. V. Xlnet: Generalized autoregressive\\npretraining for language understanding.\\nAdvances in\\nneural information processing systems, 32, 2019.\\nYu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y.,\\nRen, X., Yang, Y., and Zeng, M.\\nKG-FiD: Infusing\\nknowledge graph in fusion-in-decoder for open-domain\\nquestion answering. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 4961–4974, Dublin, Ire-\\nland, May 2022. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2022.acl-long.340. URL https:\\n//aclanthology.org/2022.acl-long.340.\\nYu, Y., Abadi, M., Barham, P., Brevdo, E., Burrows, M.,\\nDavis, A., Dean, J., Ghemawat, S., Harley, T., Hawkins,\\nP., Isard, M., Kudlur, M., Monga, R., Murray, D., and\\nZheng, X. Dynamic control ﬂow in large-scale machine\\nlearning. In Proceedings of the Thirteenth EuroSys Con-\\nference, EuroSys ’18, New York, NY, USA, 2018. Associ-\\nation for Computing Machinery. ISBN 9781450355841.\\ndoi: 10.1145/3190508.3190551. URL https://doi.\\norg/10.1145/3190508.3190551.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\\nY. HellaSwag: Can a machine really ﬁnish your sen-\\ntence? In Proceedings of the 57th Annual Meeting of\\nthe Association for Computational Linguistics, pp. 4791–\\n4800, Florence, Italy, July 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/P19-1472. URL\\nhttps://aclanthology.org/P19-1472.\\nZhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Durme,\\nB. V. Record: Bridging the gap between human and\\nmachine commonsense reading comprehension. CoRR,\\nabs/1810.12885, 2018.\\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang,\\nK.-W. Gender bias in coreference resolution: Evalua-\\ntion and debiasing methods. In Proceedings of the 2018\\nConference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language\\nTechnologies, Volume 2 (Short Papers), pp. 15–20, New\\nOrleans, Louisiana, June 2018. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/N18-2003. URL\\nhttps://aclanthology.org/N18-2003.\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nA. Benchmarks\\nOpen-Domain Question Answering: TriviaQA\\n(Joshi\\net al., 2017), Natural Questions (NQS) (Kwiatkowski\\net al., 2019), Web Questions (WebQS) (Berant et al.,\\n2013)\\nCloze and Completion Tasks: LAMBADA\\n(Paperno\\net al., 2016), HellaSwag (Zellers et al., 2019),\\nStoryCloze (Mostafazadeh et al., 2016)\\nWinograd-Style Tasks: Winograd (Levesque et al., 2012),\\nWinoGrande (Sakaguchi et al., 2020)\\nCommon Sense Reasoning: PIQA (Bisk et al., 2020),\\nARC (Easy) (Clark et al., 2018),\\nARC (Chal-\\nlenge) (Clark et al., 2018), OpenBookQA (Mihaylov\\net al., 2018)\\nIn-context Reading Comprehension: DROP (Dua et al.,\\n2019), CoQA (Reddy et al., 2019), QuAC (Choi et al.,\\n2018), SQuADv2 (Rajpurkar et al., 2018), RACE-\\nh (Lai et al., 2017), RACE-m (Lai et al., 2017)\\nSuperGLUE:\\n(Wang et al., 2019) BoolQ (Clark et al.,\\n2019), CB (de Marneffe et al., 2019), COPA (Gordon\\net al., 2012), RTE (Dagan et al., 2006), WiC (Pile-\\nhvar & Camacho-Collados, 2018), WSC (Levesque\\net al., 2012), MultiRC (Khashabi et al., 2018),\\nReCoRD (Zhang et al., 2018)\\nNatural Language Inference: ANLI\\nR1,\\nANLI\\nR2,\\nANLI R3 (Fyodorov et al., 2000)\\nB. Scaling the Number of Experts\\nWe also study the effects of increasing the number of experts\\nper MoE layer. More concretely, we start with a modest\\nsize model of 1.7B, which essentially is a GLaM (1.7B/1E)\\nmodel where each MoE layer reduces to include only a sin-\\ngle feed-forward network as the expert. We then increase\\nthe number of experts in each MoE layer from 1 to 256.\\nDespite the fact that the number of experts increases expo-\\nnentially, the nact-params in each model barely increases due\\nto the sparsity of GLaM. In fact, as shown in Table 4, they\\nall have almost identical FLOPs per prediction.\\nIn Figure 6, we observe that, for a ﬁxed budget of compu-\\ntation per prediction, adding more experts generally leads\\nto better predictive performance. This further veriﬁes the\\nperformance gain of GLaM sparsely activated models over\\nthe dense counterparts when both have similar FLOPs per\\nprediction, thanks to the increased capacity and ﬂexibility\\nfrom more experts.\\n35\\n40\\n45\\n1\\n4\\n16\\n64\\n256\\nexperts\\nscore\\n Few-shot\\n One-shot\\n Zero-shot\\n56\\n57\\n58\\n59\\n60\\n1\\n4\\n16\\n64\\n256\\nexperts\\nscore\\n Few-shot\\n One-shot\\n Zero-shot\\nFigure 6. Average zero, one and few-shot performance versus the\\nnumber of experts per layer for a set of modest-size models from\\n1.7B/1E to 1.7B/256E.\\nC. Model Partitioning\\nWe partition the weights and computation of large GLaM\\nmodels using the 2D sharding algorithm as described in\\nXu et al. (2021), which exploits the 2D topology of the\\ndevice network of the TPU cluster. We place experts with\\nthe same index across different MoE layers on the same\\ndevice in order to generate an identical computation graph\\nfor different MoE layers. As a result, we can wrap the\\nrepetitive modules of the MoE Transformer architecture in\\na while loop control ﬂow statement (Abadi et al., 2016a; Yu\\net al., 2018) to reduce compilation time. Our experiments\\nreveal that we should grow the size of the experts to get\\nhigh quality models. Therefore, when each expert gets\\nsufﬁciently large, we have to allocate each expert across a set\\nof N\\nE devices. For example, we partition the expert weight\\ntensor with the shape [E, M, H] in the MoE layer along the\\nexpert dimension E, and hidden dimension H, and partition\\nthe input activation tensors with the shape [B, S, M] along\\nthe batch dimension B and the model dimension M. With\\nthis 2D sharding algorithm, we are then able to fully divide\\nthose large weight and activation tensors into smaller pieces\\nsuch that there is no redundancy in data or compute across\\nall devices. We rely on GSPMD’s compiler pass (Xu et al.,\\n2021) to automatically determine the sharding properties\\nfor the rest of the tensors.\\nD. Data Contamination\\nAs GLaM was trained on over 1.6 trillion tokens of text, it\\nis a valid concern that some of the test data might appear\\nexactly in the pretraining dataset, inﬂating some of the re-\\nsults. We therefore follow Brown et al. (2020) and Wei et al.\\n(2021) and quantify the overlap between pretraining data\\nand evaluation datasets.\\nOur analysis uses the same methodology as Wei et al. (2021),\\nwhich, in turn closely follows Brown et al. (2020). For\\neach evaluation dataset we report the number of examples\\nwhich overlap with the pretraining data, deﬁning overlap as\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 6. Overlap statistics for the subset of datasets that are also\\nused in GPT-3. An evaluation example was dirty if it had any\\nn-gram collision with the pretraining corpus.\\nDataset\\nSplit\\nDirty\\ncount\\nTotal\\ncount\\n% clean\\nANLI R1\\nvalidation\\n962\\n1000\\n3.8\\nANLI R2\\nvalidation\\n968\\n1000\\n3.2\\nANLI R3\\nvalidation\\n596\\n1200\\n50.33\\nARC Challenge\\nvalidation\\n95\\n299\\n68.23\\nARC Easy\\nvalidation\\n185\\n570\\n67.54\\nBoolQ\\nvalidation\\n3013\\n3270\\n7.86\\nCB\\nvalidation\\n15\\n56\\n73.21\\nCOPA\\nvalidation\\n3\\n100\\n97.0\\nCoQa\\ntest\\n375\\n500\\n25.0\\nDROP\\ndev\\n9361\\n9536\\n1.84\\nHellaSwag\\nvalidation\\n1989\\n10042\\n80.19\\nLAMBADA\\ntest\\n1125\\n5153\\n78.17\\nMultiRC\\nvalidation\\n3334\\n4848\\n31.23\\nNQs\\nvalidation\\n141\\n3610\\n96.09\\nOpenBookQA\\nvalidation\\n100\\n500\\n80.0\\nPIQA\\nvalidation\\n902\\n1838\\n50.92\\nQuac\\nvalidation\\n7353\\n7354\\n0.01\\nRACE-h\\ndev\\n2552\\n3451\\n26.05\\nRACE-m\\ndev\\n838\\n1436\\n41.64\\nRTE\\nvalidation\\n152\\n277\\n45.13\\nReCoRD\\nvalidation\\n9861\\n10000\\n1.39\\nSQuADv2\\nvalidation\\n11234\\n11873\\n5.38\\nStoryCloze\\nvalidation\\n1871\\n1871\\n0.0\\nTriviaQA\\nvalidation\\n2121\\n11313\\n81.25\\nWSC\\ntest\\n157\\n273\\n42.49\\nWiC\\nvalidation\\n46\\n638\\n92.79\\nWinograd\\nvalidation\\n70\\n104\\n32.69\\nWinogrande\\ntest\\n6\\n1767\\n99.66\\nhaving any n-gram, which also appears in the pretraining\\ndata (varying n between datasets). We ﬁnd that the number\\nof validation examples appearing verbatim in the training\\ndata roughly matches that of prior work. We report these\\nnumbers in Table 6.\\nE. Ethics and Unintended Biases\\nLike Rae et al. (2021), we also analyzed toxicity degener-\\nation with with respect to model scale. This is shown in\\nFigure 7. As with other analysis GLaM’s performance on\\nthis benchmark, it is fairly consistent across model sizes\\nand with MoE variants. The 0.1B/64E MoE variant, the\\nsmallest sparse variant analyzed, is noticeable in the plot\\nand smaller MoE models may be less stable, as noted by\\nRae et al. (2021).\\nFollowing Rae et al. (2021), we also analysed the aspect of\\nthe distribution of generated toxicity probabilities with re-\\nspect to model scale. The same pattern of scale-in-variance\\nis observed with respect to the maximal expected toxicity\\nprobability of a continuation. The distribution of toxicity\\nprobabilities from the API for 25 continuations is plotted\\n1/27/22, 6:53 PM\\nvisualization (26\\nfile:///Users/kevinrobinson/Downloads/visualization (26).svg\\n1e+8\\n1e+9\\n1e+10\\n1e+11\\n1e+12\\n1e+13\\nParameters\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nContinuation toxicity probability\\nHigh >70%\\nMedium 40-70%\\nLow 10-40%\\nVery low\\nFigure 7. Toxicity degeneration scaling plot. The different shades\\nshow different buckets of prompt toxicity probability. The y-axis\\nshows the expected probability of a continuation being toxic. The\\n0.1B/64E variant is noticeable, but as model parameters scale, the\\nrelationship to continuation toxicity constant.\\nTable 7. Prompts used for each co-occurrence category.\\nCategory Prompts\\ngender\\n{term} was very...\\n{term} would be described as...\\nrace\\nThe {term} man was very...”\\nThe {term} woman was very...”\\nPeople would describe the {term} person as...\\nreligion\\n{term} practitioners are...\\nfor low toxicity prompts in Figure 8. This shows that, even\\nfor low toxicity prompts, it is very likely that some gener-\\nated continuation would be judged as toxic by most people\\nreviewing it, according to the Perspective API’s model.\\nTable 7 shows the prompts used for the co-occurrence eval-\\nuation; these are the same as those of Brown et al. (2020).\\nThe top associations for gender templates are shown in Ta-\\nble 8, and Tables 9 and 10 show the same for the race and\\nreligion prompt templates.\\nF. Energy Usage\\nThe power usage effectiveness (PUE) of the datacenter at\\nthe time of training (August and September 2021) was 1.11.\\nUsing 326W measured system power per TPU-v4 chip, this\\nleads to a total energy consumption of 213 MWh for GLaM,\\n1/6 of the energy cost of GPT-3, 1287 MWh. The datacenter\\nPUE was 1.10 at the time of training GPT-3 (Patterson\\net al., 2021). The reduced energy consumption of GLaM\\nis due to the MoE architecture and computation efﬁciency\\noptimizations from TPU-v4 hardware and GSPMD software.\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nMin\\n25th\\n50th\\n75th\\nMax\\nPercentiles, aggregating across prompts\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nExpected toxicity\\nExpected toxicity for non-toxic prompts, 8B-dense\\nFigure 8. Expected toxicity probability given low toxicity proba-\\nbility prompts for 8B Dense variant. This chart shows distributions\\nunderlying the expected maximum toxicity metric for the 8B Dense\\nmodel. The y-axis shows expected toxicity and the x-axis shows\\nthe distribution aggregated at different percentiles. At the left, the\\nminimum continuation toxicity reﬂects that after repeated eval-\\nuations of 25 samples the least toxic response for some outlier\\nnon-toxic prompts was 0.8 likely to be perceived as toxicity. At\\nthe right we see that the worst-case toxicity has an almost uniform\\ndistribution across non-toxic prompts. In other words, in 25 sam-\\nples across low probability toxic prompts, for the majority of trials,\\nthere will be a high toxicity probability continuation.\\nAs a result of low energy consumption, GLaM training has\\nlower CO2 emissions as well. The net tCO2e per MWh of\\nthe datacenter at the time was 0.088, training GLaM with\\n280B tokens emits a total of 18.7 net tCO2e, compared\\nto 552 net tCO2e for GPT-3 (Patterson et al., 2021). The\\ncomplete GLaM training using 600B tokens consumes only\\n456 MWh and emits 40.2 net tCO2e.\\nG. Results on All Tasks for All Model Sizes\\nWe include the zero/one/few-shot results of different model\\nsizes on all the tasks in Table 11, 12, 13 and 14.\\nTable 8. Gender: top co-occurrences for prompts like “{term} was\\nvery...”\\n“He”\\n“She”\\nThe top 10\\nmost common\\ndescriptive\\nwords (and\\ncounts).\\nmuch (188)\\npretty (232)\\ngreat (130)\\nlittle (185)\\nwell (129)\\nmuch (154)\\nlittle (129)\\nbeautiful (148)\\ngood (124)\\nalways (142)\\nalways (114)\\ngood (136)\\nblack (103)\\nblack (117)\\neven (92)\\nnever (116)\\nmany (87)\\neven (111)\\nalso (83)\\nwell (110)\\nTable 9. Race: co-occurrence in response to prompts like “People\\nwould describe the {term} person as...”.\\nTerm\\nMost common descriptive words\\nAsian\\nAsian, black, white, polite, even, really,\\nChinese, good, also, nice\\nBlack\\nwhite, black, much, even, well, angry,\\ngood, also, proud, happy\\nWhite\\nwhite, black, many, even, Indian, much,\\ngood, happy, angry, never\\nLatinx\\nwhite, black, even, really, also, Spanish,\\nmuch, well, different, never\\nIndian\\nIndian, white, black, much, even, differ-\\nent, happy, really, never, good\\nMiddle-Eastern\\nwhite, black, even, eastern, polite, really,\\nmiddle, nice, brown, also\\nTable 10. Religion: co-occurrence in response to prompts like\\n“{term} practitioners are...”\\nTerm\\nMost common descriptive words\\nAtheism\\nreligious, also, bad, likely, really, much,\\nmany, moral, even, sure\\nBuddhism\\nalso, generally, many, religious, always,\\noften, even, good, ﬁrst, different\\nChristianity\\nreligious, also, Christian, many, even,\\noften, always, likely, different, bad\\nIslam\\nalso, religious, even, many, likely, still,\\ndifferent, generally, much, violent\\nHinduism\\ngenerally, also, religious, many, differ-\\nent, even, often, well, Indian, likely\\nJudaism\\nJewish, also, religious, responsible,\\nmany, even, well, generally, often, dif-\\nferent\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 11. Scores of GLaM (64B/64E), GPT-3 and Gopher across all 29 benchmarks. We include the signiﬁcantly larger and more\\ncomputationally expensive Gopher and Megatron-NLG models for reference.\\nZero-shot\\nOne-shot\\nFew-shot (shots)\\nName\\nMetric\\nSplit\\nGPT-3\\n(175B)\\nGLaM\\n(64B/64E)\\nGPT-3\\n(175B)\\nGLaM\\n(64B/64E)\\nGPT-3\\n(175B)\\nGopher\\n(280B)\\nMegatron-NLG\\n(530B)\\nGLaM\\n(64B/64E)\\nTriviaQA\\nacc (em)\\ndev\\n64.3\\n71.3\\n68.0\\n75.8\\n71.2 (64)\\n57.1 (64)\\n–\\n75.8 (1)\\nNQs\\nacc (em)\\ntest\\n14.6\\n24.7\\n23.0\\n26.3\\n29.9 (64)\\n28.2 (64)\\n–\\n32.5 (64)\\nWebQS\\nacc (em)\\ntest\\n14.4\\n19.0\\n25.3\\n24.4\\n41.5 (64)\\n–\\n–\\n41.1 (64)\\nLambada\\nacc (em)\\ntest\\n76.2\\n64.2\\n72.5\\n80.9\\n86.4 (15)\\n74.5(0)\\n87.2\\n86.6 (9)\\nHellaSwag\\nacc\\ndev\\n78.9\\n76.6\\n78.1\\n76.8\\n79.3 (20)\\n79.2(0)\\n82.4\\n77.2 (8)\\nStoryCloze\\nacc\\ntest\\n83.2\\n82.5\\n84.7\\n84.0\\n87.7 (70)\\n–\\n–\\n86.7 (16)\\nWinograd\\nacc\\ntest\\n88.3\\n87.2\\n89.7\\n83.9\\n88.6 (7)\\n–\\n–\\n88.6 (2)\\nWinoGrande\\nacc\\ndev\\n70.2\\n73.5\\n73.2\\n73.1\\n77.7 (16)\\n70.1(0)\\n78.9\\n79.2 (16)\\nDROP\\nf1\\ndev\\n23.6\\n57.3\\n34.3\\n57.8\\n36.5 (20)\\n–\\n–\\n58.6 (2)\\nCoQA\\nf1\\ndev\\n81.5\\n78.8\\n84.0\\n79.6\\n85.0 (5)\\n–\\n–\\n79.6 (1)\\nQuAC\\nf1\\ndev\\n41.5\\n40.3\\n43.4\\n42.8\\n44.3 (5)\\n–\\n–\\n42.7 (1)\\nSQuADv2\\nf1\\ndev\\n62.1\\n71.1\\n64.6\\n71.8\\n69.8 (16)\\n–\\n–\\n71.8 (10)\\nSQuADv2\\nacc (em)\\ndev\\n52.6\\n64.7\\n60.1\\n66.5\\n64.9 (16)\\n–\\n–\\n67.0 (10)\\nRACE-m\\nacc\\ntest\\n58.4\\n64.0\\n57.4\\n65.5\\n58.1 (10)\\n75.1 (5)\\n–\\n66.9 (8)\\nRACE-h\\nacc\\ntest\\n45.5\\n46.9\\n45.9\\n48.7\\n46.8 (10)\\n71.6 (5)\\n47.9\\n49.3 (2)\\nPIQA\\nacc\\ndev\\n81.0\\n80.4\\n80.5\\n81.4\\n82.3 (50)\\n81.8 (0)\\n83.2\\n81.8 (32)\\nARC-e\\nacc\\ntest\\n68.8\\n71.6\\n71.2\\n76.6\\n70.1 (50)\\n–\\n–\\n78.9 (16)\\nARC-c\\nacc\\ntest\\n51.4\\n48.0\\n53.2\\n50.3\\n51.5 (50)\\n–\\n–\\n52.0 (3)\\nOpenbookQA\\nacc\\ntest\\n57.6\\n53.4\\n58.8\\n55.2\\n65.4 (100)\\n–\\n–\\n63.0 (32)\\nBoolQ\\nacc\\ndev\\n60.5\\n83.1\\n76.7\\n82.8\\n77.5 (32)\\n–\\n84.8\\n83.1 (8)\\nCopa\\nacc\\ndev\\n91.0\\n90.0\\n87.0\\n92.0\\n92.0 (32)\\n–\\n–\\n93.0 (16)\\nRTE\\nacc\\ndev\\n63.5\\n67.9\\n70.4\\n71.5\\n72.9 (32)\\n–\\n–\\n76.2 (8)\\nWiC\\nacc\\ndev\\n0.0\\n50.3\\n48.6\\n52.7\\n55.3 (32)\\n–\\n58.5\\n56.3 (4)\\nMultirc\\nf1a\\ndev\\n72.9\\n73.7\\n72.9\\n74.7\\n74.8 (32)\\n–\\n–\\n77.5 (4)\\nWSC\\nacc\\ndev\\n65.4\\n85.3\\n69.2\\n83.9\\n75.0 (32)\\n–\\n–\\n85.6 (2)\\nReCoRD\\nacc\\ndev\\n90.2\\n90.3\\n90.2\\n90.3\\n89.0 (32)\\n–\\n–\\n90.6 (2)\\nCB\\nacc\\ndev\\n46.4\\n48.2\\n64.3\\n73.2\\n82.1 (32)\\n–\\n–\\n84.0 (8)\\nANLI R1\\nacc\\ntest\\n34.6\\n39.2\\n32.0\\n42.4\\n36.8 (50)\\n–\\n–\\n44.3 (2)\\nANLI R2\\nacc\\ntest\\n35.4\\n37.3\\n33.9\\n40.0\\n34.0 (50)\\n–\\n39.6\\n41.2 (10)\\nANLI R3\\nacc\\ntest\\n34.5\\n41.3\\n35.1\\n40.8\\n40.2 (50)\\n–\\n–\\n44.7 (4)\\nAvg NLG\\n–\\n–\\n47.6\\n54.6\\n52.9\\n58.4\\n58.8\\n–\\n–\\n61.6\\nAvg NLU\\n–\\n–\\n60.8\\n66.2\\n65.4\\n68.6\\n68.4\\n–\\n–\\n71.4\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 12. Zero-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models.\\nGLaM (MoE)\\nGLaM (Dense)\\nGPT3\\nName\\nMetric\\nSplit\\n0.1B/64E\\n1.7B/64E\\n8B/64E\\n64B/64E\\n0.1B\\n1.7B\\n8B\\n137B\\n175B\\nTriviaQA\\nacc (em)\\ndev\\n9.42\\n44.0\\n55.1\\n71.3\\n2.3\\n27.0\\n48.1\\n64.0\\n64.3\\nNQs\\nacc (em)\\ntest\\n2.24\\n9.2\\n11.9\\n24.7\\n1.1\\n5.6\\n9.0\\n17.3\\n14.6\\nWebQS\\nacc (em)\\ntest\\n3.44\\n8.3\\n10.7\\n19.0\\n0.7\\n5.9\\n7.7\\n13.8\\n14.4\\nLambada\\nacc (em)\\ntest\\n41.4\\n63.7\\n67.3\\n64.2\\n37.8\\n60.1\\n69.3\\n70.9\\n76.2\\nHellaSwag\\nacc\\ndev\\n43.1\\n65.8\\n74.0\\n76.6\\n34.7\\n60.6\\n72.2\\n76.9\\n78.9\\nStoryCloze\\nacc\\ntest\\n66.4\\n76.2\\n78.9\\n82.5\\n63.3\\n75.1\\n79.5\\n81.1\\n83.2\\nWinograd\\nacc\\ntest\\n66.3\\n80.2\\n83.9\\n87.2\\n67\\n78.7\\n81.6\\n84.3\\n88.3\\nWinoGrande\\nacc\\ndev\\n51.0\\n63.9\\n67.8\\n73.5\\n49.7\\n62.6\\n70.1\\n71.5\\n70.2\\nDROP\\nf1\\ndev\\n9.43\\n13.4\\n16.8\\n57.3\\n5.67\\n14.0\\n17.0\\n21.8\\n23.6\\nCoQA\\nf1\\ndev\\n45.9\\n65.3\\n65.5\\n78.8\\n40.7\\n66.5\\n68.7\\n72.1\\n81.5\\nQuAC\\nf1\\ndev\\n25.2\\n32.8\\n33.8\\n40.3\\n25.4\\n33.3\\n30.7\\n38.3\\n41.5\\nSQuADv2\\nf1\\ndev\\n22.9\\n49.2\\n57.1\\n71.1\\n16.8\\n44.9\\n55.7\\n65.5\\n59.5\\nSQuADv2\\nacc (em)\\ndev\\n7.06\\n29.6\\n38\\n64.7\\n3.4\\n24\\n35.8\\n48.2\\n52.6\\nRACE-m\\nacc\\ntest\\n43.4\\n56.1\\n61.9\\n64.0\\n40.6\\n53.6\\n63.0\\n67.8\\n58.4\\nRACE-h\\nacc\\ntest\\n30.4\\n40.4\\n43.4\\n46.9\\n29.4\\n40.0\\n45.0\\n47.2\\n45.5\\nPIQA\\nacc\\ndev\\n70.0\\n76.9\\n78.6\\n80.4\\n64.4\\n73.6\\n78.2\\n78.5\\n80.4\\nARC-e\\nacc\\ntest\\n52.0\\n66.2\\n66.2\\n71.6\\n44.5\\n62.2\\n67.9\\n71.7\\n68.8\\nARC-c\\nacc\\ntest\\n26.5\\n37.6\\n42.8\\n48.0\\n23.2\\n35.1\\n42.7\\n47.2\\n51.4\\nOpenbookqa\\nacc\\ntest\\n40.0\\n46.4\\n50.0\\n53.4\\n36.8\\n46.7\\n49.8\\n52.0\\n57.6\\nBoolQ\\nacc\\ndev\\n56.6\\n62.7\\n72.2\\n83.1\\n56.6\\n56.1\\n73.6\\n78\\n60.5\\nCopa\\nacc\\ndev\\n73\\n85\\n86\\n90\\n67\\n80\\n86\\n90\\n91\\nRTE\\nacc\\ndev\\n45.8\\n58.8\\n60.3\\n67.9\\n51.3\\n49.1\\n63.8\\n50.5\\n63.5\\nWiC\\nacc\\ndev\\n50.0\\n49.8\\n49.5\\n50.3\\n50.8\\n50.3\\n44\\n50.6\\n0.0\\nMultirc\\nf1a\\ndev\\n57.7\\n58.0\\n52.4\\n73.7\\n58.6\\n53.0\\n39.0\\n54.8\\n72.9\\nWSC\\nacc\\ndev\\n65.6\\n79.3\\n81.8\\n85.3\\n66.3\\n77.2\\n80.7\\n82.8\\n65.4\\nReCoRD\\nacc\\ndev\\n77.5\\n87.1\\n88.9\\n90.3\\n71.6\\n86.7\\n89.2\\n90.3\\n90.2\\nCB\\nacc\\ndev\\n66.1\\n33.9\\n40.7\\n48.2\\n42.9\\n37.5\\n33.9\\n42.9\\n46.4\\nANLI R1\\nacc\\ndev\\n34.1\\n33.9\\n33.4\\n39.2\\n36.1\\n33.2\\n34.7\\n39.4\\n34.6\\nANLI R2\\nacc\\ndev\\n33.8\\n32.4\\n34.9\\n37.3\\n36.7\\n33.6\\n34.8\\n35.7\\n35.4\\nANLI R3\\nacc\\ndev\\n32.8\\n34.0\\n34.6\\n41.3\\n34.8\\n34.1\\n34.9\\n34.6\\n34.5\\nAvg NLG\\n-\\n-\\n18.6\\n35.1\\n39.6\\n54.6\\n14.9\\n31.3\\n38.0\\n45.8\\n47.6\\nAvg NLU\\n-\\n-\\n51.5\\n58.3\\n61.1\\n66.2\\n48.9\\n56.1\\n60.2\\n63.2\\n60.8\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 13. One-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models.\\nGLaM (MoE)\\nGLaM (Dense)\\nGPT3\\nName\\nMetric\\nSplit\\n0.1B/64E\\n1.7B/64E\\n8B/64E\\n64B/64E\\n0.1B\\n1.7B\\n8B\\n137B\\nGPT-3 (175B)\\nTriviaQA\\nacc (em)\\ndev\\n15.2\\n54.1\\n65.9\\n75.8\\n8.3\\n36.3\\n56.4\\n70.0\\n68.0\\nNQs\\nacc (em)\\ntest\\n2.5\\n10.7\\n16.0\\n26.3\\n1.19\\n6.5\\n10.7\\n19.1\\n23.0\\nWebQS\\nacc (em)\\ntest\\n5.9\\n13.9\\n17.0\\n24.4\\n3.44\\n9.3\\n11.6\\n18.8\\n25.3\\nLambada\\nacc (em)\\ntest\\n36.9\\n57.4\\n64.1\\n80.9\\n21.8\\n52.3\\n64.7\\n68.5\\n72.5\\nHellaSwag\\nacc\\ndev\\n43.5\\n66.4\\n74.0\\n76.8\\n34.7\\n60.5\\n72.6\\n76.8\\n78.1\\nStoryCloze\\nacc\\ntest\\n67.0\\n77.9\\n80.0\\n84.0\\n63.7\\n76.4\\n82.1\\n82.6\\n84.7\\nWinograd\\nacc\\ntest\\n69.2\\n80.2\\n85.3\\n83.9\\n65.6\\n80.2\\n84\\n85.3\\n89.7\\nWinoGrande\\nacc\\ndev\\n51.7\\n63.5\\n68.7\\n73.0\\n49.8\\n62.8\\n70.0\\n73.1\\n73.2\\nDROP\\nf1\\ndev\\n16.3\\n24.8\\n28.4\\n57.8\\n19.3\\n24.9\\n41.2\\n49.4\\n34.3\\nCoQA\\nf1\\ndev\\n48.3\\n72.8\\n76\\n79.6\\n33.3\\n72.7\\n74.4\\n78.8\\n84.0\\nQuAC\\nf1\\ndev\\n28.7\\n35.2\\n43.1\\n42.7\\n23.7\\n35.7\\n35.1\\n44.6\\n43.4\\nSQuADv2\\nf1\\ndev\\n35.5\\n69.5\\n76.3\\n71.8\\n34.2\\n67.1\\n69.2\\n70.0\\n65.4\\nSQuADv2\\nacc (em)\\ndev\\n21.8\\n53.6\\n60.9\\n66.5\\n29.0\\n50.8\\n64.2\\n63.7\\n60.1\\nRACE-m\\nacc\\ntest\\n42.7\\n60.9\\n60.6\\n65.5\\n43.1\\n56.4\\n63.1\\n69.0\\n57.4\\nRACE-h\\nacc\\ntest\\n29.1\\n41.9\\n44.6\\n48.7\\n29.4\\n40.8\\n45.3\\n47.7\\n45.9\\nPIQA\\nacc\\ndev\\n69.0\\n76.0\\n78.1\\n81.4\\n63.7\\n73.1\\n76.3\\n79.5\\n80.5\\nARC-e\\nacc\\ntest\\n53.5\\n68.1\\n73.4\\n76.6\\n45.9\\n63.8\\n62.6\\n77.2\\n71.2\\nARC-c\\nacc\\ntest\\n27.0\\n39.3\\n44.8\\n50.3\\n24.5\\n35.2\\n41.5\\n50.7\\n53.2\\nOpenbookqa\\nacc\\ntest\\n39.6\\n47.6\\n50.6\\n55.2\\n37.8\\n47.2\\n53.0\\n55.4\\n58.8\\nBoolQ\\nacc\\ndev\\n53.6\\n62.0\\n70.8\\n82.8\\n55.7\\n58.1\\n76.4\\n77.5\\n76.7\\nCopa\\nacc\\ndev\\n75\\n81\\n86\\n92\\n71\\n81\\n86\\n91\\n87\\nRTE\\nacc\\ndev\\n53.1\\n54.5\\n57.0\\n71.5\\n53.4\\n55.2\\n62.0\\n58.4\\n70.4\\nWiC\\nacc\\ndev\\n47.3\\n47.0\\n48.0\\n52.7\\n47.3\\n46.8\\n48.0\\n48.7\\n48.6\\nMultirc\\nf1a\\ndev\\n58.5\\n59.6\\n62.0\\n74.7\\n56.3\\n59.4\\n61.9\\n64.2\\n72.9\\nWSC\\nacc\\ndev\\n67.7\\n77.5\\n83.8\\n83.9\\n63.8\\n78.5\\n83.0\\n86.3\\n69.2\\nReCoRD\\nacc\\ndev\\n77.5\\n87.3\\n89.0\\n90.3\\n71.6\\n86.2\\n89.2\\n90.2\\n90.1\\nCB\\nacc\\ndev\\n41.1\\n35.7\\n44.6\\n73.2\\n42.9\\n41.1\\n30.4\\n48.2\\n64.3\\nANLI R1\\nacc\\ndev\\n32.1\\n31.1\\n32.3\\n42.4\\n32.5\\n31.4\\n31.9\\n34.8\\n32.0\\nANLI R2\\nacc\\ndev\\n31.1\\n30.7\\n32.5\\n40.0\\n30.7\\n31.2\\n30.7\\n32.6\\n33.9\\nANLI R3\\nacc\\ndev\\n30.5\\n31.6\\n34.8\\n40.8\\n30.9\\n30.3\\n32.4\\n35.0\\n35.1\\nAvg NLG\\n-\\n-\\n23.5\\n43.6\\n49.7\\n58.4\\n19.4\\n39.5\\n47.5\\n52.8\\n52.7\\nAvg NLU\\n-\\n-\\n50.4\\n58.1\\n61.9\\n68.6\\n48.3\\n56.9\\n61.7\\n65.0\\n65.4\\nGLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts\\nTable 14. Few-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models. We tune the number of shots up\\nto the respective value in each task used by GPT3.\\nGLaM (MoE)\\nGLaM (Dense)\\nGPT3\\nName\\nMetric\\nSplit\\n0.1B/64E\\n1.7B/64E\\n8B/64E\\n64B/64E\\n0.1B\\n1.7B\\n8B\\n137B\\nGPT-3 (175B)\\nTriviaQA\\nacc (em)\\ndev\\n21.7\\n60.1\\n67.7\\n75.8\\n8.3\\n38.8\\n56.4\\n70.0\\n71.2\\nNQs\\nacc (em)\\ntest\\n5.3\\n17.7\\n24.4\\n32.5\\n1.50\\n9.0\\n20.1\\n27.9\\n29.9\\nWebQS\\nacc (em)\\ntest\\n12.1\\n24.4\\n29.6\\n41.1\\n6.90\\n9.3\\n25.5\\n32.9\\n41.5\\nLambada\\nacc (em)\\ntest\\n36.9\\n64.3\\n79.0\\n86.6\\n21.8\\n63.0\\n77.1\\n84.2\\n86.4\\nHellaSwag\\nacc\\ndev\\n45.6\\n66.2\\n74.0\\n77.2\\n34.7\\n60.7\\n72.6\\n76.8\\n79.3\\nStoryCloze\\nacc\\ntest\\n69.4\\n80.0\\n82.8\\n86.7\\n63.7\\n78.7\\n83.7\\n85.7\\n87.7\\nWinograd\\nacc\\ntest\\n69.2\\n82.8\\n85.3\\n88.6\\n65.6\\n80.5\\n85.4\\n85.3\\n88.6\\nWinoGrande\\nacc\\ndev\\n52.6\\n66.2\\n71.4\\n79.2\\n49.8\\n64.2\\n72.3\\n76.6\\n77.7\\nDROP\\nf1\\ndev\\n23.5\\n37.0\\n40.0\\n58.6\\n19.3\\n41.4\\n49.4\\n49.4\\n36.5\\nCoQA\\nf1\\ndev\\n48.3\\n66.0\\n72\\n79.6\\n33.3\\n66.0\\n74.4\\n78.8\\n85.0\\nQuAC\\nf1\\ndev\\n26.0\\n34.2\\n43.1\\n42.8\\n23.7\\n34.3\\n35.1\\n37.2\\n44.3\\nSQuADv2\\nf1\\ndev\\n38.7\\n61.8\\n67.1\\n71.8\\n34.2\\n60.0\\n69.6\\n70.0\\n69.8\\nSQuADv2\\nacc (em)\\ndev\\n32.7\\n55.5\\n60.9\\n67.0\\n29.0\\n53.9\\n64.2\\n63.7\\n64.9\\nRACE-m\\nacc\\ntest\\n41.8\\n53.6\\n60.6\\n66.9\\n43.1\\n56.5\\n56\\n65.1\\n58.1\\nRACE-h\\nacc\\ntest\\n31.5\\n40.2\\n44.6\\n49.3\\n29.5\\n40.8\\n43\\n48.1\\n46.8\\nPIQA\\nacc\\ndev\\n69.0\\n76.1\\n78.1\\n81.8\\n64.2\\n73.1\\n77\\n80.8\\n82.3\\nARC-e\\nacc\\ntest\\n57.8\\n70.1\\n75.3\\n78.9\\n48.9\\n66.0\\n74\\n79.0\\n70.1\\nARC-c\\nacc\\ntest\\n29.7\\n38.3\\n45.5\\n52.0\\n24.8\\n35.2\\n41.5\\n45.7\\n51.5\\nOpenbookqa\\nacc\\ntest\\n41.6\\n49.6\\n53.0\\n63.0\\n37.8\\n54\\n54.0\\n58.8\\n65.4\\nBoolQ\\nacc\\ndev\\n53.6\\n62.0\\n70.5\\n83.1\\n59.9\\n63.1\\n76.4\\n80.5\\n77.5\\nCopa\\nacc\\ndev\\n75\\n82\\n88\\n93.0\\n71\\n83\\n92.0\\n91.0\\n92.0\\nRTE\\nacc\\ndev\\n53.1\\n54.5\\n60.0\\n76.2\\n54.9\\n55.2\\n64.0\\n63.9\\n72.9\\nWiC\\nacc\\ndev\\n49.4\\n51.3\\n53.3\\n56.3\\n51.9\\n50.9\\n50.0\\n53.6\\n55.3\\nMultirc\\nf1a\\ndev\\n58.5\\n59.7\\n62.0\\n77.5\\n56.3\\n59.4\\n61.5\\n68.1\\n74.8\\nWSC\\nacc\\ndev\\n67.7\\n80.4\\n83.8\\n85.6\\n65.6\\n80.0\\n82.0\\n87.4\\n75.0\\nReCoRD\\nacc\\ndev\\n77.5\\n87.3\\n89.0\\n90.6\\n71.8\\n86.2\\n89.0\\n90.5\\n89.0\\nCB\\nacc\\ndev\\n43.0\\n53.6\\n60.7\\n84.0\\n42.9\\n55.4\\n58\\n53.6\\n82.1\\nANLI R1\\nacc\\ndev\\n34.3\\n31.4\\n34.0\\n44.3\\n33.5\\n33.1\\n33.2\\n35.8\\n36.8\\nANLI R2\\nacc\\ndev\\n32.3\\n33.0\\n32.0\\n41.2\\n34.4\\n33.7\\n33.9\\n35.6\\n34.0\\nANLI R3\\nacc\\ndev\\n33.9\\n35.8\\n33.0\\n44.7\\n32.9\\n33.3\\n35.0\\n34.7\\n40.2\\nAvg NLG\\n-\\n-\\n27.2\\n46.8\\n53.0\\n61.6\\n19.8\\n42.7\\n52.4\\n57.1\\n58.8\\nAvg NLU\\n-\\n-\\n51.7\\n59.7\\n63.6\\n71.4\\n49.2\\n59.2\\n63.7\\n66.8\\n68.4\\n', 'source_name': 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts', 'source_url': 'https://arxiv.org/abs/2112.06905'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "GShard.pdf #4\n",
      "{'content': 'GShard: Scaling Giant Models with Conditional\\nComputation and Automatic Sharding\\nDmitry Lepikhin\\nlepikhin@google.com\\nHyoukJoong Lee\\nhyouklee@google.com\\nYuanzhong Xu\\nyuanzx@google.com\\nDehao Chen\\ndehao@google.com\\nOrhan Firat\\norhanf@google.com\\nYanping Huang\\nhuangyp@google.com\\nMaxim Krikun\\nkrikun@google.com\\nNoam Shazeer\\nnoam@google.com\\nZhifeng Chen\\nzhifengc@google.com\\nAbstract\\nNeural network scaling has been critical for improving the model quality in many\\nreal-world machine learning applications with vast amounts of training data and\\ncompute. Although this trend of scaling is afﬁrmed to be a sure-ﬁre approach for\\nbetter model quality, there are challenges on the path such as the computation cost,\\nease of programming, and efﬁcient implementation on parallel devices. GShard\\nis a module composed of a set of lightweight annotation APIs and an extension\\nto the XLA compiler. It provides an elegant way to express a wide range of\\nparallel computation patterns with minimal changes to the existing model code.\\nGShard enabled us to scale up multilingual neural machine translation Transformer\\nmodel with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters\\nusing automatic sharding. We demonstrate that such a giant model can efﬁcienctly\\nbe trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality\\nfor translation from 100 languages to English compared to the prior art.\\n1\\nIntroduction\\nScaling neural networks brings dramatic quality gains over a wide array of machine learning problems\\n[1, 2, 3, 4, 5, 6]. For computer vision, increasing the model capacity has led to better image classiﬁ-\\ncation and detection accuracy for various computer vision architectures [7, 8, 9]. Similarly in natural\\nlanguage processing, scaling Transformers [10] yielded consistent gains on language understanding\\ntasks [4, 11, 12], cross-lingual down-stream transfer [4, 13] and (massively-)multilingual neural\\nmachine translation [14, 15, 16]. This general tendency motivated recent studies to scrutinize the\\nfactors playing a critical role in the success of scaling [17, 18, 19, 20, 3], including the amounts of\\ntraining data, the model size, and the computation being utilized as found by past studies. While the\\nﬁnal model quality was found to have a power-law relationship with the amount of data, compute\\nand model size [18, 3], the signiﬁcant quality gains brought by larger models also come with various\\npractical challenges. Training efﬁciency among the most important ones, which we deﬁne as the\\namount of compute and training time being used to achieve a superior model quality against the best\\nsystem existed, is oftentimes left out.\\nPreprint. Under review.\\narXiv:2006.16668v1  [cs.CL]  30 Jun 2020\\nFigure 1: Multilingual translation quality (average ∆BLEU comparing to bilingual baselines) im-\\nproved as MoE model size grows up to 600B, while the end-to-end training cost (in terms of TPU v3\\ncore-year) only increased sublinearly. Increasing the model size from 37.5B to 600B (16x), results in\\ncomputation cost increase from 6 to 22 years (3.6x). The 600B parameters model that achieved the\\nbest translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3\\ncore-years. In contrast, training all 100 bilingual baseline models would have required 29 TPU v3\\ncore-years. Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU\\nof 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3\\ncore-years.\\n1.1\\nPractical Challenges for Scaling\\nHere we enumerate major practical challenges faced especially when training massive-scale models\\nthat are orders of magnitude larger than the capacity limit of a single accelerator memory (e.g., GPUs\\nor TPUs).\\nArchitecture-speciﬁc model parallelism support\\nThere is a lack of support for efﬁcient model\\nparallelism algorithms under commonly used deep learning frameworks such as TensorFlow [21] and\\nPyTorch [22]. Naive model parallelism with graph partition is supported but it would lead to severe\\nunder-utilization due to the sequential dependency of the network and gradient based optimization.\\nIn order to scale up the existing models efﬁciently, users typically need to invest a lot of engineering\\nwork, for example, migrating the model code to special frameworks [23, 15].\\nSuper-linear scaling of computation cost vs model size\\nStraightforward scaling of the mode size\\nby increasing the depth or width [6, 15] generally results in at least linear increase of training step\\ntime. Model parallelism by splitting layer weights and computation across multiple devices generally\\nbecomes necessary, leading to network communication overhead and device under-utilization. Device\\nunder-utilization stems from imbalanced assignment and sequential dependencies of the underlying\\nneural network. This super-linear relationship between the computation cost and the model size can\\nnot be resolved by simply using more devices, making training massive models impractical.\\nInfrastructure scalability for giant model representation\\nA naive graph representation for the\\nmassive-scale model distributed across thousands of devices may become a bottleneck for both deep\\nlearning frameworks and their optimizing compilers. For example, adding D times more layers with\\ninter-op partitioning or increasing model dimensions with intra-op partitioning across D devices may\\nresult in a graph with O(D) nodes. Communication channels between devices could further increase\\nthe graph size by up to O(D2) (e.g., partitioning gather or transpose). Such increase in the graph\\nsize would result in an infeasible amount of graph building and compilation time for massive-scale\\nmodels.\\nNon-trivial efforts for implementing partitioning strategies\\nPartitioning a model to run on\\nmany devices efﬁciently is challenging, as it requires coordinating communications across devices.\\nFor graph-level partitioning, sophisticated algorithms [15, 24] are needed to reduce the overhead\\n2\\nintroduced by the sequential dependencies between different partitions of graphs allocated on different\\ndevices. For operator-level parallelism, there are different communication patterns for different\\npartitioned operators, depending on the semantics, e.g., whether it needs to accumulate partial results,\\nor to rearrange data shards. According to our experience, manually handling these issues in the model\\nrequires substantial amount of effort, given the fact that the frameworks like TensorFlow have a\\nlarge sets of operators with ad-hoc semantics. In all cases, implementing model partitioning would\\nparticularly be a burden for practitioners, as changing model architecture would require changing the\\nunderlying device communications, causing a ripple effect.\\n1.2\\nDesign Principles for Efﬁcient Training at Scale\\nIn this paper, we demonstrate how to overcome these challenges by building a 600 billion parameters\\nsequence-to-sequence Transformer model with Sparsely-Gated Mixture-of-Experts layers, which\\nenjoys sub-linear computation cost and O(1) compilation time. We trained this model with 2048 TPU\\nv3 devices for 4 days on a multilingual machine translation task and achieved far superior translation\\nquality compared to prior art when translating 100 languages to English with a single non-ensemble\\nmodel. We conducted experiments with various model sizes and found that the translation quality\\nincreases as the model gets bigger, yet the total wall-time to train only increases sub-linearly with\\nrespect to the model size, as illustrated in Figure 1. To build such an extremely large model, we made\\nthe following key design choices.\\nSub-linear Scaling\\nFirst, model architecture should be designed to keep the computation and\\ncommunication requirements sublinear in the model capacity. Conditional computation [25, 16,\\n26, 27] enables us to satisfy training and inference efﬁciency by having a sub-network activated\\non the per-input basis. Scaling capacity of RNN-based machine translation and language models\\nby adding Position-wise Sparsely Gated Mixture-of-Experts (MoE) layers [16] allowed to achieve\\nstate-of-the-art results with sublinear computation cost. We therefore present our approach to extend\\nTransformer architecture with MoE layers in Section 2.\\nThe Power of Abstraction\\nSecond, the model description should be separated from the partitioning\\nimplementation and optimization. This separation of concerns let model developers focus on the\\nnetwork architecture and ﬂexibly change the partitioning strategy, while the underlying system applies\\nsemantic-preserving transformations and implements efﬁcient parallel execution. To this end we\\npropose a module, GShard, which only requires the user to annotate a few critical tensors in the\\nmodel with partitioning policies. It consists of a set of simple APIs for annotations, and a compiler\\nextension in XLA [28] for automatic parallelization. Model developers write models as if there is a\\nsingle device with huge memory and computation capacity, and the compiler automatically partitions\\nthe computation for the target based on the annotations and their own heuristics. We provide more\\nannotation examples in Section 3.2.\\nScalable Compilers\\nThird, the system infrastructure, including the computation representation\\nand compilation, must scale with thousands of devices for parallel execution. For example, Figure 2\\nillustrates two different ways of partitioning a dot-product operation across 4 devices (color-coded).\\nNotice that with the usual MPMD (Multiple Program Multiple Data) approach in Figure 2a scaling\\nbecomes more challenging since the number of nodes in the graph increases linearly with the number\\nof devices. Instead, we developed a compiler technique for SPMD (Single Program Multiple Data)\\ntransformation that generates a single program to run on all devices, keeping the compilation time\\nconstant independent of the number of devices, as illustrated in Figure 2b. We will discuss our SPMD\\nframework in more details in Section 3.3.\\nThe rest of the paper is organized as the following. Section 2 describes our Transformer architecture\\nwith Sparsely-Gated MoE layer in more details. Section 3 introduces our development module GShard.\\nSection 4 demonstrates the application of our mixture of expert models on the multilingual machine\\ntranslation task over 100 language pairs. Section 5 has performance and memory measurements of\\nour implementation. Section 6 discusses related work.\\n3\\nslice\\nslice\\ndot\\ndot\\ndot\\ndot\\nall-\\nreduce\\nall-\\nreduce\\nall-\\nreduce\\nall-\\nreduce\\nN\\nK\\nN\\nM\\nK\\nM\\nM\\nK/4\\nK/4\\nN\\n(a) MPMD Partition\\ndynamic\\nslice\\ndot\\nall-\\nreduce\\nN\\nK\\nN\\nM\\nK\\nM\\nM\\nK/4\\ndynamic\\nslice\\nK/4\\nN\\n(b) SPMD Partition\\nFigure 2: Comparison between MPMD and our proposed SPMD partitioning of a Dot operator\\n([M, K] × [K, N] = [M, N]) across 4 devices. In this example, both operands are partitioned along\\nthe contracting dimension K, where each device computes the local result and globally combines\\nwith an AllReduce. MPMD partitioning generates separate operators for each device, limiting its\\nscalability, whereas SPMD partitioning generates one program to run on all devices. Note that the\\ncompilation time with our SPMD partitioning is not-dependent of the number of devices being used.\\n2\\nModel\\n2.1\\nSparse scaling of the Transformer architecture\\nThe Transformer [10] architecture has been widely used for natural language processing. It has\\nbecome the de-facto standard for many sequence-to-sequence tasks, such as machine translation.\\nTransformer makes use of two computational blocks, an encoder and a decoder, both implemented\\nby stacking multiple Transformer layers. Transformer encoder layer consists of two consecutive\\nlayers, namely a self-attention layer followed by a position-wise feed-forward layer. Decoder adds\\nthird cross-attention layer, which attends over encoder output. We sparsely scale Transformer with\\nconditional computation by replacing every other feed-forward layer with a Position-wise Mixture of\\nExperts (MoE) layer [16] with a variant of top-2 gating in both the encoder and the decoder (Figure 3).\\nWe vary the number of Transformer layers and the number of experts per MoE layer in order to scale\\nthe model capacity.\\nEach training example consists of a pair of sequences of subword tokens. Each token activates a\\nsub-network of the MoE Transformer during both training and inference. The size of the sub-network\\nis roughly independent of the number of experts per MoE Layer, allowing sublinear scaling of the\\ncomputation cost as described in the previous section. Computation complexity is further analyzed in\\nSection 3.1 and training performance in Section 5.\\n2.2\\nPosition-wise Mixture-of-Experts Layer\\nThe Mixture-of-Experts (MoE) layer used in our model is based on [16] with variations in the sparse\\ngating function and the auxiliary loss being used. A MoE layer for Transformer consists of E\\nfeed-forward networks FFN1 . . . FFNE:\\nGs,E = GATE(xs)\\n(1)\\nFFNe(xs) = woe · ReLU(wie · xs)\\n(2)\\nys =\\nE\\nX\\ne=1\\nGs,e · FFNe(xs)\\n(3)\\n4\\nFigure 3: Illustration of scaling of Transformer Encoder with MoE Layers. The MoE layer replaces\\nthe every other Transformer feed-forward layer. Decoder modiﬁcation is similar. (a) The encoder of\\na standard Transformer model is a stack of self-attention and feed forward layers interleaved with\\nresidual connections and layer normalization. (b) By replacing every other feed forward layer with\\na MoE layer, we get the model structure of the MoE Transformer Encoder. (c) When scaling to\\nmultiple devices, the MoE layer is sharded across devices, while all other layers are replicated.\\nwhere xs is the input token to the MoE layer, wiand wobeing the input and output projection matrices\\nfor the feed-forward layer (an expert). Vector Gs,E is computed by a gating network. Gs,E has one\\nnon-negative for each expert, most of which are zeros meaning the token is not dispatched to that\\nexpert. The token is dispatched to a very small number of experts. We choose to let each token\\ndispatched to at most two experts. The corresponding entries in Gs,E are non-zeros, representing\\nhow much an expert contributes to the ﬁnal network output. Every expert FFNe applies to xs a\\nfully-connected 2-layer network using ReLU [29] activation function. The output of the MoE layer,\\nys, is the weighted average of outputs from all the selected experts.\\nThe gating function GATE(·) is critical to the MoE layer, which is modeled by a softmax activation\\nfunction to indicate the weights of each expert in processing incoming tokens. In other words, to\\nindicate how good an expert is at processing the incoming token. Furthermore, the gating function\\nmust satisfy two goals:\\n• Balanced load It is desirable that the MoE layer to sparsely activate the experts for a given\\ntoken. A naive solution would be just to choose the top-k experts according to the softmax\\nprobability distribution. However, it is known that this approach leads to load imbalance\\nproblem for training [16]: most tokens seen during training would have been dispatched to a\\nsmall number of experts, amassing a very large input buffer for only a few (busy) experts\\nleaving other experts untrained, slowing down the training. Meanwhile many other experts\\ndo not get sufﬁciently trained at all. A better design of the gating function would distribute\\nprocessing burden more evenly across all experts.\\n• Efﬁciency at scale It would be rather trivial to achieve a balanced load if the gating function\\nis done sequentially. The computation cost for the gating function alone is at least O(NE)\\nfor all N tokens in the input batch given E experts. However, in our study, N is in the order\\nof millions and E is in the order of thousands, a sequential implementation of the gating\\nfunction would keep most of the computational resources idle most of the time. Therefore,\\nwe need an efﬁcient parallel implementation of the gating function to leverage many devices.\\n5\\nWe designed the following mechanisms in the gating function GATE(·) to meet the above require-\\nments (details illustrated in Algorithm 1):\\n• Expert capacity To ensure the load is balanced, we enforce that the number of tokens\\nprocessed by one expert is below some uniform threshold, which we deﬁne as expert\\ncapacity. Assuming that the total number of tokens in a training batch is N, and each\\ntoken is dispatched to at most two experts, then the expert capacity is set to be O(N/E).\\nGATE(·) keeps a running counter ce for how many tokens are dispatched to an expert. When\\nboth experts selected by a token already exceed their capacity, the token is considered as\\nan overﬂowed token, where Gs,E degenerates into a zero vector. Such tokens have their\\nrepresentation xs passed on to the next layer via residual connections.\\n• Local group dispatching GATE(·) partitions all tokens in a training batch evenly into G\\ngroups, i.e., each group contains S = N/G tokens. All groups are processed independently\\nin parallel. Each group is given a fractional capacity of each expert, 2N/(G · E). Each\\ngroup ensures that at most this many tokens are dispatched to an expert. In this way, we can\\nensure that expert capacity is still enforced and the overall load is balanced.\\n• Auxiliary loss It is important that the gating function does not always choose the same few\\nexperts, as this would lead to a capacity overﬂow for only a few experts and under-utilization\\nfor the remaining ones. Following [16], we deﬁne an auxiliary loss term ℓaux to enforce\\nthis constraint. It is added to the overall loss function of the model L = ℓnll + k ∗ℓaux\\nwith a constant multiplier k. The particular form of the auxiliary loss term ℓaux in line (13)\\nof algorithm 1 is motivated by the following consideration: the term ce/S represents the\\nfraction of input routed to each expert, and we want to minimize mean square of ce/S. But\\nbecause ce is derived from top-2 operation and is not differentiable, we use the mean gates\\nper expert me as a differentiable approximation and replace (ce/S)2 with me(ce/S), which\\ncan now be optimized with gradient descent.\\n• Random routing Intuitively, because ys is a weighted average of what selected experts\\nreturn, if the weight for the 2nd expert is very small, we can simply ignore the 2nd expert to\\nconserve the overall expert capacity. Hence, in addition to respecting the expert capacity\\nconstraint, GATE(·) dispatches to the 2nd-best expert with the probability proportional to\\nits weight g2.\\n3\\nHighly Parallel Implementation using GShard\\nThis section describes the implementation of the model in Section 2 that runs efﬁciently on a cluster\\nof TPU devices.\\nThe ﬁrst step is to express the model in terms of linear algebra operations, in which our software\\nstack (TensorFlow [21]) and the hardware platform (TPU) are highly tailored and optimized. It\\nis readily easy to code up most of the model in terms of linear algebra in the same way as the\\noriginal Transformer. However, it requires some effort to express the MoE Layer, in particular\\nGATE(·) function presented in Algorithm 1 due to its sequential nature, and we describe the details\\nin Section 3.1.\\nNext, we annotate the linear algebra computation to express parallelism. Each tensor in the com-\\nputation can be annotated for replication or distribution across a cluster of devices using sharding\\nAPIs in Section 3.2. Using sharding annotations enables separation of concerns between the model\\ndescription and the efﬁcient parallel implementation, and allows users to ﬂexibly express diverse\\nparallelization strategies. For example, (1) the attention layer is parallelized by splitting along the\\nbatch dimension and replicating its weights to all devices. On the other hand, (2) experts in the MoE\\nlayer are infeasible to be replicated in all the devices due to its sheer size and the only viable strategy\\nis to shard experts into many devices. Furthermore, the whole model alternates between these two\\nmodes (1)-(2). Using annotations frees model developers from the system optimization efforts and\\navoids baking the parallel implementation and low-level details into the model code.\\nFinally, the compiler infrastructure takes a (partially) annotated linear algebra computation and\\nproduces an efﬁcient parallel program that scales to thousands of devices. As will be described in\\nSection 3.3, the compiler applies SPMD (Single Program Multiple Data) partitioning transformation\\nto express per-device computation, inserts necessary cross-device communication, handles irregular\\n6\\nAlgorithm 1: Group-level top-2 gating with auxiliary loss\\nData: xS, a group of tokens of size S\\nData: C, Expert capacity allocated to this group\\nResult: GS,E, group combine weights\\nResult: ℓaux, group auxiliary loss\\n(1) cE ←0\\n▷gating decisions per expert\\n(2) gS,E ←softmax(wg · xS)\\n▷gates per token per expert, wg are trainable weights\\n(3) mE ←1\\nS\\nPs\\ns=1 gs,E\\n▷mean gates per expert\\n(4) for s ←1 to S do\\n(5)\\ng1, e1, g2, e2 = top_2(gs,E)\\n▷top-2 gates and expert indices\\n(6)\\ng1 ←g1/(g1 + g2)\\n▷normalized g1\\n(7)\\nc ←ce1\\n▷position in e1 expert buffer\\n(8)\\nif ce1 < C then\\n(9)\\nGs,e1 ←g1\\n▷e1 expert combine weight for xs\\n(10)\\nend\\n(11)\\nce1 ←c + 1\\n▷incrementing e1 expert decisions count\\n(12) end\\n(13) ℓaux = 1\\nE\\nPE\\ne=1\\nce\\nS · me\\n(14) for s ←1 to S do\\n(15)\\ng1, e1, g2, e2 = top_2(gs,E)\\n▷top-2 gates and expert indices\\n(16)\\ng2 ←g2/(g1 + g2)\\n▷normalized g2\\n(17)\\nrnd ←uniform(0, 1)\\n▷dispatch to second-best expert with probability ∝2 · g2\\n(18)\\nc ←ce2\\n▷position in e2 expert buffer\\n(19)\\nif c < C ∧2 · g2 > rnd then\\n(20)\\nGs,e2 ←g2\\n▷e2 expert combine weight for xs\\n(21)\\nend\\n(22)\\nce2 ←c + 1\\n(23) end\\npatterns such as uneven partitions, and ﬁnally generates a single program to be launched on all devices\\nfor parallel execution.\\n3.1\\nPositions-wise Mixture-of-Expert Layer Expressed in Linear Algebra\\nOur model implementation (Algorithm 2) views the whole accelerator cluster as a single device and\\nexpresses its core mathematical algorithm in a few tensor operations independent of the concrete\\nsetup of the cluster. Einstein summation notation [30] (i.e., tf.einsum) is a powerful construct to\\nconcisely express the model and we use it extensively in our implementation. The softmax gates\\ncomputation is trivially expressed by one einsum followed by the softmax function. Dispatching\\nof inputs to selected experts is expressed by a single einsum between the dispatching mask and the\\ninput. All FFNe weights are combined into single 3-D tensors wi amd wo and the computation by\\nFFN1 . . . FFNE is expressed using 3 operators (two einsum and one relu). Finally, taking weighted\\naverage of all experts output into the ﬁnal output is expressed in another einsum.\\nTop2Gating in Algorithm 2 computes the union of all group-local GS,E described in Algorithm 1.\\ncombine_weights is a 4-D tensor with shape [G, S, E, C]. The value combine_weights[g,\\ns, e, c] is non-zero when the input token s in group g is sent to the input buffer of expert e at buffer\\nposition c. For a speciﬁc g and s, a slice combine_weight[g, s, :, :] contains at most two\\nnon-zero vaules. Binary dispatch_mask is produced from combine_weights by simply setting all\\nnon-zero values to 1.\\nWe need to choose the number of groups G and the number of experts E properly so that the algorithm\\ncan scale to a cluster with D devices. It is worthwhile to analyze its overall computation complexity\\n(the total number of ﬂoating point operations) for a training step given a training batch of N tokens.\\n7\\nAlgorithm 2: Forward pass of the Positions-wise MoE layer. The underscored letter (e.g., G and\\nE) indicates the dimension along which a tensor will be partitioned.\\n1\\ngates = softmax(einsum(\"GSM ,ME ->GSE\", inputs , wg))\\n2\\ncombine_weights , dispatch_mask = Top2Gating(gates)\\n3\\ndispatched_expert_inputs = einsum(\\n4\\n\"GSEC ,GSM ->EGCM\", dispatch_mask , reshaped_inputs )\\n5\\nh = einsum(\"EGCM ,EMH ->EGCH\", dispatched_expert_inputs , wi)\\n6\\nh = relu(h)\\n7\\nexpert_outputs = einsum(\"EGCH ,EHM ->GECM\", h, wo)\\n8\\noutputs = einsum(\\n9\\n\"GSEC ,GECM ->GSM\", combine_weights , expert_outputs )\\nWe analyze Algorithm 2 computation complexity scaling with number the of devices D with the\\nfollowing assumptions: a) number of tokens per device N\\nD = O(1) is constant1; b) G = O(D),\\nS = O(1) and N = O(GS) = O(D); c) M = O(1), H = O(1); d) E = O(D); and e)\\nC = O( 2S\\nE ) = O( 1\\nD), D < S and is a positive integer2 .\\nThe total number of ﬂoating point operations FLOPS in Algorithm 2:\\nFLOPSSoftmax +FLOPSTop2Gating+FLOPSDispatch|Combine+FLOPSFFN =\\nO(GSME)\\n+O(GSEC)\\n+O(GSMEC)\\n+O(EGCHM) =\\nO(D · 1 · 1 · D)+O(D · 1 · D · 1\\nD)+O(D · 1 · 1 · D · 1\\nD) +O(D · D · 1\\nD · 1 · 1) =\\nO(D2)\\n+O(D)\\n+O(D)\\n+O(D)\\nand consequently per-device FLOPS/D = O(D) + O(1) + O(1) + O(1). Per-device softmax\\ncomplexity FLOPSsoftmax/D = O(D) is linear in number of devices, but in practice is dominated\\nby other terms since D << H and D < S. As a result FLOPS/D could be considered O(1),\\nsatisfying sublinear scaling design requirements. Section 5 veriﬁes this analysis empirically.\\nIn addition to the computation cost, we have non-constant cross-device communication cost, but it\\ngrows at a modest rate O(\\n√\\nD) when we increase D (Section 5).\\n3.2\\nGShard Annotation API for Parallel Execution\\nDue to the daunting size and computation demand of tensors in Algorithm 1, we have to parallelize\\nthe algorithm over many devices. An immediate solution of how to shard each tensor in the algorithm\\nis illustrated by underscored letters in Algorithm 2. The sharding API in GShard allows us to annotate\\ntensors in the program to selectively specify how they should be partitioned. This information is\\npropagated to the compiler so that the compiler can automatically apply transformations for parallel\\nexecution. We use the following APIs in TensorFlow/Lingvo [31] in our work.\\n• replicate(tensor) annotates tensor to be replicated across partitions, and returns the an-\\nnotated tensor. This is often used for the non-MoE layers in our model to replicate the\\nweights.\\n• split(tensor, split_dimension, num_partitions) annotates tensor to be partitioned along\\nsplit_dimension, and returns the annotated tensor. Partition i is placed on the i’th device,\\nand num_partitions must not exceed the number of devices on the system.\\n• shard(tensor, device_assignment) generalizes split() to allow partitioning multiple\\ndimensions and specifying the placement of each partition. Appendix A.3 describes this\\nAPI with more details.\\n1This is oftentimes necessary in practice to avoid overﬂowing device memory.\\n2Scaling D > S would require different use of fractional expert capacity.\\n8\\nNote that the invocations to split or shard only adds annotations and does not change the logical\\nshape in the user program. The user still works with full shapes and does not need to worry about\\nissues like uneven partitioning.\\nGShard is general in the sense that the simple APIs apply to all dimensions in the same way.\\nThe sharded dimensions could include batch (data-parallelism), feature, expert, and even spatial\\ndimensions in image models, depending on the use cases. Also, since the sharding annotation is per\\ntensor, different parts of the model can be partitioned in different ways. This ﬂexibility enables us to\\npartition the giant MoE weights and switch partition modes between MoE and non-MoE layers, as\\nwell as uses cases beyond this paper, e.g., spatial partitioning of large images [32] (Appendix A.4).\\nWith the above sharding APIs, we can express the sharding strategy shown in Algorithm 2 as below.\\nThe input tensor is split along the ﬁrst dimension and the gating weight tensor is replicated. After\\ncomputing the dispatched expert inputs, we apply split to change the sharding from the group (G)\\ndimension to the expert (E) dimension. D is device count.\\n1\\n# Partition\\ninputs\\nalong\\ngroup (G) dim.\\n2\\n+ inputs = split(inputs , 0, D)\\n3\\n# Replicate\\nthe gating\\nweights\\n4\\n+ wg = replicate(wg)\\n5\\ngates = softmax(einsum(\"GSM ,ME ->GSE\", inputs , wg))\\n6\\ncombine_weights , dispatch_mask = Top2Gating( gating_logits )\\n7\\ndispatched_expert_inputs = einsum(\\n8\\n\"GSEC ,GSM ->EGCM\", dispatch_mask , reshaped_inputs )\\n9\\n# Partition\\ndispatched\\ninputs\\nalong\\nexpert (E) dim.\\n10\\n+ dispatched_expert_inputs = split(dispatched_expert_inputs , 0, D)\\n11\\nh = einsum(\"EGCM ,EMH ->EGCH\", dispatched_expert_inputs , wi)\\n12\\n...\\nPer-tensor sharding assignment\\nAs shown in the example above, users are not required to annotate\\nevery tensor in the program. Annotations are typically only required on a few important operators\\nlike Einsums in our model and the compiler uses its own heuristics to infer sharding for the rest of\\nthe tensors 3. For example, since the input tensor is partitioned along G and the weight tensor is\\nreplicated, the compiler chooses to partition the einsum output along the same G dimension (Line\\n5). Similarly, since both inputs are partitioned along the G dimension for the input dispatch einsum\\n(Line 7), the output sharding is inferred to be split along the G dimension, and then we add the split\\nannotation on the output to reshard along the E dimension. Some annotations in the above example\\ncould also be determined by the compiler (e.g., replicate(wg)) but it is recommended to annotate\\nthe initial input and ﬁnal output tensors of the computation.\\nThe compiler currently uses an iterative data-ﬂow analysis to propagate sharding information from\\nan operator to its neighbors (operands and users), starting from the user-annotated operators. The\\nanalysis tries to minimize the chance of resharding by aligning the sharding decisions of adjacent\\noperators. There could be other approaches such as integer programming or machine-learning\\nmethods, but improving the automatic sharding assignment is not the focus of this paper and we leave\\nit as future work.\\nMixing manual and automatic sharding\\nAutomatic partitioning with sharding annotations is\\noften enough for common cases, but GShard also has the ﬂexibility to allow mixing manually\\npartitioned operators with auto-partitioned operators. This provides users with more controls on how\\noperators are partitioned, and one example is that the user has more run-time knowledge beyond\\nthe operators’ semantics. For example, neither XLA’s nor TensorFlow’s Gather operator deﬁnition\\nconveys information about the index bounds for different ranges in the input, but the user might\\nknow that a speciﬁc Gather operator shufﬂes data only within each partition. In this case, the\\nuser can trivially partition the operator by simply shrinking the dimension size and performing a\\nlocal Gather; otherwise, the compiler would need to be conservative about the index range and add\\nunnecessary communication overhead. For example, the dispatching Einsum (Line 3) in Algorithm 2\\n3It is also important for the compiler to infer missing shardings since the backpropagation computation is\\noften automatically generated by the frontend framework and users don’t have access to those tensors.\\n9\\nin Algorithm 2, which uses an one-hot matrix to dispatch inputs, can be alternatively implemented\\nwith a Gather operator using trivial manual partitioning, while the rest of the model is partitioned\\nautomatically. Below is the pseudocode illustrating this use case.\\n1\\n# input has shape [G, S, M]. split () does not change\\nlogical\\nshape.\\n2\\ninput = split(input , 0, num_devices)\\n3\\n# s_indices\\nhas shape [E, G, C, 1]. Values: indices to S in input.\\n4\\ns_indices = split(s_indices , 1, num_devices)\\n5\\n6\\n# Begin\\nmanual\\npartitioning.\\n7\\n# partitioned_input\\nhas shape [G/num_devices , S, M]\\n8\\npartitioned_input = auto_to_manual_spmd_partition (input)\\n9\\n# partitioned_s_indices\\nhas shape [E, G/num_devices , C, 1]\\n10\\npartitioned_s_indices = auto_to_manual_spmd_partition (s_indices)\\n11\\n# Concat\\nwith G indices in partitioned_input : Iota on G dimension.\\n12\\npartitioned_gs_indices = concat(\\n13\\niota ([E, G/num_devices , C, 1], 1), partitioned_s_indices , 3)\\n14\\n# partitioned_data\\nhas shape [E, G/num_devices , C, M]\\n15\\npartitioned_data = gather(\\n16\\npartitioned_input , partitioned_gs_indices )\\n17\\n18\\n# Switch\\nback to auto\\npartitioning .\\n19\\n# data has shape [E, G, C, M]\\n20\\ndata = manual_to_auto_spmd_partition ( partitioned_data )\\n21\\n...\\n3.3\\nThe XLA SPMD Partitioner for GShard\\nThis section describes the compiler infrastructure that automatically partitions a computation graph\\nbased on sharding annotations. Sharding annotations inform the compiler about how each tensor\\nshould be distributed across devices. The SPMD (Single Program Multiple Data) partitioner (or\\n“partitioner” for simplicity) is a compiler component that transforms a computation graph into a single\\nprogram to be executed on all devices in parallel. This makes the compilation time near constant\\nregardless of the number of partitions, which allows us to scale to thousands of partitions. 4\\nWe implemented the partitioner in the XLA compiler [28]. Multiple frontend frameworks including\\nTensorFlow, JAX, PyTorch and Julia already have lowering logic to transform their graph representa-\\ntion to XLA HLO graph. XLA also has a much smaller set of operators compared to popular frontend\\nframeworks like TensorFlow, which reduces the burden of implementing a partitioner without harm-\\ning generality, because the existing lowering from frontends performs the heavy-lifting to make it\\nexpressive. Although we developed the infrastructure in XLA, the techniques we describe here can\\nbe applied to intermediate representations in other machine learning frameworks (e.g., ONNX [33],\\nTVM Relay [34], Glow IR [35]).\\nXLA models a computation as a dataﬂow graph where nodes are operators and edges are tensors\\nﬂowing between operators. The core of the partitioner is per-operation handling that transforms a\\nfull-sized operator into a partition-sized operator according to the sharding speciﬁed on the input\\nand output. When a computation is partitioned, various patterns of cross-device data transfers are\\nintroduced. In order to maximize the performance at large scale, it is essential to deﬁne a core set of\\ncommunication primitives and optimize those for the target platform.\\n3.3.1\\nCommunication Primitives\\nSince the partitioner forces all the devices to run the same program, the communication patterns are\\nalso regular and XLA deﬁnes a set of collective operators that perform MPI-style communications [36].\\nWe list the common communication primitives we use in the SPMD partitioner below.\\n4An alternative is MPMD (Multiple Program Multiple Data), which does not scale as shown in Figure 2.\\n10\\nCollectivePermute\\nThis operator speciﬁes a list of source-destination pairs, and the input data of a\\nsource is sent to the corresponding destination. It is used in two places: changing a sharded tensor’s\\ndevice order among partitions, and halo exchange as discussed later in this section.\\nAllGather\\nThis operator concatenates tensors from all participants following a speciﬁed order. It is\\nused to change a sharded tensor to a replicated tensor.\\nAllReduce\\nThis operator performs elementwise reduction (e.g., summation) over the inputs from\\nall participants. It is used to combine partially reduced intermediate tensors from different partitions.\\nIn a TPU device network, AllReduce has a constant cost when the number of partition grows\\n(Section 5.2). It is also a commonly used primitive with efﬁcient implementation in other types of\\nnetwork topology [37].\\nAllToAll\\nThis operator logically splits the input of each participant along one dimension, then\\nsends each piece to a different participant. On receiving data pieces from others, each participant\\nconcatenates the pieces to produce its result. It is used to reshard a sharded tensor from one dimension\\nto another dimension. AllToAll is an efﬁcient way for such resharding in a TPU device network,\\nwhere its cost increases sublinearly when the number of partitions grows (Section 5.2).\\n3.3.2\\nPer-Operator SPMD Partitioning\\nThe core of the partitioner is the per-operator transformation from a full-sized operator into a\\npartition-sized operator according to the speciﬁed sharding. While some operators (e.g., elementwise)\\nare trivial to support, we discuss several common cases where cross-partition communications are\\nrequired.\\nThere are a few important technical challenges in general cases, which we will cover in Section 3.3.3.\\nTo keep the discussion more relevant to the MoE model, this section focuses on Einsum partitioning\\nto illustrate a few communication patterns. And to keep it simple for now, we assume that all tensors\\nare evenly partitioned, which means the size of the dimension to partitition is a multiple of the\\npartition count.\\nEinsum Case Study\\nEinsum is the most critical operator in implementing the MoE model. They\\nare represented as a Dot operation in XLA HLO, where each operand (LHS or RHS) consists of three\\ntypes of dimensions:\\n• Batch dimensions are the embarrassingly parallel dimensions. The same set of batch\\ndimensions must exist in all of LHS, RHS and the output, and each element in the output\\nonly depends on the corresponding batch in LHS and RHS.\\n• Contracting dimensions only exist in the operands. LHS and RHS must have the same set\\nof contracting dimensions, and they are summed up and collapsed in the output.\\n• Non-contracting dimensions are also parallel dimensions that exist in one of the operands\\nand the output. Each of LHS and RHS has its own set of non-contracting dimensions, which\\nare inherited by the output.\\nSharding propagation prioritizes choosing the same sharding on batch dimensions of LHS, RHS and\\noutput, because that would avoid any cross-partition communication. However, that is not always\\npossible, and we need cross-partition communication in the following three cases.\\n• Resharding. In the MoE model we built, the expert dispatching logic (Line 3 in Algorithm 2)\\nrequires switching the partitioned dimension after an Einsum. Since resharding is efﬁcient\\n(Section 5.2) with AllToAll, we ﬁrst execute the Einsum locally, then reshard it to the\\ndesired dimension, as shown in Figure 4a.\\n• Accumulating partial results. If the inputs are partitioned along contracting dimensions,\\nthe local result is partial and we need to use an AllReduce to combine them and produce\\nthe ﬁnal result, as shown in Figure 4b.\\n• Slicing in a loop. For certain scenarios, we also implemented an algorithm similar to\\nCannon’s algorithm [38], in order to limit the size of tensors on each partition. For example,\\n11\\nEinsum: GSEC,GSM->EGCM\\nG\\nE\\nC\\n0\\n1\\n2\\n3\\nG\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\nM\\nG\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\nE\\nC\\nX\\n(S omitted)\\n(S omitted)\\n(M omitted)\\n2\\n3\\n2\\n3\\n3\\n0\\n1\\n0\\n1\\n0\\n1\\n2\\n0\\n1\\n2\\nG\\nE\\nC\\n(M omitted)\\nParallel, partitioned \\neinsums\\nReshard (all-to-all)\\n3\\nGSEC\\nGSM\\nEGCM\\nEGCM\\n(a) A partitioned Einsum operator. Colored letters (G and E) represent the\\npartitioned dimension of each tensor. The partitioner decides to ﬁrst execute a\\nbatch-parallel Einsum along the G dimension, then reshard the result to the E\\ndimension.\\nMatmul/Einsum: AB,BC->AC\\nB\\nA\\nC\\n0\\n1\\n2\\n3\\nX\\nB\\n0\\n1\\n2\\n3\\nC\\nA\\nParallel, B-partitioned matmul\\nAll-reduce\\nPartial \\nresult\\nC\\nA\\nFull result\\n1\\nX\\n1\\nPartial \\nresult for \\nPartition 1\\nFull result\\nPartition 1 \\nlocal view\\n(b) A simple Einsum (Matmul) partitioned on the contracting dimension.\\nMatmul/Einsum: AB,BC->AC\\nB\\nA\\nC\\n0\\n1\\n2\\n3\\nB\\nC\\nA\\n1\\n1\\nPartition 1 \\nlocal view\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\nSliced Matmul\\nX\\nX\\nDynamicUpdateSlice\\ni\\nCollective \\nPermute\\nWhile loop\\n(c) An Einsum (Matmul) where we use collective-permute in a loop to compute\\none slice at a time. There is no full-sized tensor during the entire process.\\nFigure 4: Examples of Einsum partitioning with cross-device communication.\\nif both operands are partitioned on a non-contracting dimension, we cannot compute the\\nlocal Einsum directly since operands have different non-contracting dimensions. Replicating\\none of the operands would not cause redundant computation, but it requires the replicated\\noperand to ﬁt in device memory. Therefore, if the size of the operand is too large, we instead\\nkeep both operands partitioned and use a loop to iterate over each slice of the result, and use\\nCollectivePermute to communicate the input slices (Figure 4c).\\n3.3.3\\nSupporting a Complete Set of Operators\\nWe solved several additional challenges to enable the SPMD partitioner to support a complete set of\\noperators without extra constraints of tensor shapes or operator conﬁgurations. These challenges often\\ninvolve asymmetric compute or communication patterns between partitions, which are particularly\\n12\\nPartitioned input\\nInput with halo\\nPadding\\nDynamicSlice\\nCollectivePermute\\nConcat\\nConvolution\\n(a) Convolution\\nConcat\\nCollectivePermute\\nDynamicSlice\\nPadding\\nPad\\nDynamicSlice\\n(b) Pad\\nLocal reshape\\nDynamicSlice\\nCollectivePermute\\nConcat\\nDynamicSlice\\nPadding\\nReshape from [3, 2] to [6]\\n(c) Reshape with unevenly partitioned\\ninput and evenly partitioned output\\nFigure 5: Halo exchange examples.\\nhard to express in SPMD, since the single program needs to be general enough for all partitions. We\\ncannot simply create many branches in the single program based on the run-time device ID, because\\nthat would lead to an explosion in program size.\\nStatic shapes and uneven partitioning\\nXLA requires tensor shapes to be static. 5 However, when\\na computation is partitioned, it’s not always the case that all partitions have the same input/output\\nshapes, because dimensions may not be evenly divisible by the number of partitions. In those cases,\\nthe size of the shape is rounded up to the next multiple of partition count, and the data in that padded\\nregion can be arbitrary.\\nWhen computing an operator, we may need to ﬁll in a known value to the padded region for correctness.\\nFor example, if we need to partition an Reduce-Add operator, the identity value of zero needs to be\\nused. Consider an example where the partitioned dimension (15) cannot be divided into 2 (partition\\ncount), so Partition 1 has one more column than needed. We create an Iota operator of range [0, 8),\\nadd the partition offset (calculated from PartitionId × 8), and compare with the full shape offset\\n(15). Based on the predicate value, we select either from the operand or from zero, and the result is\\nthe masked operand.\\nStatic operator conﬁgurations\\nXLA operators have static conﬁgurations, like the padding, stride,\\nand dilation deﬁned in Convolution. However, different partitions may not execute with the same\\noperator conﬁguration. E.g., for a Convolution, the left-most partition applies padding to its left\\nwhile the right-most partition applies padding to its right. In such cases, the partitioner may choose\\nconﬁgurations that make some partitions to produce slightly more data than needed, then slice out the\\nthe irrelevant parts. Appendix A.4 discusses examples for Convolution and similar operators.\\nHalo exchange\\nCertain operators have a communication pattern which involves partial data ex-\\nchange with neighboring partitions, which we call halo exchange. We use the CollectivePermute\\noperator to exchange halo data between partitions.\\nThe most typical use case of halo exchange is for partitinoning window-based operators (e.g.,\\nConvolution, ReduceWindow), because neighboring partitions may require overlapping input data\\n(Figure 5a). In practice, halo-exchange for these operator often needs to be coupled with proper\\npadding, slicing, and masking due to advanced use of window conﬁgurations (dilation, stride, and\\npadding), as well as uneven halo sizes. We describe various scenarios in Appendix A.4.\\nAnother use of halo exchange is for data formatting operators that change the size of the shape. For\\nexample, after a Slice or Pad operator, the shape of the tensor changes, and so do the boundaries\\nbetween partitions. This requires us to realign the data on different partitions, which can be handled\\nas a form of halo exchange (Figure 5b).\\nOther data formatting operators, although logically not changing the size of the shape, may also need\\nhalo exchange, speciﬁcally due to the static shape constraint and uneven partitioning. For example,\\nthe Reverse operator reverses the order of elements in a tensor, but if it is partitioned unevenly,\\nwe need to shift data across partitions to keep the padding logically to the right of the result tensor.\\nAnother example is Reshape. Consider reshaping a tensor from [3, 2] to [6], where the input is\\n5The limited dynamism in the intermediate representation is often necessary to efﬁciently target accelerators.\\n13\\nunevenly partitioned in 2 ways on the ﬁrst dimension (partition shape [2, 2]), and the output is also\\npartitioned in 2 ways (partition shape [3]). There is padding on the input due to uneven partitioning,\\nbut after Reshape, the output tensor no longer has padding; as a result, halo exchange is required in\\na similar way to Slice (Figure 5c).\\nCompiler optimizations\\nThe SPMD partitioner creates various data formatting operators in order\\nto perform slicing, padding, concatenation, masking and halo exchange. To address the issue, we\\nleverage XLA’s fusion capabilities on TPU, as well as code motion optimizations for slicing and\\npadding, to largely hide the overhead of data formatting. As a result, the run-time overhead is\\ntypically negligible, even for convolutional networks where masking and padding are heavily used.\\n4\\nMassively Multilingual, Massive Machine Translation (M4)\\n4.1\\nMultilingual translation\\nWe chose multilingual neural machine translation (MT) [39, 40, 41] to validate our design for efﬁcient\\ntraining with GShard. Multilingual MT, which is an inherently multi-task learning problem, aims at\\nbuilding a single neural network for the goal of translating multiple language pairs simultaneously.\\nThis extends our line of work [15, 14, 16] towards a universal machine translation model [42], i.e. a\\nsingle model that can translate between more than hundred languages, in all domains. Such massively\\nmultilingual translation models are not only convenient for stress testing models at scale, but also\\nshown to be practically impactful in real-world production systems [43].\\nIn massively multilingual MT, there are two criteria that deﬁne success in terms of the model quality,\\n1) improvements attained on languages that have large amounts of training data (high resourced), and\\n2) improvements for languages with limited data (low-resource). As the number of language pairs\\n(tasks) to be modeled within a single translation model increases, positive language transfer [44]\\nstarts to deliver large gains for low-resource languages. Given the number of languages considered,\\nM4 has a clear advantage on improving the low-resource tasks. On the contrary, for high-resource\\nlanguages the increased number of tasks limits per-task capacity within the model, resulting in lower\\ntranslation quality compared to a models trained on a single language pair. This capacity bottleneck\\nfor high resourced languages can be relaxed by increasing the model size to massive scale in order to\\nsatisfy the need for additional capacity [14, 15].\\nMassively multilingual, massive MT consequently aims at striking a balance between increasing\\npositive transfer by massive multilinguality and mitigating the capacity bottleneck by massive scaling.\\nWhile doing so, scaling the model size and the number of languages considered have to be coupled\\nwith a convenient neural network architecture. In order to amplify the positive transfer and reduce the\\nnegative transfer6, one can naturally design a model architecture that harbours shared components\\nacross languages (shared sub-networks), along with some language speciﬁc ones (unshared, language\\nspeciﬁc sub-networks). However, the search space in model design (deciding on what to share) grows\\nrapidly as the number of languages increase, making heuristic-based search for a suitable architecture\\nimpractical. Thereupon the need for approaches based on learning the wiring pattern of the neural\\nnetworks from the data emerge as scalable and practical way forward.\\nIn this section, we advocate how conditional computation [45, 46] with sparsely gated mixture of\\nexperts [16] ﬁts into the above detailed desiderata and show its efﬁcacy by scaling neural machine\\ntranslation models beyond 1 trillion parameters, while keeping the training time of such massive\\nnetworks practical. E.g. a 600B GShard model for M4 can process 1T tokens7 in 250k training\\nsteps in under 4 days. We experiment with increasing the model capacity by adding more and more\\nexperts into the model and study the factors playing role in convergence, model quality and training\\nefﬁciency. Further, we demonstrate how conditional computation can speed up the training [25] and\\nhow sparsely gating/routing each token through the network can efﬁciently be learned without any\\nprior knowledge on task or language relatedness, exemplifying the capability of learning the routing\\ndecision directly from the data.\\n6Negative transfer is the notion of sharing the model capacity by unrelated tasks which in return hurts the\\nquality of such interfering tasks.\\n7Source side tokens after sub-word segmentation.\\n14\\n4.2\\nDataset and Baselines\\nThe premise of progressively larger models to attain greater quality necessitates large amounts of\\ntraining data to begin with [3]. Following the prior work on dense scaling for multilingual machine\\ntranslation [15, 14], we committed to the realistic test bed of MT in the wild, and use a web-scale\\nin-house dataset. The training corpus, mined from the web [47], contains parallel documents for\\n100 languages, to and from English, adding up to a total of 25 billion training examples. A few\\ncharacteristics of the training set is worth mentioning. Having mined from the web, the joint corpus is\\nconsiderably noisy while covering a diverse set of domains and languages. Such large coverage comes\\nwith a heavy imbalance between languages in terms of the amount of examples per language pair. This\\nimbalance follows a sharp power law, ranging from billions of examples for high-resourced languages\\nto tens of thousands examples for low-resourced ones. While the above mentioned characteristics\\nconstitute a challenge for our study, it also makes the overall attempt as realistic as possible. We refer\\nreader to [15, 14] for the additional details of the dataset being used.\\nWe focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100\\nlanguages to English. This resulted in approximately 13 billion training examples to be used for model\\ntraining8. In order to form our baselines, we trained separate bilingual Neural Machine Translation\\nmodels for each language pair (e.g. a single model for German-to-English), tuned depending on\\nthe available training data per-language9. Rather than displaying individual BLEU scores for each\\nlanguage pair, we follow the convention of placing the baselines along the x-axis at zero, and report\\nthe ∆BLEU trendline of each massively multilingual model trained with GShard (see Figure 6).\\nThe x-axis in Figure 6 is sorted from left-to-right in the decreasing order of amount of available\\ntraining data, where the left-most side corresponds to high-resourced languages, and low-resourced\\nlanguages on the right-most side respectively. To reiterate, our ultimate goal in universal machine\\ntranslation is to amass the ∆BLEU trendline of a single multilingual model above the baselines for\\nall languages considered. We also include a variant of dense 96 layer Transformer Encoder-Decoder\\nnetwork T(96L) trained with GPipe pipeline parallelism on the same dataset as another baseline\\n(dashed trendline in Figure 6). Training to convergence took over 6 weeks on 2048 TPU v3 cores 10,\\noutperforming the original GPipe T(128L)11 [15] and is the strongest single dense model baseline we\\nuse in our comparisons.\\n4.3\\nSparsely-Gated MoE Transformer: Model and Training\\nScaling Transformer architecture has been an exploratory research track recently [49, 50, 51]. Without\\nloss of generality, emerging approaches follow scaling Transformer by stacking more and more layers\\n[49, 15], widening the governing dimensions of the network (i.e. model dimension, hidden dimension\\nor number of attention heads) [4, 11] and more recently learning the wiring structure with architecture\\nsearch [52] 12. For massively multilingual machine translation, [15] demonstrated the best practices\\nof scaling using GPipe pipeline parallelism; in which a 128 layer Transformer model with 6 billion\\nparameters is shown to be effective at improving high-resource languages while exhibiting the highest\\npositive transfer towards low-resource languages. Although very promising, and satisfying our\\ndesiderata for universal translation, dense scaling of Transformer architecture has practical limitations\\nwhich we referred in Section 1 under training efﬁciency.\\nWe aim for practical training time and seek for architectures that warrant training efﬁciency. Our\\nstrategy has three pillars; increase the depth of the network by stacking more layers similar to\\nGPipe [15], increase the width of the network by introducing multiple replicas of the feed-forward\\nnetworks (experts) as described in Section 2.2 and make use of learned routing modules to (sparsely)\\nassign tokens to experts as described in Section 2.1. With this three constituents, we obtain an\\n8Compared to prior work using the same dataset, Kazakh and Latin to English language pairs were excluded\\nfrom evaluation.\\n9We tuned batch-size and different values of regularization methods (e.g. dropout) in a Transformer-Big or\\nTransformer-Base layout, for high or low-resourced languages respectively.\\n10T(96L) measured to be processing 1+ trillion tokens at 300k steps, processing around 4M tokens/step, total\\nbudget of 235.5 TPU v3 core years\\n1164 encoder + 64 decoder layers, 16384 hidden dim, 32 attention heads\\n12Since the approaches utilizing architecture search are compute intensive, they are not considered within the\\nscope of this work.\\n15\\n600B\\n1B+ examples\\nper language\\n←high-resouce languages\\n...\\nlow-resource languages →\\n10k examples\\nper language\\n0\\n5\\n10\\n15\\nΔBLEU\\nMoE(2048,36L) - 600B\\nMoE(2048,12L) - 200B\\nMoE(512E,36L) - 150B\\nMoE(512E,12L) - 50B\\nMoE(128E,36L) - 37B\\nMoE(128E,12L) - 12.5B\\nT(96L) - 2.3B\\nId\\nModel\\nBLEU\\navg.\\n∆BLEU\\navg.\\nWeights\\n(1)\\nMoE(2048E, 36L)\\n44.3\\n13.5\\n600B\\n(2)\\nMoE(2048E, 12L)\\n41.3\\n10.5\\n200B\\n(3)\\nMoE(512E, 36L)\\n43.7\\n12.9\\n150B\\n(4)\\nMoE(512E, 12L)\\n40.0\\n9.2\\n50B\\n(5)\\nMoE(128E, 36L)\\n39.0\\n8.2\\n37B\\n(6)\\nMoE(128E, 12L)\\n36.7\\n5.9\\n12.5B\\n*\\nT(96L)\\n36.9\\n6.1\\n2.3B\\n*\\nBaselines\\n30.8\\n-\\n100×0.4B\\nFigure 6: Translation quality comparison of multilingual MoE Transformer models trained with\\nGShard and monolingual baselines. Positions along the x-axis represent languages, raging from high-\\nto low-resource. ∆BLEU represents the quality gain of a single multilingual model compared to a\\nmonolingual Transformer model trained and tuned for a speciﬁc language. MoE Transformer models\\ntrained with GShard are reported with solid trend-lines. Dashed trend-line represents a single 96\\nlayer multilingual Transformer model T(96L) trained with GPipe on same dataset. Each trend-line is\\nsmoothed by a sliding window of 10 for clarity. (Best seen in color)\\neasy to scale, efﬁcient to train and highly expressive architecture, which we call Sparsely-Gated\\nMixture-of-Experts Transformer or MoE Transformer in short.\\nModel Details\\nTo detail the model speciﬁcs, each expert is designed to have the same shape of a\\nregular Transformer feed-forward network, and experts (MoE layers) are distributed once in every\\nother Transformer layer. We tied the number of devices used for training to the number of experts\\nper MoE layer for simplicity, although this is not a requirement. During training, we use ﬂoat32 for\\nboth model weights and activations in order to ensure training stability. We ran additional scalability\\nexperiments with MoE(2048E, 60L) with bﬂoat16 [53] activations with total of 1 trillion model\\nweights. Although trainable by careful and manual diagnostics, with deep 1 trillion model we\\nencountered several trainability issues with numerical stability, hence did not include the results for\\nthe sake of reproducibility. For more model and training details, please see Appendix A.2.\\n4.4\\nResults\\nBefore going into the details of training efﬁciency, we ﬁrst investigate the effect of various design\\nchoices on building MoE Transformer. In order to prune the search space, we explored varying two\\n16\\nId\\nModel\\nExperts\\nPer-layer\\nExperts\\ntotal\\nTPU v3\\nCores\\nEnc+Dec\\nlayers\\nWeights\\n(1)\\nMoE(2048E, 36L)\\n2048\\n36684\\n2048\\n36\\n600B\\n(2)\\nMoE(2048E, 12L)\\n2048\\n12228\\n2048\\n12\\n200B\\n(3)\\nMoE(512E, 36L)\\n512\\n9216\\n512\\n36\\n150B\\n(4)\\nMoE(512E, 12L)\\n512\\n3072\\n512\\n12\\n50B\\n(5)\\nMoE(128E, 36L)\\n128\\n2304\\n128\\n36\\n37B\\n(6)\\nMoE(128E, 12L)\\n128\\n768\\n128\\n12\\n12.5B\\n*\\nMoE(2048E, 60L)\\n2048\\n61440\\n2048\\n60\\n1T\\nTable 1: MoE Transformer model family. To achieve desired capacity we i) increased the depth by\\nstacking more layers, ii) increased the width of the network by scaling the number of experts per\\nMoE layer along with number of cores used for training.\\nvariables, number of layers in the Transformer encoder-decoder stack (L) and the total number of\\nexperts used for every other MoE layer (E). For depth, we tested three different options, 12 (original\\nTransformer depth, which consists of 6 encoder and 6 decoder layers), 36 and 60 layers. For the\\nnumber of experts that replaces every other feed-forward layer, we also tested three options, namely\\n128, 512 and 2048 experts. Note that, the number of devices used for training, is ﬁxed to be equal to\\nthe number of experts per-layer, using 128, 512 and 2048 cores respectively independent of the depth\\nbeing experimented. Please also see the detailed description in Table 1 for model conﬁgurations.\\nFor each experiment (rows of the Table 1), we trained the corresponding MoE Transformer model\\nuntil it has seen 1 trillion (1012) tokens. The model checkpoint at this point is used in the model\\nevaluation. We did not observe any over-ﬁtting patterns by this point in any experiment. Instead, we\\nobserved that the training loss continued to improve if we kept training longer. We evaluated BLEU\\nscores that the models achieved for all language pairs on a held-out test set. Figure 6 reports all our\\nresults.\\nHere we share a qualitative analysis for each experiment and discuss the implication of each setup on\\nhigh- and low-resource languages in order to track our progress towards universal translation. To\\nground the forthcoming analysis, it is worth restating the expected behavior of the underlying quality\\ngains. In order to improve the quality for both high- and low-resource languages simultaneously\\nwithin a single model, scaled models must mitigate capacity bottleneck issue by allocating enough\\ncapacity to high-resource tasks, while amplifying the positive transfer towards low-resource tasks\\nby facilitating sufﬁcient parameter sharing. We loosely relate the expected learning dynamics of\\nsuch systems with the long-standing memorization and generalization dilemma, which is recently\\nstudied along the lines of width vs depth scaling efforts [54]. Not only do we expect our models\\nto generalize better to the held-out test sets, we also expect them to exhibit high transfer capability\\nacross languages as another manifestation of generalization performance [55].\\nDeeper Models Bring Consistent Quality Gains Across the Board\\nWe ﬁrst investigate the rela-\\ntionship between the model depth and the model quality for both high- and low-resource languages.\\nThree different experiments are conducted in order to test the generalization performance, while\\nkeeping the number of experts per-layer ﬁxed. With an increasing number of per-layer experts for\\neach experiment (128, 512 and 2048), we tripled the depth of the network for each expert size, from\\n12 to 36. This resulted in three groups where experts per-layer ﬁxed but three times the depth within\\neach group:\\nFor each conﬁguration shown in Fig. 6, we observed that increasing the depth (L) while keeping\\nthe experts per-layer (E) ﬁxed, brings consistent gains for both low and high resourced languages\\n(upwards ∆shift along the y-axis), almost with a constant additive factor every time we scale the\\ndepth from 12L to 36L (2-to-3 BLEU points on average as shown in the last column of Table 3).\\nRelaxing the Capacity Bottleneck Grants Pronounced Quality Gains\\nEarlier in Section 4.1\\nwe highlighted the inﬂuence of the capacity bottleneck on task interference, resulting in degraded\\nquality especially for high resourced languages. Later we alleviated this complication by increasing\\nthe number of experts per-layer, which in return resulted in a dramatic increase in the number of\\nparameters (weight) of the models studied. Here we investigate whether this so called capacity\\n17\\nbottleneck is distinctly observable and explore the impact on model quality and efﬁciency once it\\nis relaxed. To that end, we ﬁrst consider three models with identical depths (12L), with increasing\\nnumber of experts per-layer: 128, 512 and 2048. As we increase the number of experts per-layer from\\n128 to 512 by a factor of four, we notice a large jump in model quality, +3.3 average BLEU score\\nacross 100 languages. However again by four folds scaling of the number of experts per-layer, from\\n512 to 2048, yields only +1.3 average BLEU scores. Despite the signiﬁcant quality improvement,\\nthis drop in gains hints the emergence of diminishing returns.\\nSpeculatively, the capacity bottleneck is expected to be residing between 128 to 512 experts, for\\nthe particular parametrization, number of languages and the amount of training data used in our\\nexperimental setup. Once the bottleneck is relaxed, models enjoy successive scaling of the depth,\\nwhich can be seen by comparing 12 versus 36 layer models both with 128 experts. Interestingly\\nincreasing the depth does not help as much if the capacity bottleneck is not relaxed.\\nHaving More Experts Improve Quality Especially for High-Resourced Tasks\\nAnother dimen-\\nsion that could shed light on the quality gains of scaling in multi-task models is the contrast between\\nhigh and low resource language improvements. As mentioned before, low resourced languages\\nbeneﬁt from transfer while high resource languages seek for added capacity. Next we examine the\\neffect of increasing the experts per-layer while ﬁxing the depth.\\nAs can be seen in Figure 6, for 12 layer models increase in the expert number yields larger gains\\nfor high resourced languages as opposed to earlier revealed diminishing returns for low-resourced\\nlanguages. A similar pattern is observed also for 36 layer models. While adding more experts relaxes\\nthe capacity bottleneck, at the same time it reduces the amount of transfer due to a reduction of the\\nshared sub-networks.\\nDeep-Dense Models are Better at Positive Transfer towards Low-Resource Tasks\\nLastly we\\nlook into the impact of the depth on low-resourced tasks as a loose corollary to our previous\\nexperiment. In order to do so, we include a dense model with 96 layers T(96L) trained with GPipe\\non the same data into our analysis. We compare T(96L) with the shallow MoE(128E, 12L) model.\\nWhile the gap between the two models measured to be almost constant for the majority of the\\nhigh-to-mid resourced languages, the gap grows in favor of the dense-deep T(96L) model as we get\\ninto the low-resourced regime. Following our previous statement, as the proportion of the shared\\nsub-networks across tasks increase, which is 100% for dense T(96L), the bandwidth for transfer gets\\nmaximized and results in a comparably better quality against its shallow counterpart. Also notice\\nthat, the same transfer quality to the low-resourced languages can be achieved with MoE(36E, 128L)\\nwhich contains 37 billion parameters.\\nWe conjecture that, increasing the depth might potentially increase the extent of transfer to low-\\nresource tasks hence generalize better along that axis. But we also want to highlight that the models\\nin comparison have a disproportionate training resource requirements. We again want to promote the\\nimportance of training efﬁciency, which is the very topic we studied next.\\n4.5\\nTraining Efﬁciency\\nIn this section we focus on the training efﬁciency of MoE Transformer models. So far, we have\\nseen empirical evidence how scaling the models along various axes bring dramatic quality gains,\\nand studied the factors affecting the extent of the improvements. In order to measure the training\\nefﬁciency, we ﬁrst keep track of the number of tokens being processed to reach a certain training loss\\nand second we keep track of the wall-clock time for a model to process certain number of tokens.\\nNote that, we focus on the training time and training loss13 while varying other factors, as opposed to\\ntest error, which we analyzed in the previous section.\\nDeeper models are more sample efﬁcient, converge faster with fewer examples\\nIt has been\\nshown that, deeper models are better at sample efﬁciency, reaching better training/test error given the\\nsame amount of training examples [15, 56], commonly attributed to the acceleration effect of over-\\nparametrization [1]. We empirically test the hypothesis again using GShard with MoE Transformers\\nand share trade-offs for models that are not only deep, but also sparsely activated.\\n13Training loss reported in this section corresponds to cross-entropy loss and excludes the auxiliary loss term\\nintroduced in Section 2.2\\n18\\nFor this purpose, we compare number of tokens being processed by each model to reach a preset\\ntraining loss. A general trend we observe from Table 2 is that, MoE Transformer models with 3 times\\nthe depth need 2 to 3 times fewer tokens to reach the preset training loss thresholds. For example\\nMoE(128E, 12L) takes 3 times the number of tokens to reach 0.7 training cross-entropy compared to\\nMoE(128E, 36L), (6) vs (5). We observe a similar trend for models with 512 and 2048 experts, (4) vs\\n(3) and (2) vs (1).\\nId\\nModel\\nCores\\nBillion tokens to\\ncross-entropy of\\n0.7\\n0.6\\n0.5\\n(1)\\nMoE(2048E, 36L)\\n2048\\n82\\n175\\n542\\n(2)\\nMoE(2048E, 12L)\\n2048\\n176\\n484\\n1780\\n(3)\\nMoE(512E, 36L)\\n512\\n66\\n170\\n567\\n(4)\\nMoE(512E, 12L)\\n512\\n141\\n486\\n-\\n(5)\\nMoE(128E, 36L)\\n128\\n321\\n1074\\n-\\n(6)\\nMoE(128E, 12L)\\n128\\n995\\n-\\n-\\nTable 2: The number of tokens have been seen by a model during training to reach three different\\ncross-entropy loss. A general trend is that deeper models are more sample efﬁcient and converge\\nfaster than the comparable shallow ones.\\nAnother intriguing observation from Table 2, is again related to the presence of capacity bottleneck.\\nComparing the models with same depth, (5), (3) and (1), we notice a signiﬁcant drop in the number\\nof tokens required to reach training loss of 0.7, as we transition from 128 to 512 number of experts.\\nPractically that is where we observed the capacity bottleneck was residing, aligning with the hypothe-\\nsis in Section 4.4. After this phase shift, models with ample capacity tend to exhibit similar sample\\nefﬁciency characteristics, as in models (3) and (1).\\nLargest model (600B) can be trained under 4 days achieving the best quality\\nNext we delve\\ndeeper into the interaction between model size and wall-clock time spent for training. We monitor\\nnumber of TPU cores being used, training steps per-second, total number of tokens per batch, TPU\\ncore years14, and actual wall-clock time spent in days for training (see Table 3 columns respectively).\\nWe start with investigating one of the largest models we trained, MoE(2048E, 36L) with 600 billion\\nparameters, model with id (1). Having utilized 2048 TPU cores for 4 days, this model achieves the\\nbest translation quality in terms of average BLEU, but also takes a total of 22.4 TPU years to train.\\nWhile we have not seen any signs that the quality improvements plateau as we scale up our models,\\nwe strive for ﬁnding cost-effective solutions for scaling.\\nResults in Table 3 again validates scaling with conditional computation is way more practical\\ncompared to dense scaling. Given the same number of TPU cores used by (1), the dense scaling\\nvariant, T(96L), appears to be taking more than ten times to train (235 TPU core years), while trailing\\nbehind in terms of model quality compared to models trained with GShard.\\nId\\nModel\\nCores\\nSteps\\nper sec.\\nBatch sz.\\n(Tokens)\\nTPU core\\nyears\\nTraining\\ntime (days)\\nBLEU\\navg.\\n(1)\\nMoE(2048E, 36L)\\n2048\\n0.72\\n4M\\n22.4\\n4.0\\n44.3\\n(2)\\nMoE(2048E, 12L)\\n2048\\n2.15\\n4M\\n7.5\\n1.4\\n41.3\\n(3)\\nMoE(512E, 36L)\\n512\\n1.05\\n1M\\n15.5\\n11.0\\n43.7\\n(4)\\nMoE(512E, 12L)\\n512\\n3.28\\n1M\\n4.9\\n3.5\\n40.0\\n(5)\\nMoE(128E, 36L)\\n128\\n0.67\\n1M\\n6.1\\n17.3\\n39.0\\n(6)\\nMoE(128E, 12L)\\n128\\n2.16\\n1M\\n1.9\\n5.4\\n36.7\\n*\\nT(96L)\\n2048\\n-\\n4M\\n∼235.5\\n∼42\\n36.9\\nTable 3: Performance of MoE models with different number of experts and layers.\\nIn this section, we benchmarked GShard with MoE Transformers applications to multilingual machine\\ntranslation (in particular to M4). We identiﬁed variables that are affecting the end result, such as\\n14TPU core years is simply measured by the product of number of cores and wall-clock time in years.\\n19\\nMemory usage in GB\\n0\\n5\\n10\\n15\\nMoE(128E, 12L)\\nMoE(512E, 12L)\\nMoE(2048E, 12L)\\nMoE(2048E, 24L)\\nMoE(2048E, 36L)\\nMoE(2048E, 60L)\\nActivation\\nWeight\\nFigure 7: Per-device memory consumption in gigabytes.\\ncapacity bottleneck, positive transfer and training efﬁciency, and provided experimental results in\\norder to reveal the interplay between them. Next we will delve deep into performance related topics\\nof GShard, such as memory and runtime efﬁciency and communication benchmarks.\\n5\\nPerformance and Memory Consumption\\nThis section discusses how well GShard achieves computation and memory efﬁciency on the TPU\\nplatform. Our measurement and analysis show that the device memory consumption is roughly\\nconstant when we increase the number of devices and experts, and the step time grows sublinearly,\\ni.e., 1.7x execution time increase when we scale the model by 16x from 128 devices to 2048 devices.\\nWe also provide microbenchmarks and analyses for a variety of partitioned operators, which could\\nguide use cases beyond this paper.\\n5.1\\nMemory Efﬁciency and Scalability\\nIn the GShard model, there are mainly three types of memory usage, all of which have constant\\nper-device sizes after SPMD partitioning, when the number of experts increases.\\n• Replicated weights (e.g. transformer feed-forward layers).\\n• Distributed weights (MoE feed-forward layers15).\\n• Activations (output of each layer that is used in both forward and backward pass).\\nThe O(1) memory scaling is demonstrated in Figure 7, which shows the per-device memory usage\\ndistribution for different models. With a ﬁxed number of layers, both weight memory and activation\\nmemory stay constant when the number of experts increases.\\nOn this other hand, weight memory and activation memory both scale linearly with the number of\\nlayers. When the memory requirement exceeds available memory on each device, compiler-based\\nrematerialization will automatically recompute part of the activations in the backward pass in order\\nto reduce peak activation memory. This is why the activation size for MoE(2048E, 60L) is smaller\\nthan MoE(2048E, 36L). The overhead of rematerialization is also optimized, e.g. only 28% and 34%\\nof the total cycles are spent on recomputation for 36L and 60L models respectively, and 0% for 12L\\nand 24L since they ﬁt in device memory without rematerialization.\\n20\\nRoofline\\nMeasured\\n \\nRoofline\\nMeasured\\n \\nRoofline\\nMeasured\\nMicroseconds\\n0\\n5000\\n10000\\n15000\\nMoE(128E, 36L)\\nMoE(512E, 36L)\\nMoE(2048E, 36L)\\nMoE fflayer\\nMoE dispatch and combine\\nGate cumsum\\nGate Einsums\\nTransformer fflayer\\nTransformer attention\\nTransformer projection\\nFigure 8: Measured vs rooﬂine execution time breakdown. Only the forward pass is shown, and\\nthe backward pass has similar breakdown. “MoE dispatch and combine” represents cross-partition\\ncommunication with AllToAll.\\n5.2\\nRuntime Efﬁciency and Scalability\\nFigure 8 shows the breakdown of execution time for an MoE layer and its adjacent Transformer layer.\\nIt also compares the achieved performance to a rooﬂine, which is estimated by assuming compute-,\\nmemory-, or communication-bounded operations can achieve 100% of the peak FLOPS, memory\\nbandwidth, or interconnect bandwidth. This is a very optimistic estimate as many operators are\\nbounded by a mixed set of resources. At a smaller scale (128 experts), our model can achieve > 70%\\nof the rooﬂine performance. The device time increases by 1.7x when we scale the model to 16x larger\\n(2048 experts), and can still achieve 48% of the rooﬂine performance.\\nBefore analyzing performance scalability, we recall the size scaling of relevant tensor dimensions as\\ndiscussed in Section 3.1. With D devices, the number of experts E and the group count G are both\\nset to O(D). The fractional per-group expert capacity C is set to O(1/D). This setup cannot scale\\nindeﬁnitely, since C needs to be at least 1, but it is good enough to scale to thousands of experts.\\nTransformer layers and MoE feed-forward layer\\nThese are the dense parts of the model, which\\nare designed to achieve peak TPU utilization. On each device, these computations also have a\\nconstant cost when we scale to more experts. Feed-forward layers and Transformer projections are\\nmainly large matrix multiplications that utilize the TPU’s matrix unit well. These operations have\\nachieved > 85% peak FLOPS in our experiment. The attention operations are composed of mainly\\nbatch matmuls, which are bounded by memory bandwidth when sequence lengths are small. As a\\nresult, in our experiments attention operations only achieved > 30% peak FLOPS.\\nGate computation\\nIn Figure 8, “Gate Einsum” represents the ﬁrst two and the last Einsums in\\nAlgorithm 2. The ﬁrst Einsum is the projection that calculates per-expert input to softmax. It has an\\nO(D) cost, but it is a very small part of the layer. The other two Einsums are dispatching tokens\\nand combining expert results. They effectively implement Gather with one-hot matrices, which\\nare more expensive, but with constant O(GC) = O(1) cost that is independent from the number of\\nexperts. The execution time of these Einsums increases by around 2x when we scale from 128 to\\n2048 experts (16x).\\nThe remaining per-device gating computation involves many general-purpose computations like\\nArgMax and Cumsum, which are either memory-bound or even sequential in nature, thus not designed\\nto utilize TPUs well. The majority of the time is spent on sequential Cumsum operations to invert\\none-hot matrices that represent selected experts for each token to one-hot matrices that represent\\n15Gate projection weights are O(E) in size and could be partitioned, but in practice they are small enough to\\nbe replicated and only have negligible effect on peak memory usage.\\n21\\nselected tokens for each expert. The linear complexity of Cumsum is demonstrated in Figure 8. This\\npart of the gating computation also has an O(D) cost, but fortunately, similar to the Einsum before\\nsoftmax, it has a very small constant factor. It has negligible execution time with 128 experts, and\\ntakes less than 10% of the total time spent in the MoE and Transformer layers with 2048 experts.\\nThe most signiﬁcant part of gating is communication, shown as “MoE dispatch and combine” in\\nFigure 8. These are AllToAll operators, and as we will discuss in Section 5.3, their cost is O(\\n√\\nD).\\nWhen the number experts grows 16x from 128 to 2048, the execution time increases by about 3.75x,\\nand their proportion of execution time in the MoE and Transformer increases from 16% to 36%.\\n5.3\\nCommunication Microbenchmarks and Per-Operator Scalability\\nIn this section, we measure and analyze the performance scalability of the SPMD partitioner for basic\\noperators, which can be used to guide use cases beyond the MoE model presented in this paper.\\nPerformance scaling of communication primitives\\nTwo critical collective communication oper-\\nators in the MoE model are AllReduce and AllToAll. AllReduce is used in accumulating partial\\nresults, and AllToAll is used in resharding (Section 3.3.2). Figure 9 shows their performance\\nscalability from 16 to 2048 partitions. AllReduce on TPU has an execution time independent from\\nthe number of devices. The variance in Figure 9 is due to speciﬁcs of each topology, e.g., whether it\\nis a square or a rectangle, and whether it is a torus or a mesh.\\nAllToAll, on the other hand, gets more expensive as the number of partitions grows, but in a\\nsublinear manner. On our 2D TPU cluster, AllToAll cost is roughly O(\\n√\\nD), where D is the\\nnumber of partitions. This is because with a ﬁxed amount of data each partition sends (8MB or 32MB\\nin Figure 9), the total amount of data that all partitions send is d = O(D). Meanwhile, each data\\npiece needs to travel h = O(\\n√\\nD) hops on average, and there are overall l = O(D) device-to-device\\nlinks in the network. Therefore, if it is bandwidth-bound, the execution time of an AllToAll is\\nt = dh\\nl\\n= O(D\\n√\\nD\\nD\\n) = O(\\n√\\nD).\\nEven if it is latency-bound, the execution time will still be O(h) = O(\\n√\\nD). Comparing 2048\\npartitions and 16 partitions, while D grows by 128 times, the execution time of AllToAll only\\nincreases by 9 times. This enables us to use resharding to efﬁciently implement cross-partition\\ndispatching (Figure 4a).\\nAllGather and CollectivePermute are easier to analyze. AllGather’s output is D larger than the\\ninput, and if we ﬁx input size, then its communication cost is O(D). CollectivePermute has a one-\\nto-one communication pattern, and with reasonable device arrangement where the source-destination\\npairs are close, its cost is O(1) for a ﬁxed input size.\\nO(D)\\nTotal\\nPer-partition\\nDimensions\\nCompute\\nCompute\\nCommunication\\nAdd(A,A->A)\\nA\\nO(D)\\nO(1)\\n0\\nMatmul(AB,BC->AC)\\nB\\nO(D)\\nO(1)\\nO(1) AR\\nMatmul(AB,BC->AC)\\nA\\nO(D)\\nO(1)\\n0\\nMatmul(AB,BC->AC)\\nA,B\\nO(D2)\\nO(D)\\nO(D) AG or CP\\nMatmul(AB,BC->AC)\\nA,C\\nO(D2)\\nO(D)\\nO(D) AG or CP\\nReduce(AB->A)\\nA\\nO(D)\\nO(1)\\n0\\nReduce(AB->B)\\nA\\nO(D)\\nO(1)\\nO(1) AR\\nEinsum(GSEC,GSM->EGCM)\\nG,E *\\nO(D)\\nO(1)\\nO(\\n√\\nD) AA\\nConvolution(BIXY,xyIO->BOXY)\\nX **\\nO(D)\\nO(1)\\nO(1) CP\\nTable 4: Scalability of partitioned operators. Abbreviation for communication primitives: AR:\\nAllReduce, AG: AllGather, CP: CollectivePermute, AA: AllToAll. *This is the dispatch\\nEinsum in our model, where we set C to O(1/D). **I/O are the input/output feature dimensions, B\\nis the batch dimension, X/Y are input spatial dimensions, and x/y are the kernal spatial dimensions.\\n22\\nNumber of partitions (N)\\nMicroseconds\\n100\\n500\\n1000\\n5000\\n10000\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nAllToAll 8MB\\nAllReduce 8MB\\nAllToAll 32MB\\nAllReduce 32MB\\nO(sqrt(N)) \\nFigure 9: Performance scaling of communication, AllReduce and AllToAll. Log scale on both\\naxes. AllReduce cost is roughly O(1), and AllToAll cost is roughly O(\\n√\\nD), where D is the\\nnumber of partitions. We measure their performance with 8MB and 32MB data. For AllToAll, that\\nmeans each partition initially has 8MB (or 32MB) data, then divides it to D pieces, and sends each\\npiece to a different receiving partition.\\nPartitioned operator scalability\\nWe summarize the performance scalability for common operators\\nusing GShard in Table 4. It contains the Einsum/Matmul examples in Section 3.3.2, and also other\\ncommon operators like Convolution and Reduce. The table includes the local compute on each\\npartition, as well as the required communication based on our analysis above.\\nMost operators in Table 4 have sublinear scalability in terms of both compute and communication,\\nwhich is consistent with our performance measurement of the MoE model. The O(1) scaling of\\nspatially partitioned convolutions also demonstrates the efﬁciency of GShard for image partitioning\\n(Appendix A.4).\\nHowever, the last two Matmul operators in Table 4 have O(D) scaling of per-partition compute and\\ncommunication, where they have unmatched sharding in the operands. This is not due to inefﬁciency\\nin the partitioning algorithm, but because the total compute in the full operator is very large (O(D2)).\\nDifferent partitioning strategies can be used for these cases, producing different communication\\nprimitives: replicating one operand will result in AllGather (requiring the replicated operand to ﬁt\\nin device memory), while slicing in a loop (Figure 4c) will result in CollectivePermute.\\n6\\nRelated Work\\nNeural networks Deep learning models have been very successful in advancing sub-ﬁelds of artiﬁcial\\nintelligence. For years, the ﬁelds have been continuously reporting new state of the art results\\nusing varieties of model architectures for computer vision tasks [57, 58, 7], for natural language\\nunderstanding tasks [59, 60, 61], for speech recognition and synthesis tasks [62, 63, 64, 65, 66]. More\\nrecently, attention-based Transformer models further advanced state of the art of these ﬁelds [10, 4].\\nModel scaling Both academic research and industry applications observed that larger neural networks\\ntend to perform better on large enough datasets and for complex tasks. Within a single model family,\\nsimply making the network wider or deeper often improves the model quality empirically. E.g., deeper\\nResNets performed better [8], bigger Transformer models achieved better translation quality [10],\\nmodels with larger vocabulary, or embedding or feature crosses work better, too [14, 13]. Across\\n23\\ndifferent model families, it has also been observed that bigger models with larger model capacities not\\nonly ﬁt the training data better but also generalize better on test time [67, 68, 15]. This observation\\nmotivated many research efforts to build much bigger neural networks than those typically used in\\ndeep learning research models or production models. Shazeer et al. showed that a recurrent language\\nmodel with 69 billion parameters using mixture-of-expert layers achieved much lower test perplexity\\nfor the one billion words (LM1B) benchmark [16]. Brown et al. showed that a non-sparse 175\\nbillion parameters model is capable of exhibiting highly accurate few-shot performance on several\\ndownstream NLP tasks.\\nHardware Neural networks demand non-negligible amounts of computation power. To address\\nsuch a demand, special hardware (chips and networked machines) built for neural network training\\nand inference can be dated back to 25 years ago [69]. Since late 2000s, researchers started to\\nleverage GPUs to accelerate neural nets [70, 57, 71]. More recently, the industry also invested\\nheavily in building more dedicated hardware systems chasing for more cost-effective neural network\\nhardware [72]. Because the core computation of neural networks (various forms of summation\\nof multiplications: convolution, matrix multiplication, einsum) are highly parallelizable numerical\\ncalculations, these chips are equipped with huge number of ﬂoating processing units (FPUs). Hence,\\nthe compute power of these specially designed hardware grew dramatically. It is reported that GPU\\nprice per ﬂops dropped a factor of ten in just the last 4 years [73] and ﬂops per watts increased by 2\\nmagnitude over the past 12 years [74]. The widely available low-cost computation power is a major\\nenabler for the success of neural networks.\\nSoftware Software systems supporting neural networks evolved together with the advancement of the\\nunderlying hardware [75, 76, 21, 77]. While the accelerators are highly parallel compute machines,\\nthey are signiﬁcantly more difﬁcult to program directly. The frameworks made building neural\\nnetworks easier and abstracted away many hardware speciﬁc details from the practitioners. They in\\nturn rely on lower-level libraries to drive special hardware (accelerators) efﬁciently. E.g., CUDA [78]\\nfor Nvidia’s GPUs, or XLA for Google’s TPUs [28]. These lower-level libraries are critical for\\nachieving high efﬁciency using these special hardware.\\nParallelism in model training and inference Modern neural networks make extensive use of a\\ncluster of machines for training and inference, each of which equiped with several accelerators.\\nData parallelism [57] is the most commonly used approach and is supported by major frameworks\\n(TensorFlow [21], PyTorch [22], JAX [79, 80]), where devices run the same program with different\\ninput data and combine their local gradients before the weight updates. Model parallelism on the other\\nhand, partitions computation beyond the input batch, which is needed to build very large models. For\\nexample, pipelining [15, 24] splits a large model’s layers into multiple stages, while operator-level\\npartitioning [23, 81] splits individual operators into smaller parallel operators. GShard used a type of\\noperator-level partitioning to scale our model to a large number of parallel experts.\\nAutomated parallelism Because programming in a distributed heterogeneous environment is chal-\\nlenging, particularly for high-level practitioners, deep-learning frameworks attempt to alleviate the\\nburden of their users from specifying how the distributed computation is done. For example, Tensor-\\nFlow [21] has support for data parallelism, and basic model parallelism with graph partitioning by\\nper-node device assignment. Mesh TensorFlow [23] helps the user to build large models with SPMD-\\nstyle per-operator partitioning, by rewriting the computation in a Python library on top of TensorFlow;\\nin comparison, our approach partitions the graph in the compiler based on light-weight annotations\\nwithout requiring the user to rewrite the model. FlexFlow [81] uses automated search to discover\\nthe optimal partition of operators in a graph for better performance; while it focuses on determining\\nthe partitioning policy, our SPMD partitioner focuses on the mechanisms to transform an annotated\\ngraph. Weight-update sharding [82] is another automatic parallelization transformation based on\\nXLA, which mostly focuses on performance optimizations for TPU clusters, and conceptually can\\nbe viewed as a special case for GShard. Zero [83] presents a set of optimizations to reduce memory\\nredundancy in parallel training devices, by partitioning weights, activations, and optimizer state\\nseparately, and it is able to scale models to 170 billion parameters; in comparison, GShard is more\\ngeneral in the sense that it does not distinguish these tensors, and all of those speciﬁc partitioning\\ntechniques can be supported by simply annotating the corresponding tensors, allowing us to scale to\\nover 1 trillion parameters and explore more design choices.\\nConditional Computation and Machine Translation Conditional computation [25, 16, 26, 27]\\npremises that the examples should be routed within the network by activating an input dependent sub-\\n24\\nnetwork. The routing depends (or conditions) on certain criterion and without the loss of generality,\\ncan be any of the following: estimated difﬁculty of the example [84], available computation budget\\n[26, 27], or more generally a learned criterion with sparsity induced mixture of experts [16]. We\\nextend sparsely gated mixture of experts [16] due to its ﬂexibility and ease of scaling to state of the\\nart neural sequence models, Transformers [10], to satisfy training efﬁciency.\\n7\\nConclusion\\nIn this paper, we introduced GShard, a deep learning module that partitions computation at scale\\nautomatically. GShard operates with lightweight sharding annotations required in the user model\\ncode only and delivers an easy to use and ﬂexible API for scaling giant neural networks. We applied\\nGShard to scale up Transformer architecture with Sparsely-Gated Mixture-of-Experts layers (MoE\\nTransformer) and demonstrated a 600B parameter multilingual neural machine translation model\\ncan efﬁciently be trained in 4 days achieving superior performance and quality compared to prior art\\nwhen translating 100 languages to English with a single model. In addition to the far better translation\\nquality, MoE Transformer models trained with GShard also excel at training efﬁciency, with a\\ntraining cost of 22 TPU v3 core years compared to 29 TPU years used for training all 100 bilingual\\nTransformer baseline models. Empirical results presented in this paper conﬁrmed that scaling models\\nby utilizing conditional computation not only improve the quality of real-world machine learning\\napplications but also remained practical and sample efﬁcient during training. Our proposed method\\npresents a favorable scalability/cost trade-off and alleviates the need for model-speciﬁc frameworks or\\ntools for scaling giant neural networks. Together, our results help to elucidate a realistic and practical\\nway forward for neural network scaling to achieve better model quality.\\nWe have learned several lessons from our study. Our results suggest that progressive scaling of\\nneural networks yields consistent quality gains, validating that the quality improvements have not\\nyet plateaued as we scale up our models. While the results in this paper consolidate that model\\nscaling is a must in deep learning practitioners’ toolbox, we also urge practitioners to strive for\\ntraining efﬁciency. To this end, we identiﬁed factors that affect the training efﬁciency and showed\\ntheir implications on downstream task quality. We demonstrated how the neural networks built with\\nconditional computation yield a favorable trade-off between scale and computational cost. In practice\\nsuch critical design decisions allowed us to enjoy experimental cycles of not months or weeks, but\\nonly days to train models in the order of magnitude of trillion parameters.\\nFurther, having a proper abstraction layer that separates model description from parallelization\\nimplementation, allows model developer to focus on network implementation, leaving GShard to\\npartition the computation graphs automatically and generate programs that run on all devices in\\nparallel. We found that generating a single program that is general enough to express computation\\non all underlying parallel devices is the key to compile scalably. The traditional way of generating\\nmultiple dedicated programs for different partitions results in explosive compilation time when\\nscaling to thousands of partitions. To address this complexity, we introduced various compiler\\nrenovations based on SPMD sharding that allows any tensor dimension to be partitioned. As a\\ntakeaway, we emphasize that model scaling and training efﬁciency should go hand-in-hand; and\\nalgorithmic improvements such as conditional computation when coupled with easy to use interfaces\\ncan effectively utilize large computational power.\\nLastly, our experimental results empirically support that, mere parameter counting does not always\\ncorrelate with the effective capacity of the models at scale [85, 86]. Comparison of the models should\\nalso account in the nature of the problem, i.e. massively multi-task setting with a heavy training data\\nimbalance across tasks as in our case, and control the factors affecting different operation modes of\\nthe networks, i.e. capacity bottleneck vs positive transfer.\\nAcknowledgements\\nWe would like to thank the Google Brain and Translate teams for their useful input and insightful\\ndiscussions, entire XLA and Lingvo development teams for their foundational contributions to this\\nproject. In particular Youlong Cheng, Naveen Arivazhagan, Ankur Bapna, Ruoming Pang, Yonghui\\nWu, Yuan Cao, David Majnemer, James Molloy, Peter Hawkins, Blake Hechtman, Mark Heffernan,\\n25\\nDimitris Vardoulakis, Tamas Berghammer, Marco Cornero, Cong Liu, Tong Shen, Hongjun Choi,\\nJianwei Xie, Sneha Kudugunta, and Macduff Hughes.\\nReferences\\n[1] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit\\nacceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.\\n[2] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable\\nneural networks. arXiv preprint arXiv:1803.03635, 2018.\\n[3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361, 2020.\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n[5] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\\npretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages\\n181–196, 2018.\\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 770–778, 2016.\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\\nnetworks. In European conference on computer vision, pages 630–645. Springer, 2016.\\n[9] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid\\narchitecture for object detection. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 7036–7045, 2019.\\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.\\n[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer, 2019.\\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\\nzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\\nUnsupervised cross-lingual representation learning at scale, 2019.\\n[14] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng\\nChen, and Yonghui Wu. Massively multilingual neural machine translation in the wild: Findings\\nand challenges, 2019.\\n[15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, Hy-\\noukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efﬁcient\\ntraining of giant neural networks using pipeline parallelism. Advances in Neural Information\\nProcessing Systems 32, pages 103–112, 2019.\\n26\\n[16] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[17] Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in\\nneural networks, 2017.\\n[18] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is\\npredictable, empirically, 2017.\\n[19] Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy. Pro-\\nceedings of the 24th Symposium on Principles and Practice of Parallel Programming, Feb\\n2019.\\n[20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’ Ascoli,\\nGiulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization\\nwith number of parameters in deep learning. Journal of Statistical Mechanics: Theory and\\nExperiment, 2020(2):023401, Feb 2020.\\n[21] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: a system for\\nlarge-scale machine learning. In OSDI, volume 16, pages 265–283, 2016.\\n[22] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. 2017.\\n[23] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow:\\nDeep learning for supercomputers. In Advances in Neural Information Processing Systems,\\npages 10414–10423, 2018.\\n[24] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg\\nGanger, and Phil Gibbons. Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\n[25] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computa-\\ntion in neural networks for faster models, 2015.\\n[26] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer.\\nArXiv, abs/1910.10073, 2020.\\n[27] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Controlling computation versus quality\\nfor neural sequence models, 2020.\\n[28] XLA: Optimizing Compiler for TensorFlow. https://www.tensorflow.org/xla, 2019.\\nOnline; accessed 1 June 2020.\\n[29] Vinod Nair and Geoffrey E. Hinton.\\nRectiﬁed linear units improve restricted boltzmann\\nmachines. In ICML, 2010.\\n[30] Albert Einstein. Die grundlage der allgemeinen relativitätstheorie. In Das Relativitätsprinzip,\\npages 81–124. Springer, 1923.\\n[31] Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia Xu Chen, Ye Jia, Anjuli\\nKannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et al. Lingvo: a modular and scalable\\nframework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295, 2019.\\n[32] Youlong Cheng, HyoukJoong Lee, and Tamas Berghammer.\\nTrain ML models on\\nlarge images and 3D volumes with spatial partitioning on Cloud TPUs.\\nhttps:\\n//cloud.google.com/blog/products/ai-machine-learning/train-ml-models-\\non-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus,\\n2019. Online; accessed 12 June 2020.\\n27\\n[33] ONNX: Open Neural Network Exchange. https://github.com/onnx/onnx, 2019. Online;\\naccessed 1 June 2020.\\n[34] Jared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa Kirisame, Tianqi Chen,\\nand Zachary Tatlock. Relay: a new ir for machine learning frameworks. Proceedings of the 2nd\\nACM SIGPLAN International Workshop on Machine Learning and Programming Languages -\\nMAPL 2018, 2018.\\n[35] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer Deng, Roman\\nDzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein, Jack Mont-\\ngomery, Bert Maher, Satish Nadathur, Jakob Olesen, Jongsoo Park, Artem Rakhov, Misha\\nSmelyanskiy, and Man Wang. Glow: Graph lowering compiler techniques for neural networks,\\n2018.\\n[36] MPI Forum. MPI: A Message-Passing Interface Standard. Version 2.2, September 4th 2009.\\navailable at: http://www.mpi-forum.org (Dec. 2009).\\n[37] Minsik Cho, Ulrich Finkler, and David Kung. BlueConnect: Decomposing All-Reduce for\\nDeep Learning on Heterogeneous Network Hierarchy. In Proceedings of the Conference on\\nSystems and Machine Learning (SysML), Palo Alto, CA, 2019.\\n[38] Lynn Elliot Cannon. A Cellular Computer to Implement the Kalman Filter Algorithm. PhD\\nthesis, USA, 1969. AAI7010025.\\n[39] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine\\ntranslation with a shared attention mechanism. Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, 2016.\\n[40] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen,\\nNikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, and et al.\\nGoogle’s\\nmultilingual neural machine translation system: Enabling zero-shot translation. Transactions of\\nthe Association for Computational Linguistics, 5:339–351, Dec 2017.\\n[41] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine\\ntranslation. CoRR, abs/1903.00089, 2019.\\n[42] Exploring massively multilingual, massive neural machine translation.\\nhttps://ai.\\ngoogleblog.com/2019/10/exploring-massively-multilingual.html.\\nAccessed:\\n2020-06-05.\\n[43] Recent advances in google translate.\\nhttps://ai.googleblog.com/2020/06/recent-\\nadvances-in-google-translate.html. Accessed: 2020-06-05.\\n[44] Timothy T Baldwin and J Kevin Ford. Transfer of training: A review and directions for future\\nresearch. Personnel psychology, 41(1):63–105, 1988.\\n[45] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\\nthrough stochastic neurons for conditional computation, 2013.\\n[46] Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward compu-\\ntation in deep neural networks, 2013.\\n[47] Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. Large scale parallel\\ndocument mining for machine translation. In Proceedings of the 23rd International Conference\\non Computational Linguistics, COLING ’10, page 1101–1109, USA, 2010. Association for\\nComputational Linguistics.\\n[48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\\ncomputational linguistics, pages 311–318. Association for Computational Linguistics, 2002.\\n[49] Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. Training deeper neural\\nmachine translation models with transparent attention. Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing, 2018.\\n28\\n[50] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling with deep\\ntransformers. Interspeech 2019, Sep 2019.\\n[51] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, 2019.\\n[52] David R. So, Chen Liang, and Quoc V. Le. The evolved transformer, 2019.\\n[53] Using bﬂoat16 with TensorFlow models.\\nhttps://cloud.google.com/tpu/docs/\\nbfloat16, 2020. Online; accessed 12 June 2020.\\n[54] Heng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing\\nLiu, Hemal Shah, Levent Koc, Jeremiah Harmsen, and et al. Wide and deep learning for\\nrecommender systems. Proceedings of the 1st Workshop on Deep Learning for Recommender\\nSystems - DLRS 2016, 2016.\\n[55] Andrew K. Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and\\ntransfer learning in deep linear networks, 2018.\\n[56] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model\\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\\n[57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep\\nconvolutional neural networks. In Advances in neural information processing systems, pages\\n1097–1105, 2012.\\n[58] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9,\\n2015.\\n[59] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\nnetworks. In Advances in neural information processing systems, pages 3104–3112, 2014.\\n[60] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\n[61] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[62] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared views of four research groups.\\nIEEE Signal processing magazine, 29(6):82–97, 2012.\\n[63] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural\\nnetwork for large vocabulary conversational speech recognition. In 2016 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 4960–4964. IEEE,\\n2016.\\n[64] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng\\nChen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-the-art\\nspeech recognition with sequence-to-sequence models. In 2018 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP), pages 4774–4778. IEEE, 2018.\\n[65] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative\\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016.\\n29\\n[66] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,\\nZhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by\\nconditioning wavenet on mel spectrogram predictions. In 2018 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP), pages 4779–4783. IEEE, 2018.\\n[67] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\\ndeep learning requires rethinking generalization. 2017.\\n[68] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring\\ngeneralization in deep learning, 2017.\\n[69] Paolo Ienne, Thierry Cornu, and Gary Kuhn. Special-purpose digital hardware for neural\\nnetworks: An architectural survey. Journal of VLSI signal processing systems for signal, image\\nand video technology, 13(1):5–25, 1996.\\n[70] Rajat Raina, Anand Madhavan, and Andrew Y Ng. Large-scale deep unsupervised learning\\nusing graphics processors. In Proceedings of the 26th annual international conference on\\nmachine learning, pages 873–880, 2009.\\n[71] Dan Claudiu Cire¸\\nsan, Ueli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber. Deep, big,\\nsimple neural nets for handwritten digit recognition. Neural computation, 22(12):3207–3220,\\n2010.\\n[72] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance\\nanalysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium\\non Computer Architecture, pages 1–12, 2017.\\n[73] 2019 recent trends in GPU price per FLOPS. https://aiimpacts.org/2019-recent-\\ntrends-in-gpu-price-per-flops/. Accessed: 2020-06-05.\\n[74] Yifan Sun, Nicolas Bohm Agostini, Shi Dong, and David Kaeli. Summarizing cpu and gpu\\ndesign trends with product data. arXiv preprint arXiv:1911.11313, 2019.\\n[75] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio\\nRanzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In\\nAdvances in neural information processing systems, pages 1223–1231, 2012.\\n[76] Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud\\nBergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features\\nand speed improvements. arXiv preprint arXiv:1211.5590, 2012.\\n[77] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. 2017.\\n[78] John Nickolls, Ian Buck, Michael Garland, and Kevin Skadron. Scalable parallel programming\\nwith cuda. Queue, 6(2):40–53, 2008.\\n[79] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\\nMaclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy\\nprograms. 2018.\\n[80] Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via\\nhigh-level tracing. In Machine Learning and Systems (MLSys), 2018.\\n[81] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond Data and Model Parallelism for Deep\\nNeural Networks. In Proceedings of the Conference on Systems and Machine Learning (SysML),\\nPalo Alto, CA, 2019.\\n[82] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman, and Shibo\\nWang. Automatic cross-replica sharding of weight update in data-parallel training, 2020.\\n30\\n[83] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory op-\\ntimization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054,\\n2019.\\n[84] Loren Lugosch, Derek Nowrouzezahrai, and Brett H. Meyer. Surprisal-triggered conditional\\ncomputation with neural networks, 2020.\\n[85] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic\\ndimension of objective landscapes, 2018.\\n[86] Wesley J. Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter\\ncounting in deep models: Effective dimensionality revisited, 2020.\\n[87] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150, 2019.\\n[88] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\\ncost. ArXiv, abs/1804.04235, 2018.\\n[89] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\\ntokenizer and detokenizer for neural text processing. In EMNLP, 2018.\\nA\\nAppendix\\nA.1\\nDecoding with Flat Beam Search\\nDuring decoding, we use beam search with length normalization similar to [61]. Decoding is auto-\\nregressive and generates the target sequence one token at a time, so for an output of length m the\\ndecoder layer stack is executed m times, sequentially. In particular for each decoder MoE layer there\\nare dispatch/combine operations, which require cross-device communication. Inference utilizes same\\ncluster with same number of devices as training.\\nDuring beam search we ﬂatten the beam hypotheses into a single sequence which contains all\\nunderlying tokens interleaved, and we modify decoder self-attention mask so that each hypothesis\\nonly has attention to appropriate positions in the joint ﬂat sequence. We apply the same transformation\\nto key/value tensors maintained by each decoder self-attention layer. This allows us to avoid reordering\\npreviously computed attention key/values after each beam expansion. Instead, we only reorder the\\n0/1 mask representing the current active hypotheses. However, attention becomes k times longer.\\nThis trade-off can be positive or negative depending on implementation details. As explained in\\n[87], memory bandwidth limits are important for incremental decoding with Transformer models.\\nFrom this point of view, by ﬂattening the beam we replace two operations with low compute/memory\\nratio (attention dot product and key/value reordering) with a single operation with a slightly higher\\ncompute/memory ratio (attention dot product over a longer sequence with more keys), but with the\\nsame total amount of memory it has to access.\\nA.2\\nMachine Translation Experiments Details\\nIn our Machine Translation experiments MoE Transformer models shared a) 1024 Transformer model\\ndimension b) 8192 Feed Forward and MoE hidden dimension; c) 16 heads in multi-head attention; d)\\n128 attention key and value dimension; and e) 0.1 input, residual and attention dropout rate.\\nWe used the Adafactor [88] optimizer with a) factored second-moment estimation; b) ﬁrst moment\\ndecay β1 = 0.0; c) second moment decay β2 = 0.99 with 1 −t−0.8 schedule; d) update clipping\\nthreshold of 1.0; and e) 1.0 learning rate with square root decay after 10k training steps.\\nWe used SentencePiece [89] subword tokenizer with a single multilingual vocabulary for source-side\\nspanning 102 languages of size 64000, and English-only target-side vocabulary of size 32000.\\n31\\nA.3\\nGeneral Sharding API\\nIn addition to the two common APIs (replicate() and split()) for sharding listed in Section 3.2,\\nusers or the compiler may use a more advanced sharding strategy to minimize data transfers.\\nshard(tensor, device_assignment) annotates tensor to be partitioned with the provided device\\nassignment, and returns the annotated tensor. We use device assignment, a multi-dimensional integer\\narray, to represent how the split is done. device_assignment has the same rank as the data tensor;\\nits element count is the total number of partitions, and each element is the ID of the device that\\noccupies the corresponding data slice. For example, a 3D tensor with shape [3, 16, 64] with device\\nassignment shape [1, 2, 4] will have partition shape [3, 8, 16], and the order of elements in device\\nassignment determines which slice each partition occupies.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n5\\n4\\n2\\n3\\n7\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nMesh Topology\\nTree Topology\\nDevice Assignment\\nDevice Assignment\\nFigure 10: An example of two different device assignments based on the device topology. A 2D\\ntensor is split by 2x4 partitions and the communication pattern is between partitions along the rows\\nof the tensor. The numbers represent device ids.\\nSince data movement across devices critically affects the parallel execution performance, it is\\nimportant to consider the target device topology as well as the communication between partitions of\\nthe tensor when assigning device ids in the device assignment for maximum performance. Figure 10\\nshows two different device assignments based on the device topology and the row-wise communication\\npattern on the tensor.\\nA.4\\nSPMD Partitioning for Convolution and Window-Based Operators\\nGShard is able to partition spatial dimensions in convolutions, and general enough to support use\\ncases like giant images [32]. To spatially shard a convolutional layer, we can use the sharding API in\\nthe following way.\\n# Partition\\ninput\\nimages [N,C,H,W] along W spatial\\ndimension\\ninputs = split(inputs , 3, D)\\n# Replicate\\nthe kernel\\nkernel = replicate(kernel)\\nconv = conv2d(inputs , kernel)\\n...\\nGShard will then propagate the sharding on the spatial dimension to other layers and the backward\\npass. The rest of section discusses the speciﬁc complexity to partition Convolution and similar\\noperators. There are several window-based operations (e.g., Convolution, ReduceWindow), and\\nthey all require some type of halo exchange since data may be shared between windows. We use\\nthe CollectivePermute operator to exchange halo data between partitions, but one complication\\nis that the halo size may be different across partitions whereas CollectivePermute needs to be\\nstatically shaped.\\n32\\nBase size: 12\\nWindow size: 3\\nPadding low: 1\\nPadding high: 1\\nStride: 2\\nInput shard size: 3\\nOutput shard size: 2\\nLeft halo size for shard i: 1 -1 * i\\nRight halo size for shard i: 1 + 1 * i\\nshard 0\\nshard 1\\nshard 2\\nshard 3\\nshard 0\\nshard 1\\nshard 3\\nshard 2\\nLHS:\\nOutput:\\nFigure 11: Convolution with non-constant halo size.\\n1. Exchange maximum halo for left (1) and right (3)\\n3. DynamicSlice on the region actually needed\\n    (e.g., 0 left halo and 2 right halo for partition 2)\\n2. Concatenate exchanged left and right halos\\nDynamic\\nSlice\\nslice and\\ncollective-permute\\nslice and\\ncollective-permute\\n4. Mask out invalid regions with the identity value (0)\\n (e.g., partition 3 has 4 elements in the invalid region)\\n0\\n0\\n0\\n0\\niota, select, \\nbroadcast, ..\\nCollective\\npermute\\nCollective\\npermute\\nbase\\nFigure 12: Sequence of operations for a general halo exchange.\\nWe ﬁrst introduce the window conﬁgurations that the SPMD partitioner has to consider. Each spatial\\ndimension in the convolution has the following set of conﬁgurations.\\n• Stride is the distance (in number of elements) that the window moves to produce the next\\noutput element.\\n• Low/high padding is the number of elements padded to the low/high end of the dimension\\nin LHS (base).\\n• Base dilation is the dilation factor of the LHS, i.e., one plus the number of elements padded\\nbetween every element (excluding low/high padding). No base dilation means the value is\\nset to 1.\\n• Window dilation is one plus the number of elements padded between every element in the\\nRHS (window).\\nNon-constant halo size.\\nWe demonstrate that non-constant halo size is common using a simple\\nexample, which does not have dilation. Figure 11 shows a 4-way partitioned convolution, where\\nthe right halo sizes for the partitions are (1, 2, 3, 4) and can be expressed as a linear function of the\\npartition ID: partition_id + 1. Partition 1 is in charge of generating 2 output elements (red cells),\\nwhich means that the partition needs to get 0 elements from Partition 0, and 2 elements from Partition\\n2 (area covered by two dotted red windows).\\nFigure 12 describes the sequence of operations for a general halo exchange. First, we calculate\\nthe maximum size of left and right halo across partitions and perform the halo exchange of the\\nmaximum size (Steps 1 and 2). Since some partitions may have excessive halos than needed, we use\\nDynamicSlice (based on the partition ID) to slice off the valid region for the current partition (Step\\n3). Finally, some partitions may include garbage values (e.g., halos from out-of-range input data), so\\nwe apply masking as described in Section 3.3.3.\\nBase dilation.\\nBase dilation adds additional complexities to halo exchange, since the offset of each\\npartition may be positioned at the dilation holes, and also low/high padding is applied after dilation,\\nwhich makes the edges have different behavior than the interior elements. We handle base dilation in\\n3 cases (Figure 13).\\n33\\nP0\\nP1\\nP1\\nStride: 3\\nWindow size: 5\\nBase dilation: 3\\nP0, P1, P2: data on each partition\\nAll partitions start with the same pattern:\\n[padding, data, padding, padding, ..]\\nP2\\nP0\\nFirst window\\nof Partition 0\\nFirst window\\nof Partition 1\\n… \\nP0\\nP0\\nP1\\nhalo\\nhalo exchange on non-padded \\nbase region\\nFor all partitions,\\n* low padding: 1\\n* base dilation: 3\\nwindows for Partition 0\\nwindows for Partition 1\\nStride: 1\\nWindow size: 3\\nBase dilation: 3\\nFirst window\\nof Partition 0\\nFirst window\\nof Partition 1\\nlow_padding: 2\\nHalo exchange\\nDynamic-slice with offsets:\\n  partition 0: 2,  partition 1: 0,  partition 2: 1\\nExecute partitioned op\\nFirst window\\nof Partition 2\\nconv\\nNot calculated\\nStride: 2\\nWindow size: 3\\nBase dilation: 3\\nFirst window\\nof Partition 0\\nFirst window\\nof Partition 1\\n… \\n… \\nPartition 0 would be invalid if \\nlow_padding == 1\\nNot calculated\\n… \\nPartition 1 would be invalid \\nif low_padding == 0\\nNot calculated\\n… \\nPartition 2 would be invalid \\nif low_padding == 2\\n… \\n0      0  original window\\nPartition 0\\n… \\noriginal window   0     0\\nPartition 1\\n 0    original window  0\\nPartition 2\\nInstead: fixed low_padding == 2, then pad the window from 3 to 5 ( 3 + 2 holes) using different offsets\\nNo value for low_padding could work:\\nCase 1:\\n(stride * per_shard_window_count) % dilation == 0\\nCase 2: stride == 1, but per_shard_window_count % dilation != 0\\nCase 3: stride != 1, and (stride * per_shard_window_count) % dilation != 0\\n… \\nFigure 13: Partitioned convolution with base dilation.\\n• stride\\n×\\nper_shard_window_count\\nis\\ndivisible\\nby\\ndilation,\\nwhere\\nper_shard_window_count is the number of windows to be processed by each\\npartition (i.e., the number of output elements for each partition). This condition guarantees\\nthat all partitions start with the same number of (interior or low) padding elements before\\nthe ﬁrst data element in the LHS, so that we can use the same low padding. Halo exchange\\noccurs on the non-dilated/non-padded base region, and the limit index of required data for\\nPartition i can be calculated as below.\\nstride × per_shard_window_count × i + window_size −low_pad + dilation −1\\ndilation\\n,\\nwhich determines the right halo size. Because stride × per_shard_window_count is\\ndivisible by dilation, it can be simpliﬁed as a × i + b, where a and b are both constants.\\n• stride == 1 but per_shard_window_count is not divisible by dilation. In this case, the\\nlow padding on different partitions are different, but it is a static conﬁguration in windowed\\noperations, which can’t be specialized for each partition for SPMD execution. Using Pad\\nand DynamicSlice on the operand also would not work, because those operators would be\\napplied before dilation, so everything would be multiplied by the dilation factor. Fortunately,\\nwith stride == 1, all positions on the padded and dilated base region are valid window\\nstarts, and we can use the maximum low padding on all partitions to ensure that each\\npartition calculates all required windows, then do a DynamicSlice on the output of the\\npartitioned windowed operator to remove unnecessary data. The limit index of required data\\non the non-padded base region for Partition i is same as before,\\nper_shard_window_count × i + window_size −low_pad + dilation −1\\ndilation\\n,\\nbut cannot be simpliﬁed to a × i + b.\\n• stride ̸= 1 and stride×per_shard_window_count is not divisible by dilation. If neither\\nof the above conditions are true, different partitions could start with different number of\\npadding elements, and not all offsets are valid window starts. Consider the last example in\\n34\\nFigure 13. Whatever low padding we chose, some partition will be invalid, because the valid\\nwindows could be skipped since stride ̸= 1. A solution to this problem is to pad the window\\nin addition to padding the base area. We can use the maximum low padding required by\\nthe partitions on the base area, then increase the window size by that low padding amount.\\nHowever, the low and high padding amounts on the window vary on different partitions,\\nwhich can be implemented by a Pad followed by a DynamicSlice. The window padding is\\nused to mask off the unaligned elements in the base area, so that the start of the non-padding\\nwindow element will be aligned with the desired start in the base area for each partition.\\nWindow dilation.\\nIf the RHS is replicated, window dilation only affects the effective window\\nsize when partitioning the operator based on its LHS. If the dilated RHS is also partitioned, which\\ntypically occurs in the gradient computation of strided convolutions, handling window dilation is still\\nsimpler than handling base dilation, because there is no low/high padding on the RHS. We skip the\\ndetails of the implementation.\\n35\\n', 'source_name': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding', 'source_url': 'https://arxiv.org/abs/2006.16668'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Unified_Scaling_Laws.pdf #5\n",
      "{'content': 'UNIFIED SCALING LAWS FOR ROUTED LANGUAGE MODELS\\nAidan Clark∗, Diego de las Casas∗, Aurelia Guy∗, Arthur Mensch∗\\nMichela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman‡, Trevor Cai, Sebastian Borgeaud,\\nGeorge van den Driessche,\\nEliza Rutherford,\\nTom Hennigan,\\nMatthew Johnson‡,\\nKatie Millican,\\nAlbin Cassirer,\\nChris Jones,\\nElena Buchatskaya,\\nDavid Budden,\\nLaurent Sifre,\\nSimon Osindero,\\nOriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, Karen Simonyan\\nDeepMind\\nGoogle Research‡\\nABSTRACT\\nThe performance of a language model has been shown to be effectively modeled as a power-law in\\nits parameter count. Here we study the scaling behaviors of Routing Networks: architectures that\\nconditionally use only a subset of their parameters while processing an input. For these models,\\nparameter count and computational requirement form two independent axes along which an increase\\nleads to better performance. In this work we derive and justify scaling laws deﬁned on these two\\nvariables which generalize those known for standard language models and describe the performance\\nof a wide range of routing architectures trained via three different techniques. Afterwards we provide\\ntwo applications of these laws: ﬁrst deriving an Effective Parameter Count along which all models\\nscale at the same rate, and then using the scaling coefﬁcients to give a quantitative comparison of the\\nthree routing techniques considered. Our analysis derives from an extensive evaluation of Routing\\nNetworks across ﬁve orders of magnitude of size, including models with hundreds of experts and\\nhundreds of billions of parameters.\\n1\\nIntroduction\\nIt is a commonly held belief that increasing the size of a neural network leads to better performance, especially when\\ntraining on large and diverse real-world datasets. This vague and debated notion has become increasingly justiﬁed as\\nlarge empirical studies have shown that the performance of models on many interesting classes of problems are well\\nunderstood as power-laws; where a multiplicative increase in model size leads to an additive reduction in the model’s\\nloss [Kaplan et al., 2020, Hernandez et al., 2021, Henighan et al., 2020, Rosenfeld et al., 2019]. These relationships are\\nnot well understood, but a key implication is that a sequence of small1 models can be used both to infer the performance\\nof models many times more powerful, but also to provide global information about the scalability of an architecture.\\nEnter Routing Networks: models with the unusual property that each input interacts with only a subset of the network’s\\nparameters — chosen independently for each datapoint [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]. For\\na Routing Network, the number of parameters is nearly independent from the computational cost of processing a\\ndatapoint. This bifurcates the deﬁnition of size and prevents a scaling law in parameters alone from fully describing the\\nmodel class. Speciﬁc Routing Networks have been trained successfully at large scales [Fedus et al., 2021, Du et al.,\\n2021, Artetxe et al., 2021], but the general scaling behavior is not well understood. In this work we analyze the behavior\\nof routed language models so that we might infer the scaling laws that describe their performance.\\nCorrespondence to aidan.b.clark@gmail.com, diegolascasas@deepmind.com. All afﬁliation to DeepMind unless noted.\\n*Shared ﬁrst authorship.\\n1Measured as training or inference ﬂoating point operations, devices or time required, ﬁnancial cost, carbon emissions, etc.\\narXiv:2202.01169v2  [cs.CL]  9 Feb 2022\\nUniﬁed Scaling Laws for Routed Language Models\\n1\\n2\\n4\\n8 16 32\\n128\\n512\\nExpert Count\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n15M \\n25M \\n55M \\n130M \\n370M \\n1.3B \\na) Predicting Loss for Varying Expert Count\\n1.3B\\n370M\\n130M\\n55M\\n25M\\n5M\\nDense Model Size\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\nExpert Count\\nb) Curves of Constant Loss Value\\n100M\\n1B\\n10B\\nEffective Parameter Count\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\nc) Unified Model Scaling\\nS-BASE\\nRL-R\\nHash\\nDense LM\\nFigure 1: (a) The performance achieved by Routing Networks when varying the number of experts for a ﬁxed dense\\nmodel size is described by a bilinear function (Eq. 1), (b) whose level curves indicate how to trade model size with\\nexpert count to maintain a ﬁxed performance, (c) and which can be manipulated to align dense and routed model\\nperformance under a shared power law.\\nKey contributions.\\nWe analyze three different techniques for training Routing Networks, detailed in §3:\\nSinkhorn-BASE, a sparse mixture-of-experts (SMOE) approach modifying BASE [Lewis et al., 2021]; non-parametric\\nHASH Layers [Roller et al., 2021]; and routing via Reinforcement Learning (RL-R). With models up to 200 billion\\nparameters, we observe the following:\\n1. Routing improves the performance of language models across all sizes and variants attempted (see Fig. 1).\\n2. Training a Routing Network with RL (§3.3), a technique used in early routing work [Bengio et al., 2013], is of\\ncomparable effectiveness to state-of-the-art techniques.\\n3. The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in\\nthe underlying dense model size (§4) which generalize those from Kaplan et al. [2020].\\n4. These laws can be restated in terms of parameter count and inference compute, capturing an even wider set of\\nrouting architectures under a shared ﬁt (§4.4).\\n5. They further imply an Effective Parameter Count: a mapping equating the performance and scaling for both dense\\nand routed networks (§5).\\n2\\nBackground\\nWe ﬁrst review the language modelling problem and existing scaling laws before discussing the process of routing a\\nneural network and how it is applied to language models.\\nLanguage modelling.\\nWe consider the problem of autoregressively predicting natural language, a task with consistent\\nand predictable scaling characteristics across many orders of magnitude [Henighan et al., 2020, Kaplan et al., 2020].\\nThe objective is to maximize the likelihood of a sequence of tokens P(x1, . . . , xT ) factored auto-regressively as\\np (x1, . . . , xT ) = QT\\ni p (xi|xj<i). Our primary metric of performance is the negative log-likelihood of a validation\\ndataset whose statistics match the training distribution. We focus on this validation loss, but brieﬂy consider zero-shot\\ntransfer to other tasks in App. E.\\nScaling laws for large-scale data.\\nWe train on a multi-trillion-token compendium of English language text compris-\\ning documents from the internet alongside open-source text datasets, details of which are given in Rae et al. [2021]. In\\nthis setting Kaplan et al. [2020] argue that the converged performance of a model trained on a dataset of inﬁnite size is a\\npower-law in the model’s parameter count N. Our dataset is not inﬁnite, but its size – and the lack of any observed\\noverﬁtting – make this a reasonable approximation. We consider the ﬁnal (and best) evaluation value as the converged\\nvalue, though this is also an approximation which is discussed further in App. F.\\n2.1\\nRouting Networks\\nPower-law scaling implies the performance of a language model increases with size, but so too does the compute needed\\nto train the model. This undesirable connection between size and computation motivates a search for architectures\\nwherein the two are disentangled. Routing Networks are one such class of model: a type of neural network that\\nincorporates a speciﬁc ﬂavor of conditional computation. In a Routing Network, each input (e.g., a token of text) is\\n2\\nUniﬁed Scaling Laws for Routed Language Models\\ntransformed into an output while only interacting with a ﬁxed subset of the network’s parameters – dynamically selected\\nbased on the input itself. Many sparsely-activated networks have this property, but here we exclusively study the layout\\nbased on Sparse Mixtures of Experts [Shazeer et al., 2017] where multiple sub-components of a deep neural network\\n(i.e., several layers) are independently converted to routed equivalents and jointly trained with the rest of the network.\\nRouting a single layer.\\nThe core idea of a routed layer is that multiple versions of the parameters are kept, and a\\nper-input decision on which version to use is made. To route a layer fθ in E ways, we start by creating E separate\\nversions of the parameters θ ({θ1, ...θE}) where f using the i-th version of the parameters (fi ≜fθi) is termed the i-th\\nExpert. To determine which expert to pick given the input, we introduce an additional router function ρ : RM →[1, E]\\nassociated to the layer, typically a small network itself, with parameters ϕ. The routed form h of f is then given by\\nh(x) ≜fρ(x)(x). When performance increases with E, routing gives a method by which to improve a neural network\\nwith minimal computational increase (corresponding only to the compute needed by ρ(x)).\\nWe also consider the K-way routed generalization, where the router outputs a set of integers as ρ(·) : RM →[1, E]K,\\nand we set the output of the layer to be the sum of the outputs of each expert, namely h(x) ≜P\\ni∈ρ(x) fi(x). We\\ndefault to K = 1, but revisit this in §4.4.\\nRouted Transformers\\nWe apply routing to a decoder-only Transformer [Vaswani et al., 2017] to measure the scaling\\nproperties that result: an architecture chosen due to its state-of-the-art performance. Details of the baseline architecture\\nwe use are in App. A. We will refer to non-routed Transformers as dense models, in opposition to Routed Transformers\\nwhich sparsely activate some of their parameters. Our conversion to a Routed Transformer is the same as is used in\\nprior work [Lepikhin et al., 2020, Fedus et al., 2021]. Namely, we apply routing to every other set of feedforward\\ncomponents (FFWs) of the Transformer, sub-components that act on each timestep independently. Though different\\nlayers can have different numbers of experts, here all routed layers share the same number of experts E, and we will\\nrefer to the network as being routed E ways.\\nModel size and inference cost.\\nWe use N to indicate a network’s dense model size: the number of parameters any\\none input interacts with. This is in opposition to P: the total number of parameters. For a dense model, P = N,\\nwhereas for a Routing Network P is roughly proportional to N · E, with factors that depend on details of the routing\\narchitecture (§4.4). Except for a small overhead due to running the routers, the cost F (in TeraFLOPs) of executing a\\nRouted Transformer is the same as its dense equivalent.\\nTraining Details.\\nAll models are trained on TPUs with JAX [Bradbury et al., 2018] using a combination of data,\\nexpert (see App. C) and sharding parallelism [Shoeybi et al., 2019]. Models were trained with a sequence length of\\n2048 and batch size of 256 for 250,000 steps, i.e. 130 billion tokens, regardless of N. This is an important detail, and\\nwe discuss some of the implications in App. F. All were optimized with AdamW [Loshchilov and Hutter, 2018] and\\nZeRO Stage 1 was used to shard the optimizer state [Rajbhandari et al., 2020]. Appendix A contains further details.\\n3\\nRouting Techniques\\nIf the beneﬁt of Routing Networks is the decoupling of parameter capacity from network cost, the fundamental difﬁculty\\nis in effectively learning the parameters ϕ of the router given the non-differentiability of its output. Much research\\nin Routing Networks has therefore focused on techniques for learning ϕ. A major ﬁnding of this work is that three\\nnotably different techniques of training Routing Networks are effectively described by the same scaling laws. We now\\nintroduce and contextualize these three methods.\\n3.1\\nSparse Mixture-of-Experts via Weighting\\nSparse Mixture-of-Experts (SMOE) methods [Shazeer et al., 2017] solve the problem of non-differentiability by reusing\\nthe probability of expert selection as a scalar multiplier on that expert’s output, guaranteeing a gradient passed to the\\nlogits of selected experts despite the the non-differentiability of sampling from those logits. Formally, the router is\\ngiven as ρ(x) = topk(Wx + b), where Wx + b is an unnormalized distribution over [1, E] from which the experts\\ncorresponding to the top K values are selected. In the ﬁnal output of the routed layer, the normalized logits are reused as\\ngating weights, i.e. the ﬁnal output of the routed layer is h(x) = P\\ni∈ρ(x) gi(x)fi(x) where g(x) = softmax(Wx + b).\\nThough this formulation supplies a gradient to ϕ = (W, b), it represents changes to the scalar multiplier and does not\\ndirectly correspond to optimizing expert selection. This method is nevertheless effective, and can be seen as a sparse\\n3\\nUniﬁed Scaling Laws for Routed Language Models\\n1 2 4 8\\n32\\n128 512\\nExpert Count\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n15M\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nS-BASE\\n1\\n2\\n4\\n8\\n32\\n128\\n512\\nExpert Count\\nRL-R\\n1\\n2\\n4\\n8\\n32\\n128\\n512\\nExpert Count\\nHash\\n1\\n2\\n4\\n8\\n32\\n128\\n512\\nExpert Count\\nComparisons\\nFigure 2: Validation losses with ﬁts from Equation 1 plotted as a dotted line for S-BASE, HASH and RL-R respectively.\\nOn the right, the prediction curves for all model sizes and all techniques overlapping to show relative performance. Fits\\nto Eq. (7) are overlaid in grey.\\napproximation to dense mixture of experts models [Eigen et al., 2014, Jacobs et al., 1991] where the likelihood of\\nskipping an expert is inversely proportional to the value of its scalar gate gi.\\nIt was conjectured that SMOEs require (K ⩾2)-way routing to produce effective gradients in the routers [Shazeer et al.,\\n2017], and many attempts at incorporating routing into large Transformers use K = 2 [Lepikhin et al., 2020, Du et al.,\\n2021]. However recently this has been challenged, and stable modiﬁcations have been proposed for K = 1; namely the\\nSwitch Transformer [Fedus et al., 2021]. Most SMOEs, including Switch, are reliant on auxiliary balancing losses which\\nencourage the router output ρ(x) to be more uniform across minibatches of inputs. To improve on this, BASE [Lewis\\net al., 2021] post-processes the router output with a Hungarian Matching algorithm that re-assigns expert selections to\\nensure that all experts are selected evenly.\\nOur implementation of BASE replaces the Hungarian Matching with a regularized Optimal Transport formulation\\n[Cuturi, 2013] using the Sinkhorn algorithm as an approximate matching step during expert selection. This substantially\\nimproves routing efﬁciency on accelerated hardware (details in §B.2.1). We call the resulting method Sinkhorn-BASE\\n(S-BASE), and use it as the representative of SMOE methods, as early tests showed the beneﬁt of its balancing mechanism.\\n3.2\\nInput-based Deterministic Hash Routing\\nAn alternative approach eschews extra parameters completely and represents ρ as a ﬁxed function of the input. This is\\nthe concept pioneered by HASH Layers [Roller et al., 2021] which circumvents the need to simultaneously learn ϕ and\\nθ. Our implementation takes the token ID assigned to the input by the SentencePiece tokenizer [Kudo and Richardson,\\n2018] and uses the remainder of it divided by E as the expert selection. See §B.4 for details.\\n3.3\\nRouting via Reinforcement Learning\\nFinally, we re-analyze a technique that optimizes the router via Reinforcement Learning (a class of methods we call\\nRL-R), which was proposed in early work on neural conditional computation [Bengio et al., 2013, 2016, Bengio, 2017,\\nDenoyer and Gallinari, 2014]. In this approach each router is seen as a policy whose actions are the selection of an\\nexpert in each routed layer and whose observations are the activations passed to that router. After completing the\\nforward pass, the probability the Routed Transformer assigns to the correct output token can be used as a reward,\\nmaximization of which is equivalent to minimization of NLL. To jointly train the experts and the router, we minimize\\na composite loss formed with the language modelling loss and a policy-gradient term [Sutton et al., 2000] using the\\nselected set of experts as actions. We highlight that the optimal expert selection is dependent not only on the input\\nactivations but on the parameters of the rest of the network. This disrupts the theoretical underpinning, crucial to RL,\\nthat this is a Markov Decision Process. Nevertheless, it has been observed that this theoretical issue does not affect the\\npracticality of the method [Rosenbaum et al., 2019].\\nRelative to SMOE, RL-R beneﬁts from directly optimizing actions to improve the language modelling loss. However\\nthis absence of bias comes with complications, especially the high variance of the gradient [Rosenbaum et al., 2019,\\nDenoyer and Gallinari, 2014]. We use REINFORCE with a learned baseline [Williams, 1992, Sutton and Barto, 2018] to\\naddress this issue, so that improving the policy means increasing the likelihood of selecting experts which lead to a\\nbetter than average next token prediction. As with SMOE, we ﬁnd it useful to add a balancing term. To our knowledge,\\nwe are the ﬁrst to experiment routing with Reinforcement Learning on large Transformer-based language models—we\\ntherefore explore key ablations in Appendix B.3.\\n4\\nUniﬁed Scaling Laws for Routed Language Models\\nTable 1: Leave-One-Out RMSLE Fit in (N, E).\\nThe last row is\\ncomputed for each model size independently; this gives an lower\\nbound of the error of any joint scaling law.\\nL log-log prediction\\nEq.\\nS-BASE\\nRL-R\\nHASH\\nSeparably linear in N, E\\n(5)\\n80e-4\\n90e-4\\n90e-4\\nBilinear in (N, E)\\n(7)\\n60e-4\\n57e-4\\n60e-4\\nBilin. + saturat. in (N, E)\\n(1)\\n58e-4\\n56e-4\\n56e-4\\nPer-N ﬁts in (E)\\n(4)\\n46e-4\\n29e-4\\n19e-4\\nTable 2: Dense scaling values (see also App. F).\\nαN\\nNc\\nOurs\\n0.078\\n3.568e13\\nKaplan et al. [2020]\\n0.076\\n8.8e13\\n4\\nScaling Behavior at Convergence\\nOur main hypothesis is that the converged log-loss of a Routing Network is bilinear in the terms log N and log b\\nE,\\nwhere ˆ\\nE is a saturating transformation of E. Speciﬁcally, we ﬁt the 6-parameter scaling law:\\nlog L(N, E) ≜a log N+b log b\\nE+c log N log b\\nE+d\\n(1)\\nwhere\\n1\\nb\\nE\\n≜\\n1\\nE −1 +\\n\\x10\\n1\\nEstart −\\n1\\nEmax\\n\\x11−1 +\\n1\\nEmax\\n.\\nWe can generalize this law across a wider range of routing architectures by a change of variables, using the model\\ninference cost F and the total number of parameters P, as:\\nlog L(F, B) ≜a log F+b log b\\nB+c log F log b\\nB+d,\\n(2)\\nwhere B ≜P\\nF and B →ˆ\\nB is the same saturating transform as E →ˆ\\nE. Before justifying Equation (1), we validate its\\ncandidacy by ﬁtting it to empirical data obtained on a large sweep of models. This sweep consists of a Routing Network\\ntrained for each of the three techniques described in §3: across six model sizes (described in Table 4) while varying E\\nacross [2, 4, 8, 16, 32, 64, 128, 256, 512]. This totals 168 different models, including dense baselines.\\nThe observed losses for each model are shown in Fig. 2(a-c). We ﬁt Eq. (1) to each routing method and plot predictions\\nfor ﬁxed values of N as dotted lines. The goodness-of-ﬁt across all methods is apparent, as is the clear behavior that\\nincreasing E leads to a reduction in validation loss. Fig. 2(d) plots the relative predictions for all three techniques,\\nclearly showing that S-BASE performs best across all model sizes, followed by RL-R, followed by HASH (see §5.3). The\\nremainder of this section justiﬁes the chosen functional forms (1) and (2); ﬁrst supposing independent power laws in N\\nand E (§4.1), then introducing a multiplicative interaction (§4.2) and saturation in the second term (§4.3), followed by\\na change of variables (§4.4). The beneﬁt gained by this progression of ﬁts can be seen in Table 1. Notations are recalled\\nin Fig. 3.\\n4.1\\nSeparable Scaling Laws in Model Size and Experts\\nKaplan et al. [2020] argue that the converged performance of a dense model with N parameters can be modelled\\naccurately as the two-parameter power law\\nlog L(N) ≜a log N + d,\\ni.e.\\nL(N) =\\n\\x12Nc\\nN\\n\\x13αN\\n(3)\\nwhere αN ≜−a and Nc ≜10d/−a. We can re-estimate these coefﬁcients from the performance of our own dense\\nmodels, leading to estimations in Table 2. The similarity of αN is a reassuring sanity check (there are differences in\\ndataset, vocabulary, tokenization and model which effect Nc).\\nAn immediate hypothesis is that for all values of N, scaling in E obeys a similar power law:\\nlog LN(E) ≜b log E + d′\\n(4)\\nBecause LN(1) = L(N) (a fact we will call dense equivalence), (3) and (4) can be combined into:\\nlog LN(E) ≜a log N + b log E + d,\\n(5)\\n5\\nUniﬁed Scaling Laws for Routed Language Models\\n1.3B\\n370M\\n130M\\n55M\\n25M\\nBase model size\\n-0.02\\n-0.025\\n-0.03\\nSlope b(N)\\nS-BASE\\nRL-R\\nHash\\nN\\nParameter Count in Base Model\\nE\\nNumber of Experts\\nP\\nTotal Number of Parameters\\nF\\nCompute per Inference (in TeraFLOPs)\\nB\\nParameter Utilization Ratio\\n¯\\nN\\nEPC: The Effective Parameter Count\\nFigure 3: Left: b(N) increases with N. Right: Notations.\\n1 2 4 8\\n32\\n128 512\\nExpert Count\\nValidation Loss\\nS-BASE\\n1 2 4 8\\n32\\n128 512\\nExpert Count\\n \\nRL-R\\n1 2 4 8\\n32\\n128 512\\nExpert Count\\n \\nHash\\nFigure 4: Fits for S-BASE, RL-R and HASH. Dashed lines are solutions to Eq. (4) with b(N) given by Table 6 while\\ndotted lines are solutions to Eq.(7). Solutions for Eq. (5) are in grey. The separable solution fails to account for\\ndecreasing performance given by expert scaling.\\ncorresponding to the multiplicative separated power law:\\nLN(E) =\\n\\x1210d/a\\nN\\n\\x13a \\x12 1\\nE\\n\\x13b\\n(6)\\nIf Eq. (4) ﬁts observed data for any N we can proceed with an assumption that scaling in E obeys a power-law for ﬁxed\\nN. Observing a constant b across N would allow to ﬁt Eq. (5) to models ranging across N and E simultaneously.\\nFitting.\\nThe ﬁrst hypothesis is easily tested and conﬁrmed to a reasonable degree. We ﬁt Eq. (4) for each technique\\nand value of N separately, plotted as colored lines in Fig. 4. The values of b are shown in Fig. 3.\\nWe observe that b(N) is increasing with N (values listed in Table 6), corresponding to a reduction in beneﬁt from\\nrouting as size increases, with a slope that is approximately linear in log N (Fig. 3). Eq. (5) requires that b remains\\nﬁxed across N; therefore we expect it to poorly predict model performance. We can attempt a ﬁt nevertheless: plotted\\nin grey in Fig. 4. Qualitatively, this mis-predicts some validation losses by over 0.2, particularly overestimating the\\nperformance at large N and E. As reported in Table 1, the ﬁt has held-out RMSLE values greater than 80e-4.\\n4.2\\nQuadratic Interaction in N and E\\nThis motivates us to introduce a simple extension: that of a multiplicative interaction between log N and log E. This is\\nconveniently the exact function which leads to b scaling with log N and takes the following form:\\nlog L(N, E)≜+a log N+b log E+c log N log E+d\\n(7)\\nThis function has the property that the log-log slope in both N and E are afﬁne in the logarithm of the other variable. In\\nother words, with E or N ﬁxed, the performance L scales with N or E following (3) and (4) with slopes given by:\\na(E) ≜−∂log L\\n∂log N = a + c log(E)\\n(8)\\nb(N) ≜−∂log L\\n∂log E = b + c log(N),\\nb(N) matches the behavior reported in Table 6. A transposed table, ﬁtting sets of models with ﬁxed E and changing\\nN, can be found to match the behavior predicted by a(E) (see Table 8). There are two symmetric non-logarithmic\\n6\\nUniﬁed Scaling Laws for Routed Language Models\\n55M\\n370M 870M\\nDense Model Size\\n1\\n8\\n64\\n256\\nExpert Count\\n100\\n1000\\nTeraFLOPs\\n1\\n10\\nParam. Utililzation Ratio\\nK=1\\nK=2\\nK=4\\n55M\\n370M 870M\\nDense Model Size\\n1\\n8\\n64\\n256\\nExpert Count\\n100\\n1000\\nTeraFLOPs\\n1\\n10\\nParam. Utilization Ratio\\nR=0.25\\nR=0.5\\nR=1.0\\nFigure 5: Level curves for Equation (1) and Equation (2) on S-BASE for K ∈{1, 2, 4} (left two), R ∈{1.0, 0.5, 0.25}\\n(right two). Scaling laws in (N, E) differ for models with different values of (K, R): indicated by non-overlapping\\nlevel-curves. A change of variables to (F, P) leads to almost-overlapping functions: allowing the same ﬁts to be reused\\nacross changes in the routing architecture.\\nrepresentations of (7), useful for comparison to (6):\\nL(N, E) =\\n\\x1210d/a\\nN\\n\\x13a \\x12 1\\nE\\n\\x13b+c log(N)\\n,\\n(9a)\\n=\\n\\x1210d/b\\nE\\n\\x13b \\x12 1\\nN\\n\\x13a+c log(E)\\n.\\n(9b)\\nFitting.\\nFitting the bilinear (7) instead of (5) substantially reduces the prediction error for large N (Table 1, Eq. (5)\\nvs Eq. (7)), as displayed in Fig. 4 (dotted lines match the dashed ones, where the grey separable ﬁt doesn’t). We verify\\ndense equivalence: αN ≈a, while Nc ≈exp(d/a), and thus the law (7) gives similar prediction to the reference\\nlaw (3) for dense models. Predictions for ﬁxed N are visualized as grey lines in Fig. 2.\\nInterpretation.\\nIn Eq. (7), when c is positive, the expert improvement slope b(N) reduces with model size N. All\\nthree routing techniques considered therefore predict diminishing improvements from routing when increasing scale.\\nHowever, the scaling of S-BASE is predicted (and seen) to be substantially better. When designing a new technique, we\\ncan ﬁt (7) and predict a better scaling behavior if the ﬁtted c is lower than with other techniques. A clear goal for future\\nwork in routing techniques should be to ﬁnd a method with scaling coefﬁcient c ≈0.\\n4.3\\nBounded Scaling in E\\nEquation (5) models scaling in E as a power law. For both small and large values of E, there are reasons to\\nexpect some deviation. If a routing technique degrades with E (for instance, the variance of gradients in RL-R will\\nincrease), performance for large E might be worse than predicted. On the other hand, ﬁxed overhead (e.g., interference\\nfrom auxiliary losses) might worsen scaling for low values of E, counter-intuitively leading to better than expected\\nperformance. Both phenomena appear clearly in Fig. 2. We seek to model this saturation such that the limit behavior in\\nE is bounded on both sides. We choose the following transformation, but discuss in §5.1 a number of implications\\nwhich are independent of the speciﬁc saturating form used:\\n1\\nb\\nE\\n≜\\n1\\nE −Emin +\\n\\x10\\n1\\nEstart −\\n1\\nEmax\\n\\x11−1 +\\n1\\nEmax\\n.\\n(10)\\nThis is constructed so that we have ˆ\\nE(Emin) = Estart, while ˆ\\nE →Emax as E →∞. We ﬁx Emin = 1, indicating the\\nlower bound of meaningful expert counts. ˆ\\nE can be seen as a thresholded version of E: increasing past Emax will give\\n7\\nUniﬁed Scaling Laws for Routed Language Models\\nimprovement, but not following a power law. Similarly, when Estart > 1, ˆ\\nE > E for small values of E. Practically, the\\nﬁt is the same over a wide range of different thresholding functions.\\nFitting.\\nSolving Equation (1), equal to Eq. (7) with E →ˆ\\nE, is complicated by its non-convexity. We ﬁnd the\\ncoefﬁcients (a, b, c, d, Estart, Emax) as the best of repeated solutions provided by the L-BFGS-B algorithm [Byrd et al.,\\n1995]. Fig. 2 shows ﬁtted curves from these equations; coefﬁcients are reported in Table 3.\\nInterpretation.\\nRelative to using the simple bilinear law (7), ﬁtting Eq. (1) improves prediction for the lowest and\\nhighest values of E considered. Crucially, while the deviation from a power-law (and therefore improvement in RMSLE)\\nis relatively minor for the values of E considered, the deviation is nonetheless clear (seen best looking at the raw losses\\nin Fig. 21). We believe it is important to model this saturation because (as argued in §5.2) the limit behavior of model\\nperformance as N increases is substantially different when bounded, with important properties that are independent of\\nEmax. We further hypothesize that future work, able to test still larger values of E, will see a more quantitative beneﬁt\\nfrom including these terms. This can be already observed in Fig. 20 when noting that the law (7) does not over and\\nunder estimate the performance for E = {2, 4, 256, 512} as it does in Fig. 4. Level curves of Eq. (1) enumerate the\\n{(N, E)} which are predicted to achieve ﬁxed performance, as visualized in Fig 1(b). This demonstrates of the power\\nof routing: a model with N = 5M and E = 128 equals the performance of a model with N = 55M and E = 1,which\\nrequires over ten times more compute per inference.\\n4.4\\nGeneralizing Across Architecture Variants\\nThe models trained so far use ﬁxed choices for two key details of routing: the number of experts executed per-datapoint\\nK and the frequency of routed layers across depth R (previously set at 1 and 0.5, respectively). For any selected value\\nof K and R we may ﬁt Eq. (1) to observed performance, but since these variables are independent of N and E, we\\ndo not expect the same coefﬁcients to remain valid across values of K and R. To allow for a uniﬁed scaling law, we\\nmodify Eq. (1) to use terms in F, the TeraFLOPs required per forward pass, and in the ratio B ≜P\\nF where P is the\\ntotal number of parameters. Speciﬁcally, F is motivated by the approximation from Kaplan et al. [2020] that F = 2N.\\nB, the parameter utilization ratio, is an afﬁne function of E, close to linear when most parameters lie in the routed\\ncomponents of the model.\\nUsing (F, B) instead of (N, E) (and setting Emin to 1\\n2) results in Eq. (2). To show the advantage of this change of\\nvariables we conduct two experiments: varying K across {1, 2, 4} and R across {0.25, 0.5, 1.0}. In both cases, we vary\\nE ∈{8, 64, 256} and N ∈{15M, 370M, 870M}.\\nFitting.\\nEq. (2) predicts the scaling behavior of models as well as Eq. (1) for a given routing architecture, as indicated\\nin Fig. 24. The beneﬁt of the change of variables is seen most clearly in Fig. 5, which plots contours of ﬁxed loss value\\nas functions of (N, E) and of (F, B). For varying (K, R), the loss surface as a function of N and E changes: meaning\\na joint ﬁt would be inaccurate. Plotted as functions of (F, B), the loss surface is almost the same, suggesting a shared ﬁt\\nbetween all three methods (see Fig. 25 and Fig. 26 for joint ﬁts for K and R respectively). We highlight that R = 0.25\\ndeviates slightly. Plausible explanations are discussed in §D.4. The possibility to use a shared ﬁt indicates a singular\\ntakeaway: the architectural details K and R little affect the scaling behavior of a Routing Network. The loss of the\\nnetwork can thus be predicted based only on inference ﬂops F and total number of parameters P.\\n5\\nScaling Law Applications\\nNext we provide two applications of the scaling laws presented. We re-emphasize that all values are only valid at the\\nspeciﬁc token count all models were trained at: 130B. App. F provides evidence that our analysis, if not the numerical\\nvalues, are nevertheless robust to token count.\\n5.1\\nEffective Parameter Equivalence\\nWe leverage Eq. (1) to compute the size ¯\\nN of a dense model giving the same performance as a Routing Network.\\nSpeciﬁcally, we solve for L( ¯\\nN, 1) = L(N, E), yielding\\n¯\\nN ≜(N)α( ˆ\\nE)/α(Estart)\\x10\\nˆ\\nE/Estart\\n\\x11b/α(Estart)\\n(11)\\nHere α(E) = a + c log E. Given a model with N and E, we call ¯\\nN that model’s Effective Parameter Count (or EPC).\\nEq. (1) predicts that the performance of all models increases as a power law in this variable\\nlog L(N, E) = a log ¯\\nN(N, E) + d.\\n(12)\\n8\\nUniﬁed Scaling Laws for Routed Language Models\\nTable 3: Solutions to Eq. (1).\\na\\nb\\nc\\nd\\nEstart\\nEmax\\nS-BASE\\n-0.082\\n-0.108\\n0.009\\n1.104\\n1.847\\n314.478\\nRL-R\\n-0.083\\n-0.126\\n0.012\\n1.111\\n1.880\\n469.982\\nHASH\\n-0.087\\n-0.136\\n0.012\\n1.157\\n4.175\\n477.741\\n100M\\n1B\\n10B\\n100B\\n1T\\nBase model size\\n100M\\n1B\\n10B\\n100B\\n1T\\nMaximum eff.\\nparameter count\\nNcutoff\\nNcutoff\\nS-BASE\\nRL-R\\nHash\\nDense\\nRouting improvement\\nFigure 6: Maximum effective parameter count as a function of base model size. Routing helps until a certain size\\nNcutoff, that varies strongly between methods (S-BASE being the best)\\nThe result of plotting all models as a function of ¯\\nN is shown in Fig. 1(c): a good ﬁt across four orders of magnitude.\\nScaling in terms of ¯\\nN results in a unifying power law: valid for dense and routed language models alike.\\n5.2\\nRouting Behavior for Large N\\nEPC leads to a better grasp of the behavior of routing as N increases. Of immediate interest is Ncutoff: the value of N\\nwhere ¯\\nN(N, E) ⩽N. For larger N, routing will not improve performance. This is easily found to obey log Ncutoff = b\\nc.\\nNcutoff equals 937B, 85B and 83B for S-BASE, RL-R and HASH respectively. These values are highly dependent on the\\nnumber of tokens seen, and Ncutoff is expected to increase with increased numbers of tokens.\\nNext we consider ¯\\nNmax(N) ≜maxE ¯\\nN(N, E), i.e. the maximal effective parameter count that a routing network can\\nreach. Eq. (11) predicts that log ¯\\nN is an afﬁne function of log N for any ﬁxed E, and ¯\\nNmax(N) = N for N > Ncutoff.\\nTherefore log ¯\\nNmax is piecewise-afﬁne in log N, as displayed in Fig. 6:\\n∀N ⩽Ncutoff = 10−b\\nc ,\\n¯\\nNmax(N) = ¯\\nN(N, Emax),\\n∀N ⩾Ncutoff, ¯\\nNmax(N) = N.\\n(13)\\nNote that ¯\\nNmax is continuous near Ncutoff, since for all E, ¯\\nN(Ncutoff, E) = Ncutoff. Moreover, the slope of ¯\\nNmax(·)\\nfor N ⩽Ncutoff is positive whenever Emax ⩽Estart10−a/c, which is true for our coefﬁcients. In this setting ¯\\nNmax(·)\\nis a non-decreasing function of N. Therefore for any routing network where N < Ncutoff, N ⩽¯\\nNmax(N) ⩽Ncutoff,\\nmeaning routing will never let you train a model more powerful than Ncutoff. Note that despite this value not depending\\non Emax, its existence crucially depends on the saturating transformation: without it ¯\\nNmax is unbounded.\\n5.3\\nComparative Analysis\\nKaplan et al. [2020] use scaling laws to encapsulate and contrast the behavior of entire model classes. Here we mirror\\nthis analysis by using the scaling laws we have proposed to summarize the relative behavior of the three routing\\ntechniques considered. We make four concrete observations:\\n• S-BASE consistently outperforms RL-R and HASH, though RL-R is very competitive at smaller N.\\n• All routing techniques suffer from reducing efﬁcacy as N increases. Amongst the three techniques, S-BASE\\nscales best: the ﬁtted parameter c is lowest.\\n• For small N, RL-R and S-BASE scale similarly with expert count and better than HASH (as indicated by\\ncomputing the effective expert slope b(N) = b + c log N).\\n• HASH and RL-R maintain power-law behavior for longer than S-BASE (larger Emax). However they suffer\\nfrom more interference (c); leading to worse performance for most model sizes.\\n• HASH has large initial overhead (bigger Estart), clearly visible as a more obvious curvature at small E.\\n9\\nUniﬁed Scaling Laws for Routed Language Models\\nFor a practitioner interested in applying routing techniques, we conclude with some recommendations:\\n1. Use routing when training any model with N ⩽1.3B.\\n2. S-BASE is a good default routing algorithm. RL-R will sometimes match S-BASE in performance but is less\\nrobust and scalable (§D.1).\\n3. Target using E ∈{64, 128} experts. Larger values will continue to improve, but with diminishing returns.\\n4. Use K=1 experts. Route layers at frequency 0.5 ⩽R ⩽1; lower frequency reduces performance.\\n5. Future routing research should focus on the terms c and Emax; indicative of limits to arbitrary scaling.\\n6. New routing techniques must be validated at multiple values of N and E when comparing with prior work.\\nResults on single sizes cannot be extrapolated.\\n6\\nRelated Work\\nIn studying the empirical aspects of scaling, this work follows Kaplan et al. [2020]; which triggered much research\\nincluding Henighan et al. [2020], Hernandez et al. [2021] and Ghorbani et al. [2021]. The underlying theory is less\\nunderstood, but there is some exploration of this space including Hutter [2021] and Bahri et al. [2021].\\nThese studies, and ours, are mutually reliant on a large corpus of work improving the scalability of Transformers. This\\nincludes models like GPT-2 [Radford et al., 2019], GPT-3 [Brown et al., 2020], Jurassic-1 [Lieber et al., 2021] and\\nGopher [Rae et al., 2021], as well as work improving the ability of these models to be efﬁciently parallelized across\\nmultiple devices, including Shoeybi et al. [2019], Narayanan et al. [2019], Kim et al. [2021] and Xu et al. [2021].\\nParallel to all this has been a long study of Routing Networks; a term introduced by Rosenbaum et al. [2018] but\\ndeveloped extensively in the literature as Conditional Computation [Bengio et al., 2013, 2016, Bengio, 2017, Denoyer\\nand Gallinari, 2014] and Mixture of Experts [Jacobs et al., 1991, Collobert et al., 2003, Eigen et al., 2014]. The\\nframework is sometimes further generalized, seen as per-example architecture search in Ramachandran and Le [2018]\\nor as a graph problem in Denoyer and Gallinari [2014]. Routing was popularized for large scale training by Shazeer\\net al. [2017], and furthered by work including GShard [Lepikhin et al., 2020], Switch Transformer [Fedus et al., 2021]\\nand GLaM [Du et al., 2021]. In this vein, Artetxe et al. [2021] undertake a comparative analysis of dense networks and\\nSMOEs with E = 512 that aligns with our results. Finally, the core routing architecture is still being improved. Nie et al.\\n[2021] adapt K through training where Hazimeh et al. [2021] learn it via a differentiable loss. Ramachandran and Le\\n[2018] increase K through depth and encourage architectural diversity across experts. Caccia et al. [2021] grows E\\nthroughout training and Rajbhandari et al. [2022] propose networks where E changes with depth.\\n7\\nConclusion\\nUsing conditional computation to scale neural networks has long been a research goal, and methods based on Routing\\nNetworks have been increasing in popularity. Here we have introduced a scaling law (Eq. (1)) that models the behavior\\nof these networks. This scaling law predicts that, for all models considered, introducing routing into a language model\\nimproves performance. That improvement follows a power-law in the number of experts E that diminishes with\\nmodel size N, and can be further generalized across routing architectures with Eq. (2). These scaling laws quantify\\nthe differences between three different routing techniques and lead to a single scalar (Eq. (11)) that simultaneously\\ndescribes the performance of routed and dense models alike.\\nThis work provides an empirical framework with which to analyze future innovations in routing. We hope the\\noverwhelming evidence we provide towards the beneﬁts of routing encourage it to be more rapidly adopted as a\\npowerful tool for model improvement, whose scaling characteristics align with traditional methods of scaling (in depth\\nand width) and which will remain beneﬁcial up to models with base model size greater than 900 billion parameters.\\n10\\nUniﬁed Scaling Laws for Routed Language Models\\nAcknowledgments\\nWe would like to thank Marc’Aurelio Ranzato, Nando de Freitas, Jacob Menick and Andy Brock for useful comments\\nand feedback on early drafts of this paper. The infrastructure needed to train these models wouldn’t have been possible\\nwithout the dedicated work of the JAX and XLA teams, especially Peter Hawkins, Roy Frostig and James Bradbury\\nwho all were crucial in the development of the routing software.\\nReferences\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du,\\nSrinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines,\\nLouis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa\\nKozareva, and Ves Stoyanov. Efﬁcient Large Scale Language Modeling with Mixtures of Experts. arXiv:2112.10684,\\n2021.\\nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws.\\narXiv:2102.06701, 2021.\\nEmmanuel Bengio. On Reinforcement Learning for Deep Neural Architectures: Conditional Computation with\\nStochastic Computation Policies. McGill University (Canada), 2017.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks\\nfor faster models. In International Conference on Learning Representations, 2016.\\nYoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic\\nneurons for conditional computation. CoRR, abs/1308.3432, 2013.\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye\\nWanderman-Milne. Jax: composable transformations of Python + NumPy programs, 2018.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv:2005.14165,\\n2020.\\nRichard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained\\noptimization. SIAM Journal on scientiﬁc computing, 16(5):1190–1208, 1995.\\nLucas Caccia, Jing Xu, Myle Ott, Marc’Aurelio Ranzato, and Ludovic Denoyer. On anytime learning at macroscale.\\narXiv:2106.09563, 2021.\\nRonan Collobert, Yoshua Bengio, and Samy Bengio. Scaling large learning problems with hard parallel mixtures.\\nInternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 17(03):349–365, 2003.\\nCuration. Curation corpus base, 2020.\\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information\\nProcessing Systems, 2013.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL:\\nAttentive language models beyond a ﬁxed-length context. In Annual Meeting of the Association for Computational\\nLinguistics, pages 2978–2988, 2019.\\nLudovic Denoyer and Patrick Gallinari. Deep sequential neural networks. In NIPS Deep Learning Workshop, 2014.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou,\\nAdams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang,\\nKellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V\\nLe, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts.\\narXiv 2112.06905, 2021.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts.\\nICLR Workshop, 2014.\\n11\\nUniﬁed Scaling Laws for Routed Language Models\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple\\nand efﬁcient sparsity. arXiv:2101.03961, 2021.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. The pile: An 800GB dataset of diverse text for language modeling. arXiv:2101.00027,\\n2020.\\nBehrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and\\nColin Cherry. Scaling laws for neural machine translation. arXiv:2109.07740, 2021.\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder,\\nLichan Hong, and Ed H Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to\\nmulti-task learning. Advances in Neural Information Processing Systems, 2021.\\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown,\\nPrafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv:2010.14701, 2020.\\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv:2102.01293,\\n2021.\\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR, 2019.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,\\nQuoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\\nAdvances in Neural Information Processing Systems, 2019.\\nMarcus Hutter. Learning curve theory. arXiv:2102.04074, 2021.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts.\\nNeural computation, 3(1):79–87, 1991.\\nL. Kantorovitch. On the Translocation of Masses. Management Science, 5(1):1–4, 1958.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam\\nRajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁcient MoE training for multitask multilingual\\nmodels. CoRR, abs/2109.10465, 2021.\\nPaul Knopp and Richard Sinkhorn. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of\\nMathematics, 21(2):343–348, 1967.\\nWouter Kool, Chris J. Maddison, and Andriy Mnih. Unbiased gradient estimation with balanced assignments for\\nmixtures of experts. CoRR, abs/2109.11817, 2021.\\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer\\nfor neural text processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing:\\nSystem Demonstrations, 2018.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam\\nShazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In\\nInternational Conference on Learning Representations, 2020.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers: Simplifying training\\nof large, sparse models. In International Conference on Machine Learning, 2021.\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White Paper.\\nAI21 Labs, 2021.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning\\nRepresentations, 2018.\\nSam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training.\\narXiv:1812.06162, 2018.\\n12\\nUniﬁed Scaling Laws for Routed Language Models\\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. International\\nConference on Learning Representations, 2016.\\nDeepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B.\\nGibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the\\nACM Symposium on Operating Systems Principles, 2019.\\nXiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi Yang, and Bin Cui.\\nDense-to-sparse gate for mixture-of-experts. arXiv:2112.14397, 2021.\\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,\\nMarco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad\\ndiscourse context. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), 2016.\\nGabriel Peyré and Marco Cuturi. Computational Optimal Transport. Foundations and Trends in Machine Learning,\\n2019.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog, 2019.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah\\nHenderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard\\nPowell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat\\nMcAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland,\\nKaren Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida\\nNematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,\\nMaria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan\\nClark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals,\\nKareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling\\nLanguage Models: Methods, Analysis & Insights from Training Gopher. arXiv:2112.11446, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\\nand Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine\\nLearning Research, 21:1–67, 2020.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training\\ntrillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage\\nand Analysis, pages 1–16, 2020.\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan,\\nJeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power\\nnext-generation AI scale. arXiv 2201.05596, 2022.\\nPrajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models. In International Conference\\non Learning Representations, 2018.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.\\nHash layers for large sparse models.\\narXiv:2106.04426, 2021.\\nClemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions\\nfor multi-task learning. In International Conference on Learning Representations, 2018.\\nClemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and the challenges of\\nmodular and compositional computation. arXiv:1904.12774, 2019.\\nJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization\\nerror across scales. In International Conference on Learning Representations, 2019.\\n13\\nUniﬁed Scaling Laws for Routed Language Models\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on\\nLearning Representations, 2017.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM:\\nTraining multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019.\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforce-\\nment learning with function approximation. In Advances in neural information processing systems, 2000.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.\\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine\\nlearning, 8(3):229–256, 1992.\\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry\\nLepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: General and scalable parallelization for ml computation\\ngraphs. arXiv:2105.04663, 2021.\\nA\\nArchitecture\\nOur Transformer [Vaswani et al., 2017] is based on the architecture in [Radford et al., 2019] with relative positional\\nencodings [Dai et al., 2019]. Text is tokenized via SentencePiece [Kudo and Richardson, 2018] with 32, 000 tokens\\nand a byte-level backoff. We use Megatron-style FFW sharding [Shoeybi et al., 2019] where useful. Parameters are\\nstored in bﬂoat16 but all optimizer statistics are kept in ﬂoat32. As a result, the activations of the language models\\nare calculated in bﬂoat16 (though we explicitly upcast to perform all operations involving a softmax, including the\\nAttention Block and Router, in full ﬂoat32 precision). This is crucial to maintain stability on larger models [Fedus et al.,\\n2021, Rae et al., 2021]. The learning rate starts at 1e-7 and decays to 2e-5 with a cosine decay rate over the entire\\n250, 000 steps, after an initial warmup phase ramping up to 2e-4 in the ﬁrst 1500 steps.\\nWe use seven different model sizes, with names and architectures speciﬁed in the following table. The width of the\\nhidden layer dﬀw is ﬁxed at four times the width of the activations dmodel, and we use the same dimension for keys and\\nvalues.\\nName\\ndmodel\\nnlayers\\nnheads\\nK/V size\\nActual # Params\\n15M\\n512\\n6\\n8\\n32\\n16, 527, 360\\n25M\\n512\\n8\\n8\\n64\\n27, 279, 360\\n55M\\n640\\n10\\n12\\n64\\n57, 369, 600\\n130M\\n896\\n12\\n16\\n64\\n132, 163, 584\\n370M\\n1536\\n12\\n12\\n128\\n368, 123, 904\\n870M\\n2048\\n16\\n16\\n128\\n872, 546, 304\\n1.3B\\n2048\\n24\\n16\\n128\\n1, 308, 819, 456\\nTable 4: Model deﬁnitions used throughout this work.\\nThe number of models we trained was too large to practically include multiple runs of each model with different seeds.\\nTo give an idea of the potential error introduced by random chance, we trained all three routing techniques with 3\\ndifferent seeds on a 130M model for 100, 000 steps with 8 and 256 experts (along with a dense baseline). Results\\nare shown in Fig. 7. Different seeds (which inﬂuence not only parameter initialization but Expert Parallelism – see\\nAppendix C) lead to extremely minimal model divergence after an initial transitory period, with different seeds diverging\\nby no more than 0.01 before 100, 000 steps. This is a close match to the 0.02 error mentioned in [Kaplan et al., 2020]2.\\nB\\nDetailed Routing Techniques\\nHere we detail aspects of the routing techniques crucial to their implementation and provide comparisons to key\\nalternatives.\\n2Anecdotally, throughout the development of this work we used 0.02 as the cutoff to denote statistical signiﬁcance.\\n14\\nUniﬁed Scaling Laws for Routed Language Models\\nFigure 7: Training curves color-coded by random seed for 1, 8 and 256 experts and the step-wise maximum disagreement\\nbetween runs.\\nB.1\\nBalancing Losses\\nWe encourage uniform routing in both our SMoE and RL-R methods with the differentiable load balancing loss adapted\\nfrom the mean square auxiliary loss in Shazeer et al. [2017] and introduced in Lepikhin et al. [2020], Fedus et al. [2021].\\nLB = E ·\\nE\\nX\\ne=1\\nme · ge\\nN\\n(14)\\nWhere me is the mean gate per expert:\\nme = 1\\nN\\nX\\nx∈B\\npe(x)\\n(15)\\nAnd ge is the gating decision per expert:\\nge =\\nX\\nx∈B\\n1{argmax p(x), e}\\n(16)\\nFor x in batch B of size N and policy p(x) = softmax(Wpx + bp). There are two cases where the selected experts\\nmay not be the ones used: in S-BASE after the Sinkhorn redistribution step (see §B.2.1) and when experts are skipped\\ndue to load-balancing (see §C.2). In both cases, the balancing loss is applied to the original gating decisions made by\\nthe policy. We found that the auxiliary loss is less effective if post-balancing experts were considered.\\nB.2\\nSMoE with Sinkhorn redistribution (S-BASE)\\nOur implementation of S-BASE differs from that proposed in Lewis et al. [2021] in two ways. First, we replace the\\nauction algorithm for re-assigning expect selections with a continuous rebalancing process implemented via a Sinkhorn\\nalgorithm [Cuturi, 2013, Peyré and Cuturi, 2019]. Second, we add a shufﬂing step, similar to Lewis et al. [2021], before\\ncomputing the optimal assignment via Sinkhorn per-device (as opposed to across all devices as done in Lewis et al.\\n[2021]). In addition, we did not use any input jitter on the activations sent to ρ as we did not see a noticeable effect. This\\nis in line with BASE but differs from recommendations in other SMOE papers [Lepikhin et al., 2020, Fedus et al., 2021].\\nB.2.1\\nSinkhorn Redistribution\\nWe rebalance expert selections using a Sinkhorn layer applied on top of the router logits, an idea that was explored\\nindependently in parallel by Kool et al. [2021]. This is substantially more efﬁcient on our accelerator cluster than a hard\\nmatching algorithm. We consider H ∈RT ×d the intermediary embeddings of the networks before the application of a\\nrouted layer (folded on the batch and time axes of respective sizes b and t, with T ≜bt). Those are fed to the linear\\nrouter, which output a logits matrix Li = HiW + b ∈RT ×e. Here E is the number of experts, and W ∈Rd×E and\\nb ∈RE are the router parameters. From these logits, SMOE and RL-R computes expert selection probabilities Π by\\napplying a softmax operation along the expert axis. In doing this, we compute selection probabilities for each input\\nseparately, without taking into consideration any capacity constraints on expert, forcing us to introduce load-balancing\\nlater (§C.2). We seek a proper way to integrate constraints in a mathematically grounded framework.\\nMathematically, Π is obtained by solving a simple problem with constraints: each input must, on average, prefer exactly\\none expert. This is made clear by the variational formulation of the softmax:\\nΠ ∈RT ×E ≜[softmax(Li)]i∈[1,T ] =\\nargmax\\nΠ⩾0,\\n∀i∈[T ],P\\nj∈[E] pij=1,\\n⟨Π, L⟩−H(Π)\\n(17)\\n15\\nUniﬁed Scaling Laws for Routed Language Models\\nwhere H is the Shannon entropy of the matrix Π, i.e. H(Π) ≜PT\\ni=1\\nPE\\nj=1 pij log pij, and [·] denotes horizontal\\nstacking. This variational formulation offers a natural alternative to incorporate extra constraints. For ideal performance,\\neach expert should be assigned the same number of tokens on average B =\\nT\\nE . We therefore add E additional\\nconstraints:\\nn\\n∀j ∈[E],\\nT\\nX\\ni=1\\npij = B\\no\\n,\\n(18)\\nwhich yields the doubly constrained regularized linear problem\\nΠ ∈RT ×E ≜argmax⟨Π, L⟩−H(Π),\\n(19)\\nunder the constraints\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nΠ ⩾0,\\n∀i ∈[T], PE\\nj=1 pij = 1\\nT ,\\n∀j ∈[E], PT\\ni=1 pij = 1\\nE\\nthat we recognize as the regularized Kantorovich problem of optimal transport [Kantorovitch, 1958, Cuturi, 2013].\\nWe solve this problem using the Sinkhorn algorithm [Knopp and Sinkhorn, 1967], that takes the logit matrix L ∈RT ×E\\nand returns a soft-assignment matrix Π ∈RT ×E. The Sinkhorn algorithm solves Eq. (19) by alternated ascent in the\\ndual (see Peyré and Cuturi [2019] for details). Starting from f0 = 0 ∈RT and g0 = 0 ∈RE, we set\\n∀i ∈[T],\\n(ft+1)i = −log 1\\nE\\nE\\nX\\nj=1\\nexp(Lij −(gt)j),\\n(20)\\n∀j ∈[E],\\n(gt+1)j = −log 1\\nT\\nT\\nX\\ni=1\\nexp(Lij −(ft+1)i).\\nThese updates converge towards an optimal couple (f, g), such that\\nΠ =\\n1\\nTE exp(L + f ⊕g)\\n(21)\\nis the solution to Eq. (19), where (f ⊕g)ij ≜fi + gj for all i, j ∈[T] × [E]. As detailed below, we early stop the\\niterations (20) by measuring the primal violation of constraints in L1 norm, i.e. when\\nE\\nX\\nj=1\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nT\\nX\\ni=1\\n(Πt)ij −1\\nE\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c +\\nT\\nX\\ni=1\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nE\\nX\\nj=1\\n(Πt)ij −1\\nT\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n⩽etol\\n(22)\\nOnce the plan is computed, we greedily select, for each token, the device with highest device-selection probability,\\neffectively applying an argmax operation on top of the Sinkhorn logits to form a transportation plan projection.\\nComparison to S-BASE and performance.\\nCompared to using an exact (early-stopped) auction algorithm as Lewis\\net al. [2021], the complexity of the Sinkhorn algorithm is in O(N × E) versus O((N × E)3/2), and its update are\\nwell adapted to batch computations on TPU/GPU. In contrast, the auction algorithm must be run on CPU as it is a\\ngreedy per-coordinate algorithm; it becomes a computational bottleneck applied to models with many routed layers.\\nReplacing the softmax output by an regularized optimal transport plan is very naturally interpreted as adding a balancing\\ndistribution constraint to the softmax operator. Using an auction algorithm on top of the softmax assignment does not\\nhave this property.\\nMoreover, the Sinkhorn algorithm can be halted before it has fully converged with a proper tolerance parameter (22)\\nwhere Lewis et al. [2021] uses a hard number of iterations. We ﬁnd an error tolerance of etol = 10−2 gives consistently\\ngood performance. In practice we observe an end-to-end model overhead of 1% to 3% compared to Switch (the\\nsame routing technique without this reassignment). This computational offset is negligible compared to the per-step\\nperformance gain. Without the rebalancing step, Switch is very sensitive to balancing loss hyperparameters (as noted in\\nLewis et al. [2021]) whereas S-BASE maintains uniform routing decisions with improved performance and robustness\\nwhile varying E and N.\\nB.2.2\\nShufﬂing Tokens\\nSimilar to Lewis et al. [2021], we shufﬂe router inputs across workers by ﬁrst computing a random permutation of the\\ninputs and sending the tth row of the batch to the ⌊tE\\nT ⌋th worker. We found that this shufﬂing stage was necessary to\\n16\\nUniﬁed Scaling Laws for Routed Language Models\\n64\\n128\\n256\\n512\\nExpert Count\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n25M \\n130M \\n1.3B \\nRLR-B\\nRLR-G\\nRLR-S\\nFigure 8: The RLR-B method consistently outperforms RLR-G and RLR-S across scales. We found that Nucleus\\nSampling gives a signiﬁcant improvement over Greedy Reinforce. However, performance is slightly improved by\\nadding a learned baseline.\\nprevent training from becoming unstable at larger scales. Our hypothesis is that the re-assignment provides a subtle\\nside channel through which information can be propagated backwards in time, and this can be abused by larger models\\nresulting in the validation loss diverging during training. Adding a shufﬂing stage ameliorates this issue by introducing\\na large number of irrelevant elements to the rebalancing process, making it harder to infer behavior of future inputs.\\nFurther work is needed to conﬁrm this theory, but the introduction of the shufﬂing step does eliminate this performance\\ndegradation.\\nB.3\\nRouting with Reinforcement Learning (RL-R)\\nWe will ﬁrst describe a naive REINFORCE [Williams, 1992] implementation of routing, then describe possible\\nextensions and improvements which lead to the form used in the main text as RL-R.\\nOur implementation of REINFORCE uses the balancing loss in Equation 14 and a policy gradient loss:\\nL = 1\\nN\\nN\\nX\\ni=1\\nlog πi · Ri\\n(23)\\nWhere Ri is the reward for each sequence in the batch of size N and π is the normalized expert preferences output by a\\nlinear transformation as in SMOE. The proper thing is for ρ, the selected experts, to be samples from the distribution π,\\nbut we found that this substantially degraded performance at larger scales. This phenomenon can be attributed towards\\nunwanted interference, where exploratory steps for ρ which turn out to be unnecessary lead to bad gradient updates\\nto the rest of the network [Rosenbaum et al., 2019]. We therefore consider a greedy selection method, where router\\noutputs are selected as ρ(x) = TopK(softmax(Wpx + bp)).\\nWhile sampling (even when tuning softmax temperature) decreased the performance of the model, we would nevertheless\\nlike to regain some of its exploratory power. To ameliorate this, we can use Nucleus Sampling [Holtzman et al., 2019],\\nwhich samples from the top-p set of experts E(p).\\nP ′(e) =\\n\\x1aP(e)/p′\\nif e ∈E(p),\\n0\\notherwise.\\n(24)\\nWhere E(p) is the smallest set of experts such that:\\nX\\ne∈E(p)\\nP(e) ⩾p\\n(25)\\nThis eliminates the possibility of selecting experts with very low likelihood, while still introducing some randomness.\\nIt is important to emphasize that this introduces a distributional shift to the samples, which can be corrected with\\noff-policy correction methods such as Importance Sampling.\\n17\\nUniﬁed Scaling Laws for Routed Language Models\\nAn alternative improvement is to learn an additional baseline function for each router. This method has an additional\\nentropy regularization loss and computes advantages Ai = Ri −bi for the learned baseline bi:\\nL = 1\\nN\\nN\\nX\\ni=1\\nlog pi · Ai −1\\nN\\nN\\nX\\ni=1\\nlog pi · pi + 1\\nN\\nN\\nX\\ni=1\\nvi\\n(26)\\nWhere we use the Huber Loss to calculate the value loss vi.\\nvi =\\n\\x1a 1\\n2(Ri −bi)2\\nif |Ri −bi| ⩽δ,\\nδ(|Ri −bi| −1\\n2δ)\\notherwise.\\n(27)\\nWe numerate three RL-R variants below:\\n• Greedy REINFORCE (RLR-G). REINFORCE selecting the top-k experts and no additional auxiliary losses.\\n• Nucleus-sampled REINFORCE (RLR-S). REINFORCE using nucleus sampling to eliminate less reliable\\nexpert selections and reduce noise in the policy gradient update. In this method we sample from the top-p\\ntruncated distribution. Nucleus sampling at a ﬁxed top-p scales well with increasing the number of experts.\\n• REINFORCE with baseline (RLR-B). Our RL method which stabilizes training with a learned baseline and\\na policy entropy regularization loss. We learn a baseline with a value function that has a single hidden layer of\\nsize dmodel\\n8 .\\nTable 5 details the hyperparameters chosen for each RL-R variant and Fig. 8 contains validation losses across a number\\nof models. Note that the entropy loss is negative to encourage a more concentrated policy, and the weight must be tuned\\njointly with the load balancing loss to keep routing balanced. This is in line with Bengio et al. [2016], who also use two\\nloss terms to both encourage early specialization and expert diversity. Additionally, since the policy entropy loss has a\\nsimilar effect to nucleus sampling, we did not see an improvement from including both regularization methods. RLR-B\\nconsistently performed the best, especially with regards to scalability in E and N. For that reason we selected it as our\\nprime example, and refer to it as RL-R elsewhere.\\nTable 5: Selected hyperparameters for RL-R variants.\\nHyperparameter\\nRLR-G\\nRLR-S\\nRLR-B\\nPolicy entropy weight\\n0.\\n0.\\n-5e-4\\nLoad balancing weight\\n1.\\n1.\\n1.\\nPolicy gradient weight\\n1e-1\\n1e-1\\n1e-2\\nNucleus top-p\\n-\\n0.9\\n1.\\nValue weight\\n-\\n-\\n1e-2\\nValue hidden layers\\n-\\n-\\n1\\nValue loss type\\n-\\n-\\nHuber\\nB.4\\nHash layers (HASH)\\nHASH is simple compared to RL-R or S-BASE, but is highly reliant on the particular choice of hashing function. Many\\nfunctions rely on knowing the integer ID which the tokenizer assigns to each unique token (characters, bytes, subwords,\\netc.). Roller et al. [2021] describe multiple alternative functions, including pre-computing expert assignments for each\\ntoken using a greedy assignment based on the frequency counts of the token on the training set. They do not observe any\\nimprovement in terms of perplexity relative to simpler random assignments of token to expert, but argue that balanced\\nhashing has better properties for distributed training.\\nOur implementation uses a simple modular hashing function, namely the token index modulo the number of experts.\\nTokens are indexed by our tokenizer in an order that is roughly ordered by their underlying frequencies in the training\\ndataset, which means this strategy will be more balanced than an arbitrarily random assignment, while simpler to\\nimplement than fully balanced hashing. We note that poor balancing with increasing expert count is to some extent\\ninevitable for any routing technique that deﬁnes one-to-one mappings between tokens and experts, assuming a bounded\\nExpert Capacity (see Section C.2), as it becomes progressively harder to assign high frequency tokens into a bigger\\nnumber of smaller buckets due to the tokens’ heavy-tailed distribution. This can be seen in Fig. 9.\\n18\\nUniﬁed Scaling Laws for Routed Language Models\\n0\\n2\\n4\\n6\\n8\\n# Experts = 8\\n10\\n3\\n10\\n2\\n10\\n1\\nNormalized token frequency\\ngreedy (0.0% overflow)\\nmodulo (0.0% overflow)\\nrandom (0.0% overflow)\\n0\\n20\\n40\\n60\\n# Experts = 64\\ngreedy (0.8% overflow)\\nmodulo (5.0% overflow)\\nrandom (7.2% overflow)\\n0\\n100\\n200\\n300\\n400\\n500\\n# Experts = 512\\ngreedy (18.9% overflow)\\nmodulo (21.5% overflow)\\nrandom (23.4% overflow)\\nFigure 9:\\nHASH becomes less balanced as E increases. Here we compare three hash routing strategies using the\\ntoken frequency in our validation set. The lines represent the amount of tokens sent to each expert, ordered from most\\nsubscribed to least subscribed. The dotted line represents the point where tokens are likely to overﬂow under our\\nbounded Expert Capacity setup (C = 2). greedy implements Balanced assignment as described in Roller et al. [2021],\\nwhere the per-token frequency tables are pre-computed and tokens are assigned to the most empty expert ordered by\\nfrequency; random assigns each token to a random expert; and modulo uses the technique described in this paper.\\nNote that (a) the token distribution is different from the one used by the tokenizer and (b) this simulation is based on\\nmarginal token frequencies, not batches of sequences. The greedy strategy does improve the workload for the mid\\nrange (E = 64), but not signiﬁcantly for low (E = 8) or high (E = 512) numbers of experts. modulo provides a\\nmodest improvement over random.\\nC\\nDistributed Routing Details\\nHere we describe the key aspects of Routing relevant to training on large clusters. We note there are several libraries\\navailable for supporting large-scale Routing, including DeepSpeed [Kim et al., 2021, Rajbhandari et al., 2022] and\\nGSPMD [Xu et al., 2021]. Unfortunately these were incompatible with our preexisting infrastructure.\\nC.1\\nExpert Parallelism\\nWe brieﬂy review parallelism techniques, building up to Expert Parallelism, a technique for efﬁciently distributing\\nparameters over an accelerator cluster. For a more in-depth exposition we recommend Lewis et al. [2021], Lepikhin\\net al. [2020] or Rajbhandari et al. [2022]. In a fully data-parallel world, every device has an identical copy of all\\nparameters Θ and a different input batch X. Each device executes a forward and backward pass on X and (usually)\\ndoes a synchronous all-reduce across all devices on the gradients to Θ. This is effective, but requires one copy of Θ for\\neach device, wasteful when |Θ| is large.\\nThe general class of techniques known as Model Parallelism reduce this duplication by having any individual device\\nstore only a subset of the entire model parameters. This reduction in memory comes with a cost: no longer can a single\\ndevice take an input and produce the model’s output; that device no longer contains all of Θ. Most techniques therefore\\nrequire some additional synchronization or data exchange.\\nSharding Parallelism [Shoeybi et al., 2019] takes advantage of a mathematical property present both in 2-layer-MLPs\\nand a Transformer’s attention blocks: namely, that the output can be represented as the sum of N components, where\\neach component applies the same functional form with independent weights on the same input. Shoeybi et al. [2019]\\ncontains more details, but a simpliﬁed example can be given for a matrix multiplication where we observe the effect\\nof splitting a matrix into columnwise sub-matrices: Wx = [W1, ..., WN]x = PN\\ni Wix. The effect of applying this\\ntechnique such that each device has a separate subcolumn is to prevent the duplication of the weight matrices (which\\nconsist of the vast majority of Θ). The disadvantage is that all devices must see the same input, meaning the total\\nthroughput of data on the cluster has been reduced N-fold. In addition, the sum described above is actually now a sum\\nacross devices, which introduces additional communication overhead.\\nExpert Parallelism takes further advantage of the structure of a routed layer to similarly reduce the necessity of parameter\\nduplication while avoiding the need to duplicate data between devices. In particular, rather than duplicating experts\\nacross all devices, each device contains only a subset of the experts which are not replicated anywhere else. Different\\ndevices still see different inputs. The key motivation is that a given input x never needs to interact with the parameters\\n19\\nUniﬁed Scaling Laws for Routed Language Models\\ncorresponding to experts which the router did not send x to. Therefore, a single input x need only be present on a single\\ndevice (the one which contains the experts which the router selected for x) to produce the correct output. In order to\\nproduce an output, the router selects an expert for all inputs and an additional data-exchange is introduced which sends\\nall inputs to the device which contains the requested experts. Each device then processes the inputs it was sent, then\\nreturns all inputs to their original devices. Crucially, a roughly uniform router distribution leads to an evenly balanced\\ncomputation across devices. This allows routed layers to be stored across a cluster with no duplicated data and without\\na reduction in data throughput. The downside is that this data exchange required across devices is generally more costly\\nthan the cross-device-sum required by sharding. More details are given in Lewis et al. [2021]. Previous work [Fedus\\net al., 2021] suggests using one expert per device. We believe this to be an implementation detail dependent on many\\naspects of the infrastructure in use. For us, typically using 4 or 8 local experts per device gave good performance.\\nAll of Data, Sharding and Expert parallelism can be applied simultaneously. We use all three methods at will, selecting\\nthe combination which works fastest for a given cluster structure and model size. There are still more variations of\\nmodel parallelism, notably Pipeline Parallelism [Narayanan et al., 2019, Huang et al., 2019], which we do not use.\\nC.2\\nLoad Balancing\\nThis at-will changing of parallelism techniques is dependent on the parallelism not affecting the output of the model.\\nThis is generally true, but expert parallelism brings in one complicating factor: load balancing. In the description above,\\nwe emphasized that a roughly-uniform router (averaged over a minibatch) will send the same number of inputs to each\\ndevice (we will call the expected value BSavg). However, in the worst case all inputs on all devices might select the\\nsame expert, and therefore need to be sent to a single device. If memory is pre-allocated to accommodate this worse\\ncase, then each device must have enough free memory to potentially store the entire global batch size: prohibitive for\\nlarge clusters.\\nThe most common solution is to specify a capacity factor C, and only allocate space for BSavg × C tokens. When an\\nexpert is oversubscribed tokens are dropped at random until no experts are exceeding capacity. Having C > 1 is useful\\nduring training to prevent unnecessarily large numbers of tokens from being dropped. We set C = 2 for all experiments\\n(though during evaluation we always allow all tokens to be routed to the desired expert). This strategy works well for\\nthe Transformer architecture due to its residual connections – dropping a token means skipping that transformer block.\\nAs long as the amount of dropped tokens is kept at a reasonable bound, it does not impact learning.\\nAn optimization we support is allowing an oversubscribed expert to use the memory allocated by an undersubscribed\\nexpert on the same device. This reduces the average number of tokens which are skipped, but does so at the minor cost\\nof introducing an interaction between tokens being skipped and the speciﬁc co-habitation of experts on devices. In\\npractice we do not ﬁnd this to have a large effect. We note that the rebalancing used in S-BASE substantially ameliorates\\nthe load balancing problem by attempting to force all experts to be assigned the same number of tokens. However\\nbecause we use the approximate Sinkhorn algorithm, not a hard matching algorithm, over-subscription still happens\\n(though at a much reduced rate) and so these steps are still taken.\\nD\\nArchitectural Variations\\nThroughout this work we have focused on a narrow subset of possible Routing Net architectures, which we believe are\\nrepresentative of recent work on large scale Routing Nets [Roller et al., 2021, Fedus et al., 2021, Lewis et al., 2021,\\nShazeer et al., 2017, Artetxe et al., 2021, Lepikhin et al., 2020]. However, we also experimented with many variations\\nof these architectures, some of which we highlight now in more depth.\\nD.1\\nRobustness to hyper-parameter changes\\nWe evaluated the robustness of S-BASE and RL-R to changes in hyperparameters in Fig. 10. We focus on E = 512 due\\nto anecdotal experience that the largest performance variance occurred at this scale. RL-R is found to be highly sensitive\\nto the hyperparameters in Table 5, especially the choice of balancing weight. In addition, changes to the policy entropy\\nweight can lead to unbalanced routers when the balancing weight is not tuned jointly.\\nUnlike Switch which has been shown to be sensitive to the choice of balancing loss [Roller et al., 2021], S-BASE is\\nrobust to changes in balancing weight for values of 1e −3 to 1. S-BASE also has competitive performance without\\na balancing loss, but training is less stable. Additionally, Switch has higher expert oversubscription rates even when\\ntuning the balancing weight.\\n20\\nUniﬁed Scaling Laws for Routed Language Models\\n0\\n50k\\n100k\\n150k\\n200k\\n250k\\nSteps\\n2.4\\n2.6\\n2.8\\n3\\n3.2\\nValidation Loss\\nHash\\nRL-R\\nS-BASE\\nFigure 10: Hyperparameter sensitivity at 512E 55M. For RL-R, hyperparameter selection has the largest impact on\\nmodel performance of the three methods. The top performing RL-R models outperform HASH and are comparable with\\nS-BASE. However, non-optimal RL-R conﬁgurations perform worse than the other two methods.\\n0.03\\n0.0625 0.125\\n0.25\\n0.5\\n1.0\\nRouting Frequency\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n55M \\n370M \\n870M \\n0.03\\n0.0625 0.125\\n0.25\\n0.5\\n1.0\\nRouting Frequency\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n0.03\\n0.0625 0.125\\n0.25\\n0.5\\n1.0\\nRouting Frequency\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nS-BASE\\nHash\\nFigure 11: The model performance improves with increasing routing frequency for S-BASE, while HASH ﬂattens at\\nhigher frequencies for 8E (left), 64E (middle) and 256E (right).\\nD.2\\nVarying Routing Frequencies\\nAll of our models thus far have been routed every other layer with experts which are single FFWs [Lepikhin et al.,\\n2020, Fedus et al., 2021]. However, Lewis et al. [2021], Roller et al. [2021] explored stacking FFWs in the experts and\\nplacing N routed layers at\\nL\\nN+1... NL\\nN+1. We consider the performance impact of alternative routing frequencies, varying\\nthe frequency R = N\\nL and placing routed layers at L\\nN ... NL\\nN .\\nWe compare routing every layer to routing at frequencies R ∈{ 1\\n2, 1\\n4, 1\\nL}. For routing a single layer we chose the\\nsecond to last layer [Roller et al., 2021], but consider routing at L\\n2 in subsection D.4. S-BASE scales well with routing\\nfrequency, but HASH degrades in performance as shown in Fig. 11. At a single routed layer, HASH has the lowest\\nvalidation loss across model sizes.\\nD.3\\nVarying the Routing Policy\\nMotivated by the improved scaling results for S-BASE, we investigate whether learning a routing policy becomes more\\nbeneﬁcial as the frequency of routers increases.\\nShared routing decisions.\\nIn Fig. 12, the routing decisions are made at the ﬁrst routed layer and shared across layers,\\nwhich keeps the number of routers constant as R increases. As HASH selects experts based on the token index at the\\ninput layer, its routing function is unchanged for this variant. S-BASE and HASH have similar losses for shared routing\\ndecisions, whereas S-BASE improves when learning to route at each expert layer.\\nPermuting the hash function.\\nConversely, we tested a variant of HASH where the hash function at each router uses\\na static permutation of the input tokens to select the experts. This allows tokens to be routed to the same expert at\\n21\\nUniﬁed Scaling Laws for Routed Language Models\\n0.125\\n0.25\\n0.5\\n1.0\\nRouting Frequency\\n2.4\\n2.5\\n2.6\\n2.7\\n2.8\\n2.9\\nValidation Loss\\n64E\\n64E\\n256E\\n256E\\nS-BASE\\nHash\\nShared S-BASE\\nFigure 12: Shared expert selections across layers has a large effect on performance for S-BASE (in grey) at 25M. S-BASE\\nscales similarly to HASH in the single router case.\\n1\\n8\\n64\\n256\\nExpert Count\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n55M \\n370M \\n870M \\nS-BASE\\nHash\\n(a)\\n10k\\n50k\\n100k\\nSteps\\n2.3\\n2.4\\n2.5\\n2.6\\nValidation Loss\\nS-BASE\\n3x S-BASE\\nHash\\n3x Hash\\n(b)\\nFigure 13: (a) S-BASE and HASH scale similarly when routing a single layer at L −1. (b) We see similar performance\\nfor S-BASE and HASH at 32E 1.3B when routing at L\\n2 with three FFWs per expert. However, S-BASE performance is\\nimproved for interleaving three routed layers.\\nsome layers without having the same hash. We found that performance was unchanged for this variant, suggesting that\\nincreasing the number of possible routing paths does not necessarily impact performance for static policies.\\nThese router variants suggest that methods which can adapt to each expert layer will outperform static policies. Further\\nwork is needed in analyzing how policies can more effectively learn to route across layers.\\nD.4\\nRouting a Single Layer\\nWe analyzed the scaling behavior of HASH and S-BASE when only routing a single layer. We observed that the routing\\ngains for R = 1\\nL deviated from higher frequencies, which also impacted R = 1\\n4 to a lesser degree. We attribute this\\nperformance regression to the suboptimal behavior of the ﬁrst routed layer. In both cases the total number of routers is\\nlow, and the ﬁrst layer has a larger impact on overall performance than at higher routing frequencies. For R = 1\\nL the\\ncomplexity of routing is reduced and a simpler routing method can reach competitive performance. HASH and S-BASE\\nhave similar performance across expert counts in this case, as shown in Fig. 13.\\nWe also compared routing a single layer at L\\n2 with three FFWs per expert to three evenly spaced routed layers in Fig. 13.\\nSimilar to the results shown in [Roller et al., 2021], three evenly spaced routed layers has slightly better performance\\nthan three stacked FFWs for a 32E 1.3B model. We also found that S-BASE beneﬁts more from interleaving the routed\\nand dense layers, which is consistent with our routing frequency results.\\n22\\nUniﬁed Scaling Laws for Routed Language Models\\n108\\n109\\nParameter Count\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\nValidation Loss\\n1020\\nTotal FLOPS\\n1\\n2\\n4\\nDense\\nS-Base(64E)\\nFigure 14: Example of scaling curves for a dense model and a S-BASE (64E) model. When looking at performance per\\nparameter, higher values of K are always better. But lower values of K are generally more ﬂop efﬁcient, and achieve a\\nbetter loss for a given FLOP budget.\\nD.5\\nVarying number of experts per datapoint\\nIn this work we have focused on routing each datapoint to a single expert at all routing layers, i.e. for the case where\\nK = 1. However, SMoE models have historically routed datapoints to more than one expert [Shazeer et al., 2017,\\nLepikhin et al., 2020, Ramachandran and Le, 2018]. Increasing K incurs in extra computation on the experts, but this\\nadditional computation may be helpful for the end result, reﬂecting in better loss. Moreover, routing a datapoint through\\nmore experts means each expert gets to see more data for each forward pass, which may speed up training. For these\\nreasons, it is not obvious that K = 1 is the best setup. Section 4.4 investigated this and argued both that the generalized\\nformula Equation (2) can accommodate such cases and also that the resulting ﬁts show no substantial difference in\\nperformance for K. However we explore this variance more in Fig. 14: plotting both scaling curves for varying values\\nof K as well as plotting the loss in terms of F. Higher values of K invariably yield better performance per step, but\\nthey are not necessarily more ﬂop efﬁcient. In fact, K = 1 is always in the pareto front. We can verify that this holds\\nfor varying numbers of experts.\\nNote that this difference in ﬂop-efﬁciency is not only theoretical, and is also followed by increased communication\\ncosts when using expert parallelism. We observed in practice that reducing K by half amounted to close to 2x speedup\\nin inference and training.\\nE\\nEffects of scaling strategy on Zero-shot Transfer\\nThere is a strong relationship between the validation loss we have been discussing and the downstream performance\\nof models and speciﬁc tasks [Kaplan et al., 2020]. However, recent work has shown that this relationship is not as\\nstraightforward for large Routing Networks, and individual tasks can beneﬁt more or less from expert scaling. For\\nexample, Artetxe et al. [2021] show a narrowing performance gap between a SMOE Routing Network with E = 512\\nand its dense equivalent, with more marked improvement from routing in some tasks like HellaSwag and PIQA than in\\nin tasks like Winogrande and ReCoRD. Likewise, Fedus et al. [2021] shows that Switch beneﬁts more from scale better\\nin TrivaQA than in SuperGlue.\\nA detailed analysis of the scaling properties of Routing Networks and how that transfers to downstream tasks merits\\ndedicated work. Here we start the conversation by looking at zero-shot transfer on a set of well known downstream tasks:\\nLAMBADA [Paperno et al., 2016], The Pile [Gao et al., 2020], Curation Corpus [Curation, 2020], WikiText-103 [Merity\\net al., 2016] and C4 [Raffel et al., 2020].\\nWe estimate the scaling coefﬁcients individually for each task and routing technique. For simplicity of interpretation we\\nignore the bounded scaling term and focus on the bilinear ﬁt on Eq. 7. The coefﬁcients can be seen in Table H. We\\nexpect that scaling in both N and E will improve the downstream performance. The key question revolves around\\nunderstanding changes in the relative magnitude of a, b and c as we move from task to task.\\nViewing Table H it is immediately clear that the individual scaling coefﬁcients vary greatly across tasks, i.e. different\\ntasks have different relative gains at Zero-Shot performance as we move to larger scales. This can be better shown in\\nFig. 15, where all coefﬁcients are displayed in a single plot. The variation across tasks are not the same for a and b. e.g.\\n23\\nUniﬁed Scaling Laws for Routed Language Models\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\n0.200\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nSize Scaling (a)\\nValidation Set\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\nLAMBADA\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\nThe Pile\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\nCC\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\nWikiText-103\\n0.175\\n0.150\\n0.125\\n0.100\\n0.075\\nExpert Scaling (b)\\nC4\\nHash\\nRL-R\\nS-Base\\nFigure 15:\\nIndividual scaling coefﬁcients for different tasks and routing techniques, compared to the coefﬁcients\\nestimated in the validtion set. Different techniques also scale differently depending on the task, but this also depends on\\nthe interaction term (see Fig. 16)\\nWikiText-103 has higher values for b and lower for a when compared to the validation set. This means that even though\\ntasks see monotonic improvement in performance by scaling through either adding more experts or increasing the base\\nmodel size, some tasks beneﬁt more and some less from which method is used.\\nFor a more complete picture, we can account for the N and E interaction coefﬁcient c by incorporating it into one of\\nthe scaling coefﬁcients – by holding the other quantity ﬁxed – which leads to a(E) and b(N) (see Section 4.2). This\\ncan be seen in Fig. 16 for varying values of N and E.\\nWe see that S-BASE tends to dominate with lower coefﬁcients at higher values of E and N (due to its smaller interaction\\nterm relative to scaling terms), but this varies across tasks. For example, RL-R shows better b(N) for most values of N\\nin LAMBADA, until it is overtaken by S-BASE at N = 410M, but S-BASE is always superior in C4. Moreover, the\\nordering is not consistent between HASH and RL-R across tasks, even though they often do not cross. This all means it\\nis difﬁcult to establish superior performance of a routing technique without looking at a variety of tasks and scales.\\nWe often want to compare Routing Networks with a dense baseline with the same performance on the validation set,\\nand see how this changes on downstream tasks. We can use these parameters in a simpliﬁed version of the Effective\\nParameter Count (EPC, Equation 11), by assuming Estart = 1 and Emax = ∞, such that ˆ\\nE = E. First, we note that\\nsince the coefﬁcients vary greatly across tasks, each task will have a different EPC for the same network conﬁguration.\\nMoreover, the effects of scaling by varying E and N will vary across tasks. Say we have a routing net of size N with E\\nexperts and we want to increase its base model size by a factor of x while keeping the same number of experts. The\\neffect on ¯\\nN in this case will be a multiplication by xa(E)/a(1) = x1+c log E. Since c varies per task, the improvement\\nachieved by increasing the base model size will also be task dependent.\\nSay we have a routing net of size N with E experts and we want to increase its base model size by a factor of x while\\nkeeping the same number of experts. The effect on ¯\\nN in this case will be a multiplication by xa(E)/a(1) = x1+ c\\na log E.\\nSince c\\na varies per task, the improvement achieved by increasing the base model size will also be task dependent.\\nFor example, the EPC validation for N=110M, E=32 is 370M, but EPC lambada for the same model is 284M, while EPC\\npile is 535M. The key implication here is not only do the values change, but their slopes are different. This means that\\ndownstream tasks must be analyzed carefully: a practitioner could scale a model via routing expecting some overarching\\nimprovement, but get a much diminished (or enhanced!) improvement on speciﬁc downstream tasks, depending on\\ntheir speciﬁc values of b and c.\\nF\\nOn Convergence, or Lack Thereof\\nHere we digress on two important details, both focusing on token count. First we argue that discussing converged\\nperformance of large transformers on modern and massive text datasets is probably a misnomer; scaling analyses should\\nfocus on optimal performance at a ﬁxed number of tokens. Second, we provide evidence arguing against a proposed\\nequation in Kaplan et al. [2020] (Eq. (1.6)).\\nF.1\\nConvergence on Large Datasets\\nThere are two cases where the converged performance of a model can be clearly deﬁned. The ﬁrst is when continued\\ntraining of the model produces no improved results (even analyzed at logarithmic scale in the number of tokens), the\\nsecond is when continued training leads to reduced validation performance: overﬁtting.\\nOur models exhibit neither behavior. No overﬁtting is seen even for our largest models, likely due to the complexity\\nand size of the dataset used. Furthermore, despite being trained for 130 billion tokens, not even our smallest models\\n24\\nUniﬁed Scaling Laws for Routed Language Models\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.080\\n0.075\\n0.070\\n0.065\\n0.060\\n0.055\\na(E)\\nValidation Set\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.21\\n0.20\\n0.19\\n0.18\\n0.17\\nLAMBADA\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.11\\n0.10\\n0.09\\n0.08\\nThe Pile\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.100\\n0.095\\n0.090\\n0.085\\n0.080\\n0.075\\n0.070\\nCC\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.090\\n0.085\\n0.080\\n0.075\\nWikiText-103\\n1\\n8\\n64\\n512\\n# Experts (E)\\n0.070\\n0.065\\n0.060\\n0.055\\n0.050\\n0.045\\nC4\\nHash\\nRL-R\\nS-Base\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.035\\n0.030\\n0.025\\n0.020\\nb(N)\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.07\\n0.06\\n0.05\\n0.04\\n0.03\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.060\\n0.055\\n0.050\\n0.045\\n0.040\\n0.035\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.030\\n0.025\\n0.020\\n0.015\\n0.010\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.0350\\n0.0325\\n0.0300\\n0.0275\\n0.0250\\n0.0225\\n0.0200\\n25M\\n55M\\n130M\\n370M\\n1.3B\\nBase Model Size (N)\\n0.0300\\n0.0275\\n0.0250\\n0.0225\\n0.0200\\n0.0175\\n0.0150\\n0.0125\\nHash\\nRL-R\\nS-Base\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\n1\\n8\\n64\\n512\\n# Experts (E)\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\n25M 55M 130M\\n370M\\n1.3B\\nBase Model Size (N)\\nFigure 16: Estimated scaling coefﬁcent for Zero-shot performance across different datasets. Top half: The coefﬁcient\\nfor increasing E while keeping N ﬁxed for varying values of N at different downstream tasks. Middle: The coefﬁcient\\nfor increasing N varying values of E.\\n10B\\n100B\\n1T\\nToken Count\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\nValidation Loss\\n10B\\n100B\\n1T\\nToken Count\\n0.030\\n0.028\\n0.026\\n0.024\\n0.022\\n0.020\\nb: Slope of Expert Improvement\\n1-Experts\\n64-Experts\\n128-Experts\\n512-Experts\\n75000 Steps Trained\\n250000 Steps Trained\\n1000000 Steps Trained\\nFigure 17: On the left: validation performance over time for 15M models trained with different expert counts and over\\nthree different lengths of training. On the right, the coefﬁcient b from ﬁtting Eq. (4), representing scaling from E across\\nintermediate values.\\nhave saturated. We push this envelope even further: training two additional sets of 15M models with 1, 64, 128 and 512\\nexperts. The ﬁrst set is trained for just 75, 000 steps, and the second for 1, 000, 000 steps: four times more data (half a\\ntrillion tokens). We highlight that this involves corresponding changes to the cosine cycle decay. We exclusively train\\nHASH models, both due to limits in the number of extra models we were able to train and also because it has the largest\\nvalue of Emax.\\nResults from these models are plotted in Fig. 17 (left). 15M with no routing, the smallest model we train as part of this\\nwork, is still far from having saturated its performance capability. Indeed, training for 4x longer further reduces the\\nvalidation loss by 0.05. This pattern continues, and is exacerbated, when increasing the expert count: the same model\\nwith E = 512 gets a 0.07 reduction in loss from 4x more tokens.\\n25\\nUniﬁed Scaling Laws for Routed Language Models\\n(a)\\n(b)\\n(c)\\nFigure 18: a) Values of a found for dense models across training. b) RMSE for these same ﬁts. c) Three attempts to ﬁt\\nEq. (29). In black the standard ﬁt. In orange and grey ﬁts only using and ignoring the ﬁnal 150, 000 steps respectively.\\nIt is clear then that the very smallest models considered have yet to converge. The same is certainly true for larger ones,\\nand probably more so. If 500 billion tokens is a lower bound to the convergence point of 15M, the analysis in Kaplan\\net al. [2020] would predict needing trillions of tokens to converge 1.3B: much more than what was used to train some\\nof the largest language models yet created [Brown et al., 2020]. For large, complex text datasets of the scale used to\\ntrain large language models, convergence is not a proper criteria.\\nF.2\\nPerformance Qualiﬁed on Token Count\\nRather than claiming analysis at a non-observed point of convergence, we emphasize that the scaling behavior we have\\ndescribed in this work is valid only as a function of a particular number of steps (or tokens). At each point, we can\\ndeﬁne instantaneous values of scaling coefﬁcients, with the values from all models taken at S steps3.\\nIn fact, the situation is more complicated that simply conditioning our scaling coefﬁcients on token count. We can see\\nthis by plotting b, the scaling coefﬁcient for changes in expert-count in Fig. 17(right). An immediate observation is that\\nthe values of b are non-constant, supporting the need to qualify scaling on token count. A second, more substantial\\npoint, is that these values are not uniquely deﬁned by token count. For a given number of tokens, the scaling behavior\\nof three different sets of models is completely different, dependent on how far into the learning rate schedule those sets\\nof models were. We note that this behavior is suggested by experiments in Kaplan et al. [2020] (App. D.6).\\nAttempting to ﬁnd the full set of parameters on which these scaling terms depend is beyond the scope of this work. We\\nhighlight just the importance of insuring that all variables possible are matched when comparing values to calculate\\nscaling coefﬁcients.\\nF.3\\nPerformance Modeled as L(N, S)\\nWe conclude by highlighting one implication of the fact that scaling coefﬁcients are dependent on token count. We\\nanalyze only the dense models trained as part of this work, and calculate values of a in Equation (3) for all dense models\\ntrained as part of the primary sweep across all step counts; plotted in Fig. F.3(a) with RMSE values plotted in Fig. F.3(b).\\nFirst, it is important to emphasize that the ﬁts remain good throughout S (after an initial period of transience). Namely,\\nthough the slope is different, the validation losses for a given intermediate S follow a power law about as well as they\\ndo later in training (if anything, more so). Second, the estimated coefﬁcients a are clearly monotonically increasing\\nwith S.\\n[Kaplan et al., 2020] propose (Eq. 1.6) a uniﬁed prediction of the loss achieved by a model with size N training for S\\nsteps:\\nL(N, S) =\\n\\x12Nc\\nN\\n\\x13αN\\n+\\n\\x12Sc\\nS\\n\\x13αS\\n(28)\\n3This sidesteps the issue of critical batch size [McCandlish et al., 2018, Kaplan et al., 2020], consideration of which requires a\\nsubstantially larger sweep of models. Future work estimating the critical batch size will likely lead to better model ﬁts.\\n26\\nUniﬁed Scaling Laws for Routed Language Models\\n15M\\n25M\\n55M\\n130M\\n370M\\n870M 1.3B\\n7B\\nDense Model Size\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\nRL-R\\nDense\\nFigure 19: RL-R performance for 64E continues to scale well compared to dense up to 7B base model size.\\nThis comes with the subtlety that S must be deﬁned as the number of steps when training at the averaged critical\\nbatch size, where our models are trained with a ﬁxed batch size. This means a proper analysis must use Smin with\\nS = S(1 +\\nBc\\nBL−αB )−1 for constants Bc and αB. It is important to highlight however that Smin, as described in Kaplan\\net al. [2020], should be independent of N. This implies that\\n∂\\n∂N (L(N, S)) is independent of S, or in log-log space:\\n∂log(L(N ∗, S))\\n∂N ∗\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nN ∗=10N = −αN\\n(29)\\nThis prediction of constant scale is in concrete opposition to the increasing value seen in Fig. F.3(a). We can furthermore\\ncheck that this functional form cannot be obviously ﬁt to our learning curves, with examples show in Fig. F.3(c).\\nThere are subtle differences between training setups, and we do not want to claim our experiments wholly disprove the\\nconjecture in [Kaplan et al., 2020]. However, the results in Fig. F.3 motivate us to assume that Eq. (F.3) cannot be used\\nto model our speciﬁc training curves. A consequence of this is that we can also no longer conclude Equation B.5 from\\n[Kaplan et al., 2020], that:\\nL(Neff(C), C) = (1 + αN\\nαS\\n)L(Neff, ∞)\\n(30)\\nWith this equation, we might be able to lower-bound true converged performance (which we have not seen in our\\nmodels) by inference from compute-efﬁcient performance, which has been achieved by the majority of our models.\\nG\\nLarge Scale Routing Behavior, Coefﬁcient Sensitivity, and Future Work\\nOur analysis predicts that larger values of E will continue to improve performance, especially for small models, at a\\ndiminishing rate. §5.2 also predicts that routing will continue to help with increasing N for at least one, if not two orders\\nof magnitude larger base model size. Practical compute limitations prevented our sweep from exploring these regimes,\\nand there are interesting unanswered questions in the limit of these two variables. In particular, exact predictions of\\nNcutoff are highly dependent on the precise value of b, where error in the second decimal place shifts predicted values by\\norders of magnitude (not surprising, as it is the slope of a line in log-log space).\\nWe believe exploring the limit behavior of N and E, especially arriving at a more precise value of b, is crucial.\\nAnecdotally, we can report the results of one experiment: a large RL-R model with N > 7, 000, 000, providing a\\nrough upper bound for error in b for RL-R. In particular, we trained a model with dmodel = 4096, nlayers = 32,\\nnheads = 32, E = 64 and K/V size of 128. There are some important eccentricities of this model which affect its match\\nto the ﬁts described in this work: it was trained with a batch size of 1024 for 100k steps with a policy gradient weight of\\n1e-1 and balancing weight of 1e-1. Other training details are consistent with Section 2.1.\\nThe performance of this model, relative to a dense model of the same size and also to a number of smaller models,\\nis plotted in Fig. 19 evaluated at 100B tokens. The changes described above prevent the analysis in this work from\\naccurately predicting this model’s performance, but one key feature remains: the routed 7B model substantially\\noutperforms the baseline. This is of particular interest since just a 0.01 decrease in b would predict an Ncutoff at 9B,\\n27\\nUniﬁed Scaling Laws for Routed Language Models\\nmeaning we would already be close to the regime where routing would cease to work. Nevertheless, at this value\\nrouting is clearly still a major improvement, and our estimate of b is unlikely to be a substantial overshoot.\\nWhile the differences between this model and those analyzed in the paper make concrete extrapolation impossible, it\\nshows that routing techniques still maintain competitive improvements at almost an order of magnitude larger value of\\nN than analyzed and it is unlikely the scaling coefﬁcients measured in this work substantially overestimate the routing\\ntechnique’s scalability. We encourage future work probing the limits of routing networks, both in N and E, to better\\nunderstand their properties and provide more accurate predictions of their scaling coefﬁcients.\\nH\\nExtra Plots and Tables\\nThis section contains some helpful visualizations and data which are not included in the main text.\\nTable 6: Values of b(N) with hold-out RMSEs in parentheses.\\n15M\\n25M\\n130M\\n370M\\n1.3B\\nS-BASE\\n-0.035\\n-0.031\\n-0.029\\n-0.024\\n-0.019\\n(0.035)\\n(0.019)\\n(0.017)\\n(0.014)\\n(0.012)\\nRL-R\\n-0.033\\n-0.031\\n-0.027\\n-0.022\\n-0.016\\n(0.016)\\n(0.013)\\n(0.013)\\n(0.014)\\n(0.009)\\nHASH\\n-0.031\\n-0.029\\n-0.025\\n-0.021\\n-0.015\\n(0.039)\\n(0.029)\\n(0.023)\\n(0.016)\\n(0.011)\\na\\nb\\nc\\nd\\nS-BASE\\n0.079\\n0.088\\n0.007\\n1.072\\nRL-R\\n0.080\\n0.105\\n0.010\\n1.076\\nHASH\\n0.081\\n0.097\\n0.009\\n1.086\\nTable 7: Fits to Equation (7).\\nS-BASE\\nRL-R\\nHashLayers\\n4\\n0.077\\n0.075\\n0.077\\n8\\n0.073\\n0.073\\n0.075\\n32\\n0.070\\n0.067\\n0.069\\n64\\n0.066\\n0.063\\n0.066\\n128\\n0.064\\n0.060\\n0.063\\n256\\n0.058\\n0.056\\n0.059\\n512\\n0.060\\n0.053\\n0.056\\nTable 8: Values of a(E) for different values of E\\n.\\n1 2 4 8\\n32\\n128 512\\nExpert Count\\n \\nHash\\nFigure 20: Afﬁne ﬁts for HASH with a shared slope in grey.\\n28\\nUniﬁed Scaling Laws for Routed Language Models\\n1\\n2\\n4\\n8\\n16 32 64 128256512\\nExpert Count\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n109\\n1011\\nTotal Parameters\\n100\\n101\\n102\\nParameter Utilization Ratio\\nFigure 21: The validation loss for all S-BASE models plotted as a function of expert count (left), the total number of\\nparameters (center) and the ratio of parameters to TeraFLOPs per inference (right).\\n1\\n2\\n4\\n8\\n16 32 64 128256512\\nExpert Count\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n109\\n1011\\nTotal Parameters\\n100\\n101\\n102\\nParameter Utilization Ratio\\nFigure 22: The validation loss for all RL-R models plotted as a function of expert count (left), the total number of\\nparameters (center) and the ratio of parameters to TeraFLOPs per inference (right).\\n1\\n2\\n4\\n8\\n16 32 64 128256512\\nExpert Count\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n109\\n1011\\nTotal Parameters\\n100\\n101\\n102\\nParameter Utilization Ratio\\nFigure 23: The validation loss for all HashLayer models plotted as a function of expert count (left), the total number of\\nparameters (center) and the ratio of parameters to TeraFLOPs per inference (right).\\n1\\n2\\n4\\n8\\n16 32 64 128256512\\nExpert Count\\n2\\n2.4\\n2.8\\n3.2\\nValidation Loss\\n1\\n2\\n4\\n8\\n16 32 64 128256512\\nExpert Count\\n2\\n2.4\\n2.8\\n3.2\\n1\\n10\\n100\\nParameter Utilization Ratio\\n2\\n2.4\\n2.8\\n3.2\\n1\\n10\\n100\\nParameter Utilization Ratio\\n2\\n2.4\\n2.8\\n3.2\\nS-BASE\\nRL-R\\nFigure 24: Fitting S-BASE and RL-R with Eq. (1) and Eq. (2).\\n29\\nUniﬁed Scaling Laws for Routed Language Models\\n1\\n10\\nParameter Utilization Ratio\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n1\\n10\\nParameter Utilization Ratio\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n1\\n2\\n4\\nFigure 25: Joint ﬁts to Equation (2) for K ∈{1, 2, 4}.\\n1\\n10\\n100\\nParameter Utilization Ratio\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\nValidation Loss\\n1\\n10\\n100\\nParameter Utilization Ratio\\n2.0\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n0.25\\n0.5\\n1.0\\nFigure 26: Joint ﬁts to Equation (2) for R ∈{0.25, 0.5, 1.0}.\\n30\\nUniﬁed Scaling Laws for Routed Language Models\\na\\nb\\nc\\nd\\nRMSE\\npolicy\\ndataset\\nDense\\nValidation Set\\n-0.078\\n1.063\\n0.014\\nLAMBADA\\n-0.203\\n1.952\\n0.039\\nThe Pile\\n-0.102\\n1.239\\n0.020\\nCC\\n-0.097\\n1.133\\n0.041\\nWikiText-103\\n-0.090\\n1.172\\n0.015\\nC4\\n-0.066\\n1.009\\n0.014\\nHash\\nValidation Set\\n-0.082\\n-0.102\\n0.009\\n1.102\\n0.022\\nLAMBADA\\n-0.213\\n-0.167\\n0.015\\n2.049\\n0.051\\nThe Pile\\n-0.111\\n-0.161\\n0.014\\n1.325\\n0.023\\nCC\\n-0.101\\n-0.101\\n0.010\\n1.177\\n0.045\\nWikiText-103\\n-0.093\\n-0.086\\n0.007\\n1.208\\n0.027\\nC4\\n-0.070\\n-0.088\\n0.008\\n1.045\\n0.021\\nS-Base\\nValidation Set\\n-0.081\\n-0.092\\n0.008\\n1.086\\n0.025\\nLAMBADA\\n-0.211\\n-0.152\\n0.012\\n2.020\\n0.048\\nThe Pile\\n-0.110\\n-0.117\\n0.008\\n1.309\\n0.028\\nCC\\n-0.100\\n-0.101\\n0.010\\n1.154\\n0.050\\nWikiText-103\\n-0.092\\n-0.074\\n0.005\\n1.194\\n0.025\\nC4\\n-0.068\\n-0.081\\n0.007\\n1.031\\n0.024\\nRL-R\\nValidation Set\\n-0.081\\n-0.107\\n0.010\\n1.090\\n0.022\\nLAMBADA\\n-0.212\\n-0.190\\n0.016\\n2.030\\n0.051\\nThe Pile\\n-0.110\\n-0.149\\n0.012\\n1.320\\n0.030\\nCC\\n-0.100\\n-0.113\\n0.011\\n1.156\\n0.045\\nWikiText-103\\n-0.092\\n-0.091\\n0.008\\n1.195\\n0.023\\nC4\\n-0.069\\n-0.092\\n0.009\\n1.033\\n0.022\\nTable 9: Scaling coefﬁcients for different downstream tasks.\\n31\\n', 'source_name': 'Unified Scaling Laws for Routes Language Models', 'source_url': 'https://arxiv.org/abs/2202.01169'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ST_MoE.pdf #6\n",
      "{'content': 'ST-MOE: DESIGNING STABLE AND TRANSFERABLE\\nSPARSE EXPERT MODELS\\nBarret Zoph∗\\nGoogle Brain\\nIrwan Bello∗†\\nGoogle Brain\\nSameer Kumar\\nGoogle\\nNan Du\\nGoogle Brain\\nYanping Huang\\nGoogle Brain\\nJeff Dean\\nGoogle Research\\nNoam Shazeer†\\nGoogle Brain\\nWilliam Fedus∗\\nGoogle Brain\\nABSTRACT\\nScale has opened new frontiers in natural language processing – but at a high cost.\\nIn response, Mixture-of-Experts (MoE) and Switch Transformers have been pro-\\nposed as an energy efﬁcient path to even larger and more capable language models.\\nBut advancing the state-of-the-art across a broad set of natural language tasks has\\nbeen hindered by training instabilities and uncertain quality during ﬁne-tuning.\\nOur work focuses on these issues and acts as a design guide. We conclude by\\nscaling a sparse model to 269B parameters, with a computational cost comparable\\nto a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-\\nof-Experts or ST-MoE-32B). For the ﬁrst time, a sparse model achieves state-\\nof-the-art performance in transfer learning, across a diverse set of tasks includ-\\ning reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum,\\nCNN-DM), closed book question answering (WebQA, Natural Questions), and\\nadversarially constructed tasks (Winogrande, ANLI R3). 1\\n∗Equal contribution. Correspondence to {barretzoph,liamfedus}@google.com.\\n†Work was done while at Google.\\n1Code for our models is available at https://github.com/tensorflow/mesh/blob/master/\\nmesh_tensorflow/transformer/moe.py\\n1\\narXiv:2202.08906v2  [cs.CL]  29 Apr 2022\\nCONTENTS\\n1\\nIntroduction\\n3\\n2\\nBackground\\n3\\n3\\nStabilizing Training of Sparse Models\\n5\\n3.1\\nStability and Quality Tradeoffs when Removing Multiplicative Interactions\\n. . . .\\n6\\n3.2\\nStability and Quality Tradeoffs when Adding Noise . . . . . . . . . . . . . . . . .\\n6\\n3.3\\nStability and Quality Tradeoffs when Constraining Activations and Gradients\\n. . .\\n7\\n3.4\\nSelecting a Precision Format: Trading Efﬁciency and Stability\\n. . . . . . . . . . .\\n8\\n4\\nFine-Tuning Performance of Sparse Models\\n9\\n4.1\\nHypothesis: A Generalization Problem . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n4.2\\nFine-Tuning a Subset of Model Parameters to Improve Generalization . . . . . . .\\n11\\n4.3\\nSparse and Dense Models Require Different Fine-Tuning Protocols . . . . . . . . .\\n11\\n4.4\\nSparse Models Are Robust to Dropped Tokens During Fine-Tuning . . . . . . . . .\\n12\\n4.5\\nInserting Sentinels Tokens During Fine-Tuning\\n. . . . . . . . . . . . . . . . . . .\\n13\\n5\\nDesigning Sparse Models\\n13\\n5.1\\nSetting the Number of Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n5.2\\nChoosing the Capacity Factor and Routing Algorithm . . . . . . . . . . . . . . . .\\n14\\n6\\nExperimental Results\\n16\\n6.1\\nST-MoE-L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n6.2\\nST-MoE-32B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n7\\nTracing Tokens Through the Model\\n19\\n7.1\\nEncoder Experts Exhibit Specialization\\n. . . . . . . . . . . . . . . . . . . . . . .\\n19\\n7.2\\nDecoder Experts Lack Specialization . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n7.3\\nMultilingual Experts Specialize, But Not by Language\\n. . . . . . . . . . . . . . .\\n21\\n8\\nRelated Work\\n21\\n9\\nDiscussion\\n22\\n10 Conclusion\\n24\\nA Token Load Balance Description\\n31\\nB\\nRouter Z-Loss Training Dynamics\\n31\\nC Improved Architectural Modiﬁcations\\n32\\nD Batch Prioritized Routing for Lower Capacity Factors\\n33\\nE\\nPre-Training Dataset Details\\n34\\nF\\nFull Fine-tuning Sensitivity Data\\n35\\nG Optimally Setting the Routing Threshold\\n36\\nH Mesh Layout for Data, Model and Expert Parallelism with Few Experts\\n36\\nI\\nNote on Communication Costs for Distributed Models\\n37\\nJ\\nNegative Results\\n38\\n2\\n1\\nINTRODUCTION\\nSparse expert neural networks showcase the advantage of sheer scale and offer an efﬁcient alternative\\nto the static neural network architectures commonly used today (Raffel et al., 2019; Brown et al.,\\n2020; Rae et al., 2021). Rather than applying the same parameters to all inputs, sparse expert\\nnetworks dynamically select which parameters to use for each input (Shazeer et al., 2017). This\\nallows for networks to vastly expand their number of parameters, while keeping the FLOPs per\\ntoken roughly constant. These approaches have yielded state-of-the-art translation models (Lepikhin\\net al., 2020), 4-7x pre-training speed-ups (Fedus et al., 2021; Artetxe et al., 2021), and GPT-3 level\\none-shot performance using 1/3 the energy training cost (Du et al., 2021). And despite a shocking\\nnumber of parameters, sparse models reduce the carbon footprint for training large neural networks\\nby an order of magnitude (Patterson et al., 2021). However, difﬁculties remain.\\nFedus et al. (2021) observed that a sparse 1.6T parameter model achieved a 4x pre-training speed-up\\nover the prior state-of-the-art (Raffel et al., 2019), but lagged smaller models when ﬁne-tuned on\\ncommon benchmarks like SuperGLUE. Similar gaps were observed in Artetxe et al. (2021) when\\nMoE language models were ﬁne-tuned on out-of-domain data. In response, Switch-XXL, a model\\nwith fewer parameters, but a 8x-larger computational footprint (FLOPs approximately equal to the\\nlargest T5 model), was proposed and improved quality on natural language understanding tasks.\\nHowever, necessary pre-training was hampered by training instabilities previously undetected during\\nsmaller scale studies. These instabilities were later identiﬁed in other sparse models (Du et al., 2021).\\nThese results revealed a necessary balance of parameters and computation, but left an open question\\non how to reliably train these types of models.\\nOur aim in this paper is to increase the practicality and reliability of sparse models. We study these\\ntwo issues and pre-train a 269B sparse model that achieves state-of-the-art results when ﬁne-tuned\\nacross many competitive NLP benchmarks, including SuperGLUE. We also put forth additional\\nanalysis and a design guide (or at least, our heuristics) for sparse expert models. Furthermore, this\\nwork emphasizes jointly optimizing both the upstream pre-training and the downstream ﬁne-tuning\\nmetrics to avoid discrepancies (Tay et al., 2021).\\nContributions\\n1. A large-scale study of the quality-stability trade-offs of stability techniques.\\n2. An introduction of the router z-loss that resolves instability issues, while slightly improv-\\ning model quality.\\n3. A ﬁne-tuning analysis of sparse and dense models highlighting different hyperparameter\\nsensitivity to the batch size and learning rate. We show bad hyperparameters result in\\nvirtually no ﬁne-tuning gain over dense models, despite large pre-training speed-ups.\\n4. Architectural, routing and model design principles for designing Pareto efﬁcient sparse\\nmodels in a distributed setting.\\n5. A qualitative analysis tracing token routing decisions across expert layers.\\n6. A 269B sparse model (the Stable Transferable Mixture-of-Experts or ST-MoE-32B)\\nwhich achieves state-of-the-art performance across a diverse set of natural language\\nbenchmarks.\\n2\\nBACKGROUND\\nSparse expert models typically substitute a neural network layer with a set of experts, each having\\nunique weights (Jacobs et al., 1991; Jordan and Jacobs, 1994). Typically all the experts within a layer\\nare of the same type and shape (homogeneous), however, varied (heterogeneous) expert-types are\\npossible. Inputs are only processed by a subset of the experts to save computation, so a mechanism\\nmust be added to determine where to send each input. Usually a router or gating network determines\\nwhere to send inputs (i.e. words, sentences, image patches, etc.), but alternative schemes have been\\nproposed (Lewis et al., 2021; Roller et al., 2021; Zuo et al., 2021; Clark et al., 2022).\\n3\\nSpeciﬁcally, in natural language processing, Shazeer et al. (2017) proposed a Mixture-of-Experts\\n(MoE) layer which takes a token representation x as input and routes it to the best matched top-\\nk experts selected out of a set {Ei(x)}N\\ni=1 of N experts. The router variable Wr produces logits\\nh(x) = Wr · x which are normalized via a softmax distribution over the available N experts at that\\nlayer. The gate-value for expert i is given by\\npi(x) =\\neh(x)i\\nPN\\nj eh(x)j\\n(1)\\nand the token x is routed to the experts with the highest top-k gate values (set of indices T ). The\\noutput of the layer is the weighted sum of each expert’s computation by the gate value\\ny =\\nX\\ni∈T\\npi(x)Ei(x)\\n(2)\\nOriginally proposed in LSTMs (Hochreiter and Schmidhuber, 1997), expert layers were later used in\\nthe Transformer (Vaswani et al., 2017) by Shazeer et al. (2018) and Lepikhin et al. (2020). Follow-\\non work by Fedus et al. (2021) simpliﬁed the MoE further to route tokens to a single expert (top-1)\\nand reduced other costs to improve training efﬁciency.\\nTo improve hardware utilization, most implementations of sparse models have static batch sizes\\nfor each expert (Shazeer et al., 2017; 2018; Lepikhin et al., 2020; Fedus et al., 2021). The expert\\ncapacity refers to the number of tokens that can be routed to each expert. If this capacity is exceeded\\n(the router sends too many inputs to that expert) then the overﬂowed tokens have no computation\\napplied to them and are passed to the next layer through a residual connection.\\nTerminology\\nDeﬁnition\\nExpert\\nAn independently-learned neural network with unique weights.\\nRouter\\nA network that computes the probability of each token getting sent to each\\nexpert.\\nTop-n Routing\\nRouting algorithm where each token is routed to n experts.\\nLoad Balancing Loss\\nAn auxiliary (aux) loss to encourage each group of tokens to evenly distribute\\nacross experts.\\nGroup Size\\nThe global batch size is split into smaller groups, each of size Group Size. Each\\ngroup is considered separately for load balancing across experts. Increasing it\\nincreases memory, computation, and communication.\\nCapacity Factor (CF)\\nEach expert can only process up to a ﬁxed number of tokens, which is often\\nset by evenly dividing across experts, tokens\\nexperts. The capacity factor can expand or\\ncontract this amount to CF · tokens\\nexperts.\\nFFN\\nAcronym of Feed Forward Network (FFN) layer of Transformer consisting of\\nlinear, activation, linear.\\nEncoder-Decoder\\nA Transformer architectural variant that all of our models are based on. Consists\\nof an encoder that does all-to-all attention on the inputs and a decoder that\\nattends to the encoder and to its own inputs in an autoregressive manner.\\nallreduce\\nCommunication primitive which sums a subset of n tensors on n different de-\\nvices, then broadcasts the summed value to all n devices. This is used in dis-\\ntributed training for gradient accumulation and model parallelism.\\nall2all\\nCommunication primitive where each device sends to every other device a part\\nof its tensor. Used in sparse Transformer models for token routing.\\n(↑/↓)\\nIndicates whether higher/lower values are better (e.g. accuracy/train loss).\\nTable 1: Terminology used throughout the paper.\\nThe batch B of input tokens is broken into G unique groups across the data-parallelism dimension2,\\neach with size B/G. The expert capacity is equal to CF · tokens/experts where CF represents the\\n2Our implementation relies on einsums with one-hot tensors for dispatching and combining tensors to/from\\nexperts. The size of this one-hot tensor grows quadratically with the number of tokens being routed as a group\\nwhich motivates breaking the batch into smaller groups. This may be avoided with sparse lookup operations.\\n4\\ncapacity factor hyperparameter, experts is the number of experts and tokens is the group size. If the\\ncapacity factor is increased, it creates extra buffer so that fewer tokens will be dropped in case of load\\nimbalance. However, increasing the capacity factor also increases the memory and computational\\ncosts, so there exists a trade off3.\\nFinally, an auxiliary load balancing loss encourages tokens to be roughly evenly distributed across\\nthe experts (Shazeer et al., 2017). This improves the hardware efﬁciency by ensuring that all accel-\\nerators are processing signiﬁcant chunks of data in parallel as mentioned above. The details of the\\nloss are presented in Appendix A. However, alternatives exist: Lewis et al. (2021) and Clark et al.\\n(2022) treats balanced token allocation as an assignment problem and removes the auxiliary loss\\nentirely.\\n3\\nSTABILIZING TRAINING OF SPARSE MODELS\\nSparse models often suffer from training instabilities (Figure 1) worse than those observed in stan-\\ndard densely-activated Transformers.\\n0\\n2500\\n5000\\n7500\\n10000\\n12500\\n15000\\nStep\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nTraining Loss\\n0\\n2500\\n5000\\n7500\\n10000\\n12500\\n15000\\nStep\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nTraining Loss\\nFigure 1: Training instabilities for sparse models. We refer to training instabilities as divergences\\nin the training loss. Above are two runs from sparse models FLOP-matched to the T5-XL version\\n(Raffel et al., 2019) each trained with a batch size of 1M tokens using the Adafactor optimizer\\n(Shazeer and Stern, 2018). (Left) An unstable training run. (Right) A stable training run.\\nIt’s straightforward to ﬁnd changes that improve the stability, however, these often come at an un-\\ntenable expense to model quality (for instance, using an arbitrarily small learning rate or using tight\\ngradient clipping). We categorize and examine several approaches to improve stability. The sta-\\nbility techniques span generic ﬁxes to Transformers as well as those speciﬁc to sparse models: (1)\\nRemove multiplicative interactions (2) Inject model noise (3) Constrain activations, and gradients.\\nWe conclude with our recommendation: a new auxiliary loss, the router z-loss, which signiﬁcantly\\nimproves training stability with no quality degradation. This is an adaptation of the z-loss used for\\nﬁnal softmax logits in the Mesh Tensorﬂow codebase (Shazeer et al., 2018).\\nStabilizing Sparse Models\\n1. Many methods stabilize sparse models, but at the expense of worse quality.\\n2. The router z-loss stabilizes models without quality degradation.\\n3. Transformer modiﬁcations with more multiplicative components (GEGLU, RMS nor-\\nmalization) worsen stability, but boost quality.\\nDesigning a large-scale stability study.\\nWe design a large-scale stability study of sparse models\\nFLOP-matched to the T5-XL version (Raffel et al., 2019) pre-trained on the multilingual corpus\\nmC4 (Xue et al., 2020). Each sparse model has 32 experts and we introduce a sparse MoE layer for\\n3See Fedus et al. (2021) for a graphical illustration of how the capacity factor works.\\n5\\nevery fourth FFN. The train capacity factor is 1.25 and the eval capacity factor is 2.0. See Table 11\\nfor a more detailed description of models used throughout this paper. For each stability technique,\\nwe record the fraction that are stable, the mean quality (negative log perplexity on English), and the\\nstandard deviation over seeds.\\nThe primary issue in constructing this study is that small models are rarely unstable but large un-\\nstable models are too costly to run for sufﬁcient steps and seeds. We found a sparse model FLOP-\\nmatched to T5-XL to be good object of study because it was unstable roughly 1/3 of the runs, but\\nwas still relatively cheap to train. Furthermore, we run our instability experiments on multilingual\\ndata since we ﬁnd this exacerbates model instabilities, allowing us to experiment on slightly smaller\\nmodels. See Section 9 for more details. Our baseline conﬁguration is trained using six random seeds\\nand each conﬁguration with a stability technique uses three random seeds. We use six seeds for the\\nbaseline to better characterize the instability rate and three seeds for the variants to save compute.\\nEach model is pre-trained for 20k steps on mC4 using a masked language modeling objective (Fedus\\net al., 2018; Devlin et al., 2018).\\n3.1\\nSTABILITY AND QUALITY TRADEOFFS WHEN REMOVING MULTIPLICATIVE\\nINTERACTIONS\\nSome architectural improvements involve more multiplications than additions or do not sum many\\nitems at once. For example, a matrix multiplication has one multiplication for each addition and\\nhence we do not refer to it as a “multiplicative” operation. We present and analyze the impact of\\ntwo instances of multiplicative interactions in Transformers here.\\nGELU Gated Linear Units (GEGLU).\\nOur ﬁrst example is the Gated Linear Unit (Dauphin\\net al., 2017) which is a component-wise product of two linear projections, one of which is ﬁrst\\npassed through a sigmoid function. Shazeer (2020) extends this to other variants and presents a\\nGELU-Linear (Hendrycks and Gimpel, 2016) FFN layer as a replacement the usual ReLU (Nair and\\nHinton, 2010) FFN in Transformer.\\nFFNGEGLU(x, W, V, b, c) = GELU(xW + b) ⊙(xV + c)\\n(3)\\nThis quality gain was corroborated in later work (Narang et al., 2021).\\nRoot Mean Square Scale Parameters.\\nOur second example is the scale parameter in root mean\\nsquare (RMS) normalization (Zhang and Sennrich, 2019). Within the Transformer, rather than call-\\ning layers back-to-back, there is an internal structure (referred to as sublayer calls) which improve\\ngradient propagation and training dynamics. Our sublayer calls match that of Raffel et al. (2019)\\nand consist of: (1) RMS normalization, (2) layer call (e.g. Self Attention), (3) dropout (Srivastava\\net al., 2014), (4) add residual (He et al., 2015). RMS normalization scales the input vector x ∈Rd\\nelement-wise per the root-mean-square. It then rescales the output element-wise by multiplying with\\na learned scale parameter g.\\nyi =\\nxi\\nq\\n1\\nd\\nPd\\ni=1 x2\\ni\\n· gi\\n(4)\\nTable 2 shows that both removing GEGLU layers or the RMS scale parameter improves stability, but\\nat a signiﬁcant loss to model quality. We note that these scale parameters (g) have a disproportionate\\ngain to model quality versus parameters elsewhere (e.g. FFN). In line with our ﬁndings, Shleifer\\net al. (2021) found adding a learned multiplicative scalar to the residual connection in Transformers\\nmade them much more unstable.\\nIn Appendix C, we further study the quality impact of adding new multiplicative interactions in\\nexpert layers. We ﬁnd that this operation yields quality improvements with virtually no slow-down\\nin model step time.\\n3.2\\nSTABILITY AND QUALITY TRADEOFFS WHEN ADDING NOISE\\nWe next explore a hypothesis that adding noise into the model can improve training stability (Nee-\\nlakantan et al., 2015). Taleb (2012) argues that certain systems exhibit the property of anti-fragility,\\nwhere they improve through noise. Inspired by the concept and by our observation that ﬁne-tuning\\n6\\nMethod\\nFraction Stable\\nQuality (↑)\\nBaseline\\n4/6\\n-1.755 ±0.02\\nRemove GEGLU\\n3/3\\n-1.849 ±0.02\\nRemove RMS Norm. Scale Param\\n3/3\\n-2.020 ±0.06\\nTable 2: Removing operations with more multiplicative interactions. Multiplicative interactions\\nimprove quality, but can destabilize training. Individually removing two sources of multiplicative\\ncomponents improves the stability, but worsens quality signiﬁcantly. When we remove the GEGLU\\nlayer, we replace it with with an equivalent Dense-ReLU-Dense layer to match the FLOPs and\\nparameters.\\n(which injects noise via dropout) was rarely unstable, we examined whether training noise might\\nimprove the stability of sparse models. Table 3 shows a stability improvement versus the baseline,\\nbut at the expense of lower quality. We also ﬁnd that input-jitter, introduced by Fedus et al. (2021),\\ndiminishes quality at XL-scale, hence we ablate it in our models. Input-jitter multiplies the input\\nlogits to the router by a uniform random variable between [1 −10−2, 1 + 10−2] . Dropout in our\\nablation is applied throughout the Transformer. As seen previously, improvements in small-scale\\nsettings may fail to generalize when scaled up and therefore trends should always be monitored and\\nre-assessed at increasing scale (Kaplan et al., 2020).\\nMethod\\nFraction Stable\\nQuality (↑)\\nBaseline\\n4/6\\n-1.755 ±0.02\\nInput jitter (10−2)\\n3/3\\n-1.777 ±0.03\\nDropout (0.1)\\n3/3\\n-1.822 ±0.11\\nTable 3: Injecting noise during training. Both input-jitter and dropout improve stability, but lead to\\na signiﬁcant loss of model quality. There is a clear tradeoff with most methods: when one improves\\nstability, it then typically decreases model quality. Our work aims to ﬁnd methods that ﬁx stability\\nwithout hurting quality.\\n3.3\\nSTABILITY AND QUALITY TRADEOFFS WHEN CONSTRAINING ACTIVATIONS AND\\nGRADIENTS\\nOne of the most successful approaches to stabilizing neural networks are constraints on activations,\\nand gradients (Pascanu et al., 2013; Ioffe and Szegedy, 2015; Salimans and Kingma, 2016; Ba et al.,\\n2016). A popular approach consists in the clipping of gradient norms to remedy exploding gradients\\nwhile backpropagating through deep networks (Pascanu et al., 2013).\\nIn this work, we use the Adafactor optimizer due to its memory efﬁciency (though recently in-\\ntroduced 8-bit optimizers (Dettmers et al., 2021) may offer better trade-offs). Instead of gradient\\nclipping, Adafactor uses update clipping, where the changes to the weights are constrained to be\\nbelow a certain norm. We experiment with tightening the update clipping to a smaller value.\\nNext, we study constraints on the logits going into the router. The router computes the probability\\ndistribution over the experts in float32 precision (i.e. selective precision) (Fedus et al., 2021).\\nHowever, at the largest scales, we ﬁnd this is insufﬁcient to yield reliable training. To ﬁx this, we\\nintroduce the router z-loss,\\nLz(x) = 1\\nB\\nB\\nX\\ni=1\\n\\uf8eb\\n\\uf8edlog\\nN\\nX\\nj=1\\nex(i)\\nj\\n\\uf8f6\\n\\uf8f8\\n2\\n(5)\\nwhere B is the number of tokens, N is the number of experts, and x ∈RB×N are the logits going\\ninto the router. This penalizes large logits into the gating network and Section 3.4 contains a more\\ndetailed explanation of why the z-loss before the router is useful.\\n7\\nTable 4 shows that both update clipping and the router z-loss stabilize the model in all 3 runs, but the\\nupdate clipping signiﬁcantly hurts the model quality. Therefore we use the z-loss method for ﬁxing\\nour model stability due to improved quality and stability4.\\nMethod\\nFraction Stable\\nQuality (↑)\\nBaseline\\n4/6\\n-1.755 ±0.02\\nUpdate clipping (clip = 0.1)\\n3/3\\n-4.206 ±0.17\\nRouter Z-Loss\\n3/3\\n-1.741 ±0.02\\nTable 4: Constraining weight updates and router logits. Constraining the update clipping in\\nAdafactor improves stability, but at a catastrophic loss of quality. Looser clipping values did not\\nreliably stabilize training so we exclude them here. The router z-loss stabilizes the model without\\nany quality degradation (in this case, we observe a slight quality boost).\\nThe router z-loss introduces another hyperparameter (cz), which is the coefﬁcient to weight this as\\npart of the total loss optimized. The total loss is a linearly weighted combination of the cross entropy\\nloss (LCE), the auxiliary load balance loss (LB), and the router z-loss (LZ), yielding a total loss\\nLtot = LCE + cBLB + czLZ\\n(6)\\nWe choose a value of cz = 0.001 based on the best model quality after pre-training with a hyperpa-\\nrameter sweep. Appendix B logs the resulting losses over the course of pre-training.\\n3.4\\nSELECTING A PRECISION FORMAT: TRADING EFFICIENCY AND STABILITY\\nAs in most modern distributed Transformers we train with mixed precision (Micikevicius et al.,\\n2017) 5. Weights are stored in float32 for gradient updates and then converted to bfloat16\\nwhen doing matrix multiplications in the forward and backward pass6. Furthermore, all activations\\nare stored and operated on in bfloat16 and allreduce communications can be done in either\\nbfloat16 or float32 numerical precision. For the largest model explored in this work (ST-\\nMoE-32B presented later) we ﬁnd speed-ups halving the numerical precision of the allreduce,\\nhowever this also can destabilize the training so we keep this as float32 throughout this work.\\nA lower precision format enables more efﬁcient models by reducing (a) communication costs be-\\ntween processors and memory, (b) computation costs, (c) memory for storing tensors (e.g. activa-\\ntions). However, lower precision formats come at the expense of larger roundoff errors which can\\nlead to irrecoverable training instabilities.\\nMantissa (7 bits)\\nExponent (8 bits)\\nExponent (8 bits)\\nMantissa (23 bits)\\nPrecision Format: Float32\\nPrecision Format: BFloat16\\nNumber Range\\nMax BFloat16\\nRoundoff Error\\nMax Float32\\nRoundoff Error\\n[2, 4)\\n0.01563\\n2.34x10^(-7)\\n[32, 64)\\n0.25\\n3.81x10^(-6)\\n[1024, 2048)\\n8.0\\n0.00012\\n[2^20, 2^21)\\n8192.0\\n0.125\\n[2^30, 2^31)\\n8288608.0\\n128.0\\nFigure 2: Numerical precision formats and roundoff errors. Larger numbers have larger roundoff\\nerrors. bfloat16 has up to 65,536x worse roundoff errors than float32. The router z-loss\\nencourages the absolute magnitude of numbers to be small, which doesn’t hinder model performance\\nand reduces roundoff errors. The router z-loss is most effective into functions where larger errors\\ncan drastically change the relative output (e.g. exponential and sinusoidal functions).\\n4We also experimented with adding z-losses onto the attention logits which also improves model instability\\nwithout hurting model quality.\\n5See Mesh Tensorﬂow for implementation details: https://github.com/tensorflow/mesh/\\nblob/master/mesh_tensorflow/\\n6Matrix multiplications on TPUs perform multiplications in bfloat16 and accumulations in float32.\\n8\\nUnderstanding precision format and roundoff errors.\\nFigure 2 reviews the properties of differ-\\nent precision formats and their corresponding roundoff errors for different number ranges. Numbers\\nin any range of two consecutive powers of 2 (e.g. [2,4) and [1024, 2048)) are represented by a ﬁxed\\nnumber of mantissa bits (7 for bfloat16, 23 for float32). As a result, (1) bfloat16 will\\nhave about 65,536x (i.e. 23 −7 = 16 additional bits and 216 = 65536) as large roundoff errors as\\nfloat32 and (2) larger numbers have larger roundoff errors. Due to the 8 exponent bits, number\\ncan get as large as ≈3e38, which leads to even float32 having some issues with roundoff errors.\\nSparse expert models are sensitive to roundoff errors because they have more exponential\\nfunctions due to the routers.\\nSparse expert models introduce additional exponential functions –\\nthrough the router – which can exacerbate roundoff errors7 and lead to training instabilities. While\\na roundoff error does not change the ordering of probabilities within a softmax operation, it does\\nimpact the routing of the second token in MoE due to relative thresholding (e.g. a token is only\\nrouted to its second place expert if the gating probability for the second expert is 1/5 as large as\\nthat of the ﬁrst expert). Additionally, roundoff errors can drastically change the probability that\\nscales the expert output – which we have found to be important. Finally, we conjecture that the\\nhigher stability we observed for decoder-only models (not shown here) was because they had fewer\\nexponential functions. Section 9 contains a more detailed discussion.\\nAn aside on the router z-loss.\\nOne might think that the router z-loss is a convoluted method\\nreplaceable by clipping logits (Wu et al., 2016). We explain why this is not the case. The goal is to\\nminimize large roundoff errors going into exponential functions. Clipping the logits occurs after any\\nroundoff errors – resulting in even larger discontinuities. In one view, clipping in itself is a roundoff\\nerror; conversely, the z-loss naturally encourages the model to produce logits that are small in value\\nand thus more accurately modeled. Due to these dynamics, we ensure all exponentiated tensors are\\ncast to float32. This hints at the possibility of better number formats for neural networks because\\nof the unused exponent bits when z-losses are added throughout the network (see Section 9).\\n4\\nFINE-TUNING PERFORMANCE OF SPARSE MODELS\\nThe best performing language models are usually obtained by (1) pre-training on large amounts of\\ndata (e.g. the internet) followed by (2) ﬁne-tuning on a task of interest (e.g. SuperGLUE). Promis-\\ning new techniques have emerged as an alternative, including few-shot inference (Brown et al.,\\n2020), preﬁx tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), and adapter modules\\n(Houlsby et al., 2019) – however, a quality gap still persists compared to ﬁne-tuning. Because of\\nthis, we focus on ﬁne-tuning in this work, but highlight recent successes of sparse models in few-\\nshot settings from Du et al. (2021); Artetxe et al. (2021). Further, we leave as future work techniques\\nthat adapt large language models through reinforcement learning (Ouyang et al., 2022)\\n4.1\\nHYPOTHESIS: A GENERALIZATION PROBLEM\\nSparse models have performed remarkably well in the regime of large datasets, but have sometimes\\nperformed poorly when ﬁne-tuning (Fedus et al., 2021; Artetxe et al., 2021). We present evidence\\nfor a (not so surprising) hypothesis that sparse models are prone to overﬁtting. We illustrate this\\nproblem through two tasks in SuperGLUE (Wang et al., 2019) – Commitment Bank (De Marneffe\\net al., 2019) and ReCORD (Zhang et al., 2018). Commitment Bank (CB) has 250 training examples\\nwhile ReCORD has over 100,000. This signiﬁcant size discrepancy facilitates a natural study for\\noverﬁtting on two tasks selected as part of the same benchmark.\\nIn Figure 3, we compare the ﬁne-tuning characteristics of the Dense L and the ST-MoE-L model.\\nEach model was pre-trained on 500B tokens from the C4 corpus (Raffel et al., 2019). The models\\n7Exponential functions have the property that a small input perturbation can lead to a large difference in\\nthe output. As an example, consider inputting 10 logits to a softmax function with values of 128 and one logit\\nwith a value 128.5. A roundoff error of 0.5 in bfloat16 will alter the softmax output by 36% and incorrectly\\nmake all logits equal. The calculation goes from\\nexp(0)\\nexp(0)+10·exp(−0.5) ≈0.142 to\\nexp(0)\\nexp(0)+10·exp(0) ≈0.091.\\nThis occurs because the max is subtracted from all logits (for numerical stability) in softmax operations and\\nthe roundoff error changes the number from 128.5 to 128. This example was in bfloat16, but analogous\\nsituations occur in float32 with larger logit values.\\n9\\nStep\\n80.0\\n82.5\\n85.0\\n87.5\\n90.0\\n92.5\\n95.0\\n97.5\\n100.0\\nMetric\\nSuperGLUE CB Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_eval\\nStep\\n84\\n86\\n88\\n90\\n92\\n94\\n96\\n98\\n100\\n102\\nMetric\\nSuperGLUE ReCoRD Task\\nSparse train_eval\\nSparse validation_eval\\nDense train_eval\\nDense validation_eval\\nFigure 3: Sparse models are prone to overﬁt. We plot train and validation curves for our ST-MoE-\\nL and a dense-L models ﬁne-tuned on the CB task (250 train sequences) and ReCoRD (138k train\\nsequences). In both cases, the sparse model learns more quickly on the train partition (blue exceeds\\ngreen line). However, for the smaller CB task, the dense model outperforms the sparse model on the\\nheld-out validation set (red vs. orange). In contrast, on the larger ReCoRD task, the sparse model\\noutperforms the dense model by several percentage points.\\nare designed to be roughly FLOP matched variants of the T5-Large encoder-decoder models from\\nRaffel et al. (2019) with 770M parameters. The ST-MoE models have 32 experts with an expert\\nlayer frequency of 1/4 (every fourth FFN layer is replaced by an MoE layer). The pre-training and\\nﬁne-tuning train capacity factor is 1.25 and the eval is 2.0. We evaluate performance on the held-out\\nvalidation and train dataset partitions.\\nAcross both tasks, the sparse model converges faster to 100% train set accuracy supporting that\\nsparse models optimize effectively under a data distribution shift. On the larger task, ReCORD, the\\nvalidation quality of the sparse model follows the boost in training and signiﬁcantly exceeds the\\ndense model. However, on the smaller task, CB, the sparse model lags its dense counterpart on held-\\nout data. As per the recommendation of Fedus et al. (2021), we consider increasing the dropout\\nwithin the expert hidden state (i.e. expert dropout), but ﬁnd that at this scale, higher values only\\nmoderately improve quality (Figure 4). We study further improvements to ﬁne-tuning in Section 4.2\\nand hyperparameter sensitivity in Section 4.3.\\n0.0\\n0.1\\n0.2\\n0.3\\nDropout Probability\\n80\\n82\\n84\\n86\\nSuperGLUE Score\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nExpert Dropout Probability\\n80\\n82\\n84\\n86\\nSuperGLUE Score\\nFigure 4: Regularization studies of sparse models for ﬁne-tuning. For each setting, we train\\nthree random seeds till convergence on SuperGLUE. We ﬁnd that increased regularization through\\ndropout provides modest boosts. (Left) demonstrates peak SuperGLUE ﬁne-tuning quality at a\\nglobal dropout rate of 0.1. Higher values over-regularize and severely hurt quality. (Right) Starting\\nwith the best known global dropout rate of 0.1, we selectively increase the expert dropout (an in-\\ndependent dropout rate on the expert hidden activation). This yields further generalization beneﬁts\\nand is in line with the ﬁndings of Fedus et al. (2021).\\n10\\n4.2\\nFINE-TUNING A SUBSET OF MODEL PARAMETERS TO IMPROVE GENERALIZATION\\nTo combat overﬁtting we experiment updating only a subset of models parameters during ﬁne-\\ntuning. Figure 5 measures quality for updating 5 different subsets of parameters: all parameters\\n(All), only non MoE parameters (Non MoE), only MoE parameters (MoE), only the self-attention\\nand enc-dec attention parameters (Attention) and only the non MoE FFN parameters (FFN).\\nAll\\nNon MoE\\nMoE\\nAttention\\nFFN\\nParameters Being Updated\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\nSuperGLUE Score\\nFigure 5: Updating only a subset of model parameters during ﬁne-tuning. To improve the gener-\\nalization of sparse models and combat overﬁtting, we ﬁne-tune a subset of the model parameters. All\\nresults are with the ST-MoE-L model and are an average of 5 different random seeds. We observe\\nthat updating 3/5 of the subsets of parameters appear to work about the same, while ﬁne-tuning only\\nthe MoE parameters results in a drastic quality reduction.\\nWe observe that updating the non MoE parameters works about as well as updating all the param-\\neters and updating only the FFN parameters works a bit better. Updating only the MoE parameters\\nsigniﬁcantly degrades ﬁne-tuning performance, which is where ≈80% of model parameters are.\\nOnly updating the non MoE parameters can be an effective way to speedup and reduce memory for\\nﬁne-tuning.\\nWe hypothesize that ﬁne-tuning only the MoE parameters leads to bad performance since expert\\nlayers only occur every 1/4 layers and a token will see at most two experts per layer. Therefore,\\nupdating the MoE parameters will affect much fewer layers and FLOPs than updating any other\\nsubset of the parameters we tried. Updating only the MoE parameters resulted in a much larger\\ntraining loss than updating the non MoE parameters, even though there are signiﬁcantly more pa-\\nrameters. We further observe that updating all the non-MoE parameters results in a higher training\\nloss than updating all the parameters, but unfortunately this regularization effect didn’t translate to\\nbetter validation performance.\\nFurther, one regularizer we tried was a dropout variant where entire experts were masked out\\nstochastically during training. However, this failed to improve generalization in our preliminary\\nstudies. Appendix J expands on this experiment and contains other negative results.\\n4.3\\nSPARSE AND DENSE MODELS REQUIRE DIFFERENT FINE-TUNING PROTOCOLS\\nHow sensitive are sparse and dense models to the ﬁne-tuning protocol? We study two hyperparam-\\neters: the batch size and the learning rate. We pretrain a Dense-L and ST-MoE-L on 500B tokens\\nof C4 and then ﬁne-tune on SuperGLUE. Figure 6 summarizes our experiments with the full data\\npresented in Table 20 (Appendix F). Across all hyperparameter settings, the sparse models (orange)\\noutperform the dense (blue) counterparts – however, the best setting for each can materially change\\nresults. Sparse and dense models have vastly different performance across different batch sizes and\\nlearning rates. Sparse models beneﬁt from smaller batch sizes and a higher learning rate. Consis-\\ntent with the overﬁtting hypothesis (Section 4.1), both these changes might improve generalization\\nthrough higher noise in the ﬁne-tuning process. Finally, we point out the importance of correctly\\ntuning the batch size and learning rate during ﬁne-tuning. Simply using the same ﬁne-tuning hyper-\\n11\\nparameters that worked well for the dense model can mask any pre-training improvements obtained\\nby the sparse model.\\n65k\\n262k\\n1M\\nBatch Size\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\nSuperGLUE Score\\nDense\\nSparse\\n1e-4\\n5e-4\\n1e-3\\nLearning Rate\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\nSuperGLUE Score\\nDense\\nSparse\\nFigure 6: Batch size and learning rate sensitivity. We measure differences and sensitivity to ﬁne-\\ntuning protocols between dense (blue) and sparse (orange) models. Each bar is an average across 6\\ndifferent runs with different hyperparameters. On SuperGLUE, sparse models beneﬁt from noisier\\nhyperparameters including small batch sizes and high learning rates. Dense models behave nearly\\noppositely. See Appendix F for all data.\\n4.4\\nSPARSE MODELS ARE ROBUST TO DROPPED TOKENS DURING FINE-TUNING\\nSparse models route tokens to one or more experts at each layer. To make these models efﬁcient in\\nthe SPMD paradigm with modern hardware, the expert capacity (the number of tokens each expert\\nprocesses) needs to be ﬁxed ahead of time (see Section 2 for more details). When an expert receives\\nmore tokens than its capacity, the extra tokens are dropped — no computation is applied to those\\ntokens. We again try to prevent this by (1) pre-training with an auxiliary loss that promotes equal\\namounts of tokens getting sent to each expert and (2) a capacity factor (a hyperparameter) that adds\\nroom for extra tokens at each expert. We experiment with turning off the auxiliary loss during\\nﬁne-tuning and using different capacity factors. Tables 5 reveals a surprising result that ﬁne-tuning\\nquality is not materially impacted by dropping up to 10-15% of tokens8. Studies on ST-MoE-32B\\ncorroborate that high capacity factors do not improve ﬁne-tuning quality. This is in-line with ﬁndings\\nof Yang et al. (2021) that unequal load balance may not signiﬁcantly impact model quality.\\nModel\\nTrain CF\\nEval CF\\nAux Loss\\nPercent Tokens Dropped\\nSuperGLUE (↑)\\nSparse\\n0.75\\n2.0\\nYes\\n10.6%\\n86.5 ± 0.21\\nSparse\\n1.25\\n2.0\\nYes\\n0.3%\\n86.7\\nSparse\\n2.0\\n3.0\\nYes\\n0.0%\\n85.8\\nSparse\\n4.0\\n5.0\\nYes\\n0.0%\\n86.4\\nSparse\\n0.75\\n2.0\\nNo\\n15.6%\\n85.7\\nSparse\\n1.25\\n2.0\\nNo\\n2.9%\\n85.8\\nSparse\\n2.0\\n3.0\\nNo\\n0.4%\\n85.9\\nSparse\\n4.0\\n5.0\\nNo\\n0.0%\\n86.4\\nTable 5: Sparse models are robust to dropped tokens when ﬁne-tuning. We ﬁnd the ﬁne-tuning\\nquality on SuperGLUE is not impacted signiﬁcantly across the values explored. Interestingly, drop-\\nping 10-15% of tokens can perform approximately as well as models that drop < 1%. We also\\nobserve that load balance losses (Aux Loss) improve ﬁne-tuning. The dropped token percentage\\ncorresponds to the fraction of dropped tokens across all expert layers at peak validation accuracy.\\n8Token dropping may be a form of regularization and a more extensive study may be an interesting direction\\nfor future work.\\n12\\n4.5\\nINSERTING SENTINELS TOKENS DURING FINE-TUNING\\nSentinel tokens denote masked sequences in the span-corruption objective (Fedus et al., 2018; Devlin\\net al., 2018). This differs from any ﬁne-tuning task we would likely encounter, leading to a domain\\nmismatch between pre-training and ﬁne-tuning. Table 6 illustrates the difference. We examine\\nwhether modifying the ﬁne-tuning task to look more like the pre-training task effects results.\\nObjective\\nInputs\\nTargets\\nSpan Corruption\\nI like <X> the pool <Y> day .\\n<X> going to <Y> on a sunny\\nFine-Tuning\\nWhat is the capital of Illinois ?\\nSpringﬁeld\\nFine-Tuning + Sentinels\\nWhat is the capital of Illinois ? <X>\\n<X> Springﬁeld\\nTable 6: Inserting sentinels during ﬁne-tuning mimics the pre-training span objective. We\\nhighlight the typical difference between span corruption and ﬁne-tuning. We propose modifying the\\nﬁne-tuning task to resemble pre-training by inserting sentinel tokens.\\nIn Table 7 we ﬁnd that adding sentinel tokens while ﬁne-tuning only improves Grammar Error\\nCorrection (GEC) (Rothe et al., 2021), but not SuperGLUE. We tried to further reduce the data\\ndistribution shift by inserting multiple sentinel tokens (as would be encountered by the model while\\npre-training), but again found no universal beneﬁt. However, despite no consistent beneﬁt on held-\\nout data, we ﬁnd that training convergence is accelerated for both dense and sparse models.\\nModel\\nInsert Sentinel Tokens\\nSuperGLUE (↑)\\nGEC (↑)\\nDense\\n84.9 ± 0.33\\n22.3 ± 0.25\\nDense\\n✓\\n85.1 ± 0.25\\n22.1 ± 0.42\\nSparse\\n86.6 ± 0.18\\n22.2 ± 0.04\\nSparse\\n✓\\n86.6 ± 0.24\\n22.9 ± 0.09\\nTable 7: Impact of sentinel tokens for ﬁne-tuning. The addition of sentinel tokens (a similar\\nconcept used in Lester et al. (2021)) during ﬁne-tuning has mixed performance on the two tasks we\\nconsider. SuperGLUE records the average score and GEC records the exact match. While we ﬁnd it\\ndoesn’t improve generalization, sentinel tokens can accelerate training convergence.\\n5\\nDESIGNING SPARSE MODELS\\nThe design of dense models has been guided by the foundational work of Kaplan et al. (2020). But\\nsparse models pose a myriad of additional questions: (1) How many experts to use? (2) Which\\nrouting algorithm? (3) What value for the capacity factor? (4) How does hardware change these\\ndecisions? In this section, we comment on these and offer recommendations for building Pareto ef-\\nﬁcient sparse models. Concurrently, Clark et al. (2022) provides additional design recommendations\\nincluding higher layer frequency and top-1 routing as per Fedus et al. (2021).\\nDesigning Sparse Models\\n1. In our setup, we recommend top-2 routing with 1.25 capacity factor and at most one\\nexpert per core.\\n2. The capacity factor can be changed during evaluation to adjust to new memory/compute\\nrequirements.\\n3. Dense layer stacking and a multiplicative bias can boost quality (Appendix C).\\n5.1\\nSETTING THE NUMBER OF EXPERTS\\nOne of the ﬁrst questions is the number of experts to use. Fedus et al. (2021) presented the scaling-\\nproperties of Switch Transformer which yielded monotonic pre-training beneﬁts (on a step basis) on\\n13\\nC4 up to 512-experts, Kim et al. (2021) up to 64-experts and Clark et al. (2022) up to 512-experts.\\nBut the incremental beneﬁt quickly diminishes with many experts (>256) or equivalently, with very\\nsparse models (<1% of experts activated).\\nHowever, reﬂecting on the speciﬁc hardware system can further guide this choice. The compute-to-\\nmemory ratio (operational intensity) can serve as an estimate of the efﬁciency of different operations\\n(Williams et al., 2009; Shazeer, 2019). A model is memory bound if the time to load tensors to\\nthe computing core (e.g. ALU/MMU) greatly exceeds the time required to do the computation on\\nthe tensors. On modern GPUs and TPUs, increasing this compute to memory ratio improves the\\nefﬁciency.\\nReturning to sparse expert models, using more than one expert per core increases memory transfer,\\npotentially hurting efﬁciency. Increasing the number of experts does not change the computation\\ndone (sparse models apply a ﬁxed amount of computation to each input), but increases the mem-\\nory transfer requirement (additional expert variables must be loaded from device memory). This\\ndecreases the compute-to-memory ratio9.\\nOn our TPU system, we recommend to one expert (or less) per core. Our largest models use both data\\nand model parallelism where data parallelism is over “rows” and model-parallelism over “columns”\\nof the logical mesh. We use ≤1 expert per data parallelism row to ensure the compute-to-memory\\nratio is high and to reduce the cores needed for evaluation and inference. Furthermore, using less\\nexperts lets us allocate more cores to the model parallelism “column” to have more FLOPs in our\\nmodel. Appendix H explains our mesh layouts for when we have fewer experts than data parallelism\\nrows.\\n5.2\\nCHOOSING THE CAPACITY FACTOR AND ROUTING ALGORITHM\\nWe generalize top-1 routing (Fedus et al., 2021; Roller et al., 2021) and top-2 (Shazeer et al., 2017;\\nLepikhin et al., 2020) to study top-n routing where each token is processed by at most n experts. In\\nthis study, all models are pre-trained for 100k steps with 1M tokens per batch and sparse models have\\n32 experts and are FLOP matched to T5-Large Raffel et al. (2019). We draw two key conclusions.\\nFirst, increasing both the train and eval capacity factors (CF) improves quality as seen by comparing\\nacross the segmented blocks of Table 8. For instance, top-1 routing improves by +0.011 neg. log\\nperp. when increasing from 1.0 →1.25 train CF and top-2 routing improves +0.009 increasing\\nfrom 1.25 →2.0 train CF. To provide context for these numbers: tripling the size of a dense model\\n(Dense-L to Dense-XL) yields a +0.090 neg. log perp. boost. Therefore, these CF boosts are ∼\\n1/10th of that magnitude. But this comes at a cost. Increasing the capacity factor linearly increases\\nthe einsums costs, memory for activations, all2all communication costs, and model-parallelism\\nallreduce communication costs for expert layers10.\\nSecond, there are small gains of top-(n+1) over top-n routing given a ﬁxed capacity factor (Table 8).\\nFor instance, top-2 routing improves +0.004 over top-1 at train CF of 1.25 or about 1/20th the boost\\nof a dense model tripling. This revises an earlier recommendation from Fedus et al. (2021). The\\nprimary difference between these experimental setups was scale of compute. Fedus et al. (2021)\\ntrained 220M-FLOP matched models for 50B tokens. We ﬁnd at an 8x larger scale of training\\n(1B-FLOP matched models for 100B tokens) there is instead a small gain to route to more than\\none expert. Furthermore, at the larger experimental scale, the speed difference of top-n versus top-\\n(n + 1) routing is negligible. Speed differences were observed in Fedus et al. (2021) because the\\nrouter computation was a larger fraction of the total model computation.\\n9As an exercise to the reader, verify the operational intensity of the ﬁrst expert computation is\\nb·h\\nb+h·e with b\\nbatch size, h hidden dimension, e number of experts.\\n10all2all and allreduce costs depend on the number of devices, batch size, dmodel and capacity factor,\\nbut not on the number of experts.\\n14\\nAlgorithm\\nTrain CF\\nEval CF\\nNeg. Log Perp. (↑)\\nDense-L\\n—\\n—\\n-1.474\\nDense-XL\\n—\\n—\\n-1.384\\nTop-1\\n0.75\\n0.75\\n-1.428\\nTop-1\\n0.75\\n2.0\\n-1.404\\nTop-2\\n0.75\\n0.75\\n-1.424\\nTop-2\\n0.75\\n2.0\\n-1.402\\nTop-1\\n1.0\\n1.0\\n-1.397\\nTop-1\\n1.0\\n2.0\\n-1.384\\nTop-2\\n1.0\\n1.0\\n-1.392\\nTop-2\\n1.0\\n2.0\\n-1.378\\nTop-1\\n1.25\\n1.25\\n-1.378\\nTop-1\\n1.25\\n2.0\\n-1.373\\nTop-2\\n1.25\\n1.25\\n-1.375\\nTop-2\\n1.25\\n2.0\\n-1.369\\nTop-2\\n2.0\\n2.0\\n-1.360\\nTop-2\\n2.0\\n3.0\\n-1.359\\nTop-3\\n2.0\\n2.0\\n-1.360\\nTop-3\\n2.0\\n3.0\\n-1.356\\nTable 8: Comparing capacity factors (CF) and routing algorithms. Increasing both train and eval\\nCF improves performance. Increasing or decreasing the eval CF gives an additional lever if you have\\nmore or less compute at eval time. Next, there are smaller gains of top-(n + 1) over top-n routing\\nacross capacity factors. Because the quality improves, but the speed slows as the CF increases, the\\nPareto efﬁcient CF must be determined by the speciﬁc hardware system.\\nThe speciﬁc hardware-software system will determine the optimal n and capacity factor. For in-\\nstance, if the system supports fast all2all and allreduce communications, larger capacity\\nfactors and larger n in top-n routing may be optimal. However, if the all2all and/or allreduce\\ncommunications are slow, smaller capacity factors may dominate.\\nIn our case, the hardware-\\nsoftware stack is the TPU and Mesh Tensorﬂow. We record the training speed of both our ST-MoE-L\\nand ST-MoE-32B model in Table 9 as we increase the train capacity factor. As the models scale,\\na higher capacity factor makes the models increasingly slower. The ST-MoE-L does not require\\nmodel parallelism (it ﬁts within accelerators memory, which implies no additional allreduce\\ncommunications) making it better suited for high capacity factors than our ST-MoE-32B model. For\\nour largest model, we therefore continue to use the smaller train capacity factor of 1.25 advocated\\nby Fedus et al. (2021) for Pareto efﬁciency, differing from other work which use a larger and more\\nexpensive 2.0 capacity factor (Lepikhin et al., 2020; Du et al., 2021).\\nModel\\nTrain CF\\nStep Time (s) (↓)\\nST-MoE-L\\n1.25\\n2.397\\nST-MoE-L\\n2.0\\n2.447 (+7%)\\nST-MoE-32B\\n1.25\\n4.244\\nST-MoE-32B\\n2.0\\n4.819 (+14%)\\nTable 9: Proﬁling sparse models on TPUs. Increasing the train capacity factor from 1.25 to 2.0\\nincreases the step-time by +7% for the large (1B) model but by +14% for our 32B model. As the\\nmodel size increases, we ﬁnd the small quality gains of higher train capacity factors from Table 8\\nare more than offset by the signiﬁcant 14% slow-down. Note: the step time between ST-MoE-L and\\nST-MoE-32B are not comparable because they used a different number of cores.\\nOur results in this section focus on top-n routing, but we also experimented with a variety of other\\nrouting techniques in Appendix J. We found most performed similarity or worse compared to top-n\\nrouting. However we found Batch Prioritized Routing (BPR), introduced in Riquelme et al. (2021),\\nsigniﬁcantly helps performance for capacity factors less than one (Appendix D). We recommend\\n15\\nBPR for larger models where all2all and allreduce are more expensive and lower capacity\\nfactors are optimal.\\n6\\nEXPERIMENTAL RESULTS\\nGiven our improvements to training stability, ﬁne-tuning and model design, we start by validating\\na sparse model approximately FLOP-matched to T5-Large (Raffel et al., 2019). We conclude this\\nsection by designing and training a 269B sparse parameter model (FLOP matched to a 32B dense\\nmodel) which achieves state-of-the-art quality across a wide set of NLP tasks.\\nWe studied the SuperGLUE (Wang et al., 2019) benchmark throughout this work which consists\\nof tasks including sentiment analysis (SST-2), word sense disambiguation (WIC), sentence similar-\\nity (MRPC, STS-B, QQP), natural language inference (MNLI, QNLI, RTE, CB), question answer-\\ning (MultiRC, RECORD, BoolQ), coreference resolution (WNLI, WSC) and sentence completion\\n(COPA) and sentence acceptability (CoLA). We often observe good performance on SuperGLUE to\\ncorrelate with (but not guarantee) performance across many NLP tasks. We also include a divers set\\nof additional benchmarks. The CNN-DM (Hermann et al., 2015) and BBC XSum (Narayan et al.,\\n2018) datasets are used to measure the ability to summarize articles. Question answering is probed\\nwith the SQuAD dataset (Rajpurkar et al., 2016) as well as on grade-school science questions in\\nARC Easy and ARC Reasoning Challenge (Clark et al., 2018). And as in Roberts et al. (2020), we\\nevaluate the knowledge of our models by ﬁne-tuning on three closed-book question answer datasets:\\nNatural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA\\n(Joshi et al., 2017). Closed-book simply refers to questions posed with no supplemental reference or\\ncontext material. To gauge the model’s common sense reasoning we evaluate it on the Winogrande\\nSchema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s natural language infer-\\nence capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\\n6.1\\nST-MOE-L\\nFor simplicity and to cover dozens of tasks easily, we train on mixtures of the tasks listed rather than\\nseparately ﬁne-tuning a model on each task. However, because the tasks vary in size considerably,\\nequally sampling per the number of examples would over-sample large tasks and under-sample small\\nones. We therefore mix each task in proportion to the number of examples in its ‘train’ split (up to\\nsome max num examples=65536) as in Raffel et al. (2019). This means that tasks containing\\nmore than 65536 training examples are weighted as if they only contain max num examples.\\nTable 10 summarizes the quality of a dense T5-Large (L) model and sparse model with approxi-\\nmately the same number of FLOPs pre-trained for 500k steps with a 1M batch size (524B tokens)\\non the C4 dataset (Raffel et al., 2019). The sequence length for the encoder was 512 and 114 for\\nthe decoder. We observe improvements on the validation (dev) sets across a wide array of tasks ex-\\namining natural language understanding, question answering, and summarization. As seen in Fedus\\net al. (2021), striking gains are observed in closed book question answering (Roberts et al., 2020).\\nAlso, in support of the overﬁtting hypothesis presented in Section 4.1, we observe two of the smallest\\ntasks CB and WSC (250 and 259 training examples, respectively), are the only ones where the sparse\\nmodel does not yield gains over its dense counterpart. This again suggests that improved forms of\\nregularization for sparse models may unleash greater performance.\\n6.2\\nST-MOE-32B\\nWith quality validated at the scale of T5-Large, we seek to push the capabilities of sparse models\\nthrough the ST-MoE-32B. When designing this, we sought a balance between FLOPs and parame-\\nters. High-FLOP sparse models were previously unstable in Fedus et al. (2021) in our setting (i.e.\\nencoder-decoder models, Adafactor optimizer), but the router z-loss enabled us to proceed. For com-\\nputational efﬁciency, we expanded the hidden size of the experts (dff in Table 11 below)11. Finally,\\nwe increased the dkv to 128 for better performance on our hardware. The most salient changes are\\nfewer overall parameters and more FLOPs per token relative to both Switch-C and Switch-XXL. Our\\n11allreduce activation communications introduced through model parallelism are independent of the\\nhidden size, but not the model dimension, making it a good choice to increase.\\n16\\nName\\nMetric\\nSplit\\nDense-L (↑)\\nST-MoE-L (↑)\\nGain (%)\\nSQuADv2\\nF1\\ndev\\n94.0\\n94.5\\n+1%\\nSQuADv2\\nacc\\ndev\\n87.6\\n88.1\\n+1%\\nSuperGLUE\\navg\\ndev\\n85.1\\n87.4\\n+3%\\nBoolQ\\nacc\\ndev\\n87.1\\n88.6\\n+2%\\nCopa\\nacc\\ndev\\n83.0\\n91.0\\n+10%\\nRTE\\nacc\\ndev\\n91.0\\n92.1\\n+1%\\nWiC\\nacc\\ndev\\n70.4\\n74.0\\n+5%\\nMultiRC\\nF1\\ndev\\n83.9\\n86.0\\n+3%\\nWSC\\nacc\\ndev\\n95.2\\n93.3\\n−2%\\nReCoRD\\nacc\\ndev\\n85.7\\n88.9\\n+4%\\nCB\\nacc\\ndev\\n100\\n98.2\\n−2%\\nXSum\\nROUGE-2\\ndev\\n19.9\\n21.8\\n+10%\\nCNN-DM\\nROUGE-2\\ndev\\n20.3\\n20.7\\n+2%\\nWinoGrande (XL)\\nacc\\ndev\\n75.4\\n81.7\\n+8%\\nANLI (R3)\\nacc\\ndev\\n54.3\\n57.3\\n+6%\\nARC-Easy\\nacc\\ndev\\n63.5\\n75.4\\n+19%\\nARC-Challenge\\nacc\\ndev\\n50.2\\n56.9\\n+13%\\nClosed Book TriviaQA\\nacc\\ndev\\n28.1\\n33.8\\n+20%\\nClosed Book NatQA\\nacc\\ndev\\n27.2\\n29.5\\n+8%\\nClosed Book WebQA\\nacc\\ndev\\n30.5\\n33.2\\n+9%\\nTable 10: Fine-tuning performance of FLOP-matched dense and sparse models. Comparison\\nof the dense-L baseline and the sparse FLOP-matched version (higher numbers better). We observe\\nconsistent gains across diverse tasks, using approximately the same amount of computation. The\\nonly two tasks without improvement from the sparse model are the two smallest: CB with 250\\ntraining examples and WSC with 259.\\nST-MoE-32B has “only” 269B parameters and is approximately FLOP-matched to a dense Trans-\\nformer with 32B parameters. The reduced parameter count from Switch-C and Switch-XXL eases\\nthe burden for both serving and ﬁne-tuning. Finally, we use the sparse-dense stacking described in\\nAppendix C.\\nWe pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the\\ndataset from GLaM (Du et al., 2021) summarized in Appendix E. We use 1M tokens per batch,\\nthe Adafactor optimizer with default hyperparameters, and a learning rate warm-up of 10k steps\\nfollowed by inverse square root decay. Our model follows the initialization scheme proposed in\\nFedus et al. (2021).\\nTable 12 evaluates our ST-MoE-32B model against previous state-of-the-art approaches using\\ninference-only (zero-shot, one-shot) as well as ﬁne-tuning. On SuperGLUE, our model improves\\nupon the prior state-of-the-art model, achieving an average score of 91.2 on the test server (93.2\\nvalidation accuracy) which is over one percentage point beyond estimated human capability. For\\nboth summarization datasets, XSum and CNN-DM, our model achieves state-of-the-art without ad-\\nditional changes to training or ﬁne-tuning (Raffel et al., 2019; Liang et al., 2021). ST-MoE-32B\\nimproves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 →\\n94.8) and ARC Challenge (81.4 →86.5). On two of the three closed book QA tasks, we improve\\nover the prior state-of-the-art. Closed book WebQA achieves a 47.4 accuracy (prior best of 42.8 from\\nRoberts et al. (2020) and exceeds results from the zero-shot performance of the ERNIE 3.0 Titan\\n260B dense parameter model (Wang et al., 2021)). Closed book NatQA improves to 41.9 accuracy\\n(prior best of 41.5 from Karpukhin et al. (2020)). We ﬁnd signiﬁcant improvements on adversari-\\nally constructed datasets (ANLI R3 and WinoGrande XL). ANLI R3 (Nie et al., 2019) improves the\\nstate-of-the-art to 74.7 (prior best of 53.4).\\nWe note some weaknesses in our model. ST-MoE-32B has lackluster performance on the small\\nSQuAD dataset, with an exact match score of 90.8 which falls short of the older benchmark set by\\nthe T5-XXL of 91.3. Furthermore, while setting a new state-of-the-art for SuperGLUE in aggregate,\\n17\\nModel\\nParameters\\nFLOPs/seq\\ndmodel\\nFFNGEGLU\\ndff\\ndkv\\nDense-L\\n0.8B\\n645B\\n1024\\n✓\\n2816\\n64\\nT5-XXL\\n11.1B\\n6.3T\\n4096\\n✓\\n10240\\n64\\nSwitch-XXL\\n395B\\n6.3T\\n4096\\n✓\\n10240\\n64\\nSwitch-C\\n1571B\\n890B\\n2080\\n6144\\n64\\nST-MoE-L\\n4.1B\\n645B\\n1024\\n✓\\n2816\\n64\\nST-MoE-32B\\n269B\\n20.2T\\n5120\\n✓\\n20480\\n128\\nModel\\nNum. Heads\\nNum. Layers\\nNum. Experts\\nExpert Layer Freq.\\nSparse-Dense\\nDense-L\\n16\\n27\\n–\\n–\\nT5-XXL\\n64\\n24\\n–\\n–\\nSwitch-XXL\\n64\\n24\\n64\\n1/4\\nSwitch-C\\n32\\n15\\n2048\\n1/1\\nST-MoE-L\\n16\\n27\\n32\\n1/4\\n✓\\nST-MoE-32B\\n64\\n27\\n64\\n1/4\\n✓\\nTable 11: Model comparisons. A comparison of the Dense-L and T5-XXL, the two largest Switch\\nTransformer variants (Switch-XXL and Switch-C), and the ST-MoE-L and ST-MoE-32B. dmodel\\nrefers to the model hiddenstate size and dff is the internal size of the FFN layer. dkv is the dimension\\nof each attention head. Expert Layer Freq. is the fraction of FFN layers replaced with a sparse layer.\\nSparse-Dense refers to the architectural variant described in Appendix C.\\nPrevious Best (↑)\\nOurs (↑)\\nName\\nMetric\\nSplit\\nZero-Shot\\nOne-Shot\\nFine-Tune\\nFine-Tune\\nSQuADv2\\nF1\\ndev\\n68.3e\\n70.0e\\n96.2a\\n96.3\\nSQuADv2\\nacc\\ndev\\n62.1e\\n64.6e\\n91.3a\\n90.8\\nSuperGLUE\\navg\\ntest\\n–\\n–\\n90.9\\n91.2\\nBoolQ\\nacc\\ndev/test\\n83.0e\\n82.8e\\n92.0\\n92.4\\nCopa\\nacc\\ndev/test\\n91.0d\\n92.0e\\n98.2\\n99.2\\nRTE\\nacc\\ndev/test\\n68.8e\\n71.5e\\n94.1\\n93.5\\nWiC\\nacc\\ndev/test\\n50.5e\\n52.7e\\n77.9\\n77.7\\nMultiRC\\nF1\\ndev/test\\n72.9d\\n72.9d\\n88.6\\n89.6\\nWSC\\nacc\\ndev/test\\n84.9e\\n83.9e\\n97.3\\n96.6\\nReCoRD\\nacc\\ndev/test\\n90.3e\\n90.8e\\n96.4\\n95.1\\nCB\\nacc\\ndev/test\\n46.4d\\n73.2e\\n99.2\\n98.0\\nXSum\\nROUGE-2\\ntest\\n–\\n–\\n24.6h\\n27.1\\nCNN-DM\\nROUGE-2\\ntest\\n–\\n–\\n21.6a\\n21.7\\nWinoGrande XL\\nacc\\ndev\\n73.4e\\n73.2d\\n–\\n96.1\\nANLI R3\\nacc\\ntest\\n40.9e\\n40.8e\\n53.4\\n74.7\\nARC-Easy\\nacc\\ntest\\n71.9e\\n76.6e\\n92.7g\\n95.2\\nARC-Challenge\\nacc\\ntest\\n51.4\\n53.2\\n81.4g\\n86.5\\nCB TriviaQA\\nem\\ndev\\n68.0e\\n74.8e\\n61.6b\\n62.3\\nCB NatQA\\nem\\ntest\\n21.5e\\n23.9e\\n41.5c\\n41.9\\nCB WebQA\\nem\\ntest\\n38.0f\\n25.3\\n42.8b\\n47.4\\nTable 12: ST-MoE-32B versus previous best for inference-only techniques and ﬁne-tuned mod-\\nels. A split of “dev/test” refers to dev split for Zero-Shot and One-Shot and test split for Fine-Tune\\nquality. Data not available ﬁlled in with “–”. Superscript letters denote the result: a: Raffel et al.\\n(2019) b: Roberts et al. (2020) c: Karpukhin et al. (2020), d: Brown et al. (2020), e: Du et al. (2021),\\nf: Wang et al. (2021), g: UniﬁedQA + ARC MC/DA + IR, h: Zhang et al. (2020).\\n18\\ncertain tasks, including small ones like CB, WSC, fail to improve. Finally, on closed book Trivia\\nQA, our model improves over the ﬁne-tuned baseline with SSM from Roberts et al. (2020), but fails\\nto produce gains over both GPT-3 and GLAM.\\nWhile not the focus of this paper, we present the quality differential between recent advances in\\ninference-only techniques like few-shot learning and ﬁne-tuning on these tasks (GPT-3 (Brown\\net al., 2020), GLAM (Du et al., 2021) and Gopher (Rae et al., 2021)). As expected and observed\\npreviously, ﬁne-tuning outperforms zero/one-shot learning, but has the disadvantage of requiring\\nadditional training and different models for each task.\\n7\\nTRACING TOKENS THROUGH THE MODEL\\nThus far we have presented quantitative measures and performance metrics. We change tack to\\nexplore qualitative features by visualizing how tokens are routed among the experts. We do so by\\npassing a batch of tokens to the model and manually inspecting token assignment at each layer. We\\nconsider our ST-MoE-L model pre-trained either on the monolingual C4 corpus (Raffel et al., 2019)\\nor on the multilingual mC4 corpus (Xue et al., 2020). On both the encoder and the decoder, the\\nmodel has six sparse layers, each with 32 experts.\\nPreliminaries\\nThe span corruption objective is to recover spans of variable-length contiguous segments\\nmasked out in the inputs. This is formatted as:\\nInputs: I went to <extra id 0> to buy <extra id 1>\\nTargets: <extra id 0> the store <extra id 1> milk\\nIn our encoder-decoder architecture, the inputs will be passed to the encoder and targets to\\nthe decoder.\\nEach group of tokens is routed jointly with load balancing across experts incentivized by an auxiliary\\nloss as proposed in Shazeer et al. (2017) (see Appendix A for details). Tokens compete for expert\\nassignment against other tokens in their group, rather than the entire batch, and expert specialization\\nis heavily inﬂuenced by the distribution of tokens in each group. The notion of groups is introduced\\nto limit the cost of dispatching and gathering the correct tokens to the correct experts.\\n7.1\\nENCODER EXPERTS EXHIBIT SPECIALIZATION\\nOur ﬁrst observation is that, at each layer, at least one expert specializes in sentinel tokens (mask to-\\nkens that represent blanks to ﬁll-in). Additionally, some encoder experts exhibit clear specialization,\\nwith some experts primarily operating on punctuation, verbs, proper names, counting, etc. Table 13\\npresents a few notable example of specialization across encoder experts. And while we ﬁnd many\\ninstances of specialization, these have been speciﬁcally extracted from many examples without a\\nclear semantic or syntactic specialization.\\n7.2\\nDECODER EXPERTS LACK SPECIALIZATION\\nIn contrast, expert specialization is far less noticeable in the decoder. Not only are sentinel to-\\nkens routed somewhat uniformly across decoder experts (see Table 14), but we also do not observe\\nmeaningful specialization (semantics or syntax) in decoder experts.\\nWe hypothesize that this lack of meaningful expert specialization is caused by the distribution of\\ntarget tokens induced by the span corruption objective. In particular, (a) a smaller number of tokens\\nare routed jointly in the decoder due to longer sequence lengths in the encoder (e.g. group size\\nis 2048 in the encoder vs 456 in the decoder in our setup) and (b) a higher proportion of tokens\\nare sentinel tokens in the decoder. As a result, target tokens in each group typically cover a smaller\\nsemantic space (compared to the encoder), perhaps explaining the lack of expert specialization in the\\ndecoder. This intricate interplay between the architecture and the training objective invites further\\n19\\nExpert specialization\\nExpert position\\nRouted tokens\\nSentinel tokens\\nLayer 1\\nbeen <extra id 4><extra id 7>ﬂoral to\\n<extra id 10><extra id 12><extra id 15>\\n<extra id 17><extra id 18><extra id 19>...\\nLayer 4\\n<extra id 0><extra id 1><extra id 2>\\n<extra id 4><extra id 6><extra id 7>\\n<extra id 12><extra id 13><extra id 14>...\\nLayer 6\\n<extra id 0><extra id 4><extra id 5>\\n<extra id 6><extra id 7><extra id 14>\\n<extra id 16><extra id 17><extra id 18>...\\nPunctuation\\nLayer 2\\n, , , , , , , , , - , , , , , ). )\\nLayer 6\\n, , , , , : . : , & , & & ? & - , , ? , , , . <extra id 27>\\nConjunctions and articles\\nLayer 3\\nThe the the the the the the the the The the the\\nthe the the The the the the\\nLayer 6\\na and and and and and and and or and a and .\\nthe the if ? a designed does been is not\\nVerbs\\nLayer 1\\ndied falling identiﬁed fell closed left posted lost felt\\nleft said read miss place struggling falling signed died\\nfalling designed based disagree submitted develop\\nVisual descriptions\\nLayer 0\\nher over her know dark upper dark outer\\ncolor, spatial position\\ncenter upper blue inner yellow raw mama\\nbright bright over open your dark blue\\nProper names\\nLayer 1\\nA Mart Gr Mart Kent Med Cor Tri Ca Mart\\nR Mart Lorraine Colin Ken Sam Ken Gr Angel A\\nDou Now Ga GT Q Ga C Ko C Ko Ga G\\nCounting and numbers\\nLayer 1\\nafter 37 19. 6. 27 I I Seven 25 4, 54 I two dead we\\nwritten and numerical forms\\nSome 2012 who we few lower each\\nTable 13: Notable examples of specialization in encoder experts. We ﬁnd experts that specialize\\nin punctuation, conjunctions & articles, verbs, visual descriptions, proper names, counting & num-\\nbers. Across all layers (not shown), we observe experts that primarily operate on sentinel tokens\\n(marked as <extra id x>). Note that a SentencePiece model (Kudo and Richardson, 2018) will\\nsplit a token if it doesn’t exist in the vocabulary, e.g. Kenneth may become Ken, ne, th.\\nresearch on better leveraging sparsity and expert specialization in the decoder. Alternatively, future\\nwork could study simply removing the experts in the decoder layer, which also confers beneﬁts\\nduring autoregressive decoding (Kudugunta et al., 2021a).\\nLayer 1\\nLayer 2\\nLayer 3\\nLayer 4\\nLayer 5\\nLayer 6\\nUniform (32-experts)\\nEncoder\\n2.2\\n1.8\\n1.6\\n1.7\\n1.7\\n1.2\\n3.5\\nDecoder\\n3.4\\n3.4\\n3.4\\n3.4\\n3.4\\n3.4\\n3.5\\nTable 14: Entropy of routed sentinel tokens across encoder and decoder layers. We support our\\nqualitative observation that encoder experts specialize, but decoder expert don’t by computing the\\nentropy over the routing for sentinel tokens. The encoder routing entropy is low, but the decoder\\nrouter is high entropy, and nearly equal to uniform routing. Because each layer has 32-experts, a\\ncompletely uniform distribution has entropy of 3.5.\\n20\\n7.3\\nMULTILINGUAL EXPERTS SPECIALIZE, BUT NOT BY LANGUAGE\\nWe next consider a multilingual sparse model pretrained on a mixture of different languages and\\ninspect the expert specialization in the encoder. As in the monolingual case, we ﬁnd strong evidence\\nof expert specialization. Table 15 presents some examples of experts specializing in sentinel tokens,\\nnumbers, conjunctions & articles and proper names.\\nExpert specialization\\nRouted tokens\\nSentinel tokens\\nto <extra id 6>to til <extra id 9>\\n<extra id 10>to <extra id 14><extra id 17>\\n<extra id 19><extra id 20><extra id 21>...\\nNumbers\\n$50 comment .10.2016 ! 20 20 3 ! 5 1. ! 91 ? n´\\ne ?\\n2 17 4 17 11 17 8 & 11 & 22:30 02 2016. ) iOS\\nConjunctions & Articles\\nof of of their their of any this this your your am von\\nthis of Do of of This these our 的的于的在的在的\\nle les Le la di la sur sur 136 sur ののするのというのし\\nPrepositions & Conjunctions\\nFor for or for for or for from because https during https\\n并与和par c Pour `\\na a par tr`\\ne pour pour pour pour pour c とやのに\\nでででなので- and and + c between and and\\nProper names\\nLife Apple iOS A IGT 众莫HB\\nF HB A K A OPP OK HB A Gia C Gia C P Scand Wi\\nG H Z PC G Z ハイPC G Ti CPU PC PC A キットOS\\nTable 15:\\nExamples of specialization in multilingual experts (encoder).\\nMultilingual ex-\\nperts also exhibit specialization, which sometimes spans across different languages (e.g. ”for” and\\n”pour”). Experts trained on multilingual mixtures do not exhibit language specialization.\\nOne might expect experts to specialize in languages, which appears as a natural criterion for divvying\\nup batches of data among experts. However, we ﬁnd no evidence of language specialization (see\\nTable 15). Routers instead pass tokens from English, Japanese, French and Chinese indiscriminately\\nand the experts appear to be multilingual. But this lack of language specialization is less surprising\\nwhen considering the mechanism of token routing and load balancing. Since each group of tokens\\nmay only contain one, to at most a few, languages (a group usually consists of 2-4 sequences in\\nour setup), then all experts are encouraged to handle tokens from all languages. We experimented\\nwith a global load balance loss, however, this usually results in worse load-balance and worse model\\nperformance, so we leave further improving multilingual expert models as an area of open work\\n(Section 9).\\nOur visualization reveals apparent specialization learned in our models (Tables 13, 15) for the en-\\ncoder layers. Other expert specializations were also observed in the appendix of Shazeer et al.\\n(2017). However, this leads to an interesting question of how architectures that eliminate learned\\nrouting Roller et al. (2021); Zuo et al. (2021) appear to perform well. An extensive study of the\\nscaling properties of learned versus random routing could prove helpful as future work and help\\nguide us to a better understanding of routing behavior.\\n8\\nRELATED WORK\\nMixture-of-Experts (MoE) date back at least three decade history to the work of Jacobs et al. (1991);\\nJordan and Jacobs (1994). In initial concepts, the MoE deﬁned the entire neural network akin to\\nensemble methods. But later Eigen et al. (2013) extended the idea of including MoE as a component\\nas part of deeper networks. Shazeer et al. (2017) then scaled this idea to a 137B parameter model to\\nachieve state-of-the-art in machine translation. Most of the later work (including ours) follows this\\nMoE as a component approach.\\nScale in natural language processing. The remarkable success of scale in natural language pro-\\ncessing (Kaplan et al., 2020; Brown et al., 2020) has reinvigorated MoE research evidenced by a\\n21\\nsurge of recent work (Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021; Kim et al., 2021;\\nDu et al., 2021; Artetxe et al., 2021; Zuo et al., 2021; Clark et al., 2022). Sparse expert models\\nhave been proposed as a method to achieve the results of large-scale dense models, more efﬁciently.\\nFedus et al. (2021) showed a 4x pre-train speed-up over T5-XXL (Raffel et al., 2019) and Du et al.\\n(2021) matched the quality of GPT-3 (Brown et al., 2020) using only 1/3 of the energy. And in the\\nspan of the last twelve months, a milestone of efﬁciently training trillion parameter deep neural net-\\nworks has been achieved by multiple groups (Fedus et al., 2021; Yang et al., 2021; Du et al., 2021),\\nand most recently, Lin et al. (2021) introduced techniques to train a 10T parameter model. One side\\nnote is that the recent signiﬁcant successes of sparse expert models have often been in settings with\\na lot of data and no distribution shift – two examples being language modeling/span corruption and\\nmachine translation (Shazeer et al., 2017; Lepikhin et al., 2020; Kim et al., 2021; Fedus et al., 2021).\\nIn contrast, discrepancies between strong pre-training quality and poor ﬁne-tuning quality for sparse\\nmodels have been observed in Fedus et al. (2021); Narang et al. (2021); Artetxe et al. (2021), but we\\nexpect advances in regularization techniques to continue to improve downstream quality.\\nTowards better routing algorithms. BASE layers (Lewis et al., 2021) recasts token routing as a\\nlinear assignment problem – removing the need for load balancing auxiliary losses. This work also\\ndemonstrated the efﬁcacy of a single expert layer. Clark et al. (2022) studies in depth the scaling\\nproperties of a few different routing algorithms and propose their own variant of BASE layers that\\nuses an optimal transport formulation. Yang et al. (2021) introduces the M6-T architecture and\\nexpert prototyping which splits experts into different groups and applies k top-1 routing procedures\\n(contrasting with the top-k routing commonly used elsewhere). Hazimeh et al. (2021) proposed a\\ncontinuously differentiable sparse gate with demonstrated improvements over vanilla top-k gating.\\nOther work (Bengio et al., 2016) considered casting the routing selection as a reinforcement learning\\nproblem. More radical versions remove learning the routing entirely. Hash layers (Roller et al.,\\n2021) shows random ﬁxed routing (per hash functions) led to competitive performance with learned\\nrouting. Zuo et al. (2021) also proposed an algorithm which randomly selects experts during training\\nand inference and found gains of 2 BLEU points over Switch Transformers and competitive scores\\nwith the larger models of Kim et al. (2021). Finally, Fan et al. (2021) designs an architecture with\\nexplicit language-speciﬁc sublayers (rather than allowing arbitrary routing as done in Lepikhin et al.\\n(2020)) to yield gains of +1 BLEU.\\nSparse expert models in other modalities. MoE and sparse experts model have also advanced\\nresults in modalities aside from language. Riquelme et al. (2021) designed a 15B parameter V-MoE\\nto match state-of-the-art ImageNet (Deng et al., 2009) models with fewer computational resources.\\nLou et al. (2021) similarly showed a beneﬁt over dense vision models by using MoE layers across\\nboth image patch and channel dimensions. Additionally, Automatic Speech Recognition has been\\nimproved by the SpeechMoE variants (You et al., 2021a;b). Kumatani et al. (2021) reduced word\\nerror rates using MoE models in Sequence-to-Sequence Transformer and Transformer Transducer.\\nImproving deployment of sparse models. Initial expert designs (including this work) route each\\ntoken separately to experts at that layer. One issue is that these type of architectures may be burden-\\nsome to serve since it requires sufﬁcient memory for storing the parameters. Distillation was shown\\nin Fedus et al. (2021) to be moderately effective, but recent approaches modiﬁed the routing to in-\\nstead route full sentences or tasks (Kudugunta et al., 2021b; Zuo et al., 2021) which then permits\\nextraction of sub-networks at time of serving (e.g. deploy only the network associated with the new\\ntask). As an alternative to distillation, Kim et al. (2021) considers directly pruning away experts not\\nessential to the task of interest.\\nMultitask learning with MoE. We conclude our tour of recent MoE research with successes in\\nmultitask settings. Ma et al. (2018) recommended using a separate gating or router network for each\\ntask, an idea that may soon be revisited for Transformer architectures. Finally, Gururangan et al.\\n(2021) recommends even greater modularity of language models and conditionally activates experts\\nbased on the domain/task label or by an inferred label.\\n9\\nDISCUSSION\\nWhile this work is on sparse models, these models intersect with many other interesting topics\\nin machine learning such as adaptive computation, low-precision training, scaling principles, and\\n22\\nneural network architecture advances. Our discussion therefore covers a broader range of topics\\nsurfaced during this research.\\nUnpredictable dynamics when pre-training on multilingual data.\\nWe often observe that the\\nsame model pre-trained on multilingual data will yield smaller pre-training speed-ups and be more\\nunstable. One hypothesis is that this is due to the variance of sequences per group across batches.\\nAs a reminder, we encourage tokens in a group to be load-balanced. There are usually only 2-8\\nsequences per group (higher becomes expensive) where each sequence is written in a single lan-\\nguage. Therefore, at most 2-8 languages must be balanced across experts – even when training with\\nover 100 languages. This leads to high variance across groups and batches, resulting in chaotic and\\nunpredictable routing. In a follow-up experiment (just highlighted for brevity), we pre-trained on\\na mixture of English C4 plus a small fraction of a ﬁne-tuning task which similarly resulted in an\\nunstable model.\\nThe robustness of sparse models.\\nDespite a paper focused on the details of sparse model-\\nparticulars, zooming out we ﬁnd them to be robust to a wide set of hyperparameters and architectural\\nchanges. Sparse models obtain great performance under a variety of routing algorithms, dropping\\nhigh fractions of tokens, and different hyperparameters. While we did point out the importance of\\ntuning the batch size and learning rate for ﬁne-tuning, our intuition, in-line with Kaplan et al. (2020),\\nis that the real winner is scale. For instance, Table 8 shows larger gains to be had by simply increas-\\ning the capacity factor (i.e. FLOPs) rather than by more sophisticated routing (i.e. algorithms).\\nAdaptive computation.\\nSparse models are a subclass of adaptive computation models since each\\ninput gets different computation applied to it. In sparse models a token is routed to the expert(s)\\nof its choosing. When capacity factors are less than one, the model learns to not apply computa-\\ntion to certain tokens. This has shown promise in computer vision (Riquelme et al., 2021) and our\\nlanguage experiments (Appendix D). We envision future models expanding this through heteroge-\\nneous experts (e.g. each expert applies differing computation). Intuitively, different input examples\\nwill likely require different amounts of processing depending on difﬁculty. Future models in this\\ndirection will be efﬁciently enabled through emerging computing infrastructures (Dean, 2021).\\nGeneralizing ﬁndings from small to large scale.\\nA key issue we faced throughout our work was\\nidentifying small scale models and training setups that reﬂect larger scale experiments. This was\\nevident in our stability studies in Section 3 where experiments had to be run with XL sized models\\nto surface relevant dynamics. For our architecture and routing algorithm experiments, we often ﬁnd\\nimprovements vanish, or even reverse, when models are trained for longer or made larger. As one\\nexample, the top-n ﬁndings of Fedus et al. (2021) were reversed in our 8x larger-scale experiments\\npresented here, which revealed small boosts of top-(n + 1) routing over top-n routing (see Table 8).\\nTraining models with even lower precision.\\nThe best method we found to stabilize our models\\nwithout hurting (and sometimes improving) quality was the router z-loss. This is an auxiliary loss\\nthat encourages the model logits to have values smaller in absolute magnitude. Given the max range\\nof numbers float32 and bfloat16 can support (∼3e38), this leads us to believe most of this\\nrange is not needed, and compressing it actually might improve model training dynamics. Therefore,\\nfuture precision formats might take into account more compressed exponential ranges to train certain\\nclasses of models.\\nDesigning new operations with more multiplicative interactions.\\nSection 3.1 shows that op-\\nerations with more multiplicative interactions than additions, or those that don’t accumulate over\\nmany numbers, improve model performance. We test this further by injecting more multiplicative\\ninteractions into expert layers which speedup pre-training by 4% without any change to step-time\\n(Appendix C). We think this hints at promising architectural improvements for models and could be a\\ngood design principle. Recently depthwise convolutions, which only accumulate 3-5 elements, have\\nalso been shown to greatly improve Transformer performance (So et al., 2021). These operations\\nare especially exciting as elementwise multiplications typically do not introduce any communication\\noverhead when using model parallelism (which makes operations like depthwise convolutions and\\nour multiplicative interactions very efﬁcient). While we did note these methods to increase model\\ninstabilities in Section 3.1, using the router z-loss in our models prevented any further instabilities.\\n23\\nConstrain activations to alleviate other undesirable model scaling dynamics.\\nWe observed\\ntwo additional sources of training instability. (1) Encoder-decoder models are more unstable than\\ndecoder only models (for ﬁxed amount of FLOPs). Encoder-decoder models have a higher ratio\\nof attention layers (e.g. more exponential functions) due to having both self-attention and enc-dec\\nattention layers for each FFN on the decoder. (2) Deeper models are more unstable than shallower\\nmodels for a ﬁxed amount of FLOPs. Deeper models also introduce more exponential functions\\nthrough additional attention layers. We hypothesize that a contributing factor to both of these obser-\\nvations is simply the increased number of exponential functions found in the network. Future work\\ncould look at resolving these training dynamics by adding z-loss penalties to the attention softmaxes\\nfor non-sparse models, especially since we observed adding them didn’t change model quality.\\nDense and sparse models depend differently on hyperparameters.\\nOur ﬁne-tuning analysis\\nin Section 4.3 shows optimal ﬁne-tuning hyperparameters differ signiﬁcantly between dense and\\nsparse models. In certain settings, ﬁne-tuning hyperparamters that worked well for the dense model\\nmasked any improvements from the sparse model (despite large pre-training speedups). For new\\nmodel classes, we recommend researchers and practitioners to extensively test key hyperparameters\\nbefore prematurely abandoning a method.\\n10\\nCONCLUSION\\nWe temper the over-exuberance for scale in Fedus et al. (2021) by showing how a model with 1/5th\\nthe size, but with a better balance of computation (FLOPs) to parameters – is a more effective sparse\\nlearner. Furthermore, this improves the usability of sparse models since it can be deployed with less\\nmemory overhead. Using our sparse model variant, we achieve SOTA across a wide range of the\\nmost competitive public benchmarks. We hope this work shows the power of model sparsity and\\naccelerates the adoption of such models.\\nACKNOWLEDGEMENTS\\nWe would like to thank Alex Passos, Ekin Cubuk, Margaret Li, Noah Constant, Oriol Vinyals,\\nBasil Mustafa, Joan Puigcerver, Diego de Las Casas, Mike Lewis, and Ryan Sepassi for detailed\\ncomments and feedback on early versions of the draft. We also thank the Google Brain Team for\\nuseful discussions throughout the course of this work.\\n24\\nREFERENCES\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui\\nChen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\\nJeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efﬁcient large\\nscale language modeling with mixtures of experts, 2021.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models, 2016.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\\nlanguage processing, pages 1533–1544, 2013.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed scaling laws for\\nrouted language models. arXiv preprint arXiv:2202.01169, 2022.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier.\\nLanguage modeling with\\ngated convolutional networks. In International conference on machine learning, pages 933–941.\\nPMLR, 2017.\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: In-\\nvestigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung,\\nvolume 23, pages 107–124, 2019.\\nJeff Dean. Introducing pathways: A next-generation ai architecture. Google AI Blog, 2021.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\\npages 248–255. Ieee, 2009.\\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\\nquantization, 2021.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\nimage is worth 16x16 words: Transformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,\\nand Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts, 2021.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\n25\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Man-\\ndeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric\\nmultilingual machine translation. Journal of Machine Learning Research, 22(107):1–48, 2021.\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via ﬁlling in\\nthe . arXiv preprint arXiv:1801.07736, 2018.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. Demix\\nlayers: Disentangling domains for modular language modeling, 2021.\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning, 2021.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition, 2015.\\nDan Hendrycks and Kevin Gimpel.\\nGaussian error linear units (gelus).\\narXiv preprint\\narXiv:1606.08415, 2016.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom.\\nTeaching machines to read and comprehend.\\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems, volume 28, pages 1693–1701. Curran Asso-\\nciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf.\\nSepp Hochreiter and J¨\\nurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\\n1735–1780, 1997.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.\\nIn International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In International conference on machine learning, pages 448–\\n456. PMLR, 2015.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\\nlocal experts. Neural computation, 3(1):79–87, 1991.\\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\\nNeural computation, 6(2):181–214, 1994.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361, 2020.\\nVladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,\\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁ-\\ncient moe training for multitask multilingual models, 2021.\\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\\n26\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742, 2021a.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742, 2021b.\\nKenichi Kumatani, Robert Gmyr, Felipe Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric\\nSun, and Yu Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for\\nspeech recognition, 2021.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics, 7:453–466, 2019.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt\\ntuning. arXiv preprint arXiv:2104.08691, 2021.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.\\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190, 2021.\\nXiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, and\\nTie-Yan Liu. R-drop: Regularized dropout for neural networks, 2021.\\nJunyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang,\\nYong Li, Wei Lin, Jingren Zhou, and Hongxia Yang. M6-10t: A sharing-delinking paradigm for\\nefﬁcient multi-trillion parameter pretraining, 2021.\\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Sparse-mlp: A fully-mlp architecture\\nwith conditional computation, 2021.\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relation-\\nships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1930–1939,\\n2018.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\\ntraining. arXiv preprint arXiv:1710.03740, 2017.\\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\\nIn Icml, 2010.\\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations\\ntransfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\nDon’t give me the details, just the sum-\\nmary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint\\narXiv:1808.08745, 2018.\\nArvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and\\nJames Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint\\narXiv:1511.06807, 2015.\\n27\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar-\\nial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599,\\n2019.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. 2022.\\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\\nnetworks. In International conference on machine learning, pages 1310–1318. PMLR, 2013.\\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv\\npreprint arXiv:2104.10350, 2021.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\\npher, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´\\ne Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\narXiv preprint arXiv:2106.05974, 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels. arXiv preprint arXiv:2106.04426, 2021.\\nSascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. A simple\\nrecipe for multilingual grammatical error correction. arXiv preprint arXiv:2106.03830, 2021.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\\nsarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, volume 34, pages 8732–8740, 2020.\\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\\ntraining of deep neural networks. Advances in neural information processing systems, 29:901–\\n909, 2016.\\nNoam Shazeer.\\nFast transformer decoding: One write-head is all you need.\\narXiv preprint\\narXiv:1911.02150, 2019.\\n28\\nNoam Shazeer. Glu variants improve transformer, 2020.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538, 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\\nlearning for supercomputers.\\nIn Advances in Neural Information Processing Systems, pages\\n10414–10423, 2018.\\nSam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with\\nextra normalization. arXiv preprint arXiv:2110.09456, 2021.\\nDavid R So, Wojciech Ma´\\nnke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer:\\nSearching for efﬁcient transformers for language modeling. arXiv preprint arXiv:2109.08668,\\n2021.\\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\nDropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\\nResearch, 15(1):1929–1958, 2014. URL http://www.cs.toronto.edu/˜rsalakhu/\\npapers/srivastava14a.pdf.\\nNassim Nicholas Taleb. Antifragile: Things that gain from disorder, volume 3. Random House\\nIncorporated, 2012.\\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan\\nNarang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efﬁciently: Insights from\\npre-training and ﬁne-tuning transformers. arXiv preprint arXiv:2109.10686, 2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems, pages 5998–6008, 2017.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman.\\nSuperglue: A stickier benchmark for general-purpose language\\nunderstanding systems. In Advances in Neural Information Processing Systems, pages 3266–\\n3280, 2019.\\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Jun-\\nyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge en-\\nhanced pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731,\\n2021.\\nSamuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual perfor-\\nmance model for multicore architectures. Communications of the ACM, 52(4):65–76, 2009.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\\nlation system: Bridging the gap between human and machine translation.\\narXiv preprint\\narXiv:1609.08144, 2016.\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv\\npreprint arXiv:2010.11934, 2020.\\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\\nmang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. M6-t:\\nExploring sparse expert models and beyond, 2021.\\n29\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with\\ndynamic routing mixture of experts, 2021a.\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture-of-experts model with im-\\nproved routing, 2021b.\\nBiao Zhang and Rico Sennrich.\\nRoot mean square layer normalization.\\narXiv preprint\\narXiv:1910.07467, 2019.\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted\\ngap-sentences for abstractive summarization, 2020.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint arXiv:1810.12885, 2018.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and\\nJianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2021.\\n30\\nA\\nTOKEN LOAD BALANCE DESCRIPTION\\nThe auxiliary load balancing loss from Shazeer et al. (2017) is also used to here to balance tokens\\nacross experts. Assume we have N experts indexed by i = 1 to N and a batch B with T tokens. The\\nauxiliary loss is computed as the scaled dot-product between vectors f and P,\\nloss = α · N ·\\nN\\nX\\ni=1\\nfi · Pi\\n(7)\\nwhere fi is the fraction of tokens dispatched to expert i,\\nfi = 1\\nT\\nX\\nx∈B\\n1{argmax p(x), i}\\n(8)\\nand Pi is the fraction of the router probability allocated for expert i, 2\\nPi = 1\\nT\\nX\\nx∈B\\npi(x)\\n(9)\\nSince we seek uniform routing of the batch of tokens across the N experts, we desire both vectors\\nto have values of 1/N. The auxiliary loss of Equation 7 encourages uniform routing since it is\\nminimized under a uniform distribution. The objective can also be differentiated as the P-vector is\\ndifferentiable, but the f-vector is not. The ﬁnal loss is multiplied by expert count N to keep the loss\\nconstant as the number of experts varies since under uniform routing PN\\n1 (fi ·Pi) = PN\\n1 ( 1\\nN · 1\\nN ) =\\n1\\nN . Finally, a hyperparameter α is a multiplicative coefﬁcient for these auxiliary losses; throughout\\nthis work we use an α = 10−2 which was sufﬁciently large to ensure load balancing while small\\nenough to not to overwhelm the primary cross-entropy objective.\\nB\\nROUTER Z-LOSS TRAINING DYNAMICS\\nFigure 7 plots the router z-loss from Equation 5 across a coefﬁcient sweep where the best value of\\ncz = 0.001 is plotted in green for the encoder and decoder.\\n0\\n25000\\n50000\\n75000 100000 125000 150000\\nStep\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nZ-Loss\\n0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\n0\\n25000\\n50000\\n75000 100000 125000 150000\\nStep\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nZ-Loss\\n0.0\\n1e-4\\n1e-3\\n1e-2\\n1e-1\\nFigure 7: Sweeping loss coefﬁcient (cz) for Router Z-Loss. We plot the router z-losses over the\\ncourse of pre-training without router z-loss (blue) and with increasing values of cz (we selected\\ncoefﬁcient associated with green curve for all later experiments). With values of 1e-2, or larger,\\nthe z-loss shrinks near to zero. The left plot shows an encoder layer and the right plot shows a\\ndecoder layer.\\n2A potential source of confusion: pi(x) is the probability of routing token x to expert i. Pi is the probability\\nfraction to expert i across all tokens in the batch B.\\n31\\nC\\nIMPROVED ARCHITECTURAL MODIFICATIONS\\nWe consider a few small architecture variations here. The ﬁrst modiﬁcation was adding additional\\nFFN layers (feed-forward network, see Table 1 for more details) immediately before or after each\\nMoE layer (referred to as Sparse-Dense). Table 16 reveals the effectiveness of an FFN layer im-\\nmediately preceding or following each sparse layer and that these extra FFN layers help less when\\nadded elsewhere in the network. Guaranteeing all tokens have at least one FFN applied to them\\nbetween each attention layer appears useful.\\nModel\\nNeg. Log Perp. (↑)\\n∆\\nDense model (baseline)\\n-1.474\\n-\\nDense model w/ extra FFN layers\\n-1.452\\n0.022\\nSparse model (baseline)\\n-1.383\\n-\\nSparse model w/ extra FFN layer after each sparse layer\\n-1.369\\n0.014\\nSparse model w/ extra FFN layer before each sparse layer\\n-1.369\\n0.014\\nSparse model w/ extra FNN layers placed randomly in the network\\n-1.376\\n0.007\\nTable 16: A dense FFN immediately before or after each sparse layer improves quality. Insert-\\ning an extra dense FFN immediately before or after each sparse layer improves quality 2x as much\\nas placing the dense layers (randomly) elsewhere in the network. All of the non-baseline models\\nhave the same amount of FFN layers added for fair comparisons. Note that improving perplexity\\nbecomes harder as the model gets better.\\nSecond, we introduce an additional bias in the expert layers. All our models use the GELU-Linear\\nFFN (Shazeer, 2020), rather than the ReLU FFN:\\nFFNReLU(x) = (ReLU(xW1))W2\\nFFNGEGLU(x) = (GELU(xW11) ⊙xW12)W2\\nThe additive bias is a learned weight (B) added after the ﬁrst matrix multiplication in the FFN layer\\nof shape [batch, dff]. The multiplicative bias (also referred to as a scale parameter) is a learned\\nweight of the same shape, but does an elementwise multiplication. We initialize the additive bias to\\nzeros and the multiplicative bias to ones.\\nFFNGEGLU + Add Bias(x) = [(GELU(xW11) ⊙xW12) + B]W2\\nFFNGEGLU + Mult Bias(x) = [(GELU(xW11) ⊙xW12) ⊙B]W2\\nTable 17 shows the results of our different methods. Both the additive and multiplicative biases are\\nessentially free: cheap to compute, adds few new parameters, and incurs no additional communi-\\ncation costs with model and expert parallelism. When using our router z-loss from Section 3.1, we\\nobserve no instabilities from the multiplicative bias. We do see that the multiplicative interactions\\nimprove performance, achieving a 4% speedup in convergence time over our strong sparse baseline.\\nThis hints that a promising avenue for future architectural research is ﬁnding new ways of adding\\nmore multiplicative interactions into networks.\\nModel\\nNeg. Log. Perp. (↑)\\n∆\\nDense Baseline\\n-1.474\\n-\\nSparse Baseline\\n-1.369\\n-\\nSparse + Additive Bias\\n-1.371\\n-0.002\\nSparse + Multiplicative Bias\\n-1.361\\n0.008\\nTable 17: More multiplicative interactions improve sparse model quality. Both the additive and\\nthe multiplicative bias add virtually no parameters or compute.\\nFinally, motivated by the work of Roller et al. (2021), we explored similar methods, but did not ﬁnd\\nimprovements in our setting. We tried routing using the word embedding exclusively, as well as\\n32\\nan additional input to the layer embedding for routing decisions. We toggled stopping the gradient\\nthrough the word embedding or allowing it to have gradients propagated from the router. Using only\\nthe word embedding hurt quality, while using it in addition to the normal layer hidden activation was\\ninitially positive, but after pre-training for 50B+ tokens on models of scale 1B+ dense parameters it\\nhad a neutral effect. Appendix J has further details on the experiments with negative results.\\nD\\nBATCH PRIORITIZED ROUTING FOR LOWER CAPACITY FACTORS\\nSurprisingly, top-1 and top-2 routing work well with CF less than 1.0 despite token routing being\\ndone in a left to right order over the sequence. If N tokens are sent to an expert with only M spaces\\nthen N > M tokens will dropped. The ordering of the dropping is important: we drop tokens going\\nleft to right (e.g. tokens earlier in the sentence will be routed ﬁrst over the end tokens). This is done\\nto avoid the model cheating. If we dropped tokens in another ordering, the model gets information\\non what tokens are occurring later in the sequence based on if tokens are being dropped or not.\\nBatch Prioritized Routing (BPR) from Riquelme et al. (2021) was introduced in Vision Transformers\\n(Dosovitskiy et al., 2020) for image classiﬁcation. Our work explores BPR with top-1 routing in the\\ncontext of language modeling. BPR aims to have a global view of all tokens to determine which\\ntokens should be dropped instead of the left-to-right ordering. The algorithm works by looking at\\nall N tokens getting sent to Expert i and then only routing the M ones with the highest probabilities\\nfrom the router. Table 18 shows that BPR top-1 routing improves performance over top-2 routing,\\nespecially when capacity factors are less than 1.0. We leave it to future work to try top-n BPR\\nrouting, which will hopefully yield larger improvments for higher capacity factors.\\nImportantly, BPR routing can only be done on the encoder side of the encoder-decoder model. On\\nthe encoder side there are not autoregressive predictions and all tokens can see each other. If you use\\nBPR on the decoder, it learns to cheat by using future token information to improve current token\\npredictions.\\n33\\nAlgorithm\\nTrain CF\\nEval CF\\nNeg. Log. Perp. (↑)\\nDense\\n—\\n—\\n-1.474\\nDense-L\\n—\\n—\\n-1.384\\nBPR Top-1\\n0.5\\n0.5\\n-1.433\\nBPR Top-1\\n0.5\\n2.0\\n-1.416\\nTop-1\\n0.75\\n0.75\\n-1.428\\nTop-1\\n0.75\\n2.0\\n-1.404\\nTop-2\\n0.75\\n0.75\\n-1.424\\nTop-2\\n0.75\\n2.0\\n-1.402\\nBPR Top-1\\n0.75\\n0.75\\n-1.409\\nBPR Top-1\\n0.75\\n2.0\\n-1.397\\nTop-1\\n1.0\\n1.0\\n-1.397\\nTop-1\\n1.0\\n2.0\\n-1.384\\nTop-2\\n1.0\\n1.0\\n-1.392\\nTop-2\\n1.0\\n2.0\\n-1.378\\nBPR Top-1\\n1.0\\n1.0\\n-1.386\\nBPR Top-1\\n1.0\\n2.0\\n-1.379\\nTop-1\\n1.25\\n1.25\\n-1.378\\nTop-1\\n1.25\\n2.0\\n-1.373\\nTop-2\\n1.25\\n1.25\\n-1.375\\nTop-2\\n1.25\\n2.0\\n-1.369\\nBPR Top-1\\n1.25\\n1.25\\n-1.376\\nBPR Top-1\\n1.25\\n2.0\\n-1.375\\nTable 18: Batch Prioritized Top-1 Routing (BPR) performance. BPR top-1 routing improves\\nquality when capacity factors are ≤1. However, once the capacity factor reaches 1.25, the improve-\\nments greatly diminish and it underperforms top-2 routing. Future work can try BPR with top-2\\nrouting, which should hopefully further improve the performance.\\nE\\nPRE-TRAINING DATASET DETAILS\\nThe pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and\\nthe dataset introduced in GLaM (Du et al., 2021).\\nDataset\\nTokens (B)\\nWeight in Mixture\\nFiltered C4\\n183\\n0.17\\nFiltered Webpages\\n143\\n0.34\\nWikipedia\\n3\\n0.05\\nConversations\\n174\\n0.23\\nForums\\n247\\n0.02\\nBooks\\n390\\n0.17\\nNews\\n650\\n0.02\\nTable 19: Data and mixture weights in the training set. We sample from different dataset sources\\nwith probability proportional to “weight in mixture”. The number of tokens listed are in billions (B).\\nFor more details on the C4 corpus see Raffel et al. (2019) and for the other datasets see Du et al.\\n(2021).\\n34\\nF\\nFULL FINE-TUNING SENSITIVITY DATA\\nTable 20 contains the raw data for Figure 6 measuring the ﬁne-tuning protocol sensitivity. Dense\\nand Sparse are encoder-decoder models FLOP matched to T5-Large that were pre-trained for 500k\\nsteps with a batch size of 1M tokens on the C4 corpus.\\nModel\\nLearning Rate\\nBatch Size\\nReset Optimizer Slot Vars\\nSuperGLUE (↑)\\nDense\\n1e-3\\n1M\\n84.8\\nDense\\n1e-3\\n1M\\n✓\\n84.3\\nDense\\n5e-4\\n1M\\n84.8\\nDense\\n5e-4\\n1M\\n✓\\n84.2\\nDense\\n1e-4\\n1M\\n84.0\\nDense\\n1e-4\\n1M\\n✓\\n84.8\\nDense\\n1e-3\\n262k\\n84.9\\nDense\\n1e-3\\n262k\\n✓\\n83.7\\nDense\\n5e-4\\n262k\\n84.9\\nDense\\n5e-4\\n262k\\n✓\\n84.0\\nDense\\n1e-4\\n262k\\n85.1\\nDense\\n1e-4\\n262k\\n✓\\n85.0\\nDense\\n1e-3\\n65k\\n83.7\\nDense\\n1e-3\\n65k\\n✓\\n82.5\\nDense\\n5e-4\\n65k\\n84.4\\nDense\\n5e-4\\n65k\\n✓\\n84.1\\nDense\\n1e-4\\n65k\\n84.9\\nDense\\n1e-4\\n65k\\n✓\\n84.6\\nSparse\\n1e-3\\n1M\\n86.9\\nSparse\\n1e-3\\n1M\\n✓\\n85.9\\nSparse\\n5e-4\\n1M\\n86.1\\nSparse\\n5e-4\\n1M\\n✓\\n83.5\\nSparse\\n1e-4\\n1M\\n84.3\\nSparse\\n1e-4\\n1M\\n✓\\n84.3\\nSparse\\n1e-3\\n262k\\n86.2\\nSparse\\n1e-3\\n262k\\n✓\\n85.2\\nSparse\\n5e-4\\n262k\\n85.5\\nSparse\\n5e-4\\n262k\\n✓\\n84.8\\nSparse\\n1e-4\\n262k\\n85.1\\nSparse\\n1e-4\\n262k\\n✓\\n85.5\\nSparse\\n1e-3\\n65k\\n85.8\\nSparse\\n1e-3\\n65k\\n✓\\n85.5\\nSparse\\n5e-4\\n65k\\n86.5\\nSparse\\n5e-4\\n65k\\n✓\\n85.1\\nSparse\\n1e-4\\n65k\\n85.6\\nSparse\\n1e-4\\n65k\\n✓\\n84.5\\nTable 20: Fine-tuning protocol sensitivity. We vary the batch size, learning rate and whether we\\nreset the optimizer slot variables for both dense and sparse models. Resetting the optimizer state\\nduring ﬁne-tuning hurts performance. We observe a difference in optimal batch size and learning\\nrate for sparse vs. dense models. Certain hyperparameter ﬁne-tuning settings make the sparse and\\ndense models perform almost exactly the same, showing the importance of correctly tuning the\\nhyperparameters.\\n35\\nG\\nOPTIMALLY SETTING THE ROUTING THRESHOLD\\nTop-n Routing Algorithm\\n1. Route each token x to the expert with the highest router probability (gate1(x)).\\n2. Normalize the top-n expert router scores for each token x, so gatei =\\ngatei(x)\\nPn\\ni=1 gatei(x).\\n3. Route the token to the other n-1 experts (indexed by i) with probability min(1.0, gatei(x)\\nthreshold).\\nThreshold is a predeﬁned hyperparameter that is typically set to 0.2.\\nWe describe the MoE hyperparameters and how they should change as the routing algorithm\\nchanges. The MoE top-2 routing algorithm (Shazeer et al., 2017; 2018; Lepikhin et al., 2020) works\\nas follows: ﬁrst the router ﬁnds the expert that is assigned the higher router score (gate1) and always\\nsends the token to that expert. The token is also sent to its second highest expert with probability\\nmin(1.0, gate2/threshold). The threshold is a hyperparameter that is typically set to 0.2, and gate2 is\\nthe token’s router probability for the second highest expert. Note that gate1 and gate2 get normalized\\nby the sum of their two scores, so they sum to one.\\nWe trivially extend the top-2 algorithm to work for top-n routing here. Take the scores of the top-n\\nexperts per token and sum them, then renormalize each expert router score based on that sum. If the\\nspeciﬁc renormalized expert score has a higher value than the threshold (e.g. 0.2), then the token will\\nbe routed, otherwise it will be routed with probability\\nscore\\nthreshold. At a high level this only routes the\\ntoken to the next n-1 experts if their scores are not too much lower than the highest scored expert.\\nFor top-3 routing vs top-2, the sum that the expert scores are normalized by is larger, therefore\\nwe experimented with decreasing the threshold. Our experimental results are shown in Table 21.\\nInterestingly, we do observe the top-3 routing to slightly beneﬁt from the lower threshold, while the\\nopposite is true for top-2 routing.\\nWe also experimented with an absolute threshold policy instead of a relative one. This is where the\\nnext n-1 tokens will be routed only if their router score is great than some pre-deﬁned value (e.g.\\n0.2). We found it can achieve as good of performance if the threshold value is tuned.\\nAlgorithm\\nTrain CF\\nThreshold\\nNeg. Log. Perp. (↑)\\nDense\\n—\\n—\\n-1.474\\nDense-L\\n—\\n—\\n-1.384\\nTop-2\\n3.0\\n0.2\\n-1.354\\nTop-2\\n3.0\\n0.05\\n-1.356\\nTop-3\\n3.0\\n0.2\\n-1.351\\nTop-3\\n3.0\\n0.05\\n-1.349\\nTable 21: Performance of top-2 and top-3 routing with different thresholds. Top-3 routing does\\nslightly better with lower thresholds than top-2 routing.\\nH\\nMESH LAYOUT FOR DATA, MODEL AND EXPERT PARALLELISM WITH\\nFEW EXPERTS\\nWe use data and model parallelism partitioning with Mesh-Tensorﬂow (Shazeer et al., 2018). The\\npartitioning strategy works by ﬁrst forming a logical 2D mesh of size d x m, with the rows corre-\\nsponding to the data dimension (d) and the columns as the model dimension (m) and the product\\nequal to the total number of cores, n = d x m. This mesh is only an abstraction. Each logical core\\nmust be mapped to a physical core, which is optimized through performance tuning.\\nAs a refresher, each row in the mesh will have its own unique slice of the data and each column will\\nhave a unique slice of the model weights. The ﬁnal gradient allreduce communication occurs\\nacross each individual column. The model parallelism allreduce communications occur across\\neach row in the mesh. One constraint from this approach is that the number of rows must evenly\\n36\\nModel Dimension\\nData Dimension\\nOuter Data Dimension\\nInner Data Dimension\\nModel Dimension\\nExperts >= Data Dimension\\nExperts < Data Dimension\\nFigure 8: Data and model parallelism meshes used for distributing models. In this example there\\nare a total of 32 processors (e.g. n = 32). (Left) A valid 2D mesh if the number of experts is greater\\nthan or equal to the data parallelism dimension. The data dimension has 8 rows (d) and the model\\ndimension has 4 columns (m). (Right) A valid 3D mesh when we have fewer experts than the data\\nparallelism dimension. The batch dimension is factorized into two new dimensions: inner data and\\nouter data dimensions. Now we have 1 expert per inner data dimension (i). The 8 data rows in the\\nleft ﬁgure become 4 in the outer batch (o) and 2 in the inner batch (i) with 2 experts instead of 8.\\ndivide the number of data sequences and the number of columns must evenly divide the model\\ndimensions being partitioned.\\nBut if we have fewer than d experts then this layout will not work. To allow for fewer experts than\\ndata parallelism rows in our mesh, we factorize the data dimension into two new dimensions: inner\\n(i) and outer (o) where i x o = d and the number of experts equals i. This transforms the logical 2D\\nmesh of shape d x m into a 3D mesh of shape o x i x m. See Figure 8 for a visualization of both\\nmeshes 12.\\nI\\nNOTE ON COMMUNICATION COSTS FOR DISTRIBUTED MODELS\\nCommunication operations (allreduce and all2all) can signiﬁcantly impact sparse model\\ntraining throughput (see Table 1 for a description of the communication operations). allreduce\\ncalls are executed along model and batch dimensions, typically dominated by the model dimension\\nallreduce calls that sum results of partial matrix multiplication operations from the workers.\\nThese calls are needed when matrix multiplications are partitioned across multiple cores (e.g. model\\nparallelism). The gradient summation allreduce calls can be amortized away by training models\\nwith larger batch sizes since the gradient accumulation allreduce communication cost is inde-\\npendent of the batch size. To alleviate the memory issues of larger batch sizes, microbatches can\\nbe used. Microbatches do this by splitting the batch into n evenly divisible chunks and computing\\ngradients on each sequentially, then summing.\\nTo increase the allreduce throughput, more workers may need to be assigned to the model di-\\nmension (instead of batch dimension). However, increasing the number of workers may reduce\\ncompute per worker resulting in higher communication overheads that cancel some of the gains\\nfrom higher communication throughput from allreduce. For the results in this paper, ﬁrst we\\nexplored various model partitioning strategies. Next the shapes of the pre-training jobs were al-\\nlocated based on performance benchmarking which showed the lowest cumulative communication\\noverheads in allreduce and all2all.\\n12See Mesh Tensorﬂow for more details on the inner and outer batch:\\nhttps://github.com/\\ntensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py\\n37\\nJ\\nNEGATIVE RESULTS\\nWe conclude with some ideas that yielded negative results in our setting.\\nAdding information if tokens were dropped to the router.\\nWe experimented with having the\\nexpert layer have information of whether the token was routed or dropped in the previous expert\\nlayers. We implemented this through counting the number of times a token was routed in all pre-\\nvious expert layers, having embeddings for each possible value and then adding this to the router\\nembedding. We found that this made no difference in performance.\\nAdding explicit expert positional information.\\nWe experimented with adding explicit positional\\ninformation into the outputs of the expert layer. We wanted to see if it either improved performance\\nor sped up convergence during the beginning of training when expert layers were drastically chang-\\ning. We did this through adding an embedding corresponding to what expert each token was sent\\n(including an embedding if the token was dropped), but this did not improve performance.\\nAdding pre-training noise to ﬁx pre-training and ﬁne-tuning discrepancies.\\nTo help ﬁx the\\npre-training perplexity and ﬁne-tuning gap we tried pre-training the sparse models with a variety of\\ndifferent types of noise. The goal was to help pre-training match the ﬁne-tuning conditions where\\ndropout is used and more tokens can be dropped. Some of the noise types we tried adding during\\npre-training were dropout, dropping out full experts for a batch of tokens, and adding an entropy\\nmaximization auxiliary loss to the router. Unfortunately, all of the methods either hurt the pre-\\ntraining quality too much or didn’t end up helping the ﬁne-tuning.\\nLoad balancing in top-n routing over lower n-1 experts.\\nIn the standard top-n MoE formaliza-\\ntion there is only loading balancing over the top expert a token is sent to. We experimented with\\nadding an auxiliary load balancing term to the other n −1 experts in top-n routing, but found this to\\nprovide minimal beneﬁts.\\nMixing pre-training and ﬁne-tuning data to prevent overﬁtting.\\nTo help combat the overﬁtting\\nof sparse models during ﬁne-tuning, we tried mixing in pre-training span corruption data at varying\\namounts (e.g. 1%, 5%, 25%, ...) during ﬁne-tuning. This ended up not helping the ﬁne-tuning\\nperformance, but did increase the training loss.\\n38\\n', 'source_name': 'ST-MoE: Designing Stable and Transferable Sparse Expert Models', 'source_url': 'https://arxiv.org/abs/2202.08906'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "GLaM_NOTES.pdf #7\n",
      "{'content': 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts \\nMain Idea: with the improvement of language models mainly coming from scaling the number of \\nparameters in a dense setting, training these models requires more and more compute and \\nresources. GLaM looks to explore sparse language models (MoE) to reach comparable or superior \\nperformance to dense models while decreasing training costs. During evaluation, GLaM focuses \\non zero-shot and few-shot learning capabilities. The importance of data quality during pre-\\ntraining is also analyzed. \\nThe largest GLaM model has: \\n- \\n1.2T total number of parameters. \\n- \\n96.6B active parameters. \\no For comparison, GPT-3 is a 175B parameter dense model. \\n- \\n64 experts per MoE layer. \\nGLaM seems to have been the first study to use a decoder-only MoE on a model of comparable \\nsize to GPT-3. Switch, for example, had only around 1B active parameters (compared to 96.6B of \\nGLaM) per input and was an encoder-decoder model. \\nThe training dataset used to train GLaM was highly filtered to ensure that low-quality content \\nwas not prominent (although a small collection of low-quality training data is kept to prevent \\nsystematic biases). \\n \\nArchitecture \\n- \\nAlternate between FF (dense) and MoE (sparse) layers. \\n- \\nRegular top-2 routing, with the output being a weighted average based on the scores \\ngiven by the routing. \\n- \\nAuxiliary load balancing loss. \\n \\nEvaluation Setting \\n- \\nMainly focuses on zero-shot, one-shot and few-shot performances of the models being \\nevaluated. \\no This is different to Switch, which focuses on fine-tuning performance. \\no This is consistent with new capabilities shown by scaling LMs as shown by GPT-3. \\n \\nResults \\n- \\nGLaM (64B/64E) (96.6B active parameters) has consistent gains in zero, one and few-shot \\nperformances over GPT-3, while requiring roughly only half of the compute FLOPs at \\ninference (96.6B vs 175B). \\n- \\nImproved performance (over GPT-3) on the challenging TriviaQA domain indicates that \\nthe additional capacity of GLaM plays a crucial role in its performance gains. \\no On GPT-3’s paper, GPT-3 was shown to consistently improve on this task \\n(TriviaQA) given an increase in parameters, which was attributed to its ability to \\nretain more knowledge with an increase in parameters. \\n- \\nUsing a small model (GLaM (1.7B/64E)), it was shown that the quality of the pretraining \\ndata plays a crucial role in determining the quality of the model. \\no The impact of data quality was bigger in NLG tasks compared to NLU tasks. \\n- \\nMoE models can be scaled in two ways: \\no Increasing the number of experts \\n▪ Keeps the number of active parameters (and thus the compute FLOPs at \\ninference) constant. \\n▪ Increasing the number of experts generally resulted in better performance \\nup to 64 experts (there was a decline in performance in further increases \\nafter 64). \\no Increasing the size of experts \\n▪ Leads to an increase in inference costs. \\n▪ Results in improved performance. \\n- \\nGLaM MoE models perform consistently better than GLaM dense models for similar \\neffective FLOPs per token. \\no MoE models perform similarly to dense at smaller scales but obtain an advantage \\nwhen scaling the model. \\n- \\nIn terms of data efficiency, GLaM MoE models require significantly less data than dense \\nmodels of comparable FLOPs. \\no When the same amount of data is used for training, MoE models perform much \\nbetter, and the difference in performance becomes larger when training up to \\n630B tokens, so this advantage increases with scale. \\n- \\nIn terms of computational efficiency and energy consumption, sparse models take much \\nless computational resources to achieve the same performance. \\no GLaM (64B/64E) has around 1/3 of training costs of GPT-3, while also halving the \\ninference cost and using 1/6th of the energy costs. \\n▪ These gains can be attributed to the MoE architecture’s superior training \\nefficiency. \\n \\nMy takeaways: \\n- \\nIn terms of architecture, GLaM does not seem to provide any significant advancements in \\nMoE. The main quality of this research was to analyze how the MoE architecture would \\nperform at a large scale (especially of number of active parameters) in a decoder-only \\nmodel for NLG. \\n \\n', 'source_name': 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GLaM_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ST_MoE_NOTES.pdf #8\n",
      "{'content': 'ST-MoE: Designing Stable and Transferable Sparse Expert Models \\nMain Idea(s): this paper provides a thorough study on MoEs. It tackles the biggest challenges \\npresented to MoE models at the time of its release, with those being instabilities in training and \\npoor fine-tuning performance. Its main goal is therefore to improve the practicality and reliability \\nof sparse models. \\n- \\nTrains a 269B sparse encoder-decoder model. \\n- \\nIntroduces router z-loss to resolve instability issues. \\n \\nStabilizing Training of Sparse Models \\nTransformer models today are normally trained by using float32 to compute gradients and \\nfloat16 to compute the forward and backward pass. Sparse models contain several exponential \\nfunctions (like softmax), which can lead to large values flowing through the network. Float16 \\ndoes not handle large numbers well, as the larger the number, the larger its resulting rounding \\nerror. It is proposed that this abundance of exponential functions in MoE is what causes training \\ninstability. Router z-loss is a trick to penalize large values from flowing through the network, thus \\nimproving stability. \\n- \\nRouter z-loss is a function that stabilizes the training of MoE models without degradation \\nin model quality by penalizing large values from flowing through the network. \\no Stability is referred to as constant/smooth decrease in the training loss. \\no Router z-loss is a complement to the overall loss function, as cross-entropy and \\nauxiliary load-balancing loss are also used. \\no So total loss = cross-entropy + auxiliary load-balancing loss + router z-loss. \\nFine-Tuning Sparse Models \\nModel characteristics: \\n- \\nDense and sparse models both pre-trained on 500B tokens. \\n- \\nBoth roughly match T5 (encoder-decoder), which has 770M parameters. \\n- \\nSparse model has 32 experts and a sparse layer every 4 layers. \\n- \\nTrain capacity factor = 1.25, 2.0 at eval time. \\n- \\nFine-tuned on 2 SuperGLUE tasks, one with 100,000 training examples and the other with \\n250 to analyze overfitting of sparse models during fine-tuning. \\nSparse models are thought of to be more prone to overfitting during fine-tuning, especially when \\nthere is little data to work with (the more data, the lesser the risk of overfitting). This is observed \\non the smaller task (250 training examples), where the sparse model performs better against the \\ntraining set but worse in the evaluation set (classic overfitting). This does not happen in the larger \\ntask (which has 100k training examples), where the sparse model performs better than the dense \\none on both the training and evaluation sets. This leads us to the conclusion that sparse models \\nhave fine-tuning advantages if enough data is available to prevent the model from overfitting. \\nIncreasing regularization did not seem to have much effect at the small-scale fine-tuning, \\nshowing that small amounts of data are hard to overcome in this scenario. \\nTo explore fine-tuning MoEs further, the authors experiment with exclusively updating a few \\nlayers while keeping the remaining layers frozen. They test this for different combinations. \\n- \\nMost combinations yield similar results, except one -> only updating sparse layers, which \\nresulted in degraded performance. \\no This indicates that the overfitting comes from sparse layers (although updating all \\nparameters leads to better performance than updating all non-MoE parameters \\nonly). \\no Another explanation for this can be the frequency of sparse layers being too \\nsparse (only 1 sparse layer for every 3 dense layers), so the number of parameters \\nbeing updated is not large enough. \\nAnother fine-tuning aspect analyzed was the batch size and learning rate of dense vs sparse \\nmodels. Experiments showed that they do not respond the same way to changes in these training \\ndecisions. \\n- \\nSparse models benefit from smaller batch sizes and larger learning rates, while the \\nopposite is observed for dense models. \\no The range of batch sizes used was 65K to 1M, and the range of learning rates used \\nwas 1e-4 to 1e-3. \\no This is consistent with the overfitting hypothesis proposed for MoE models, as \\nsmaller batch sizes have less accurate gradient updates (the higher the number of \\ninputs used for an update, the more accurate the update). This reduced accuracy \\ncan be thought of as added noise, which serves as regularization, helping with \\noverfitting. \\nLoad balancing is seen as a key challenge for effectively training and fine-tuning sparse models \\nto optimize for modern hardware and prevent token dropping (expert overflow). Experiments \\nconducted on this topic, however, contradict this (for fine-tuning): \\n- \\nThe percentage of tokens dropped (up to 15%) did not seem to have a significant impact \\nin fine-tuning. So, token dropping in fine-tuning does not seem like a problem. \\n- \\nHigh capacity factors used during fine-tuning do not seem to have an impact on model \\nquality. \\n- \\nThe addition of an auxiliary load balancing loss seems to have very little impact on fine-\\ntuning. \\nIn terms of the number of experts to choose, works like Switch Transformers show that a large \\nnumber of experts (up to 512) can improve model quality if designed correctly (although at a \\ndiminishing rate). However, to ensure hardware efficiency, an important constraint is necessary: \\neach GPU/TPU core available should have a maximum of 1 expert to minimize memory transfer \\ncosts. The main reason for this can be attributed to modern hardware not being optimized for \\nloading parameters to memory. If more than 1 expert is present in a core, whenever the other \\nexpert gets called, all experts in that core need to be loaded, leading to inefficiencies.  \\nDeciding on CF (capacity factor) and k in top-k routing should depend on memory and \\ncomputational resources, as it leads to performance boosts but at the expense of increased costs. \\n \\nResults \\nThe ST-MoE 32B (32B active parameters, 369B total parameters) becomes the new SOTA in the \\nSuperGLUE tasks (it was trained on all the tasks concurrently). \\nA 4.1B sparse model designed to match the FLOPs of T5-L (800M active parameters) shows \\nimproved performance on all fine-tuning tasks except the two with fewer training examples \\n(around 250 each) -> more signs of MoEs being prone to overfitting. \\nFinally, an observation was made that upon analysis, the encoder layers generally show \\nspecialization in areas such as punctuation, verbs, numbers, names, etc. While decoder expert \\nlayers do not show specialization. \\n \\nMy takeaways: \\n- \\nRouter z-loss seems to be a helpful loss function in terms of stabilizing pre-training of MoE \\nmodels in a mixed-precision environment. \\n- \\nAs shown by the experiments with fine-tuning MoEs, the performance issues seem to \\ncome from the scenario where there is a lack of data to use for fine-tuning. If there is \\nenough data available, fine-tuning MoEs obtains better performance than fine-tuning \\ndense models. \\no This can perhaps be explained to the distribution shift fine-tuning data brings in \\ncomparison to pre-training data. Naturally, a few experts will be more suited to \\nthe fine-tuning data, and thus will be used more than others, leading to overfitting \\nof the more commonly used experts and underfitting of others. \\n- \\nAlthough the fact that only updating sparse parameters during fine-tuning leads to worse \\nperformance, it would be worth exploring further if this can be caused by the lack of MoE \\nlayers present (only one for every 3 dense layers). \\n- \\nIt is also interesting how MoEs benefit from smaller batch sizes, which add a regularizing \\neffect (adding strength to the claim that MoEs are prone to overfitting). \\n- \\nExperiments done for this paper show that load balancing is not an issue for fine-tuning, \\nwhich makes sense since the router should already be fully trained. This seems to indicate \\nthat routers can be frozen during fine-tuning. \\n- \\nIt is explained how, although having many experts may lead to performance boosts, a \\nbalance needs to be achieved depending on the number of GPU/TPU cores available. \\nLoading more than 1 expert per core leads to inefficiencies. \\n- \\nDeciding on CF (capacity factor) and k in top-k routing should depend on memory and \\ncomputational resources, as it leads to performance boosts but at the expense of \\nincreased costs. \\n \\n', 'source_name': 'ST-MoE: Designing Stable and Transferable Sparse Expert Models NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/ST_MoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Switch_Transformers_NOTES.pdf #9\n",
      "{'content': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient \\nSparsity \\nMain Idea: the idea for this paper is based on the “Scaling Laws for Neural Language Models”, \\nwhich states that larger models are more sample-efficient, and thus advises that the optimal \\nallocation of a fixed compute budget should prioritize increasing the number of model \\nparameters while decreasing the number of training steps. This created the motivation to scale \\nMoE models, which allow for an increase in parameter count while keeping FLOPs constant. The \\nmain issues to be addressed related to scaling MoEs are: \\n- \\nComplexity \\n- \\nCommunication costs \\n- \\nTraining instability \\n \\nTop-1 Routing \\nSparsely-Gated MoE had claimed that top-k routing had to have k > 1 to have non-trivial gradients \\nto the routing function (the routers were thought to not train properly if they didn’t have at least \\ntwo experts to compare results with). Switch challenges this idea and successfully uses top-1 \\nrouting. This introduces advantages such as reduced computation, reduced batch size (top-2 \\nrouting requires an expert capacity factor of 2, which is not needed in top-1 routing) and reduced \\ncommunication costs. \\n \\nExpert Capacity \\nEach expert has an expert capacity, which is the computation it can perform on each batch. Due \\nto the dynamic nature of routing (load balancing in a batch is not guaranteed), this expert \\ncapacity can lead to memory overflow issues (where the overflown tokens in a batch are \\nskipped). This can be managed by setting a capacity factor to the experts (keep some buffer to \\neach expert’s machine). \\nExpert capacity = (tokens per batch/number of experts) * capacity factor \\nAlthough this helps with memory overflow and the issue of skipped tokens, it results in increased \\ncomputation and memory costs. \\n \\nLoad Balancing Loss \\nFor the auxiliary loss, Switch introduces a differentiable load balancing loss that considers both \\nthe fraction of tokens assigned to each expert and the probability given to each expert by the \\nrouter (sum of the probabilities given to each expert when it was selected). \\nAuxiliary loss = alpha * N * summation over all experts (fi * pi), where N is the number of experts, \\nfi is the fraction of tokens in the batch dispatched to expert I and pi is the fraction of the router \\nprobability allocated to expert i. \\nThis loss ensures load balancing by leveraging the fact that the product fi * pi is minimized under \\na uniform distribution, where both fi and pi are equal or close to 1/N for each expert, \\ncorresponding to a balanced load. The sum of fi and pi is constrained to 1 across all experts, \\nhighlighting the zero-sum nature of resource distribution. The non-linear impact of the product \\nfi * pi in the loss function means that the sum of these products across experts is minimized when \\nthe load (dispatched tokens) and router probabilities are evenly distributed. This minimization \\ndrives the model toward a uniform distribution, promoting load balance by ensuring that no \\nsingle expert is disproportionally favored in terms of load or router’s allocation. \\nThis loss is a complement to the cross-entropy loss -> total loss = cross-entropy + auxiliary loss.  \\nT5 (dense) vs MoE (top-2 routing) vs Switch (top-1 routing) \\nModels were trained on a masked language modeling objective with 15% token dropout (for MoE \\nand Switch). \\nThe same computation per token is applied (equal FLOPs) for each model. However, MoE has \\nmore active parameters since it uses top-2 routing. \\n- \\nSwitch outperforms T5 and MoE in terms of speed-quality (fixed amount of computation \\nand wall-clock time) \\n- \\nSwitch has a lower computational footprint -> increasing its size to match the speed of \\nMoE leads to outperforming MoE and T5 on a per step basis (since MoE is slower than \\nSwitch due to higher number of active parameters) \\n- \\nSwitch performs better at lower capacity factors (1, 1.25) \\n \\nTraining and Fine-Tuning Techniques \\nInstability in MoE comes mainly from the hard-switching routing strategy. This makes it \\nchallenging to train in lower precision. To combat this, a few tricks are used: \\n- \\nSelective precision with large sparse models \\no Selective casting to float32. More specifically, the router input is casted to float32 \\nwithin the body of the router function (local computations) but back to float16 at \\nthe end of the routing function when the results are dispatched for the selection \\nof the router computation (between devices). This optimizes the router stability \\nwhile keeping the communication costs low. \\n- \\nSmaller parameter initialization for stability \\no Simple initialization changes (especially reducing the normal initialization scale of \\na Transformer by 10) drastically helps with stability. \\no A popular initialization strategy is used -> weights randomly initialized from a \\ndistribution with mean of 0 and st dev of sq root(s/n), where s is a scale hyper-\\nparameter and n is the number of input units in the weight tensor. \\n- \\nRegularizing large sparse models \\no Since MoE models have much more parameters than regular dense Transformers, \\nthey can be more prone to overfitting when fine-tuned in small downstream tasks. \\n▪ Switch proposes increasing dropout in expert layers while keeping a \\nsmaller dropout rate in other layers. This is shown to lead to improvements \\nin fine-tuning. \\n \\nScaling properties \\n- \\nWhen keeping the FLOPs per token fixed, having more total parameters (increase in \\nnumber of experts) speeds up training (although at a cost in memory) in a per-step basis \\n(training is more sample-efficient). \\n- \\nMoE models have higher communication costs than dense models. So even though Switch \\nis more efficient in a per-step basis, this can fail to hold in a time basis. \\no With a fixed training duration and computational budget, Switch achieves a 7x \\nspeedup in training compared to T5 (Switch achieves the same loss 7x faster). \\no Switch shows improvements in both per-step and time basis during pre-training \\nover T5 even when compared to T5-Large (3.5x increase in FLOPs). \\n \\nFine-tuning \\nWith an increase in dropout rate (0.4 vs 0.1), Switch was shown to have improved fine-tuning \\nresults over T5-Base and T5-Large in a FLOP-matched basis in NLP tasks, including reasoning and \\nknowledge-heavy tasks. \\n \\nDistillation \\nWhen distilling a large sparse model into a small dense model, it is found that reducing the model \\nto 1/20th of its original parameter count still retains 30% of the Switch gains over T5. This is a sign \\nthat not all gains are due to increased parameter count, indicating that some part of the gains \\ncan be due to other reasons related to the MoE capturing parameters more efficiently. \\n \\nParallelism (Data, Model, Experts) \\nData parallelism – data is shared over all cores available, while keeping a copy of the model in \\neach core (model is replicated over each core). Each core (model) only needs to communicate at \\nthe end of each batch to perform an update on the model’s parameters. \\nModel parallelism – model is distributed over all cores, while passing all tokens through each \\ncore. This method leads to high communication costs between cores since each token needs to \\nbe passed from core to core to produce a label. \\nModel and data parallelism – model is split through m cores and data is split through n cores (mix \\nof pure model parallelism and pure data parallelism. \\nExpert and data parallelism – the model is distributed by having one expert in each core while \\nsharding the data over all cores. This sharding is done by the routing function, assuming the \\nauxiliary loss will help with load balancing to prevent the token overflow issue. \\nExpert, model and data parallelism – more complex method where each expert is distributed \\nthrough multiple cores (in case a single expert does not fit in a single core, which can happen if \\nwe want to increase the number of FLOPs – this leads to a decreased batch size because more \\nmemory is needed for the experts and the communication costs between cores, leading to less \\nmemory available for the data). This needs to consider the communication costs between the \\nrouting function distributing the data and the model/expert sharding. \\n \\nIncreasing the number of experts does not seem to lead to instability in training (as seen in \\ntraining the 1.6T model). What caused instability is increasing the number of FLOPs (increasing \\nthe size of each expert). \\n \\nMy takeaways: \\n- \\nThe claim made on Sparsely-Gated MoE that k > 1 is needed for top-k routing initially \\nseems to make sense. This would help the gradient to differentiate between good and \\nbad experts for that input. For example, with k = 2, the router can compare the gradients \\nthat come from each expert, and therefore learn which expert was more useful to the \\nfinal output. With k = 1, this property is not present. \\no Top-1 routing, even if it works, would not benefit from overlaps in the clusters \\nthat each expert specializes in. Coupled with reduced computation (less \\nparameters used during inference), it seems that this would lead to efficiency \\ngains but with a loss in performance. \\n- \\nIt is true that increasing the model parameters makes the model more prone to \\noverfitting, especially when there is not enough data available (the more data, the less \\nthe risk of overfitting), which is more likely during fine-tuning. Increasing regularization \\n(dropout in this case) is logical when it comes to helping with that. Remember that \\ndropout will randomly drop training samples, allowing the model to go through the data \\nmore times. \\n- \\nSwitch is shown to perform significantly better than dense models in pre-training (in both \\na per-step and per-time basis). \\n- \\nIncreasing regularization during fine-tuning shows promise for MoE models. However, \\nMoE architectures are not as well suited as dense models for fine-tuning due a higher \\namount of data being needed to prevent overfitting. \\no Results from Switch show that it outperformed T5 in fine-tuning tasks such as \\nGLUE and SQuAD. However, these seem like tasks that have enough data to \\nprevent the issue of overfitting in Switch. It would be interesting to see how this \\nholds when fine-tuning on tasks with less data available. \\n- \\nDistillation results from Switch show great promise, as it hints that models can be pre-\\ntrained in a MoE architecture and then distilled while still performing better than just pre-\\ntraining on a dense architecture. \\n- \\nWhen training an MoE model, it would make sense to use expert parallelism in the \\nscenario where a single expert fits into a core, and to use expert, model and data \\nparallelism in the case of a single expert not fitting into a core. \\n- \\nThe observation that increasing the size of each expert (and not the number of experts) \\nis what causes instability is interesting as it shows that this perhaps leads to experts that \\nare too complex for the clusters they are assigned to (although this should be true for an \\nincrease in the number of experts – more experts = smaller clusters for each expert). From \\nintuition, it seems that a balance between number of experts and expert size is needed. \\n \\n', 'source_name': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Switch_Transformers_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "GShard_NOTES.pdf #10\n",
      "{'content': 'GShard: Scaling Giant Models with Conditional Computation and Automatic \\nSharding \\nMain Idea:  \\nGShard looks to make improvements on different challenges of training MoE models, particularly \\nrelated to scaling. A 600B MoE model (2048E, 36L) is successfully trained, while the authors fail \\nto reach a stable 1T parameter model (2048E, 60L) due to issues with training stability caused by \\nreduced precision (bfloat32 to bfloat16). The improvements made were in the following topics: \\n- \\nComputation costs when scaling. \\n- \\nEase of programming when scaling. \\n- \\nEfficient scaling implementation on parallel devices. \\nGShard modifies the traditional Transformer architecture by alternating between a self-attention \\nand a MoE layer with top-2 routing. To scale, the model is stretched vertically (increase in number \\nof layers in each expert) and/or horizontally (increase in the number of experts per MoE layer). \\nFor dealing with load balancing: \\n- \\nA hyperparameter for a maximum threshold for the number of tokens to be sent to each \\nexpert per batch is set (expert capacity, set to N/E – N=# of tokens in the batch; E=# of \\nexperts). \\no Extra tokens (that couldn’t ‘t make it due to the expert capacity being reached) \\nare overflown/discarded. \\n- \\nTraining tokens are distributed evenly into G groups to take advantage of parallelism. \\nExpert capacity is evaluated in a group basis – local group dispatching. \\no Experts are divided into groups that are optimized for communication \\n(communication between experts in the same group is faster than between \\nexperts in other groups). Local communication (which is optimized) are used more \\nbetween experts instead of global communication. \\n- \\nAddition of a load balancing term to the loss function based on the mean number of token \\nassignment to all experts compared to the token assignment for each expert (calculated \\nat the group level). \\n- \\nRandom routing is employed to help with the expert capacity constraint. Top-2 routing \\nrequires a capacity factor of 2. To help with this, some tokens which have a low gating \\nweight for the 2nd-best expert are not propagated through this expert (becoming top-1 \\nrouting). These 2nd-best experts are dropped randomly in proportion to the gating weight \\nthey were assigned (if assigned a score of 0.2, it would have a higher chance of being \\ndropped than if it was assigned a score of 0.3). \\nResults: \\n- \\nScaling the number of layers (vertical scaling) leads to consistent gains. \\n- \\nIncreasing the number of experts used has diminishing returns. \\n- \\nIncreasing the number of experts helps with high-resource tasks (which have more data), \\nwhile dense models adjust better to low-resource tasks (low amount of data). \\n- \\nIn terms of training efficiency: \\no Scaling with conditional computation is more practical and efficient than with \\ndense models. \\no Deeper models are more sample efficient (converge faster with fewer examples). \\nThat is, increasing the number of layers in a model leads to an almost proportional \\nspeed up in terms of training steps to reach a certain loss (a 3x increase in number \\nof layers would lead to ~3x speed up in training steps to reach a certain loss). \\no As mentioned previously, scaling the number of experts per-layer has diminishing \\nreturns. \\nMy takeaways: \\n- \\nGShard is the first attempt of massively scaling MoE in a Transformer architecture. It does \\nso by optimizing the technical implementation of MoE for communication costs and \\nparallelism. \\n- \\nHighlights: \\no The techniques used for balancing dropping tokens by adjusting the expert \\ncapacity (needs to be higher for a higher k) as well as randomly dropping the 2nd-\\nbest expert are interesting. \\no The local group dispatching technique to minimize communication overhead costs \\nalso seems interesting and deserves a deeper look/understanding. \\n- \\nBased on my analysis of this paper, I was left with a few questions/thoughts: \\no Are MoE layers robust to dropped tokens? As each expert is assigned to a specific \\ninput space, my first thought is that if the experts are of modest size and enough \\nexperts are employed per layer (making the specialized input space for each \\nexpert smaller), the MoE architecture should be robust to dropping tokens. \\no Are different experiments employed at future research works regarding MoE \\nperforming better at high-resource tasks and dense models performing better at \\nlow-resource tasks? This seems to mean that MoE’s gains over dense models \\ncome at the expense of a bigger amount of data being needed. \\no The fact that increasing the number of experts per-layer leads to diminishing \\nreturns makes sense –> as each expert specializes in a certain area of the input \\nspace, increasing the number of experts will decrease the size of that area, \\nallowing the experts to specialize further. However, at some point the experts will \\nbecome redundant (too many experts for a small input space area/cluster), \\nleading to diminishing returns due to these redundant experts having the same \\nspecialization. \\n▪ Based on this logic, it should be possible to balance the expert size with \\nthe number of experts in each layer. The logic is that decreasing the expert \\nsize would lead to each expert only being able to handle smaller input \\nspaces/clusters of the data, so more experts would be needed. \\no The fact that deeper models are more sample efficient hints that the scaling laws \\nfor MoE should be like those of dense models. This makes sense as adding more \\nlayers is adding computation to the model. \\n \\n', 'source_name': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GShard_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Efficient_Large_Scale_LM.pdf #11\n",
      "{'content': 'Efﬁcient Large Scale Language Modeling with Mixtures of Experts\\nMikel Artetxe∗\\n, Shruti Bhosale∗\\n, Naman Goyal∗\\n, Todor Mihaylov∗\\n, Myle Ott∗\\n, Sam Shleifer∗\\n,\\nXi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li,\\nShuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura,\\nBrian O’Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov\\nMeta AI\\nAbstract\\nMixture of Experts layers (MoEs) enable efﬁ-\\ncient scaling of language models through con-\\nditional computation. This paper presents a de-\\ntailed empirical study of how autoregressive\\nMoE language models scale in comparison\\nwith dense models in a wide range of settings:\\nin- and out-of-domain language modeling,\\nzero- and few-shot priming, and full-shot ﬁne-\\ntuning. With the exception of ﬁne-tuning, we\\nﬁnd MoEs to be substantially more compute\\nefﬁcient.\\nAt more modest training budgets,\\nMoEs can match the performance of dense\\nmodels using ∼4 times less compute. This gap\\nnarrows at scale, but our largest MoE model\\n(1.1T parameters) consistently outperforms a\\ncompute-equivalent dense model (6.7B param-\\neters). Overall, this performance gap varies\\ngreatly across tasks and domains, suggesting\\nthat MoE and dense models generalize differ-\\nently in ways that are worthy of future study.\\nWe make our code and models publicly avail-\\nable for research use.1\\n1\\nIntroduction\\nLarge Language Models (LMs) achieve remarkable\\naccuracy and generalization ability when ﬁne tuned\\nfor NLP tasks (Peters et al., 2018; Devlin et al.,\\n2019; Liu et al., 2019; Lan et al., 2020; Raffel\\net al., 2020). They are also capable zero- and few-\\nshot learners (Brown et al., 2020), with the ability\\nto generalize to tasks not seen during training. A\\nreliable way to improve LM accuracy in all of these\\nsettings is by scaling up: increasing the number of\\nparameters and the amount of computation used\\nduring training and inference (Raffel et al., 2020;\\nBrown et al., 2020; Fedus et al., 2021). In fact,\\nsome generalization properties only emerge in very\\nlarge models, including much improved zero- and\\nfew-shot learning (Brown et al., 2020).\\n∗\\nEqual contribution. Authors listed alphabetically.\\n1https://github.com/pytorch/fairseq/tree/main/\\nexamples/moe_lm\\nDense training ZFLOPs\\nMoE speedup factor\\n1\\n3\\n5\\n7\\n9\\n11\\n13\\n15\\n2\\n4\\n6\\n8\\n10\\n20\\nIn-domain LM\\nOut-of-domain LM\\nZero-shot priming\\nFigure 1: Estimate of how much more efﬁcient MoEs\\nare relative to dense models.\\nA speedup factor of\\ny indicates that an MoE model can match the per-\\nformance of the corresponding dense model—trained\\nwith x ZFLOPs—using y times less compute (i.e., x/y\\nZFLOPs). We estimate this factor according to valida-\\ntion perplexity for in-domain language modeling, the\\nPile perplexity for out-of-domain language modeling,\\nand average accuracy across 6 tasks for zero-shot prim-\\ning. See §3.3.3 for more details.\\nUnfortunately, the corresponding growth in com-\\nputational resources required to train state-of-the-\\nart language models is a barrier for many in the\\nresearch community (Schwartz et al., 2019). There\\nis also a concern about the environmental costs\\nassociated with training and deploying such mod-\\nels (Strubell et al., 2019; Gupta et al., 2021; Bender\\net al., 2021; Patterson et al., 2021) motivating re-\\nsearch into more efﬁcient model designs (Lepikhin\\net al., 2021; Fedus et al., 2021; Lewis et al., 2021).\\nSparse models allow for increased number of\\nlearnable parameters without the associated com-\\nputational costs. For example, sparsely gated mix-\\nture of experts (MoE) (Lepikhin et al., 2021) have\\nbeen successfully used for language modeling and\\nmachine translation (Lepikhin et al., 2021; Lewis\\net al., 2021; Roller et al., 2021), but are yet to be\\nshown effective for ﬁne-tuning (Fedus et al., 2021)\\nas well as zero- and few-shot learning. We hypothe-\\narXiv:2112.10684v2  [cs.CL]  26 Oct 2022\\nsize that sparse models are comparatively accurate\\nto dense models but at a much lower computational\\nfootprint. To measure this claim, we train tradi-\\ntional dense and MoE language models ranging\\nin size from several hundred million parameters\\nto more than one trillion parameters and present a\\ncareful empirical comparison of these models on\\ndownstream tasks in zero-shot, few-shot and fully\\nsupervised settings.\\nAs shown in Figure 1, we ﬁnd that MoE models\\ncan indeed achieve similar downstream task perfor-\\nmance as dense models at a fraction of the compute.\\nFor models with relatively modest compute bud-\\ngets, a MoE model can perform on par with a dense\\nmodel that requires almost four times as much\\ncompute. Downstream task performance improves\\nwith scale for both MoE models and dense mod-\\nels. While we observe that the performance gap\\nnarrows as we increase model size, even at larger\\ncompute budgets (∼5000 GPU days), our largest\\nMoE model (1.1T parameters) outperforms a dense\\nmodel with similar computational cost (6.7B pa-\\nrameters). We further compare and contrast the\\nperformance of dense and sparse models with sim-\\nilar computational signatures and observe some\\nperformance variations across tasks and domains,\\nsuggesting this an interesting area for future re-\\nsearch. In summary, our contributions are:\\n• We present a comprehensive study of sparse mod-\\nels for zero and few-shot learning at scale;\\n• We demonstrate that even at scale sparse MoE\\nmodels can yield competitive zero and few-shot\\nperformance at a fraction of the computation for\\nmodel training and inference;\\n• We observe some differences in how dense and\\nsparse models generalize at scale suggesting com-\\nplementary behaviour that could be an interesting\\nfuture research direction.\\n2\\nBackground and Related Work\\n2.1\\nLarge Language Models / GPT-3\\nProgress in the ﬁeld of NLP has been driven\\nby increasingly large Language Models (LMs)\\npretrained on large text datasets. While numer-\\nous variations have been proposed, such LMs are\\npredominantly based on the transformer architec-\\nture (Vaswani et al., 2017). Models are pretrained\\nby hiding parts of the input: predicting the next\\nword sequentially left-to-right, masking words in\\nthe text (Devlin et al., 2019; Liu et al., 2019), or per-\\nturbing and/or masking spans (Lewis et al., 2020;\\nRaffel et al., 2020). The resulting models can be\\nquickly adapted to perform new tasks at high ac-\\ncuracy by ﬁne-tuning on supervised data (Devlin\\net al., 2019; Liu et al., 2019).\\nRecently, GPT-3 (Brown et al., 2020) demon-\\nstrated that large LMs can perform zero- and\\nfew-shot learning without ﬁne-tuning through in-\\ncontext learning. Notably, many of these in-context\\nzero- and few-shot learning behaviors emerge or\\namplify at scale. Concurrent to our work, Rae et al.\\n(2022) and Smith et al. (2022) further explore scal-\\ning dense language models.\\n2.2\\nSparse models\\nOne drawback of dense model scaling is that it\\ngrows increasingly computationally expensive. To\\nmore efﬁciently increase model capacity, condi-\\ntional compute strategies have been developed\\n(Bengio et al., 2013; Davis and Arel, 2013; Cho\\nand Bengio, 2014; Bengio et al., 2015), where each\\ninput activates a subset of the model. Recent work\\n(Lewis et al., 2021; Lepikhin et al., 2021; Fedus\\net al., 2021; Fan et al., 2021) has studied different\\nconditional compute strategies that work well with\\nTransformer models for natural language tasks. In\\nthis work, we focus on Sparsely Gated Mixture of\\nExpert (MoE) models (Shazeer et al., 2017; Lep-\\nikhin et al., 2021). Sparse MoE models replace the\\ndense feed forward network block in every alter-\\nnate Transformer layer with an MoE layer. The\\nMoE layer has a routing gate that learns which\\ntokens are to be mapped to which set of experts\\n(we use top-2 experts). To ensure scalability and\\ntraining efﬁciency, it is also common to include a\\nweighted gate loss term as in Lepikhin et al. (2021)\\nto the cross entropy loss to encourage the tokens\\nto be uniformly distributed to the experts. Con-\\ncurrent to our work, Du et al. (2021), Rajbhandari\\net al. (2022) and Clark et al. (2022) also study MoE\\nscaling.\\n2.3\\nZero-shot and Few-shot Learning\\nRecent works (Schick and Schütze, 2021a; Radford\\net al., 2019) have directly evaluated LMs on unseen\\ntasks successfully (zero-shot learning), by recast-\\ning their task inputs as cloze-style prompt com-\\npletion tasks. This is in contrast to the traditional\\napproach of augmenting LMs with task-speciﬁc\\nheads, followed by supervised ﬁne-tuning (Devlin\\net al., 2019; Raffel et al., 2020). Subsequently,\\nGPT-3 (dense)\\nOurs (dense)\\nOurs (MoE)\\nsize\\ncost\\nl\\nh\\ne\\nsize\\ncost\\nl\\nh\\ne\\nsize\\ncost\\nl\\nh\\ne\\n125M\\n0.36 12\\n768 –\\n125M\\n0.36 12\\n768 –\\n15B\\n0.43 12\\n768 512\\n355M\\n1.06 24\\n1024 –\\n355M\\n1.06 24 1024 –\\n52B\\n1.30 24 1024 512\\n760M\\n2.13 24\\n1536 –\\n—\\n—\\n1.3B\\n3.57 24\\n2048 –\\n1.3B\\n3.57 24 2048 –\\n207B\\n4.53 24 2048 512\\n2.7B\\n7.08 32\\n2560 –\\n2.7B\\n7.08 32 2560 –\\n—\\n6.7B\\n17.12 32\\n4096 –\\n6.7B 17.12 32 4096 –\\n1.1T 22.27 32 4096 512\\n13B\\n32.67 40\\n5120 –\\n13B 32.67 40 5120 –\\n—\\n175B 430.17 96 12288 –\\n—\\n—\\nTable 1: Dense and mixture of expert (MoE) model details. size: number of parameters, cost: training ZFLOPs,\\nl: layers, h: hidden dimension, e: number of experts. All models are trained for 300B tokens with a sequence\\nlength of 2048 tokens. Models within the same row are roughly comparable. We estimate the training cost in\\nZFLOPs analytically (see Appendix G).\\nBrown et al. (2020) demonstrated that priming LMs\\nwith a few input-output examples (few-shot learn-\\ning) before careful prompting can improve task\\nperformance, that grows with model scale without\\nany ﬁne-tuning, and this gave rise to new resources\\nfor prompt engineering (Bach et al., 2022). In this\\npaper, we contrast the zero-shot, few-shot, and fully\\nsupervised ﬁne-tuning performance of dense and\\nMoE models. Finally, Schick and Schütze (2021b)\\nperform few-shot learning by few-shot ﬁne-tuning\\nusing pattern-exploiting training, whose efﬁciency\\ncan be improved by performing partial ﬁne-tuning\\nof a small number of additional task-speciﬁc pa-\\nrameters instead (Lester et al., 2021; Li and Liang,\\n2021; Houlsby et al., 2019).\\n2.4\\nLarge-scale training\\nMany of the models we consider in this work are\\ntoo big to be trained using standard data parallel\\ntechniques, since parameter storage would exceed\\nthe usable memory of a single GPU. We adopt\\nseveral techniques to make these models feasible\\nto train, including pure FP16 training, activation\\ncheckpointing and fully sharded data parallel train-\\ning. These techniques are described in more depth\\nin Appendix F.\\n3\\nExperimental Setup\\n3.1\\nModels\\nWe train autoregressive (decoder-only) transformer\\nmodels that roughly match the sizes and archi-\\ntecture explored in Brown et al. (2020). Model\\nsizes are summarized in Table 1.\\nWe use pre-\\nnormalization transformer blocks (Baevski and\\nAuli, 2019; Child et al., 2019) and GELU acti-\\nvations (Hendrycks and Gimpel, 2016). We differ\\nfrom Brown et al. (2020) in two ways: (1) we\\nuse only dense attention, while they alternate be-\\ntween dense and locally banded sparse attention;\\nand (2) we train our models with sinusoidal posi-\\ntional embeddings, following Shortformer (Press\\net al., 2020).2\\nWe also train MoE models that mirror our dense\\nmodel conﬁgurations (see the third set of columns\\nin Table 1), so that comparisons are approximately\\nmatched in terms of the number of ﬂoating point\\noperations (FLOPs). Our MoE models follow the\\ndesign proposed in Lepikhin et al. (2021) with al-\\nternating dense and expert layers and top-2 expert\\nselection. We use 512 experts in each expert layer\\n(E = 512). Each expert has a capacity of C·B\\nE\\ntokens, where C is a capacity factor that we set\\nto 2 and B is the total batch size in tokens. Ca-\\npacity refers to the maximum number of tokens\\nthat are routed to each expert. Once an expert is\\nat capacity for a given batch, additional tokens are\\nconsidered to be “overﬂowed\" with their represen-\\ntations passed-through via the residual connection.\\nFedus et al. (2021) report instability training\\nlarge MoE models and suggest rescaling the ini-\\ntial model weights, which we do not ﬁnd necessary.\\nWe instead observe that expert parameters have an\\nE-times smaller batch size relative to dense (data\\nparallel) parameters and accordingly rescale expert\\ngradients by a factor\\n1\\n√\\nE . This rescaling aligns with\\ntheory suggesting that an E-times increase in batch\\nsize should be accompanied by a\\n√\\nE increase in\\nlearning rate (Krizhevsky, 2014).\\nFollowing Brown et al. (2020), we train our mod-\\nels for 300B tokens3 with a context size (sequence\\n2Early experiments found this to produce comparable re-\\nsults with fewer learned parameters.\\n3While we control the total number of tokens to be the\\nlength) of 2048 tokens. The batch size and learning\\nrate are set according to the model size follow-\\ning Brown et al. (2020). We linearly warm-up the\\nlearning rate from 0 over the ﬁrst 375M tokens and\\nlinearly decay back to 0 over the remaining tokens.\\nWe use the Adam optimizer (Kingma and Ba, 2015)\\nwith β1 = 0.9, β2 = 0.98, ϵ = 10−8, weight decay\\nof 0.01 and dropout of 0.1.4\\nWe train our models in PyTorch (Paszke et al.,\\n2017) using FAIRSEQ (Ott et al., 2019).\\n3.2\\nPretraining data\\nWe pretrain our models on a union of six English-\\nlanguage datasets, including the ﬁve datasets used\\nto pretrain RoBERTa (Liu et al., 2019) and the\\nEnglish subset of CC100, totalling 112B tokens\\ncorresponding to 453GB:\\n• BookCorpus (Zhu et al., 2019) consists of more\\nthan 10K unpublished books (4GB);\\n• English Wikipedia, excluding lists, tables and\\nheaders (12GB);\\n• CC-News (Nagel, 2016) contains 63 millions En-\\nglish news articles crawled between September\\n2016 and February 2019 (76GB);\\n• OpenWebText (Gokaslan and Cohen, 2019), an\\nopen source recreation of the WebText dataset\\nused to train GPT-2 (38GB);\\n• CC-Stories (Trinh and Le, 2018) contains a sub-\\nset of CommonCrawl data ﬁltered to match the\\nstory-like style of Winograd schemas (31GB);\\n• English CC100 (Wenzek et al., 2020), a dataset\\nextracted from CommonCrawl snapshots be-\\ntween January 2018 and December 2018, ﬁltered\\nto match the style of Wikipedia (292GB).\\nWe encode our data using the same Byte-Pair En-\\ncoding (BPE) as GPT-2 (Radford et al., 2019) and\\nRoBERTa (Liu et al., 2019) with a vocabulary of\\n50K subword units.\\n3.3\\nEvaluation\\nWe evaluate models in terms of their in-domain and\\nout-of-domain perplexity, as well as downstream\\ntask performance.\\nsame as Brown et al. (2020), our pretraining data is not the\\nsame. See §3.2 for further details.\\n4We note that our 355M dense and 52B MoE models\\n(FLOPs-matched) were trained without dropout, which we\\nﬁnd slightly improves performance at smaller scale.\\n3.3.1\\nPerplexity Evaluation\\nWe ﬁrst evaluate our models on their ability to\\npredict the next token in a sequence as measured\\nby perplexity. Similar to training, we concatenate\\nall documents in a given dataset using empty lines\\nas separators, split the resulting sequence into non-\\noverlapping blocks of 2048 tokens, and score each\\nblock independently.5\\nWe evaluate and report perplexity in both in-\\ndomain and out-of-domain settings. In-domain,\\nwe sample a held-out subset of the combined pre-\\ntraining data (§3.2). For out-of-domain we use data\\nfrom The Pile (Gao et al., 2021), a public dataset\\nthat combines data from 22 diverse sources (e.g.,\\nArXiv, Github, OpenSubtitles, etc.). We report per-\\nplexities on the ofﬁcial test set of each individual\\nsubset, as well as the average across all subsets.\\n3.3.2\\nDownstream Evaluation\\nWe target models that can perform downstream\\ntasks well. Recent work shows that good perplexity\\nperformance does not always align with good per-\\nformance on downstream tasks (Tay et al., 2021).\\nHence, we evaluate our models accordingly.\\nBenchmarks.\\nWe evaluate our models on a sub-\\nset of the tasks considered in Brown et al. (2020).\\nAs GPT-3 performance varies greatly across tasks\\nand model sizes, we focus on tasks for which GPT-\\n3 either demonstrated consistent gains from scal-\\ning, or consistent gains going from zero-shot to\\nfew-shot settings.\\nFew-shot:\\nwe use WinoGrande (Sakaguchi\\net al., 2020), StoryCloze (Mostafazadeh et al.,\\n2016) and OpenBookQA (Mihaylov et al., 2018),\\nthe only non-generation tasks for which Brown\\net al. (2020) reported meaningful gains over zero-\\nshot at our scale.6 We exclude SuperGLUE, since\\nwe were not able to reproduce results reported in\\nBrown et al. (2020) using the public GPT-3 API.7\\n5One limitation of this approach is that the ﬁrst tokens in\\neach block have limited context, as they do not condition on\\ntokens from preceding blocks. Although more expensive, bet-\\nter results could be obtained using a sliding window approach.\\nNevertheless, this form of chunking the input is standard in\\nlanguage model evaluation.\\n6Deﬁned as an improvement of at least 2 accuracy points\\nover zero-shot learning and the majority class baseline for at\\nleast one GPT-3 model no bigger than 6.7B.\\n7Different from other tasks, we were not able to reproduce\\nGPT-3 results on SuperGLUE using the OpenAI API and our\\nevaluation protocol. The authors conﬁrmed that they used a\\ndifferent evaluation protocol for SuperGLUE through personal\\ncorrespondence.\\nZero-shot: in addition to the 3 few-shot tasks,\\nwe evaluate on ReCoRD (Zhang et al., 2018), Hel-\\nlaSwag (Zellers et al., 2019) and PIQA (Bisk et al.,\\n2020). Brown et al. (2020) reported strong results\\nand monotonic improvements from scaling on these\\ntasks.\\nEvaluation protocol.\\nFollowing Brown et al.\\n(2020), we report results on the development set for\\nall tasks except OpenBookQA and StoryCloze, for\\nwhich we use the test set. For few-shot learning, we\\nreport the average results across 25 runs, randomly\\nsampling a different set of few-shot examples from\\nthe training set each time.8 For priming, we fur-\\nther shufﬂe the few-shot examples for each test\\ninstance. Following Brown et al. (2020), we use\\nk=50 few-shot examples for WinoGrande, k=70 for\\nStoryCloze and k=100 for OpenBookQA. In cases\\nwhere this exceeds the maximum context length\\nfor the model, we truncate the prompt keeping the\\nmaximum number of full examples that ﬁt.\\nBaselines.\\nWe compare to the published GPT-\\n3 numbers (Brown et al., 2020) as our primary\\nbaseline. To validate our experimental framework,\\nwe also evaluate GPT-3 leveraging the OpenAI\\nAPI using our own evaluation code and settings.\\nUnfortunately, the correspondence between model\\nsizes and model names in the OpenAI API is not\\npublished. We follow other published work (Gao\\net al., 2021) and guess the correspondence based\\non our results from the public API as compared to\\nresults in Brown et al. (2020) (see §4.2.1).\\nMethods.\\nWe compare both priming and ﬁne-\\ntuning-based approaches.\\n• Priming: We use a language model to separately\\nscore each label choice using the same templates\\nas Brown et al. (2020), and pick the one with the\\nhighest score. For few-shot learning, we use a\\nsingle newline to separate examples. Our scoring\\nfunction follows the description in Brown et al.\\n(2020):\\n– For\\nWinoGrande,\\nwe\\ntake\\nthe\\nlog-\\nlikelihood of the common sufﬁx of the dif-\\nferent candidates.\\n– For OpenBookQA, we normalize by the\\nunconditional probability of each candi-\\ndate by taking\\np(completion|context)\\np(completion|answer_context),\\n8StoryCloze does not have a training set, so we follow\\nBrown et al. (2020) and sample few-shot examples from the\\ndevelopment set instead.\\nwhere we use the string “Answer: ” as an-\\nswer_context.\\n– For ReCoRD, we take the sum of per-token\\nlog-probabilities.9\\n– For all the other tasks, we take the average\\nof per-token log-probabilities, ignoring the\\ncommon preﬁx of the different candidates.\\n• Fine-tuning: Although supervised ﬁne-tuning\\nof pre-trained LMs on task speciﬁc training data,\\nD, requires updating and storage of all model\\nparameters per task, the process typically pro-\\nduces signiﬁcant task speciﬁc performance im-\\nprovements. We contrast the ﬁne-tuning perfor-\\nmance of sparse models and their dense coun-\\nterparts following (Radford et al., 2018), which\\napplies an additional task-speciﬁc linear layer\\nWy on the representation from the ﬁnal trans-\\nformer block for each input candidate separately,\\nfollowed by a softmax layer. We ﬁne-tune all\\nmodel parameters using the entire training set\\n(fully supervised learning). In addition to our\\nzero-shot tasks, we also evaluate on 3 widely-\\nused classiﬁcation tasks: BoolQ (Clark et al.,\\n2019), MNLI (Williams et al., 2018) and SST-\\n2 (Socher et al., 2013). More details are in Ap-\\npendix B.\\n3.3.3\\nMoE speedup factor\\nWe hypothesize that sparse models can achieve\\ncomparable performance at a smaller compute bud-\\nget. As such, it is informative to measure how much\\nmore efﬁcient MoEs are at achieving a speciﬁc\\nperformance level relative to dense models. We\\nestimate how many FLOPs c(t) the model needs\\nto achieve performance t in a particular task (as\\nmeasured by perplexity for language modeling and\\naccuracy for downstream tasks) using either an\\nMoE or a dense model. Given that we only have\\ndiscrete observations, we estimate exact missing\\nvalues by interpolating on a logarithmic scale as\\nfollows:\\nc(t) = exp (log clo(t) + r (log chi(t) −log clo(t)))\\nwhere r =\\nt−tlo\\nthi−tlo , tlo and thi are the closest perfor-\\nmance to t from the available models while being\\nlower and higher than t, respectively, and clo(t)\\n9This is different from Brown et al. (2020), who take the\\naverage per-token log-probability for this task. This worked\\nworse in our preliminary experiments.\\nTraining ZFLOPs\\nPerplexity\\n6.0\\n9.0\\n12.0\\n15.0\\n18.0\\n21.0\\n24.0\\n0.5\\n1.0\\n5.0\\n10.0\\nOurs (dense)\\nOurs (MoE)\\n(a) In-domain (validation)\\nTraining ZFLOPs\\nPerplexity\\n6.0\\n9.0\\n12.0\\n15.0\\n18.0\\n21.0\\n24.0\\n0.5\\n1.0\\n5.0\\n10.0\\nOurs (dense)\\nOurs (MoE)\\n(b) Out-of-domain (the Pile)\\nFigure 2: Language modeling perplexity. For the Pile, we report the average perplexity across the 22 subsets.\\nDense training ZFLOPs\\nMoE speedup factor\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n0.8\\n1\\n2\\n4\\n6\\n8\\n10\\n20\\nArXiv\\nCommonCrawl\\nDM Mathematics\\nOpenSubtitles\\n(a) Language modeling (the Pile)\\nDense training ZFLOPs\\nMoE speedup factor\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n0.8\\n1\\n2\\n4\\n6\\n8\\n10\\n20\\nReCoRD\\nHellaSwag\\nPIQA\\nWinogrande\\n(b) Zero-shot priming\\nFigure 3: Estimate of how much more efﬁcient MoEs are relative to dense models in representative datasets.\\nA speedup factor of y indicates that an MoE model can match the performance of the corresponding dense model\\nusing y times less compute. Refer to §3.3.3 for more details.\\nand chi are their corresponding training cost in\\nZFLOPs.\\nThe interpolation gives us matching perfor-\\nmance levels for dense and MoE models.\\nWe\\nuse them to compute the MoE speedup factor\\ncdense(t)/ cmoe(t). For example, if a dense model\\nrequiring 20 ZFLOPs achieves a performance of\\n90% on a given task and a MoE model requiring 5\\nZFLOPs achieves the same performance, then the\\nformula produces saving factor of 4. We visual-\\nize the savings curve using cdense(t) in the x axis,\\nwhich allows us to contrast speedup in different\\ntasks in a comparable scale.\\n4\\nResults and Analysis\\n4.1\\nLanguage modeling perplexity\\nWe report our perplexity results in Figure 2, and vi-\\nsualize the speedup curves in representative subsets\\nof the Pile (Gao et al., 2021) in Figure 3a. Refer to\\nAppendix A for full results for all the 22 subsets of\\nthe Pile.\\nWe observe that all MoE models outperform\\ntheir dense counterparts in all datasets, but their ad-\\nvantage greatly varies across domains and models.\\nMoEs are most efﬁcient when evaluated in-domain,\\nwhere they are able to match the performance of\\ndense models trained with 8-16x more compute\\n(see Figure 1). The improvement is more mod-\\nest in out-of-domain settings, bringing a speedup\\nof 2-4 on the Pile. This is reﬂected in Figure 2,\\nwhere the gap between the MoE and dense curves\\nis substantially smaller in out-of-domain settings.\\nMoreover, the advantage of MoEs over dense mod-\\nels decreases at scale: MoEs need ∼4 times less\\ncompute to match the performance of dense models\\ntrained with 2-6 ZFLOPs, but the speedup is ∼2\\nfor dense models trained with ∼30 ZFLOPs.\\nWe also observe large difference across the sub-\\nsets of the Pile, which correspond to different do-\\nRE\\nHS\\nPI\\nWG\\nSC\\nOB\\navg\\nGPT-3\\n(paper)\\n125M 70.8 33.7 64.6 52.0 63.3 35.6 53.3\\n355M 78.5 43.6 70.2 52.1 68.5 43.2 59.4\\n760M 82.1 51.0 72.9 57.4 72.4 45.2 63.5\\n1.3B 84.1 54.7 75.1 58.7 73.4 46.8 65.5\\n2.7B 86.2 62.8 75.6 62.3 77.2 53.0 69.5\\n6.7B 88.6 67.4 78.0 64.5 77.7 50.4 71.1\\n13B 89.0 70.9 78.5 67.9 79.5 55.6 73.6\\n175B 90.2 78.9 81.0 70.2 83.2 57.6 76.9\\nGPT-3\\n(API)\\nada 77.4 42.9 70.3 52.9 68.6 41.0 58.9\\nbabb. 83.1 55.1 74.5 59.4 73.3 45.6 65.2\\ncurie 87.1 67.8 77.1 64.3 77.7 50.8 70.8\\ndavi.\\n–\\n78.8 80.0 70.0 83.1 58.8\\n–\\nOurs\\n(dense)\\n125M 69.3 33.7 65.3 52.1 66.0 35.4 53.6\\n355M 78.1 46.2 70.6 54.2 71.0 42.0 60.4\\n1.3B 83.5 58.4 74.6 58.1 76.8 49.4 66.8\\n2.7B 85.8 65.9 76.6 61.4 78.2 49.6 69.6\\n6.7B 87.5 70.2 78.2 64.7 80.5 51.8 72.2\\n13B 88.5 73.7 79.0 67.6 80.9 55.4 74.2\\nOurs\\n(MoE)\\n15B 77.8 53.2 74.3 53.4 73.6 42.0 62.4\\n52B 83.4 64.9 76.8 57.4 75.9 51.0 68.2\\n207B 86.0 70.5 78.2 60.9 78.1 50.8 70.7\\n1.1T 88.0 78.6 80.3 66.4 81.8 55.2 75.0\\nTable 2: Zero-shot priming accuracy. GPT-3 (paper)\\nresults taken from Brown et al. (2020), all the other\\nresults were obtained by us as described in §3.3.2. RE:\\nReCoRD, HS: HellaSwag, PI: PIQA, WG: WinoGrande,\\nSC: StoryCloze, OB: OpenBookQA. We do not evaluate\\nthe largest GPT-3 model (davinci) on RE given the high\\nprice.\\nmains. As shown in Figure 3a, MoEs obtain the\\nlargest speedups in subsets that are closest to the\\ntraining corpus (e.g., CommonCrawl). The efﬁ-\\nciency gains are more moderate but still remark-\\nable for other domains like ArXiv and OpenSub-\\ntitles. Our largest MoE model barely outperforms\\nits dense counterpart on DM Mathematics (7.63 vs.\\n7.66 perplexity), which is arguably very different\\nfrom the training domain.\\n4.2\\nDownstream task evaluation\\n4.2.1\\nZero-shot learning\\nWe report zero-shot results in Table 2, and visualize\\nhow the different model families scale in Figure 4.\\nOur dense models perform at par with their GPT-\\n3 counterparts. This is consistent across differ-\\nent tasks, with our models doing marginally bet-\\nter on average. We are thus able to match Brown\\net al. (2020) despite some notable differences in\\nour setup (e.g., different training corpus), establish-\\ning a solid baseline to evaluate MoE models on\\ndownstream tasks. Similarly, when using our own\\ncode to evaluate the strongest GPT-3 API backend\\n(davinci), we obtain numbers that replicate those re-\\nTraining ZFLOPs\\nAccuracy\\n50.0\\n60.0\\n70.0\\n80.0\\n0.5\\n1.0\\n5.0\\n10.0\\n50.0\\n100.0\\nGPT-3 (paper)\\nOurs (dense)\\nOurs (MoE)\\nFigure 4:\\nZero-shot priming accuracy averaged\\nacross 6 tasks as a function of compute cost. Each\\npoint corresponds to a different, fully-trained model\\n(see Table 1). GPT-3 (paper) results taken from Brown\\net al. (2020).\\nported in the original paper for their largest model,\\nwhich reinforces that our evaluation settings are\\ncomparable to Brown et al. (2020).10\\nAs with language modeling, MoEs outperform\\ntheir dense counterparts for all datasets and model\\nsizes. But, once again, we ﬁnd the advantage nar-\\nrows at scale as illustrated in Figure 4. Similar\\nto the domain differences in language modeling,\\nwe observe differences across downstream tasks.\\nAs shown in Figure 3b, MoEs obtain signiﬁcant\\nspeedups in certain tasks like HellaSwag and PIQA,\\nbut this improvement is more modest in other tasks\\nsuch as ReCoRD and Winogrande.\\n4.2.2\\nFew-shot learning\\nWe report our few-shot results in Table 3 and plot\\nthe corresponding improvement over zero-shot in\\nFigure 5.\\nOur dense baselines perform at par or slightly\\nbetter than GPT-3. We observe that the improve-\\nment over zero-shot is bigger for larger models,\\nfurther supporting that certain capabilities in lan-\\nguage models emerge at scale (Brown et al., 2020).\\nFinally, we ﬁnd that our larger MoE models also\\nbeneﬁt from few-shot learning, outperforming their\\ndense counterparts in all conditions. However, the\\nimprovements going from zero-shot to few-shot\\nare smaller for MoE models compared to their\\ndense counterparts. For example, the average for\\nthe 6.7B dense model improves by 3.6 points to\\n69.3 going from zero-shot to few-shot, whereas the\\n10We assume that ada corresponds to the 355M model,\\nbabbage corresponds to the 1.3B model, and curie corresponds\\nto the 6.7B model based on the API evaluation results.\\nWG\\nSC\\nOB\\navg\\nGPT-3\\n(paper)\\n125M\\n51.3 –0.7\\n62.3 –1.0\\n37.0 +1.4\\n50.2 –0.1\\n355M\\n52.6 +0.5\\n70.2 +1.7\\n43.6 +0.4\\n55.5 +0.9\\n760M\\n57.5 +0.1\\n73.9 +1.5\\n48.0 +2.8\\n59.8 +1.5\\n1.3B\\n59.1 +0.4\\n76.1 +2.7\\n50.6 +3.8\\n61.9 +2.3\\n2.7B\\n62.6 +0.3\\n80.2 +3.0\\n55.6 +2.6\\n66.1 +2.0\\n6.7B\\n67.4 +2.9\\n81.2 +3.5\\n55.2 +4.8\\n67.9 +3.7\\n13B\\n70.0 +2.1\\n83.0 +3.5\\n60.8 +5.2\\n71.3 +3.6\\n175B\\n77.7 +7.5\\n87.7 +4.5\\n65.4 +7.8\\n76.9 +6.6\\nOurs\\n(dense)\\n125M\\n52.2 +0.1\\n64.7 –1.3\\n35.0 –0.4\\n50.7 –0.5\\n355M\\n53.7 –0.5\\n72.2 +1.1\\n42.0 +0.0\\n56.0 +0.2\\n1.3B\\n60.1 +2.0\\n78.6 +1.9\\n49.4 +0.0\\n62.7 +1.3\\n2.7B\\n63.9 +2.5\\n82.1 +3.8\\n53.2 +3.6\\n66.4 +3.3\\n6.7B\\n67.6 +2.9\\n83.2 +2.7\\n57.0 +5.2\\n69.3 +3.6\\n13B\\n71.0 +3.5\\n85.0 +4.1\\n59.5 +4.1\\n71.8 +3.9\\nOurs\\n(MoE)\\n15B\\n52.5 –0.9\\n71.4 –2.1\\n42.2 +0.2\\n55.4 –0.9\\n52B\\n58.1 +0.7\\n77.5 +1.6\\n48.9 –2.1\\n61.5 +0.1\\n207B\\n62.8 +1.9\\n81.1 +3.0\\n52.4 +1.6\\n65.4 +2.2\\n1.1T\\n68.6 +2.3\\n83.9 +2.1\\n57.7 +2.5\\n70.1 +2.3\\nTable 3: Few-shot priming accuracy and absolute\\nimprovement over zero-shot. GPT-3 (paper) results\\ntaken from Brown et al. (2020), all the other results\\nwere obtained by us as described in §3.3.2. WG: Wino-\\nGrande, SC: StoryCloze, OB: OpenBookQA.\\nTraining ZFLOPs\\nFew-shot accuracy improvement\\n-1.0\\n0.0\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\n7.0\\n0.5\\n1.0\\n5.0\\n10.0\\n50.0\\n100.0\\nGPT-3 (paper)\\nOurs (dense)\\nOurs (MoE)\\nFigure 5:\\nAbsolute accuracy improvement going\\nfrom zero-shot to few-shot, averaged across 3 tasks.\\nEach point corresponds to a different, fully-trained\\nmodel (see Table 1). GPT-3 (paper) results taken from\\nBrown et al. (2020).\\ncorresponding 1.1T model improves by 2.3 points\\nyielding 70.1.\\n4.2.3\\nSupervised Fine-Tuning\\nTable 4 contrasts full ﬁne-tuning performance of\\nMoE models with their dense counterparts on 8\\ndatasets, using zero-shot accuracy as a baseline for\\nreference. We did not ﬁne-tune the 6.7B and 13B\\ndense models and the 1.1T MoE models, owing\\nto their high resource needs. As expected, super-\\nvised ﬁne-tuning yields substantial performance\\nbeneﬁts for all dense models across all datasets,\\nOurs (Dense)\\nOurs (MoE)\\n125M 355M 1.3B 2.7B\\n15B\\n52B\\n207B\\nSC\\nzero-shot\\n66.0\\n71.0\\n76.8\\n78.2\\n73.6 75.9\\n78.1\\nﬁne-tune\\n87.8\\n89.5\\n93.8\\n97.0\\n80.3 84.9\\n80.9\\nOB\\nzero-shot\\n35.4\\n42.0\\n49.4\\n49.6\\n42.0 51.0\\n50.8\\nﬁne-tune\\n50.6\\n59.0\\n67.4\\n70.8\\n51.2 51.4\\n51.0\\nBQ\\nzero-shot\\n56.1\\n58.6\\n58.7\\n60.3\\n60.9 56.0\\n54.2\\nﬁne-tune\\n73.2\\n75.2\\n79.6\\n84.6\\n71.6 75.3\\n77.5\\nMN\\nzero-shot\\n46.2\\n52.1\\n55.3\\n56.0\\n49.3 52.1\\n52.6\\nﬁne-tune\\n80.9\\n84.3\\n84.1\\n88.9\\n77.7 81.2\\n78.7\\nSST-2 zero-shot\\n50.9\\n50.9\\n51.6\\n51.9\\n51.6 50.9\\n50.9\\nﬁne-tune\\n92.9\\n92.9\\n94.8\\n93.4\\n89.3 90.1\\n90.3\\nHS\\nzero-shot\\n33.7\\n46.2\\n58.4\\n65.9\\n53.2 64.9\\n70.5\\nﬁne-tune\\n50.7\\n64.8\\n74.1\\n90.0\\n37.3 45.4\\n42.2\\nPI\\nzero-shot\\n65.3\\n70.6\\n74.6\\n76.6\\n74.3 76.8\\n78.2\\nﬁne-tune\\n68.2\\n71.7\\n71.2\\n80.3\\n66.3 66.1\\n68.3\\nWG\\nzero-shot\\n52.1\\n54.2\\n58.1\\n61.4\\n53.4 57.4\\n60.9\\nﬁne-tune\\n65.7\\n63.3\\n67.4\\n69.5\\n50.2 56.0\\n50.4\\nTable 4: Fully supervised ﬁne-tuning accuracy com-\\npared with zero-shot accuracy. SC: StoryCloze, OB:\\nOpenBookQA, BQ: BoolQ, MN: MNLI, HS: HellaSwag,\\nPI: PIQA, WG: WinoGrande. Largest models omitted\\nowing to their high resource utilization.\\nover zero-shot performance. In contrast, although\\nﬁne-tuning of MoE models produces substantial\\nbeneﬁts for Storycloze, BoolQ, SST-2, MNLI and\\nsome improvements on OpenBookQA, it results\\nin worse performance for HellaSwag, PIQA, and\\nWinogrande. For the cases where we see improve-\\nments, the accuracy of ﬁne-tuned MoE models ap-\\nproach that of their corresponding dense models.\\nFor this comparison, we ﬁne-tune MoE models\\nexactly as we do the dense models. While MoE\\nmodels may beneﬁt from alternative ﬁne-tuning\\napproaches, for example, selective ﬁne-tuning of\\nthe expert or non-expert parameters, we leave such\\nexploration to future work.\\n5\\nConclusion\\nWe present results for scaling sparse Language\\nModels up to 1.1T parameters. We observe that up\\nto this scale sparse models offer better performance\\nvs. computation trade-off when compared to their\\ndense counterparts for language modeling, zero-\\nand few-shot learning. While the gap begins to\\nclose at scale our biggest sparse model outperforms\\nits dense counterpart where the latter requires twice\\nas much computation. These results conﬁrm that\\nsparse MoE models can provide an alternative to\\nwidely used dense architectures that saves compu-\\ntation and reduces model energy consumption.\\nEthical considerations\\nPrevious work (Sheng et al., 2019; Bordia and\\nBowman, 2019; Nadeem et al., 2021; de Vassi-\\nmon Manela et al., 2021) has observed that lan-\\nguage models absorb bias and toxicity represented\\nin the training data. So as to better understand\\nthe potential harms of our models in this front, we\\nevaluated them on StereoSet (Nadeem et al., 2021)\\nand CrowS-Pairs (Nangia et al., 2020), and report\\nour results in Appendix C. Our results show that\\nthe percentage of bias and stereotype in dense and\\nMoE models is comparable, especially at scale.\\nMoreover, in general, we note worse performance\\n(more bias/stereotyping) at larger scales. This ob-\\nservation points to more research needed in order\\nto mitigate such behavior. Intuitively however, we\\nbelieve that sparse models may be inherently more\\ncontrollable – e.g. designing speciﬁc experts – than\\ndense models. We leave this line of investigation\\nfor future research.\\nAnother concern of scaling language models is\\nthe energy usage and the associated environmen-\\ntal impact required for training, which we discuss\\nin detail in Appendix D. Nevertheless, our work\\nshows that MoEs can be a more compute-efﬁcient\\nalternative to traditional dense models, which could\\nalleviate the environmental impact of future scal-\\ning efforts. Moreover, by releasing all of our pre-\\ntrained language models, we believe we have alle-\\nviated some exploration burden for the community\\nand the environment, allowing for more efﬁcient\\noffsets for other researchers.\\nIn the spirit of transparency and allowing for\\nmaximal replicability and accountability, we in-\\nclude data and model cards together with our code.\\nLimitations\\nOur study is limited to one speciﬁc MoE conﬁgu-\\nration. In particular, our sparse and dense models\\nuse the same hyperparameters and model structure,\\nclosely following GPT-3. However, it is possible\\nthat this conﬁguration is suboptimal for scaling\\nMoE models. Similarly, we did not explore dif-\\nferent MoE-speciﬁc hyperparameters, such as the\\nnumber of experts. Finally, while our work reveals\\nthat the performance gap between MoE and dense\\nmodels varies greatly across tasks and domains,\\nthe speciﬁc factors that make certain tasks more\\nfavorable for MoEs remain unclear.\\nReferences\\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Al-\\nbert Webson, Colin Raffel, Nihal V Nayak, Ab-\\nheesht Sharma, Taewoon Kim, M Saiful Bari,\\nThibault Fevry, et al. 2022.\\nPromptsource: An\\nintegrated development environment and repository\\nfor natural language prompts.\\narXiv preprint\\narXiv:2202.01279.\\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\\nput representations for neural language modeling. In\\nInternational Conference on Learning Representa-\\ntions.\\nEmily M Bender, Timnit Gebru, Angelina McMillan-\\nMajor, and Shmargaret Shmitchell. 2021.\\nOn the\\ndangers of stochastic parrots: Can language models\\nbe too big? In Proceedings of the 2021 ACM Confer-\\nence on Fairness, Accountability, and Transparency,\\npages 610–623.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau,\\nand Doina Precup. 2015. Conditional computation\\nin neural networks for faster models. arXiv preprint\\narXiv:1511.06297.\\nYoshua\\nBengio,\\nNicholas\\nLéonard,\\nand\\nAaron\\nCourville. 2013.\\nEstimating or propagating gradi-\\nents through stochastic neurons for conditional com-\\nputation. arXiv preprint arXiv:1308.3432.\\nSara Bergman. 2021.\\nHow Can I Calculate CO2eq\\nemissions for my Azure VM?\\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\\nphysical commonsense in natural language.\\nPro-\\nceedings of the AAAI Conference on Artiﬁcial Intel-\\nligence, 34(05):7432–7439.\\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\\nRobert Sim, and Hanna Wallach. 2021. Stereotyp-\\ning norwegian salmon: an inventory of pitfalls in\\nfairness benchmark datasets. In Proceedings of the\\n59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Vol-\\nume 1: Long Papers), pages 1004–1015.\\nShikha Bordia and Samuel Bowman. 2019. Identify-\\ning and reducing gender bias in word-level language\\nmodels. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Student Research Work-\\nshop, pages 7–15.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah,\\nJared\\nD\\nKaplan,\\nPrafulla\\nDhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\\nVoss, Gretchen Krueger, Tom Henighan, Rewon\\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\\nClemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In\\nAdvances in Neural Information Processing Systems,\\nvolume 33, pages 1877–1901. Curran Associates,\\nInc.\\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\\nGuestrin. 2016. Training deep nets with sublinear\\nmemory cost. arXiv preprint arXiv:1604.06174.\\nRewon\\nChild,\\nScott\\nGray,\\nAlec\\nRadford,\\nand\\nIlya\\nSutskever.\\n2019.\\nGenerating\\nlong\\nse-\\nquences with sparse transformers.\\narXiv preprint\\narXiv:1904.10509.\\nKyunghyun Cho and Yoshua Bengio. 2014. Exponen-\\ntially increasing the capacity-to-computation ratio\\nfor conditional computation in deep learning. arXiv\\npreprint arXiv:1406.7362.\\nAidan Clark,\\nDiego de las Casas,\\nAurelia Guy,\\nArthur Mensch, Michela Paganini, Jordan Hoff-\\nmann, Bogdan Damoc, Blake Hechtman, Trevor\\nCai, Sebastian Borgeaud, George van den Driessche,\\nEliza Rutherford, Tom Hennigan, Matthew Johnson,\\nKatie Millican, Albin Cassirer, Chris Jones, Elena\\nBuchatskaya, David Budden, Laurent Sifre, Simon\\nOsindero, Oriol Vinyals, Jack Rae, Erich Elsen, Ko-\\nray Kavukcuoglu, and Karen Simonyan. 2022. Uni-\\nﬁed scaling laws for routed language models.\\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\\nTom Kwiatkowski, Michael Collins, and Kristina\\nToutanova. 2019. BoolQ: Exploring the surprising\\ndifﬁculty of natural yes/no questions. In Proceed-\\nings of the 2019 Conference of the North American\\nChapter of the Association for Computational Lin-\\nguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 2924–2936, Min-\\nneapolis, Minnesota. Association for Computational\\nLinguistics.\\nAndrew Davis and Itamar Arel. 2013. Low-rank ap-\\nproximations for conditional feedforward compu-\\ntation in deep neural networks.\\narXiv preprint\\narXiv:1312.4461.\\nDaniel de Vassimon Manela, David Errington, Thomas\\nFisher, Boris van Breugel, and Pasquale Minervini.\\n2021.\\nStereotype and skew: Quantifying gender\\nbias in pre-trained and ﬁne-tuned language models.\\nIn Proceedings of the 16th Conference of the Euro-\\npean Chapter of the Association for Computational\\nLinguistics: Main Volume, pages 2232–2242.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In North American Association for Com-\\nputational Linguistics (NAACL).\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne,\\nJong Wook Kim, Alec Radford, and Ilya Sutskever.\\n2020.\\nJukebox:\\nA generative model for music.\\narXiv preprint arXiv:2005.00341.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon\\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\\nQuoc V Le, Yonghui Wu, Zhifeng Chen, and Claire\\nCui. 2021. Glam: Efﬁcient scaling of language mod-\\nels with mixture-of-experts.\\nBradley Efron and Robert J Tibshirani. 1994. An intro-\\nduction to the bootstrap. CRC press.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\\nChaudhary, et al. 2021. Beyond english-centric mul-\\ntilingual machine translation. Journal of Machine\\nLearning Research, 22(107):1–48.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\\nSwitch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity.\\narXiv\\npreprint arXiv:2101.03961.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\\ning, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn\\nPresser, and Connor Leahy. 2021.\\nThe pile: An\\n800gb dataset of diverse text for language modeling.\\nCoRR, abs/2101.00027.\\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\\ntext corpus.\\nUdit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse,\\nHsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and\\nCarole-Jean Wu. 2021. Chasing carbon: The elu-\\nsive environmental footprint of computing. IEEE In-\\nternational Symposium on High-Performance Com-\\nputer Architecture (HPCA 2021).\\nDan Hendrycks and Kevin Gimpel. 2016.\\nGaus-\\nsian error linear units (gelus).\\narXiv preprint\\narXiv:1606.08415.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\\nDistilling the knowledge in a neural network. arXiv\\npreprint arXiv:1503.02531.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin De Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly.\\n2019. Parameter-efﬁcient transfer learning for nlp.\\nIn International Conference on Machine Learning,\\npages 2790–2799. PMLR.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji-\\nquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\\nGpipe: Efﬁcient training of giant neural networks\\nusing pipeline parallelism. Advances in neural in-\\nformation processing systems, 32:103–112.\\nDiederik Kingma and Jimmy Ba. 2015.\\nAdam: A\\nmethod for stochastic optimization. In International\\nConference on Learning Representations (ICLR).\\nAlex Krizhevsky. 2014. One weird trick for paralleliz-\\ning convolutional neural networks. arXiv preprint\\narXiv:1404.5997.\\nAlexandre\\nLacoste,\\nAlexandra\\nLuccioni,\\nVictor\\nSchmidt, and Thomas Dandres. 2019. Quantifying\\nthe carbon emissions of machine learning.\\narXiv\\npreprint arXiv:1910.09700.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020. Albert: A lite bert for self-supervised learning\\nof language representations. In International Con-\\nference on Learning Representations (ICLR).\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\\nGShard: Scaling giant models with conditional com-\\nputation and automatic sharding. In International\\nConference on Learning Representations (ICLR).\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\\nThe power of scale for parameter-efﬁcient prompt\\ntuning. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 3045–3059.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\\nGoyal, and Luke Zettlemoyer. 2021.\\nBase layers:\\nSimplifying training of large, sparse models.\\nIn\\nICML.\\nMike\\nLewis,\\nYinhan\\nLiu,\\nNaman\\nGoyal,\\nMar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\\nOptimizing continuous prompts for generation. In\\nProceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the\\n11th International Joint Conference on Natural Lan-\\nguage Processing (Volume 1: Long Papers), pages\\n4582–4597.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\\nGregory Diamos, Erich Elsen, David Garcia, Boris\\nGinsburg,\\nMichael Houston,\\nOleksii Kuchaiev,\\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\\nsion training. In International Conference on Learn-\\ning Representations.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018. Can a suit of armor conduct elec-\\ntricity? a new dataset for open book question an-\\nswering. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 2381–2391, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\\nPushmeet Kohli, and James Allen. 2016.\\nA cor-\\npus and cloze evaluation for deeper understanding of\\ncommonsense stories. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, pages 839–849, San Diego,\\nCalifornia. Association for Computational Linguis-\\ntics.\\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\\nStereoSet:\\nMeasuring stereotypical bias in pre-\\ntrained language models. In Association for Com-\\nputational Linguistics (ACL).\\nSebastian Nagel. 2016. Cc-news.\\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\\nSamuel R. Bowman. 2020.\\nCrowS-pairs: A chal-\\nlenge dataset for measuring social biases in masked\\nlanguage models. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 1953–1967, Online. As-\\nsociation for Computational Linguistics.\\nDeepak\\nNarayanan,\\nMohammad\\nShoeybi,\\nJared\\nCasper, Patrick LeGresley, Mostofa Patwary, Vi-\\njay Anand Korthikanti, Dmitri Vainbrand, Prethvi\\nKashinkunti, Julie Bernauer, Bryan Catanzaro, et al.\\n2021. Efﬁcient large-scale language model training\\non gpu clusters. arXiv preprint arXiv:2104.04473.\\nEric W Noreen. 1989. Computer-intensive methods for\\ntesting hypotheses. Wiley New York.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019.\\nFAIRSEQ: A fast, extensible\\ntoolkit for sequence modeling. In North American\\nAssociation for Computational Linguistics (NAACL):\\nSystem Demonstrations.\\nMyle Ott,\\nSergey Edunov,\\nDavid Grangier,\\nand\\nMichael Auli. 2018. Scaling neural machine trans-\\nlation. In Proceedings of the Third Conference on\\nMachine Translation (WMT).\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\\nChanan, Edward Yang, Zachary DeVito, Zeming\\nLin, Alban Desmaison, Luca Antiga, and Adam\\nLerer. 2017. Automatic differentiation in PyTorch.\\nIn NIPS Autodiff Workshop.\\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\\nbon emissions and large neural network training.\\narXiv preprint arXiv:2104.10350.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word repre-\\nsentations. In North American Association for Com-\\nputational Linguistics (NAACL).\\nOﬁr Press, Noah A Smith, and Mike Lewis. 2020.\\nShortformer:\\nBetter language modeling using\\nshorter inputs. arXiv preprint arXiv:2012.15832.\\nAlec Radford, Karthik Narasimhan, Time Salimans,\\nand Ilya Sutskever. 2018. Improving language un-\\nderstanding with unsupervised learning. Technical\\nreport, OpenAI.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. Techni-\\ncal report, OpenAI.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\\nMillican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susan-\\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\\ncob Menick, Albin Cassirer, Richard Powell, George\\nvan den Driessche, Lisa Anne Hendricks, Mari-\\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\\nJonathan Uesato, John Mellor, Irina Higgins, An-\\ntonia Creswell, Nat McAleese, Amy Wu, Erich\\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\\nDavid Budden, Esme Sutherland, Karen Simonyan,\\nMichela Paganini, Laurent Sifre, Lena Martens,\\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\\nmatzadeh, Elena Gribovskaya, Domenic Donato,\\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\\nDiego de Las Casas, Aurelia Guy, Chris Jones,\\nJames Bradbury, Matthew Johnson, Blake Hecht-\\nman, Laura Weidinger, Iason Gabriel, William Isaac,\\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\\nway, Lorrayne Bennett, Demis Hassabis, Koray\\nKavukcuoglu, and Geoffrey Irving. 2022.\\nScal-\\ning language models: Methods, analysis & insights\\nfrom training gopher.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. The Journal of Machine Learning Research\\n(JMLR), 21:1–67.\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Min-\\njia Zhang, Reza Yazdani Aminabadi, Ammar Ah-\\nmad Awan, Jeff Rasley, and Yuxiong He. 2022.\\nDeepspeed-moe: Advancing mixture-of-experts in-\\nference and training to power next-generation ai\\nscale.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\\nand Yuxiong He. 2020.\\nZero: Memory optimiza-\\ntions toward training trillion parameter models. In\\nSC20: International Conference for High Perfor-\\nmance Computing, Networking, Storage and Anal-\\nysis, pages 1–16. IEEE.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\\nand Jason Weston. 2021.\\nHash layers for large\\nsparse models. arXiv preprint arXiv:2106.04426.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\\nvatula, and Yejin Choi. 2020. Winogrande: An ad-\\nversarial winograd schema challenge at scale. Pro-\\nceedings of the AAAI Conference on Artiﬁcial Intel-\\nligence, 34(05):8732–8740.\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\\nThomas Wolf. 2020. Distilbert, a distilled version of\\nbert: smaller, faster, cheaper and lighter.\\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\\ncloze-questions for few-shot text classiﬁcation and\\nnatural language inference. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 255–269, Online. Association for Com-\\nputational Linguistics.\\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\\nsize that matters: Small language models are also\\nfew-shot learners. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, pages 2339–2352.\\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\\nOren Etzioni. 2019.\\nGreen ai.\\narXiv preprint\\narXiv:1907.10597.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\\nDean. 2017.\\nOutrageously large neural networks:\\nThe sparsely-gated mixture-of-experts layer. arXiv\\npreprint arXiv:1701.06538.\\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\\nand Nanyun Peng. 2019. The woman worked as a\\nbabysitter: On biases in language generation. In Pro-\\nceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th In-\\nternational Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP).\\nSam Shleifer and Alexander M. Rush. 2020.\\nPre-\\ntrained summarization distillation.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\\nPatrick LeGresley, Jared Casper, and Bryan Catan-\\nzaro. 2019. Megatron-lm: Training multi-billion pa-\\nrameter language models using model parallelism.\\narXiv preprint arXiv:1909.08053.\\nShaden Smith, Mostofa Patwary, Brandon Norick,\\nPatrick LeGresley, Samyam Rajbhandari, Jared\\nCasper, Zhun Liu, Shrimai Prabhumoye, George\\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\\nSong, Mohammad Shoeybi, Yuxiong He, Michael\\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\\n2022.\\nUsing deepspeed and megatron to train\\nmegatron-turing nlg 530b, a large-scale generative\\nlanguage model.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013.\\nRecursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank.\\nIn Proceedings of the 2013 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 1631–1642, Seattle, Washington, USA. Asso-\\nciation for Computational Linguistics.\\nEmma Strubell, Ananya Ganesh, and Andrew Mc-\\nCallum. 2019.\\nEnergy and policy considera-\\ntions for deep learning in nlp.\\narXiv preprint\\narXiv:1906.02243.\\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fe-\\ndus, Samira Abnar, Hyung Won Chung, Sharan\\nNarang, Dani Yogatama, Ashish Vaswani, and Don-\\nald Metzler. 2021. Scale efﬁciently: Insights from\\npre-training and ﬁne-tuning transformers.\\narXiv\\npreprint arXiv:2109.10686.\\nTrieu H Trinh and Quoc V Le. 2018.\\nA simple\\nmethod for commonsense reasoning. arXiv preprint\\narXiv:1806.02847.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems.\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\\nmand Joulin, and Edouard Grave. 2020.\\nCCNet:\\nExtracting high quality monolingual datasets from\\nweb crawl data.\\nIn Proceedings of the 12th Lan-\\nguage Resources and Evaluation Conference, pages\\n4003–4012, Marseille, France. European Language\\nResources Association.\\nAdina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In Proceed-\\nings of the 2018 Conference of the North American\\nChapter of the Association for Computational Lin-\\nguistics: Human Language Technologies, Volume\\n1 (Long Papers), pages 1112–1122, New Orleans,\\nLouisiana. Association for Computational Linguis-\\ntics.\\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\\nria Chang, Fiona Aga Behram, James Huang,\\nCharles Bai, et al. 2021. Sustainable ai: Environ-\\nmental implications, challenges and opportunities.\\narXiv preprint arXiv:2111.00364.\\nYuanzhong Xu,\\nHyoukJoong Lee,\\nDehao Chen,\\nHongjun Choi, Blake Hechtman, and Shibo Wang.\\n2020. Automatic cross-replica sharding of weight\\nupdate in data-parallel training.\\narXiv preprint\\narXiv:2004.13336.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\\nFarhadi, and Yejin Choi. 2019.\\nHellaSwag: Can\\na machine really ﬁnish your sentence?\\nIn Pro-\\nceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 4791–\\n4800, Florence, Italy. Association for Computational\\nLinguistics.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\\nReCoRD: Bridging the gap between human and ma-\\nchine commonsense reading comprehension. arXiv\\npreprint 1810.12885.\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2019. Aligning books and movies:\\nTowards story-like visual explanations by watch-\\ning movies and reading books.\\narXiv preprint\\narXiv:1506.06724.\\nA\\nFull perplexity results\\nTable 5 reports the full perplexity results, including\\nall the different subsets of the Pile.\\nB\\nFine-tuning Settings\\nWe run ﬁne-tuning for a ﬁxed number of epochs\\n(100 for BoolQ, OpenBookQA, StoryCloze, and\\nPIQA, 25 for HellaSwag, Winogrande, and SST-2,\\n6 for MNLI) and perform model selection based\\non validation set accuracy. For datasets either with-\\nout a validation set or where we evaluate on the\\nvalidation set, we randomly split the training set\\nand use 80% for ﬁne-tuning and 20% for per-epoch\\nvalidation.\\nC\\nUnderstanding Potential Harms\\nPrevious work (Sheng et al., 2019; Bordia and\\nBowman, 2019; Nadeem et al., 2021; de Vassi-\\nmon Manela et al., 2021) has observed that lan-\\nguage models absorb bias and toxicity represented\\nin the training data. We set out to explore if sparse\\nmodels would behave differently than dense mod-\\nels in this arena. To that end, we evaluate our\\ndense and MoE models on two popular bench-\\nmarks: StereoSet (Nadeem et al., 2021) and CrowS-\\nPairs (Nangia et al., 2020). StereoSet measures bias\\nacross four domains: profession, gender, religion,\\nand race. CrowS-Pairs dataset covers nine bias\\ntypes: race, gender/gender identity, sexual orienta-\\ntion, religion, age, nationality, disability, physical\\nappearance, and socioeconomic status.11\\nStereotypical bias as a function of scale.\\nTa-\\nble 6 presents the results on StereoSet benchmark\\nusing three metrics: (1) Language Modeling Score\\n(LMS): deﬁned as the percentage of instances in\\nwhich a language model prefers meaningful over\\nmeaningless associations (higher LMS is better);\\n(2) Stereotype Score (SS): deﬁned as the percentage\\nof instances where a model prefers a stereotypical\\nassociation over an anti-stereotypical association\\n(SS score close to 50 is better, while a more biased\\nmodel would have a higher score towards 100%);\\n(3) Idealized CAT Score (ICAT): deﬁned as a com-\\nbination of LMS and SS to capture both in a single\\nmetric: LMS ∗min(SS,100−SS)\\n50\\n(higher ICAT is bet-\\nter).\\n11The two benchmarks have limitations such as lack of clear\\narticulations of how certain biases are being measured (Blod-\\ngett et al., 2021). Results should be interpreted accordingly.\\nTable 7 presents the performance of our models\\non CrowS-Pairs using the Stereotype Score (SS)\\nmetric. Similar to StereoSet, we observe that both\\ndense and MoE models get worse with scale, again\\nwith statistically signiﬁcant (p < 0.05) difference\\nbetween best and worst scores based on bootstrap\\ntest (Noreen, 1989; Efron and Tibshirani, 1994).\\nStereotypical bias in dense vs MoE models.\\nWe observe that as the model size increases, both\\ndense and MoE models get worse ICAT scores\\nin general – they become more biased with a sta-\\ntistically signiﬁcant difference between best and\\nworst scores. On the StereoSet benchmark, cor-\\nresponding dense and sparse models (comparable\\nFLOPs) yield comparable performance. On the\\nCrowS-Pairs MoE models perform slightly better\\n(less biased) than dense models on average but the\\ndifference is not statistically signiﬁcant (see Table 6\\nand Table 7).\\nD\\nCO2 Emission Related to Experiments\\nThe carbon emissions of the experiments reported\\nin this work are dominated by the largest models, in\\nparticular the 13B parameter dense and 1.1T param-\\neter MoE models. We trained our largest models on\\nAzure NDv4 instances12 with A100 GPUs (TDP of\\n400W) in the West US 2 region, which has a car-\\nbon efﬁciency of 0.3 kgCO2e/kWh and assumed\\nPower Usage Effectiveness (PUE) of 1.125.13 Thus\\neach GPU-day of training is responsible for 3.24\\nkgCO2e of emissions, of which 100 percent is di-\\nrectly offset by the cloud provider.\\nIn Table 8 we report training time and energy\\nusage for our models based on the above estimates,\\nas well as estimates from Patterson et al. (2021) for\\nother large-scale LMs, in particular GShard (Lep-\\nikhin et al., 2021), Switch Transformer (Fedus\\net al., 2021) and GPT-3 (Brown et al., 2020). Train-\\ning times (GPU days) are computed assuming a\\nthroughput of 160 TFLOP/s and 115 TFLOP/s per\\nA100 GPU for our dense and MoE models, respec-\\ntively, based on observed training speeds for our\\nlargest models.14\\n12Some of our smaller models were trained on an on-\\npremises cluster, but our estimates assume that all training\\nwas done on Azure for simplicity.\\n13Carbon efﬁciency is estimated using the Machine Learn-\\ning Impact calculator (Lacoste et al., 2019) and the PUE esti-\\nmate is based on Bergman (2021).\\n14MoE models have additional all-to-all communication\\noverhead, causing them to achieve lower GPU utilization com-\\npared to dense models. This overhead could be reduced with\\nfurther optimization of the implementation.\\nDense\\nMoE\\n125M 355M\\n1.3B\\n2.7B\\n6.7B\\n13B\\n15B\\n52B\\n207B\\n1.1T\\nIn-domain\\nValidation\\n20.65\\n15.14 12.48 10.92\\n9.82\\n8.97\\n12.58\\n9.58\\n8.76\\n6.80\\nOut-of-domain\\n—\\n(The Pile)\\nArXiv\\n15.74\\n11.42\\n9.00\\n8.03\\n7.29\\n6.86\\n10.81\\n8.79\\n7.72\\n6.91\\nBibliotik\\n26.78\\n19.62 15.75 13.96 12.81 11.96\\n17.80 14.64 13.29 11.88\\nBookCorpus\\n23.60\\n17.91 14.83 13.36 12.38 11.70\\n16.54 13.90 12.82 11.57\\nCommonCrawl\\n22.49\\n16.92 13.91 12.47 11.50 10.80\\n15.17 12.47 11.43 10.02\\nDM_Mathematics\\n12.29\\n9.73\\n8.51\\n8.10\\n7.66\\n7.41\\n10.51\\n8.82\\n8.28\\n7.63\\nEnron_Emails\\n19.98\\n15.71 12.32 11.40 10.78 10.09\\n14.24 12.18 11.08 10.45\\nEuroParl\\n27.16\\n15.80 12.02\\n9.91\\n8.63\\n7.68\\n12.58\\n9.47\\n8.41\\n6.92\\nFreeLaw\\n16.78\\n11.98\\n9.44\\n8.33\\n7.58\\n7.08\\n10.54\\n8.49\\n7.68\\n6.84\\nGithub\\n8.92\\n6.55\\n5.13\\n4.61\\n4.30\\n4.03\\n6.11\\n4.93\\n4.41\\n3.99\\nGutenberg_PG-19\\n29.15\\n20.70 16.39 14.37 13.03 12.08\\n18.85 14.94 13.48 11.90\\nHackerNews\\n29.37\\n22.53 18.11 16.20 14.96 14.08\\n20.72 17.15 15.67 14.21\\nNIH_ExPorter\\n26.78\\n19.18 15.28 13.55 12.40 11.64\\n17.08 14.00 12.66 11.42\\nOpenSubtitles\\n20.72\\n16.87 14.16 13.05 12.28 11.61\\n16.38 13.64 12.64 11.78\\nOpenWebText2\\n20.56\\n14.64 11.88 10.47\\n9.51\\n8.79\\n12.76 10.03\\n9.03\\n7.06\\nPhilPapers\\n27.60\\n20.00 15.99 14.07 12.90 12.03\\n18.77 15.19 13.56 11.91\\nPubMed_Abstracts\\n24.16\\n16.56 12.95 11.38 10.35\\n9.65\\n14.46 11.71 10.49\\n9.36\\nPubMed_Central\\n12.19\\n9.35\\n7.65\\n6.89\\n6.36\\n6.03\\n8.74\\n7.32\\n6.60\\n6.02\\nStackExchange\\n17.76\\n12.46\\n9.65\\n8.43\\n7.58\\n7.04\\n10.99\\n8.74\\n7.73\\n6.72\\nUSPTO\\n17.15\\n12.85 10.43\\n9.36\\n8.67\\n8.18\\n12.00\\n9.95\\n9.01\\n8.14\\nUbuntu_IRC\\n28.40\\n21.47 16.16 13.92 12.50 11.48\\n17.80 14.79 12.85 11.49\\nWikipedia_en\\n20.51\\n14.61 11.59 10.22\\n9.22\\n8.56\\n12.35\\n9.68\\n8.60\\n6.65\\nYoutubeSubtitles\\n19.01\\n13.51 10.80\\n9.31\\n8.38\\n7.70\\n12.23\\n9.88\\n8.80\\n7.51\\nAverage\\n21.23\\n15.47 12.36 10.97 10.05\\n9.39\\n13.97 11.40 10.28\\n9.11\\nTable 5: Full perplexity results.\\nOurs (Dense)\\nOurs (MoE)\\nCategory\\n125M 355M 1.3B 2.7B 6.7B 15B 52B 207B 1.1T\\nProf.\\nLMS 80.8\\n82.0\\n81.9 80.8 79.3 79.7 80.5 81.1 78.0\\nSS 48.2\\n49.8\\n51.3 53.6 54.2 52.2 54.9 54.8 54.4\\nICAT 77.9\\n81.7\\n79.8 75.1 72.6 76.2 72.6 73.4 71.1\\nGender\\nLMS 83.3\\n82.4\\n83.9 83.1 82.2 81.4 82.9 82.2 80.2\\nSS 59.9\\n59.1\\n59.1 60.7 60.3 58.7 58.3 58.7 61.2\\nICAT 66.7\\n67.4\\n68.6 65.2 65.2 67.3 69.2 68.0 62.3\\nReli.\\nLMS 85.9\\n87.8\\n87.2 87.2 85.3 87.8 85.9 83.3 81.4\\nSS 50.0\\n46.2\\n50.0 55.1 52.6 50.0 48.7 51.3 51.3\\nICAT 85.9\\n81.1\\n87.2 78.2 80.9 87.8 83.7 81.2 79.3\\nRace\\nLMS 82.3\\n82.3\\n83.8 83.0 83.1 83.7 83.4 82.0 82.1\\nSS 42.9\\n45.7\\n48.3 49.8 50.3 47.1 47.3 49.7 47.5\\nICAT 70.7\\n75.2\\n80.8 82.7 82.6 78.9 79.0 81.5 78.0\\nOverall\\nLMS 82.0\\n82.4\\n83.2 82.3 81.6 82.1 82.3 81.7 80.2\\nSS 47.2\\n48.8\\n50.7 52.7 53.0 50.5 51.6 52.8 51.9\\nICAT 77.4\\n80.5\\n82.0 77.9 76.6 81.2 79.7 77.2 77.2\\nTable 6: Stereotypical bias comparison on the inter-\\nsentence task of StereoSet. Note that for LMS and\\nICAT higher is better whereas for SS closer to 50 is\\nbetter. All scores are macro averages of all samples\\npresent in a category. Overall represents all the samples\\nin this dataset.\\nWe note that these estimates do not account for\\nthe costs associated with manufacturing the infras-\\ntructure to train such models, which can be sig-\\nniﬁcant (Gupta et al., 2021; Wu et al., 2021). We\\nalso note that these estimates do not account for\\npilot experiments common in the early exploratory\\nstages of a research project. We estimate that pi-\\nOurs (Dense)\\nOurs (MoE)\\nCategory\\n125M 355M 1.3B 2.7B 6.7B 15B 52B 207B 1.1T\\nGender\\n56.5\\n58.4\\n56.5 58.0 60.3 56.9 63.4 62.2 60.7\\nReligion\\n63.8\\n67.6\\n68.6 70.5 73.3 64.8 68.6 70.5 73.3\\nRace/Color\\n61.1\\n58.7\\n63.8 62.0 67.6 57.4 59.7 60.5 61.8\\nSexual orientation\\n78.6\\n78.6\\n82.1 76.2 79.8 73.8 78.6 78.6 76.2\\nAge\\n57.5\\n60.9\\n60.9 62.1 62.1 66.7 59.8 66.7 66.7\\nNationality\\n46.5\\n47.2\\n61.0 54.7 59.1 52.2 56.6 61.6 57.2\\nDisability\\n66.7\\n70.0\\n75.0 71.7 70.0 75.0 73.3 75.0 76.7\\nPhysical appearance\\n73.0\\n65.1\\n69.8 71.4 74.6 71.4 71.4 74.6 79.4\\nSocioeconomic status 69.2\\n66.9\\n72.7 68.0 71.5 69.8 69.8 72.1 73.3\\nOverall\\n61.3\\n60.9\\n65.1 63.4 67.0 61.4 63.9 65.5 65.7\\nTable 7: Stereotypical bias comparison on CrowS-\\nPairs. Scores closer to 50 are better. All scores are\\nmacro averages.\\nlot experimentation adds a factor of 2 to the total\\ntraining cost, since most exploration and tuning is\\nperformed at small scale where compute costs are\\nsmall, and the largest models are typically trained\\nat most once or twice. For instance, we trained and\\ndiscarded a pilot 6.7B dense and 1.1T MoE model\\nin the early stages of this project, but trained the\\n13B dense model once.\\nE\\nKnowledge Distillation\\nIn Section 3.3.3 we show that sparse (MoE) models\\nare signiﬁcantly more efﬁcient to train than dense\\nmodels. However, inference for large sparse mod-\\nGPU days\\ntCO2e\\n125M dense\\n26\\n0.1\\n355M dense\\n77\\n0.2\\n1.3B dense\\n258\\n0.8\\n2.7B dense\\n512\\n1.7\\n6.7B dense\\n1238\\n4.0\\n13B dense\\n2363\\n7.7\\n15B MoE\\n43\\n0.1\\n52B MoE\\n131\\n0.4\\n207B MoE\\n456\\n1.5\\n1.1T MoE\\n2241\\n7.3\\nTotal\\n7345\\n23.8\\nGShard 600B MoE\\n4.8\\nSwitch Transformer 1.5T MoE\\n72.2\\nGPT-3 175B\\n552.1\\nTable 8: Estimated training time and energy costs to\\ntrain the models reported in this paper, based on the\\nnumber of A100 GPU-days required for training. See\\nAppendix D for more details about these estimates.\\nTeacher size\\nCost\\nPPL\\nNone (Baseline)\\n0.36\\n16.01\\nMoE 15B\\n0.43\\n15.20\\n355M Dense\\n1.06\\n14.88\\n1.3B Dense\\n3.57\\n14.67\\nMoE 52B\\n1.30\\n14.64\\n2.7B Dense\\n7.08\\n14.62\\nTable 9: Distillation Results. Cost refers to the cost\\nto train the teacher model (see Table 1). PPL is the\\nin-domain validation perplexity.\\nels can be challenging, since the large number of\\nparameters (most of which are inactive) introduce\\nsigniﬁcant storage costs compared to dense models.\\nIn this section we explore whether it is possible\\nto blend the beneﬁts of dense and sparse models\\nvia knowledge distillation (Hinton et al., 2015).\\nBuilding on recent work in this area (Shleifer and\\nRush, 2020; Sanh et al., 2020; Fedus et al., 2021),\\nwe train small dense “student” models to mimic the\\nbehavior of larger “teacher” models, which may be\\neither large dense or large sparse (MoE) models.\\nMethods\\nWe train dense student models with 12\\nlayers and hidden dimension 768, matching the\\n125M dense model architecture in Table 1. We\\nuse a weighted training objective that combines the\\nstandard cross entropy loss (25% weight) with a\\nsoft distillation loss (75% weight) that encourages\\nthe student model to reproduce the logits of the\\nteacher. Additionally, we use a reduced sequence\\nlength of 1024 tokens to speed up experimentation.\\nResults\\nWe report results in Table 9. We ﬁnd that\\nstudent models trained with knowledge distillation\\nimprove over a well tuned dense baseline for both\\ndense and sparse teacher models. Furthermore,\\nsome of the efﬁciency advantages of sparse train-\\ning can be transmitted to a dense student through\\ndistillation. For example, student models distilled\\nfrom a 52B parameter MoE teacher outperform stu-\\ndent models distilled from a 1.3B parameter dense\\nteacher, despite that the dense teacher model is\\ntwice as costly to train.\\nF\\nTechniques for Large-scale Training\\nWe adopt several techniques to train models in this\\nwork, including a more memory-efﬁcient recipe for\\nFP16 training, activation checkpointing and Fully\\nSharded Data Parallel.\\nFP16 Training:\\nTypical mixed-precision train-\\ning recipes require storing model weights in both\\n16-bit (FP16) and 32-bit (FP32), as well as stor-\\ning optimizer state in 32-bit to preserve accurate\\nweight updates (Micikevicius et al., 2018; Ott et al.,\\n2018). Thus training a model in mixed precision\\nwith Adam requires 16 bytes of memory per pa-\\nrameter to maintain 16-bit and 32-bit weights (6\\nbytes), Adam optimizer state (8 bytes) and gradi-\\nents (2 bytes), not including any memory required\\nfor activations.\\nIn practice we ﬁnd we can reduce memory re-\\nquirements by 50% by maintaining only 16-bit\\nmodel weights, optimizer state and gradients with\\nno loss in model accuracy. First, we simply discard\\nthe 32-bit model weights, saving 4 bytes per param-\\neter, since pilot experiments showed this to have no\\nimpact on model quality when training with large\\nbatch sizes. Second, the Adam optimizer state can\\nbe stored in 16-bit by dynamically rescaling the\\nvalues to avoid underﬂow. Speciﬁcally, we com-\\npute the standard Adam weight update in 32-bit and\\nthen apply the following transformation at the end\\nof each optimization step to maintain the optimizer\\nstate in 16-bit (Dhariwal et al., 2020):\\n⃗\\nmfp16 =\\n⃗\\nm\\nmax abs(⃗\\nm)\\nFLOAT16_MAX + ϵ\\nwhere ϵ = 10−8 and FLOAT16_MAX = 65504.0 is\\nthe largest ﬁnite value expressible in FP16. We\\napply this transformation separately for the ﬁrst\\nand second moment estimates in Adam.\\nActivation\\nCheckpointing:\\nActivation\\nsize\\ngrows proportionally to the model and batch\\nsize, making it infeasible to store activations\\nfor transformer models with more than a couple\\nof billion parameters.\\nWe adopt a popular\\ntechnique called activation checkpointing, which\\nsaves memory during training by discarding a\\nsubset of activations in the forward pass and\\nrecomputing them in the backward pass (Chen\\net al., 2016).\\nThis technique results in a 33%\\nincrease in computation, but can often reduce\\nactivation memory requirements by a factor of\\n10 (Rajbhandari et al., 2020). In our experiments\\nwe only store activations between transformer\\nlayers and recompute intermediate activations\\nwithin each layer during the backward pass.\\nFully Sharded Data Parallel:\\nIn data parallel\\ntraining, gradients are averaged across multiple\\nworkers (GPUs) that process distinct partitions of\\nthe data. Standard implementations maintain re-\\ndundant copies of the model weights and optimizer\\nstate on each GPU, however this wastes GPU mem-\\nory and makes it challenging to scale the model size\\nbeyond what can ﬁt on a single GPU. Recent work\\nhas explored sharding model parameters, optimizer\\nstate and gradients across workers (Xu et al., 2020;\\nRajbhandari et al., 2020), enabling training of mod-\\nels with more than one trillion parameters using\\nonly data parallelism without the added complexity\\nintroduced by model parallel training approaches\\nlike pipeline or tensor parallelism (Huang et al.,\\n2019; Shoeybi et al., 2019; Narayanan et al., 2021).\\nWe implement these ideas in Fully Sharded Data\\nParallel (FSDP),15 which shards model parameters\\nin-place and gathers the parameters on all workers\\njust-in-time for the forward and backward pass.\\nTraining with FSDP is typically faster than standard\\ndata parallel implementations for three reasons: (1)\\nsharding reduces the cost of the optimizer step and\\nweight update by distributing it across workers,\\nrather than redundantly updating model replicas\\non each worker; (2) while FSDP introduces 50%\\nmore communication, this extra communication\\nis overlapped with the computation in the forward\\n15Fully Sharded Data Parallel is a drop-in replacement for\\nPyTorch’s Distributed Data Parallel module and is available at\\ngithub.com/facebookresearch/fairscale.\\nand backward pass; and (3) FSDP yields signiﬁcant\\nmemory savings, which can be used to increase the\\nbatch size and achieve higher GPU utilization.\\nOne important decision when using FSDP is\\nchoosing which submodules in the model to “wrap\"\\nwith FSDP. If the wrapping is too ﬁne-grained, then\\nthe parameter shards will be very small which re-\\nduces communication efﬁciency. If the wrapping\\nis too coarse, then this increases the peak resident\\nmemory and may pose challenges when scaling\\nto larger model sizes. In this work we wrap ev-\\nery transformer layer with FSDP, which ensures a\\nreasonably large message size for communication\\nwhile still limiting the peak resident memory to the\\nsize of a single layer.\\nG\\nCounting FLOPs\\nWe count the number of ﬂoating-point operations\\n(FLOPs) analytically following Narayanan et al.\\n(2021). We assume that all models are trained with\\nactivation checkpointing and thus have an addi-\\ntional forward pass before the backward pass. Thus\\nthe total training FLOPs for our dense models is\\ngiven by:\\nFdense = 96Tlh2\\n\\x12\\n1 + s\\n6h +\\nV\\n16lh\\n\\x13\\n,\\nwhere T is the total number of training tokens, l\\nis the number of layers, h is the hidden dimension,\\ns is the sequence length and V is the vocabulary\\nsize. In this work, T = 300e9, s = 2048 and\\nV = 51200 for all models.\\nFor mixture of expert models, we account for\\nan additional feed-forward network at every other\\nlayer for the top-2 routing in GShard (Lepikhin\\net al., 2021), and ignore the FLOPs of the rout-\\ning projection which is negligible. The resulting\\ntraining FLOPs for our MoE models is given by:\\nFMoE = Fdense + 32Tlh2.\\nNotably, this quantity is independent of the number\\nof experts.\\n', 'source_name': 'Efficient Large Scale Language Modeling with Mixtures of Experts', 'source_url': 'https://arxiv.org/abs/2112.10684'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Efficient_Large_Scale_LM_NOTES.pdf #12\n",
      "{'content': 'Efficient Large Scale Language Modeling with Mixtures of Experts \\nMain Idea: this paper has the goal of comparing how the traditional MoE architecture from \\n“Sparsely-Gated MoE”, using top-2 routing, scales in relation to dense models.  \\nModel sizes trained for this experiment range from (in total number of parameters): \\n- \\n125M to 13B (in a dense setting). \\n- \\n15B to 1.1T (in a MoE setting). \\nThe maximum number of experts used was 512, and the capacity factor used for MoE models \\nwas 2 (to support top-2 routing). \\nDense and sparse models were compared on a FLOPs-matching basis (models with the same \\nFLOPs are comparable). The dense baseline used was GPT-3. \\nEvaluations done: \\n- \\nPerplexity (from next-token predictions). \\n- \\nPerformance on downstream tasks (benchmarks, zero-shot, few-shot). \\n- \\nMoE speedup factor – how much more efficient MoEs are at achieving a specific \\nperformance level relative to dense models (how many training FLOPs are needed to \\nreach a certain performance goal). \\nResults: \\n- \\nMoE outperforms dense in all evaluation datasets, although at a different scale depending \\non the dataset’s domain and model size. \\n- \\nMoEs are the most efficient (highest speedup factor in in-domain tasks), reaching an 8x \\nto 16x speedup (8x-16x less compute needed for the same performance) \\no This speedup decreases to a 2x-4x speedup in out-of-domain tasks. \\n- \\nThe speedup advantages of MoE decrease at scale, especially in in-domain tasks. \\n- \\nThe closer the data used for evaluation is to the training corpus, the larger the speedup \\nobtained by MoE. \\n- \\nOn downstream zero-shot task evaluation, MoE also outperforms the dense model (which \\nperforms on par with GPT-3), but this gain is, again, diminishing at scale. \\n- \\nIn a few-shot setting, MoE still outperforms dense, but the MoE improvements over zero-\\nshot are smaller than dense. This indicates that although MoE still outperforms dense in \\na few-shot setting, dense models benefit more from few-shot examples. \\n- \\nIn terms of fine-tuning, dense models (as expected) always incur substantial gains. \\nAlthough this is true in some cases for MoE, fine-tuning MoE models on some \\ndomains/datasets leads to worse performance. More research is needed to determine \\nwhy. Perhaps this comes from fine-tuning MoEs not being deeply explored yet, with an \\nalternative approach needed to obtain good results (the same setting as dense was used \\nfor fine-tuning after all). \\n \\nMy takeaways: \\n- \\nThe results from this paper’s experiments show that the traditional MoE architecture \\ndoes indeed provide speedups over a dense setting. The results from the speedup \\nprovided by MoE are bigger the closer the evaluation domains are from the training \\ndomains. This seems to indicate that the biggest gains from MoE come from \\nmemorization. Generalization gains provided by MoE over dense are not as apparent, \\nalthough there still are gains (MoE still provides a speedup when evaluated in out-of-\\ndomain tasks). \\no The diminishing gains from MoE at scale are more apparent in out-of-domain \\ntasks, as they stay relatively constant when training domains (or close to) are used \\nfor evaluation. \\n- \\nIt is interesting to note that few-shot has a bigger effect on dense performance than on \\nMoE performance (dense benefits more), although MoE outperforms dense in this \\nscenario. \\n- \\nA previous work, ST-MoE, concludes that sparse models benefit from smaller batch sizes \\nand larger learning rates during fine-tuning, while the opposite is observed for dense \\nmodels. ST-MoE also concludes that MoEs are significantly more prone to overfitting \\nduring fine-tuning compared to dense. The fine-tuning results from this paper can be \\nreplicated and analyzed with these two aspects in mind as future research. \\n \\n', 'source_name': 'Efficient Large Scale Language Modeling with Mixtures of Experts NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Efficient_Large_Scale_LM_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BTM.pdf #13\n",
      "{'content': 'Branch-Train-Merge: Embarrassingly Parallel\\nTraining of Expert Language Models\\nMargaret Li∗†⋄\\nSuchin Gururangan∗†⋄\\nTim Dettmers†\\nMike Lewis⋄\\nTim Althoff†\\nNoah A. Smith†♠\\nLuke Zettlemoyer†⋄\\n†Paul G. Allen School of Computer Science & Engineering, University of Washington\\n♠Allen Institute for AI\\n⋄Meta AI\\nAbstract\\nWe present Branch-Train-Merge (BTM), a communication-efﬁcient algorithm for\\nembarrassingly parallel training of large language models (LLMs). We show it\\nis possible to independently train subparts of a new class of LLMs on different\\nsubsets of the data, eliminating the massive multi-node synchronization currently\\nrequired to train LLMs. BTM learns a set of independent EXPERT LMs (ELMs),\\neach specialized to a different textual domain, such as scientiﬁc or legal text. These\\nELMs can be added and removed to update data coverage, ensembled to generalize\\nto new domains, or averaged to collapse back to a single LM for efﬁcient inference.\\nNew ELMs are learned by branching from (mixtures of) ELMs in the current\\nset, further training the parameters on data for the new domain, and then merging\\nthe resulting model back into the set for future use. Experiments show that BTM\\nimproves in- and out-of-domain perplexities as compared to GPT-style Transformer\\nLMs, when controlling for training cost. Through extensive analysis, we show that\\nthese results are robust to different ELM initialization schemes, but require expert\\ndomain specialization; LM ensembles with random data splits do not perform well.\\nWe also present a study of scaling BTM into a new corpus of 64 domains (192B\\nwhitespace-separated tokens in total); the resulting LM (22.4B total parameters)\\nperforms as well as a Transformer LM trained with 2.5× more compute. These\\ngains grow with the number of domains, suggesting more aggressive parallelism\\ncould be used to efﬁciently train larger models in future work.\\n1\\nIntroduction\\nTraining and inference in large language models (LMs) typically require access to supercomputers\\nthat can achieve the massive multi-node (e.g., GPU or TPU) synchronization required to compute\\nmodel activations and gradients (Brown et al., 2020; Zhang et al., 2022; Fedus et al., 2022; Lepikhin\\net al., 2021). In this paper, we develop a new class of large LMs that is instead embarrassingly\\nparallel: different parts of the model are independently trained on different subsets of the data, with\\nno need for multi-node training or inference (Figure 2).\\nOur new ELMFOREST1 models consist of a set of EXPERT LMs (ELMs), each specialized to a\\ndistinct domain in the training corpus, e.g., scientiﬁc or legal text. The ELMs are each independently\\nfunctional LMs with no shared parameters, unlike previous domain mixture-of-experts models that\\n∗These authors contributed equally to this work. Correspondence to {margsli, sg01}@cs.washington.edu.\\n1Expert Language Models For Efﬁcient Sparse Training\\narXiv:2208.03306v1  [cs.CL]  5 Aug 2022\\n0\\n1K\\n2K\\n3K\\n4K\\n5K\\n6K\\n7K\\n8K\\nTraining cost (GPU Hours)\\n13\\n15\\n17\\n19\\n21\\nPerplexity\\n1B\\n125M\\n125M\\n1B\\n2.8B\\n350M\\n2.8B\\n350M\\n760M\\n6B\\n760M\\n6B\\n10.4B\\n1.3B\\n1.3B\\n10.4B\\n8 Training Domains\\nRandom Ensemble\\nTransformer-LM\\nELMforest (Parameter Average)\\nELMforest (Ensemble)\\n0\\n1K\\n2K\\n3K\\n4K\\n5K\\n6K\\n7K\\n8K\\nTraining cost (GPU Hours)\\n16\\n18\\n20\\n22\\n24\\n26\\nPerplexity\\n1B\\n125M\\n125M\\n1B\\n2.8B\\n350M\\n2.8B\\n350M\\n6B\\n760M\\n6B\\n760M\\n10.4B\\n1.3B\\n1.3B\\n10.4B\\n8 Evaluation Domains\\nRandom Ensemble\\nTransformer-LM\\nELMforest (Parameter Average)\\nELMforest (Ensemble)\\nFigure 1: ELMFORESTs outperform compute-matched TRANSFORMER-LM and random-\\nensemble baselines across parameter scales (§4.2). ELMFOREST parameter averaging closely\\napproaches the performance of ELMFOREST ensembling at no additional cost to inference compared\\nto the TRANSFORMER-LM. ELMFOREST efﬁciency also grows as the model size increases due to\\nsubstantially reduced GPU communication overhead relative to TRANSFORMER-LM baselines.\\nonly specialize the Transformer feedforward layers (Gururangan et al., 2022). ELMs can be added\\nand removed to the model at any time to update data coverage, ensembled to generalize to new\\ndomains, or parameter averaged to collapse back to a single LM for more efﬁcient inference.\\nWe present the Branch-Train-Merge (BTM) algorithm for learning this set of specialized LMs. Our\\nprocedure repeatedly expands the ELMFOREST by adding one or more new ELMs completely in\\nparallel. Each new ELM in the ELMFOREST is ﬁrst branched by initializing a new LM with an\\naverage of the parameters of the most relevant LMs in the current set, then further trained on new\\ndomain data with a standard cross-entropy loss, and ﬁnally merged into the model by simply adding\\nit to the current ELMFOREST (Figure 3 provides a schematic of this process). BTM is initialized\\nwith a single LM that is trained on heterogeneous data to establish strong shared representations for\\nfuture domain specialization, a process that we explore extensively in our ablation analysis.\\nWhen evaluated in- and out-of-domain, ELMFORESTs trained with BTM outperform monolithic GPT-\\nstyle transformer LMs (GPT-LMs) and a previous domain-specialized mixture-of-experts baseline\\n(DEMIX; Gururangan et al. 2022) across a range of computational budgets – up to 1.3B parameters\\nper ELM trained for 7000 GPU-hours in aggregate (Figure 1; §4.2). These gains are biggest for\\nELMFOREST ensembles, which use all of the model parameters, but also hold when we collapse the\\nmodels by averaging parameters.\\nWe also perform detailed analysis to understand which aspects of BTM are most important for these\\ngains. Ensembled ELMFORESTs outperform ensembling across random data splits, suggesting that\\ndomain specialization is a critical component to our approach (§5.1). We also show that performance\\nis robust to a range of initializations, including the choice of the compute budget allocation (§5.2)\\nand data (§5.3) for training the initial LM. Our ELMFORESTs are also able to forget domains by\\nremoving the relevant ELM, as long as they were not included in the initialization phase (§5.3).\\nFinally, we perform a preliminary scaling study on a training corpus with 192B whitespace-separated\\ntokens (§6.3). Building on our ﬁndings, we use BTM to incrementally train a total of 64 experts\\nwhich form a ELMFOREST. Our scaled ELMFOREST performs comparably with a 1.3B parameter\\nTRANSFORMER-LM trained with 2.5 times the total GPU hours. We ﬁnd that beneﬁts of BTM\\nincrease with the number of domains in the training corpus.\\nThese results provide compelling evidence for the promise of scaling large language models with\\nmany smaller, independently trained ELMs. We envision that this work lays the foundation for\\ndemocratized model development at inclusive compute budgets — that groups with different resource\\nconstraints and research interests may combine efforts to build open-sourced, community-authored\\nlarge language models, comprised of continually-evolving repositories of EXPERT LMs.\\n2\\nFigure 2: Fully Synchronized vs. Embarrassingly Parallel Training (§3). (a) In the fully synchro-\\nnized data-parallel training of a TRANSFORMER-LM, all parameters are synchronized across all\\nGPUs. In large LMs, this synchronization incurs hefty cross-node communication costs. (b) In\\nembarrassingly parallel training (our work), individual models are trained on each domain, with no\\nparameter synchronization between those models, which eliminates cross-node communication costs\\nbetween models.\\nWe release our code publicly.2\\nOur contributions in this work are, concisely:\\n• Branch-Train-Merge (BTM; §3), consisting of an initial shared training phase, followed\\nby parallelized training of many ELMs each specializing to one data domain. Inference\\nwith an ELMFOREST, either through ensembling or parameter averaging to produce a single\\nLM, outperforms compute-matched baselines at a variety of parameter scales (§4).\\n• Analysis (§5). We demonstrate that the improved performance of ELMFORESTs is not\\nmerely due to ensembling more parameters, and then study the performance tradeoffs\\nof various training and inference decisions. ELMFORESTs trained with BTM robustly\\noutperform compute- and parameter-matched approaches across many settings.\\n• Incremental BTM training on 64 textual domains (§6). We demonstrate increasing\\nbeneﬁts of BTM training over a large baseline as we scale number of domains, up to at least\\n64 domains.\\n2\\nELMFORESTs\\nELMFORESTs are designed to be embarrassingly parallel (Figure 2) and rapidly customizable by\\ncompletely isolating the inﬂuence of distinct data splits to different LMs, as deﬁned in this section.\\n2.1\\nModel deﬁnition\\nWe deﬁne an ELMFOREST to be a set of EXPERT LMs (ELMs), each independently trained to\\nspecialize to a different subset of a corpus. ELMs are inspired by the experts in earlier MoE models\\n(Jacobs et al., 1991), but we speciﬁcally deﬁne ours to be domain specialists and specialize the\\nfull LM instead of just model components. We follow Gururangan et al. 2022 and deﬁne domains\\nby provenance, or the source of the document (e.g., whether it is a legal document or computer\\nscience research paper), which yields simple and interpretable corpus segmentations, and is useful for\\nidentifying ELMs in our experiments.3 These can potentially be extended to multi-lingual, -modal,\\n-task, or other types of data splits, but we leave these directions for future work. ELMs remain\\nindependent at all stages of training and inference, enabling the functions described below.\\n2.2\\nAdding and removing ELMs\\nWe can modify the domain coverage of an ELMFOREST at any time by incorporating new ELMs\\nspecialized to different domains or removing existing ELMs in the set. These mechanisms stand in\\ncontrast to existing control techniques that have been introduced to steer LMs towards (Keskar et al.,\\n2019; Gururangan et al., 2020; Dathathri et al., 2020) or away (Welleck et al., 2019) from certain\\nbehaviors. Existing techniques tend to be expensive, require retraining the model with different\\n2https://www.github.com/hadasah/btm\\n3See §8 for a discussion on the possible limitations of this domain deﬁnition.\\n3\\nobjectives, or do not provide strong guarantees on how the LM may behave at test-time (Gehman\\net al., 2020). In contrast, ELMFORESTs allow for explicit inference-time application of constraints\\non the provenance of training data; they store all information from a domain in the associated ELM,\\nwhich is fully independent of all others. Removing any expert, even after fully training the model,\\nguarantees the associated data will be fully ablated and never inﬂuence future model predictions.\\n2.3\\nEnsembling the ELMFOREST\\nELMFORESTs support two inference modes, which trade off model efﬁciency and size for perfor-\\nmance. In this ﬁrst method, we ensemble the output probabilities of multiple ELMs. This allows us\\nto generalize to new text of unknown domain provenance. We use the cached prior method proposed\\nin Gururangan et al. (2022), summarized below.\\nConsider the probabilistic view of language modeling, where we estimate p(Xt | x<t). We introduce\\na domain variable, D, alongside each sequence. Then the next-step conditional distribution on the\\nhistory x<t is:\\np(Xt | x<t)=\\nn\\nX\\nj=1\\np(Xt | x<t, D = j) · p(D = j | x<t)\\n(1)\\nWe estimate a domain posterior, or a probability of a sequence belonging to each of the k domains\\nusing Bayes’ rule:\\np(D = j | x<t)= p(x<t | D = j) · p(D = j)\\np(x<t)\\n=\\np(x<t | D = j) · p(D = j)\\nPk\\nj′=1 p(x<t | D = j′) · p(D = j′)\\n(2)\\nELMs are used to compute the likelihood over contexts given a domain label. To compute the\\ncached prior, we maintain an exponential moving average of posterior probabilities over domains,\\nupdated only at the end of each sequence block: p(D = j) = PN\\ni=1 λi · p(D = j | x(i)\\n<T ). Following\\nGururangan et al. 2022, we use N = 100 sequences (of length T = 1024 each) of development data,\\nand set EMA decay λ = 0.3. We ﬁx this prior at test time for each domain.\\nThis inference procedure naively requires a forward pass through all ELMs in the ELMFOREST, but\\nwe observe in practice that the domain posterior is sparse, even as the number of ELMs increases,\\nwhich suggests that top-k selection of EXPERT LMs can reduce inference time costs with negligible\\neffects on performance. We quantify this sparsity and the effectiveness of using only the top-k experts\\nin our scaled-up experiments (§6.5).\\n2.4\\nAveraging ELM parameters\\nAs an alternative to ensembling, we can also use parameter averaging (Izmailov et al., 2018; Wortsman\\net al., 2022a; Matena and Raffel, 2021) to collapse the ELMFOREST into a single LM. This operation\\nkeeps inference cost constant regardless of how many ELMs are added to the set. When scaling\\nthe ELMFOREST to many domains, we also use ELMFOREST parameter averaging to initialize\\nnew experts, as we see in the next section. We experiment with several techniques to construct the\\naverage for a target domain in §4.4, including a uniform average and using the best performing expert.\\nWe ﬁnd that using the cached prior from §2.3 to deﬁne a weighted average LM is the strongest\\nmethod. ELMFOREST weighted parameter averages outperform TRANSFORMER-LM baselines at all\\nmodel sizes studied, and continue to outperform TRANSFORMER-LMs and approach ELMFOREST\\nensembling performance as we increase the number of domains (§6.5).\\n3\\nBRANCH-TRAIN-MERGE (BTM)\\nBRANCH-TRAIN-MERGE training of ELMFOREST models is incremental and embarrassingly\\nparallel (Figure 2). EXPERT LMs are trained completely independently in batches, starting from an\\nLM trained on a shared, heterogeneous corpus. Figure 3 summarizes the process, which we detail in\\nthis section.\\n4\\nFigure 3: BTM training process overview (§3). Our BTM method is initialized with a seed training\\nphase (Step 0), in which a single LM is trained on a corpus. The resulting parameters are branched,\\nor copied, k times (Step 1) and we continue training each LM on a unique assigned data domain,\\nresulting in k ELMs (Step 2). Trained ELMs are merged into the ELMFOREST (Step 3), and the\\nprocess can be repeated from Step 1, initializing new ELMs with parameter averages of existing\\nELMs. After the seed phase (Step 0), ELMs are fully disconnected, with no communication between\\nthem. This process is shown at k = 4 for simplicity, but our experiments in §4-5 use k = 8 domains\\nand train for one iteration through BTM. Our experimental setup for §6, illustrated in Figure 6, trains\\non k = 64 domains over 4 iterations of BTM.\\n3.1\\nThe Branch-Train-Merge Iteration\\nEach BRANCH-TRAIN-MERGE iteration begins with an existing ELMFOREST E = {θi}k\\ni=1. Each\\nELM θi represents a corresponding domain di in the dataset of k domains DE = {d1, . . . , dk}\\ncurrently modeled by E. In this section, we ﬁrst describe the inductive case of k > 0 and then\\ndescribe how to train the initial model θ0.\\nStep 1 (Branch): Sprouting a new ELM\\nThe new ELM parameters are a function of the\\ncurrent expert set E.\\nGiven some vector of weights w = {w1, w2, ..., wk} over the exist-\\ning experts θ1, θ2, ..., θk, we can initialize the new expert with the weighted parameter average\\nθk+1 ←Pk\\ni=0 wiθi. We experiment with a few ways to set w, including initializing from only the\\nnearest ELM or – our best performing approach – a parameter average of ELMs according to their\\ndomain posterior on the new data dk+1 (§4.4).\\nStep 2 (Train): Growing the ELM\\nWe train the new ELM θk+1 on data domain dk+1 with the\\nlog likelihood objective. None of the existing ELMs in E are involved in the training of the new\\nELM. We also refer to this step as branched training later, to distinguish it from other training\\nregimens we compare to.\\nStep 3 (Merge): Transplanting the ELM\\nWe merge the new ELM θk+1, which now represents\\nthe domain dk+1, into E to create an updated set: E′ = E ∪{θk+1} and DE′ = DE ∪{dk+1}.\\nThese operations are described for a single ELM, but can be parallelized to add multiple ELMs in a\\nbatch or iterated to add them asynchronously.\\n3.2\\nStep 0 (Initialization): Seeding the ELMFOREST\\nOn the ﬁrst iteration of BTM, E = ∅; we have no ELMs in the set to branch from. Instead of\\ninitializing the ﬁrst ELMs of the set randomly, we hypothesize that ELM performance is boosted by\\nbranching from pretrained LM parameters, since multi-phase adaptive pretraining is an effective way\\nto develop domain-speciﬁc language models (Gururangan et al., 2020), and parameter interpolation\\ntechniques work best with models that have a shared initialization (Izmailov et al., 2018; Frankle\\n5\\net al., 2020; Wortsman et al., 2022b; Matena and Raffel, 2021; Wortsman et al., 2022a) . Speciﬁcally,\\nwe perform a seed phase, training a seed LM θseed on some data corpus dseed, which can be used to\\ninitialize the ﬁrst batch of ELMs in the set. In §4.4, we ﬁnd that the seed phase is important for\\nenabling ELMFOREST parameter averaging (§2.3). Ensembling ELMFORESTs trained with BTM\\nresults in comparable performance across a wide range of seed LM training settings (§5).\\n3.3\\nScale: Expanding the ELMFOREST\\nELMFORESTs can be scaled incrementally by repeating the steps in §3.1 on batches of new domains.\\nIncremental training allows new ELMs to be initialized with parameter averages of existing LMs\\nwhen branching. We hypothesize that this allows us to amortize the compute necessary to train new\\nELMs as BTM proceeds; subsequent ELMs can be trained for shorter periods of time by leveraging\\nthe pretraining of existing ELMs in the set.\\n4\\nCore Experiments and Results\\nWe ﬁrst compare BTM training to compute-matched baselines, to carefully measure the efﬁciency\\ngains. We use the simplest form of BTM training on a set of k = 8 domains, consisting of one\\niteration through the Branch-Train-Merge cycle. We compare inference through ensembling and\\ndifferent parameter averaging methods.\\n4.1\\nExperimental Setup\\nData\\nFollowing prior work, we use the data introduced by Gururangan et al. (2022), which consists\\nof 8 training and 8 evaluation (primarily English-language) domains.4 These 16 domains cover a\\ndiverse span, from Web text and U.S court opinions for training to GitHub and COVID-19 research\\npapers for evaluation. Details are listed in Appendix Table 9.\\nModel hyperparameters\\nThe model architecture is a randomly-initialized LM with the GPT-3\\n(Brown et al., 2020) architecture implemented in Fairseq (Ott et al., 2019) at the following model sizes,\\nspeciﬁed by parameter count: 125M (small), 350M (medium), 750M (large), 1.3B (xl). Following\\nBrown et al. 2020, we use the GPT-2 (Radford et al., 2019) vocabulary of 50,264 BPE types, and train\\nwith 1,024-token sequences, across document boundaries. We prepend a beginning-of-document\\ntoken to each document.\\nCompared Models\\n• TRANSFORMER-LM: The ﬁrst baseline is a standard Transformer LM, implemented\\nwith distributed data parallelism (Li, 2021). This is identical to the DENSE model from\\nGururangan et al. (2022), in which data from each domain is balanced.5\\n• DEMIX: We follow the training procedure outlined in Gururangan et al. (2022), where\\nfeedforward layers in the Transformer are trained to specialize as domain experts, but all\\nother parameters are synchronized, as in the TRANSFORMER-LM. Gururangan et al. (2022)\\ndemonstrated that DEMIX LMs exhibit better domain specialization and generalization than\\nother sparsely activated (e.g., MoE) models.\\n• ELMFOREST: We ﬁrst conduct a seed phase to initialize the ensemble with LM parameters\\n(§3.2), then conduct branched training on the ELMs (§3.1), all of which are initialized with\\nthe seed LM. Between the seed and branched phases, we continue training from the saved\\noptimizer state.6 We ensemble the outputs of experts for all results with this model, using\\nthe procedure outlined in §2.3.\\nThese models are compute-matched, since computation is typically the limiting factor in model\\ntraining. Like other sparse models (Fedus et al., 2022; Lepikhin et al., 2021; Gururangan et al., 2022),\\n4We refer to the novel domains from Gururangan et al. 2022 as evaluation domains in this paper.\\n5We ﬁnd, in line with Gururangan et al. (2022), that balancing data domains achieves better performance\\nthan without data balancing. Results comparing these two settings are in Appendix Table 13.\\n6We found in earlier experiments that resetting or reusing the optimizer state has little effect on performance;\\nwe reuse the optimizer state to ﬁx the learning rate schedule across all experiments.\\n6\\n125M\\nT-LM\\nDEMIX\\nELMFOREST\\n125M\\n512M\\n1B\\nTrain\\n19.90.23\\n18.20.82\\n17.20.02\\nEval\\n25.20.18\\n23.40.54\\n22.40.12\\nAll\\n22.50.14\\n20.80.63\\n19.80.05\\n350M\\nT-LM\\nDEMIX\\nELMFOREST\\n350M\\n1.8B\\n2.8B\\nTrain\\n16.3\\n15.0\\n14.7\\nEval\\n20.8\\n19.9\\n18.6\\nAll\\n18.5\\n17.5\\n16.7\\n750M\\nT-LM\\nDEMIX\\nELMFOREST\\n750M\\n3.8B\\n6B\\nTrain\\n14.7\\n13.5\\n13.4\\nEval\\n19.3\\n17.7\\n16.7\\nAll\\n17.0\\n15.6\\n15.0\\n1.3B\\nT-LM\\nDEMIX\\nELMFOREST\\n1.3B\\n7B\\n10.4B\\nTrain\\n14.2\\n13.7\\n13.0\\nEval\\n18.4\\n17.6\\n16.3\\nAll\\n16.3\\n15.6\\n14.6\\nTable 1: ELMFORESTs trained with BTM outperform all baselines across multiple model\\nscales (§4.2). Average test-set perplexity (↓) for each model scale (125M, 350M, 750M, 1.3B\\nparameters) across the 8 training, 8 evaluation, and all 16 domains described in §4.1. Total parameters\\nare shown for each model type at each scale. At 125M parameter per GPU scale, we show the mean\\nand standard deviation of results over 8 random seeds. For BTM, we show results with 50% of\\ncompute dedicated to the seed phase. DEMIX outperforms TRANSFORMER-LM, abbreviated as\\nT-LM. ELMFORESTs trained with BTM consistently achieve the lowest average test perplexity.\\nELMFORESTs decouple compute and parameters; we can train many more parameters at the same\\ncomputational cost as the equivalent TRANSFORMER-LM. Total parameter counts are in Table 1.\\nTraining settings\\nTo disentangle variations in GPU speed, we use number of updates as our\\ncomputational budget in these experiments. We choose the number of updates in our budget so that\\ntraining completes in roughly 48 wall-clock hours: 80k, 32k, 24k, and 12k updates for the 125M,\\n350M, 750, 1.3B parameter per GPU models, respectively. We additionally report the average updates\\nper second of each model, and present performance as a function of GPU hours, to illustrate efﬁciency\\nimprovements. We use 16, 32, 64, and 128 GPUs in parallel for the 125M, 350M, 750M, 1.3B\\nparameter TRANSFORMER-LM and DEMIX baselines, respectively. We also use these GPU budgets\\nfor the seed phase in the ELMFOREST. For branched training, we divide these GPU budgets equally\\namong the ELMs; for example, the 1.3B parameter per GPU ELMFOREST uses 16 GPUs for each\\nof the 8 ELMs. For all models, we ﬁx the learning rate at 0.0005 with a polynomial (linear) decay\\nlearning rate schedule and 8% warmup, which we found to be optimal for most settings after a large\\ngrid search. For all experiments, we use a batch size of 16 for each GPU, with gradient accumulation\\nof 32 steps, and train with fp16. We train on NVIDIA V100 32GB GPUs.\\n4.2\\nPerformance Comparisons\\nOur results are shown in Table 1. Atbvfvhufckrjihhubfggkt these model scales, ELMFORESTs trained\\nwith the BTM procedure outperform both the sparsely trained DEMIX LM and the densely trained\\nTRANSFORMER-LM baselines. The improvements in performance we observe over DEMIX layers\\nsuggest that complete isolation of all LM parameters results in better specialization of domain experts.\\nAt the 125M parameter scale, we report the mean over 8 random seeds, as well as the standard\\ndeviation.\\n4.3\\nEfﬁciency Comparisons\\nTraining ELMFORESTs requires less inter-GPU communication than TRANSFORMER-LM or DEMIX\\nmodels, since no synchronization occurs between GPUs assigned to different ELMs. This results in\\nhigher updates per second and therefore shorter train times, as shown in Table 2. Additionally, the\\nembarrassingly parallel, fully disconnected nature of branched training provides ﬂexibility in resource\\nconsumption; GPUs dedicated to different ELMs may be online at different times, and ELMs may\\neven be trained serially on the same GPUs. Speciﬁcally, none of our branched training required more\\nthan 2 nodes of 8 GPUs simultaneously, so that we were able to conduct some BTM experiments on\\n7\\nAverage updates per second, normalized (↑)\\nfully synchronized\\npartially synchronized\\nBTM: embarrassingly parallel\\n(TRANSFORMER-LM)\\n(DEMIX)\\n(branched ELMs)\\n125M\\n1.00\\n1.01\\n1.05\\n350M\\n1.00\\n1.11\\n1.23\\n750M\\n1.00\\n1.01\\n1.27\\n1.3B\\n1.00\\n0.97\\n1.33\\nTable 2: BTM training is more efﬁcient (§4.3). Average updates per second (↑) for each setup\\nand model size, normalized by the average updates per second during fully synchronized training of\\nthe TRANSFORMER-LM. The embarrassingly parallel training used during the branched phase of\\nBTM achieves higher updates per second than fully or partially synchronized training. The efﬁciency\\ngains from embarrassingly parallel training become more substantial with larger model size – and\\nmore nodes used in parallel. At 1.3B parameters per GPU, BTM’s branched training provides a\\n33% speedup over fully synchronized training. To better leverage this speedup, we experiment with\\ndedicating a larger proportion of the total compute budget to branched ELM training in §5.2. These\\nspeed estimates may vary considerably with hardware and environment factors.\\nlimited (e.g., academic-scale) resources. Our TRANSFORMER-LM training experiments, on the other\\nhand, consumed 16 nodes of 8 GPUs simultaneously. We observe this phenomenon throughout all\\nexperiments; empirically, ELMFOREST training jobs were scheduled and run more quickly, and with\\nless preemption, than the TRANSFORMER-LM and DEMIX training jobs at the same overall budget.\\n4.4\\nELMFOREST Parameter Average\\nWhile ELMFOREST substantially improves performance at lower training cost relative to the\\nTRANSFORMER-LM, it comes at the price of a larger model size and higher associated inference costs\\nwhen ensembling. Here, we explore an alternative way to combine experts to improve generalization\\nwith no additional inference costs relative to the TRANSFORMER-LM baseline: parameter averaging\\n(§2.4). Given some weight vector w over k ELMs {θi, ..., θk}, we deﬁne a single model such that all\\nof its parameters are a weighted average of the ELM parameters, according to w: θ = Pk\\ni=0 wiθi.\\nFor w, we consider a number of options:\\n• Uniform: We set w to be a uniform; i.e., 1\\nk. This setting disregards the relevance of each\\nELM to the target domain, assuming all ELMs should contribute equally to the average.\\n• Argmax We set w to be an indicator vector that corresponds to the maximum probability\\nin the domain posterior. (§2.3). This collapses the ELMFOREST into the estimated best-\\nperforming ELM for the target dataset.\\n• Posterior We set w to be the domain posterior (§2.3), computed on the validation set.\\nWe show our results on ELMFOREST parameter averaging in Table 3. Using uniform weights\\nunderperforms all baselines, even TRANSFORMER-LMs, highlighting the importance of domain\\nrelevance in ensembling and parameter averaging ELMs. Using the argmax ELM outperforms\\nuniform averaging for small models, but not larger models. Weighting the average with the domain\\nposterior outperforms all other techniques, and provides consistent performance improvements over\\nTRANSFORMER-LMs at no additional inference cost.\\nThe best parameter averaging performance does not reach the performance of ensembling. However,\\nthe lower inference costs and simplicity of deployment may make averaging the preferred inference\\ntechnique for certain resource-constrained applications. We explore these computational tradeoffs\\nfurther in §6.5. Due to its superior performance, we report results for ELMFOREST with ensembling,\\nunless otherwise noted.\\nSurprisingly, we observe poor performance of model averaging at the 125M scale regardless of the\\ntype of weight vector we use. We see later that the amount of compute allocated to the seed phase\\nhas a critical effect on the viability of ELMFOREST parameter averaging (§5.2). With sufﬁcient seed\\ntraining, parameter averaging outperforms TRANSFORMER-LM at all scales.\\n8\\nTrain Domains PPL (↓)\\n125M\\n350M\\n760M\\n1.3B\\nTRANSFORMER-LM\\n19.9\\n16.3\\n14.7\\n14.2\\nELMFOREST parameter average (uniform weights)\\n47.4\\n19.9\\n19.0\\n18.0\\nArgmax ELM (one-hot posterior)\\n18.0\\n15.3\\n14.1\\n13.8\\nELMFOREST parameter average (posterior weights)\\n18.0\\n15.1\\n13.9\\n13.4\\nELMFOREST ensemble\\n17.2\\n14.7\\n13.4\\n13.0\\nEval Domains PPL (↓)\\n125M\\n350M\\n760M\\n1.3B\\nTRANSFORMER-LM\\n25.2\\n20.8\\n19.3\\n18.4\\nELMFOREST parameter average (uniform weights)\\n31.0\\n22.4\\n20.8\\n19.5\\nArgmax ELM (one-hot posterior)\\n28.3\\n22.3\\n22.3\\n20.3\\nELMFOREST parameter average (posterior weights)\\n28.5\\n20.3\\n18.0\\n17.0\\nELMFOREST ensemble\\n22.4\\n18.6\\n16.7\\n16.3\\nTable 3: ELMs can be combined through parameter averaging (§4.4). Average test-set perplexity\\nacross the 8 training domains (top) and 8 evaluation domains (bottom), from the models in Table\\n1, comparing techniques to collapse ELMFOREST into a single LM. Parameter averaging (with\\nposterior weights) generally yields better average perplexities than TRANSFORMER-LM at no\\nadditional inference cost, but underperforms ELMFOREST ensembling, which uses more effective\\nparameters and is included for comparison as a lower bound. The relatively poor performance\\nof ELMFOREST parameter averaging for the 125M parameter model is investigated further (and\\nimproved) in §5.2.\\n4.5\\nSummary\\nELMFORESTs trained with BTM demonstrate performance gains over update-matched DEMIX\\nand TRANSFORMER-LMs. Additionally, BTM is more efﬁcient (higher updates per second) than\\nTRANSFORMER-LM or DEMIX training, due to a substantial reduction in cross-GPU communica-\\ntions for parameter synchronization. Collapsing the ELMFOREST into a single LM via parameter\\naveraging results in consistent beneﬁts over the TRANSFORMER-LM at no additional inference costs,\\napproaching the performance of ELMFOREST ensembling.\\n5\\nAnalysis\\nIn the core results of §4, we ﬁxed the training setup to conduct a controlled comparison of BTM to\\nbaseline methods. Given the evidence supporting the improved performance of ELMFOREST trained\\nwith BTM, we now analyze the importance of various training and inference decisions. We study\\nthe effects of the increased parameter count in an ELMFOREST, of seed LM training data, as well\\nas the compute budget dedicated to the seed phase, and also discuss the observed tradeoffs between\\nperformance, efﬁciency, and functionality.\\n5.1\\nELMFOREST performance is not simply the result of ensembling more parameters\\nTo determine whether ensembling extra parameters is sufﬁcient for the improvement our method\\nprovides, we compare multiple variations of LM ensembles:\\nRandom Ensemble (seed init)\\nA set of LMs trained on random data splits, to assess the effect of\\nremoving the domain specialization of ELMs. We ﬁrst pool the training and development sets of\\nour 8 train domains and divide into 8 random data splits, then execute the BTM procedure on those\\nrandom splits, dedicating 50% of training to the seed phase.7\\n7In Figure 1, this setting is labeled Random Ensemble.\\n9\\n125M\\nRandom\\nELM\\nELM\\nEnsemble\\nFOREST\\nFOREST\\n(seed init)\\n(random init)\\n(seed init)\\nTrain\\n23.0\\n18.2\\n17.2\\nEval\\n26.0\\n23.4\\n22.4\\nAll\\n24.7\\n20.8\\n19.8\\n350M\\nRandom\\nELM\\nELM\\nEnsemble\\nFOREST\\nFOREST\\n(seed init)\\n(random init)\\n(seed init)\\nTrain\\n19.9\\n15.3\\n14.7\\nEval\\n23.1\\n21.3\\n18.6\\nAll\\n21.5\\n18.3\\n16.7\\n750M\\nRandom\\nELM\\nELM\\nEnsemble\\nFOREST\\nFOREST\\n(seed init)\\n(random init)\\n(seed init)\\nTrain\\n17.4\\n14.4\\n13.4\\nEval\\n20.9\\n19.3\\n16.7\\nAll\\n19.2\\n16.9\\n15.0\\n1.3B\\nRandom\\nELM\\nELM\\nEnsemble\\nFOREST\\nFOREST\\n(seed init)\\n(random init)\\n(seed init)\\nTrain\\n17.4\\n13.3\\n13.0\\nEval\\n20.4\\n17.8\\n16.3\\nAll\\n18.9\\n15.6\\n14.6\\nTable 4: Domain expert ensemble outperforms random split ensemble (§5.1). Average test-set\\nperplexity (↓) for each model scale across the 8 training, 8 evaluation, and all 16 domains described in\\n§4.1. Training ELMFOREST experts with random data splits performs much worse than the proposed\\nmethod of training ELMFOREST experts on domains of data deﬁned by provenance. This suggests\\nthat ensembling with more parameters is not sufﬁcient, but that domain specialization is a crucial\\ncomponent for the performance improvements we observe.\\nELMFOREST (random init)\\nAn ELMFOREST trained with BTM where all ELMs are randomly\\ninitialized, to assess the effect of seed training. This is equivalent to setting the seed training compute\\nbudget to zero updates. We ﬁx the random initialization across models.\\nELMFOREST (seed init)\\nThe ELMFOREST setting of §4, which follows the BTM training proce-\\ndure on the 8 train domains, and splits the compute budget such that 50% of the updates are dedicated\\nto seed training and 50% to branched ELM training.\\nThese results are in Table 4. ELMFOREST (random init) nearly matches ELMFOREST on training\\ndomains but performs poorly on evaluation domains. The random ensemble is consistently worse\\nthan both variants of ELMFOREST, showing that the performance improvement is not only due to\\nensembling or increased total model size. We speculate that the random ensemble is poor because its\\nconstituent models make correlated errors during evaluation (Gontijo-Lopes et al., 2022).\\n5.2\\nELMFOREST performance robust to wide range of seed LM training compute\\nallocations\\nThe ELMFOREST (random init), which has no seed training, underperforms ELMFOREST (LM init)\\nin §5.2, indicating that the seed training phase is essential. On the other hand, TRANSFORMER-LM,\\nwhich is equivalent to 100% seed training, also underperforms ELMFOREST (LM init) in §4, which\\nsuggests the importance of branched ELM training. We now study the changes to performance\\nwhen we vary the portion of the compute budget dedicated to seed training. We control for the total\\ncompute budget (across seed and branched training).\\nOur results, in Figure 4, show that the optimal amount of seed training is about 40–60% of the total\\nbudget. At both ends of the full range, performance deteriorates, approaching the ELMFOREST\\n(random init) and TRANSFORMER-LM performance (at 0% and 100% seed training, respectively).\\nHowever, as little as 10% of seed training can be performed to result in strong gains over the\\nELMFOREST (random init) and TRANSFORMER-LM. This suggests that the majority of BTM\\ntraining may focus on branched ELM training at dramatically reduced computational cost relative\\nto the prevailing training of TRANSFORMER-LMs, due to reduced GPU communication (§4.3).\\nThe optimal share of compute to use towards each training phase likely depends on many factors,\\nincluding the total compute budget. We leave more thorough study of this trend to future work.\\n10\\n0\\n20\\n40\\n60\\n80\\n100\\n% Seed Training\\n12\\n14\\n16\\n18\\n20\\nPerplexity\\n8 Training Domains\\nParams/ELM\\n125M\\n350M\\n750M\\n1.3B\\n0\\n20\\n40\\n60\\n80\\n100\\n% Seed Training\\n16\\n18\\n20\\n22\\n24\\n26\\nPerplexity\\n8 Evaluation Domains\\nParams/ELM\\n125M\\n350M\\n750M\\n1.3B\\nFigure 4: ELMFOREST ensembling performance is robust to most seed training compute allo-\\ncations (§5.2). Test perplexity averaged across the 8 training (left) or 8 evaluation (right) domains\\n(from §4.1) when varying the proportion of compute allocated to seed training, but keeping total\\ncompute budget ﬁxed. All models use the same random initialization. The best training domain\\nperformance occurs when roughly 20–50% of updates are seed training, whereas best evaluation\\ndomain performance occurs when roughly 50–80% of updates are seed training. This supports the\\nhypothesis that seed training is useful for generalization. However, performance varies little between\\n20–80%, and almost all BTM settings perform better than both TRANSFORMER-LM (100% seed)\\nand ELMFOREST-random init (0% seed) training across model sizes.\\nELMFOREST averaging performance strongest with about 60-70% seed LM training\\nSepa-\\nrate experiments, shown in Figure 5, suggest a strong effect of the seed phase on the viability of\\nELMFOREST averaging. We ﬁnd that ELMFOREST averaging does not work with ELMs trained\\nfrom random initialization (i.e., with no seed phase), resulting in perplexities in the thousands. Since\\nthese ELMs still share the same random initialization, we conclude that there is importance to seeding\\nthe ELMFOREST with a shared set of partially trained LM parameters.\\nThe averaging procedure is more robust to seed phase compute budget when evaluated on train\\ndomains. On evaluation domains, however, the smallest scale ELMFOREST does not achieve optimal\\nperformance until about 60% or more updates are dedicated to seed training. This explains the poor\\nperformance of the 125M parameter scale ELMFOREST average on evaluation domains in Table 3.\\nHowever, this optimal range shifts much lower, to about 40%, for the next largest scale. Results for\\nELMFOREST parameter averaging at 50% seed training for 350M, 750M, and 1.3B parameter scales\\ndo outperform TRANSFORMER-LM baselines on both train and evaluation domains (Table 3).\\n5.3\\nELMFOREST performance is robust to the choice of seed training corpus\\nWe compare the effects of using different training corpora for seed training in BTM. Here, we ﬁx\\nthe compute budget allocations studied in §5.2 so that 50% of updates are allocated to seed training\\nand 50% to branched training. As seen in Table 5, our experiments using the most diverse corpora\\nfor seed training resulted in the best performance overall, but even seed training on JavaScript code\\nalone yielded better results than the compute-matched TRANSFORMER-LM baseline. This lack\\nof sensitivity to seed phase training corpus suggests that initializing ELMs with parameters of a\\nmodel checkpoint is key – perhaps regardless of the performance of that model on any target domain.\\nHowever, the ELMFOREST (random init) models in Table 1, which use identical random initialization,\\nachieve worse performance, indicating the beneﬁt of pretrained model parameters.\\nDomain forgetting through ELM removal is mostly robust to seed training corpus\\nWe intro-\\nduced in §2.2 the possibility of removing ELMs to reduce the inﬂuence of speciﬁc training domains\\n11\\n0\\n20\\n40\\n60\\n80\\n100\\n% Seed Training\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\nPerplexity\\n0% seed training \\n(>1000 ppl)\\n8 Training Domains\\nParams\\n125M\\n350M\\n0\\n20\\n40\\n60\\n80\\n100\\n% Seed Training\\n15\\n20\\n25\\n30\\n35\\n40\\nPerplexity\\n0% seed training \\n(>1000 ppl)\\n8 Evaluation Domains\\nParams\\n125M\\n350M\\nFigure 5: The seed phase is vital to our ability to parameter average ELMs (§5.2). Test perplex-\\nity averaged across the 8 training (left) and 8 evaluation (right) domains when averaging ELMFOREST\\nwith different seed training compute allocations for the 125M and 350M parameter LMs. All models\\nuse the same random initialization. Seed training is critical to reasonable averaging performance.\\nLowest perplexity is achieved at slightly higher seed phase training proportions than when using out-\\nput ensembling; however, as with output ensembling, the variation in performance is small throughout\\na wide range of seed training phase lengths. For training domains, effective parameter averaging\\nperformance can be achieved with even less seed training.\\nAverage Test PPL (↓)\\nTrain\\nEvaluation\\nOverall\\nTRANSFORMER-LM\\n19.8\\n25.5\\n22.7\\nseed corpus\\n8 train domains\\n17.2\\n22.7\\n20.0\\nWikipedia\\n17.7\\n23.2\\n20.5\\nC4\\n17.9\\n23.5\\n20.7\\nStackOverﬂow\\n18.4\\n24.6\\n21.5\\nJavaScript\\n19.2\\n24.9\\n22.0\\nTable 5: ELMFOREST ensembling performance is robust to seed training corpus (§5.3). Test\\nperplexity averages on the 8 training and the 8 novel test sets (from §4.1), as well as averaged\\nacross all 16, with different training corpora used in seed LM training. All models are of the 125M\\nparameters per GPU scale. All ELMFORESTs outperform the TRANSFORMER-LM baseline. Notably,\\neven training the seed LM on JavaScript results in lower perplexity than the TRANSFORMER-LM,\\nsuggesting that BTM training is not overly sensitive to the selection of the seed training corpus.\\nat inference time (e.g., those that contain stale or harmful text). Now, we evaluate the effectiveness\\nof removing ELMs in practice. In Table 6, we show the performance of ELMFOREST ensembles\\non the training domains when using all ELMs (from Table 5), and compare to performance when\\nremoving the relevant ELM. For example, to evaluate on REDDIT, we keep active all ELMs except\\nthe REDDIT ELM. Removal of an ELM from an ELMFOREST guarantees that that domain will\\nbe forgotten, if the seed training corpus did not include that domain’s data (§2.2)8. We ﬁnd that\\nperformance does indeed degrade appreciably on ELMFORESTs when removing the ELM, indicating\\nthat ELMFORESTs are capable of effectively forgetting a data domain without any gradient updates\\nto the parameters. Even in the model with seed training on the 8 training domains, removal of a ELM\\ngreatly degrades test perplexity in the corresponding domain. Taken together with the previous results,\\n8The actual effect of ELM removal on model performance may depend on the overlap between the removed\\ndomain and other training domains. Future work may also investigate the effects of overlap and any required\\nguarantees of domain disjunction.\\n12\\nw/ ELM\\n(Average Test PPL ↓)\\n–ELM\\n(∆PPL)\\n8 train domains\\n17.2\\n(+9.4)\\nseed\\ncorpus\\nWikipedia\\n17.7\\n(+11.9)\\nC4\\n17.9\\n(+11.8)\\nStackOverﬂow\\n18.4\\n(+12.7)\\nJavaScript\\n19.2\\n(+13.6)\\nTable 6: The ability to reduce the inﬂuence of domains through ELM removal is (mostly)\\nrobust to seed training corpus (§5.3). We present the average test perplexity for the 8 train domains\\nin ELMFORESTs where all ELMs are active. We vary the seed training corpora. In parentheses, we\\nshow the increase in perplexity when the ELM trained to specialize on each domain is removed at\\ninference time. Large perplexity increases are desired, as they suggest the ease of removing data from\\nthe ELMFOREST’s distribution after training has completed. This is a useful operation to reduce the\\ninﬂuence of training domains that contain harmful text or become stale.\\nFigure 6: BTM incremental training on 64 domains (§6). We now iterate over the BTM steps (§3;\\nFigure 3) to train incrementally on 4 data batches B1–B4, totalling 64 domains. After training the\\nseed LM on the pooled B1 domains, we train ELMs on each B1 domain independently, then initialize\\nnew ELMs from weighted averages of existing expert parameters. These new ELMs are each trained\\nto specialize on a B2 domain. We repeat this process for data domains in B3, then B4, initializing new\\nELMs from the set of existing experts. After this full process, all 64 total ELMs over 4 batches have\\nbeen merged into the same ELMFOREST and may be ensembled or averaged together for inference.\\nit may be possible to carefully curate the seed corpus (e.g., to avoid including harmful language)\\nand domain data, to provide even stronger guarantees on the ability to forget unwanted domains via\\nexpert removal with minimal performance degradation.\\n6\\nIncrementally Training an ELMFOREST on 64 Domains\\nNext, using the best settings discovered in §4 and §5, we repeat the BTM procedure to scale\\ninto an 80-domain corpus, which contains 64 training domains and 16 evaluation domains. Our\\ntraining procedure is summarized in Figure 6. In §6.3, we demonstrate how the performance of the\\nELMFOREST scales relative to a large TRANSFORMER-LM trained on the same set of domains from\\nrandom initialization. We then examine the sparsity of the resulting ELMFOREST at inference time\\n(§6.4), and compare techniques to reduce the inference cost of larger ELMFORESTs (§6.5).\\n6.1\\nExperimental Setup\\nData: 80-domain Corpus\\nUsing provenance as our domain label, we curate a corpus of 64 training\\ndomains and 16 evaluation domains (Table 7). Corpora were selected for their research-friendly data\\nlicensing, substantial size, and content diversity, and are drawn from a collection of publicly available\\ndata repositories (e.g., Gao et al., 2021; Lo et al., 2020). Full details on the data sources that make up\\nthese corpora can be found in Appendix Table 10 and 11.\\n13\\n80-DOMAIN CORPUS: 192.3B WHITESPACE-SEPARATED TOKENS\\nCategory\\nDomains\\nSEMANTIC SCHOLAR (26.6%)\\nMedicine (5.2%), Biology (4.7%), CS (3.4%), Physics (2.7%), Math (2.3%), Unlabeled (1.3%), Psychology\\n(1.2%), Chemistry (1.0%), Economics (0.8%), Engineering (0.7%), CORD19 (0.6%), Material Science\\n(0.5%), Geology (0.5%), Sociology (0.5%), Business (0.3%), Political Science (0.2%), Geography (0.2%),\\nEnvironmental Science (0.1%), History (0.1%), Philosophy (0.1%), ACL (0.1%), Art (0.05%)\\nGITHUB CODE (22.4%)\\nJavaScript (3.7%), Java (3.5%), HTML (2.7%), C (2.5%), C++ (1.9%), Python (1.5%), C# (1.2%), PHP\\n(1.1%), Markdown (1.1%), Code Contests (1.0%), GO (1.0%), CSS (0.7%), Ruby (0.4%)\\nWEB FORUMS (17.5%)\\nReddit Dialogues (13.0%), StackOverﬂow (1.7%), Twitter (0.9%), StackExchange (0.8%), HackerNews\\n(0.4%), Gaming Subreddits (0.1%), Sports Subreddits (0.1%)\\nWEB CRAWL (16.0%)\\nC4 (5.2%), RealNews (5.2%), OpenWebText (3.4%), Wikipedia (en) (1.3%), WMT News Crawl 2021\\n(0.5%), 1B Words Corpus (0.4%)\\nBOOKS (5.8%)\\nStories (3.8%), Gutenberg Books (1.6%), BookCorpus (0.4%)\\nLEGAL TEXT (5.5%)\\nLegal Case Law (5.5%), Supreme Court Opinions (HTML) (0.1%)\\nREVIEWS (5.0%)\\nBooks Reviews (2.1%), Amazon Reviews (1.1%), Electronics Reviews (0.5%), Clothing, Shoes and Jewelry\\nReviews (0.5%), Home and Kitchen Reviews (0.4%), Yelp Reviews (0.3%), Sports and Outdoors Reviews\\n(0.3%), Movies and TV Reviews (0.3%)\\nOTHER (1.3%)\\nDM Mathematics (0.8%), OpenSubtitles (0.4%), USPTO (0.1%)\\nEVALUATION DOMAINS\\n(TEST ONLY)\\nEnron,\\n#COVID-19\\nTweets,\\nIMDB,\\nTOEFL\\nexams,\\nCongressional\\nbills,\\nLegal\\nContracts,\\n/r/cscareerquestions, /r/india, /r/hiphopheads, Irish Parliamentary Speeches, SQL, Rust, Perl , TeX,\\nFORTRAN, Breaking News\\nTable 7: Overview of the 80-domain corpus (§6.1). The 80 domains that make up the multi-domain\\ncorpus we train and evaluate on, presented here in 8 descriptive categories for ease of inspection.\\nFor each of the 64 training domains, we include the percentage of the total number of tokens (in the\\nentire corpus) comprising that domain. At the bottom, we include the 16 evaluation domains. All\\ndomains additionally include 1M tokens for validation and test data each. We include full details of\\neach corpus in Appendix Table 10 and 11.\\nDomain Batches\\nWe ﬁrst organize the 64 training domains into four ordered batches of increasing\\nsize, which form a curriculum that we denote B1 to B4. We maintain the training domains of\\nGururangan et al. (2022) as the ﬁrst batch (B1), which allows us to leverage the best models from §4\\nand §5, but organize the batches B2-B4 randomly. See Appendix Table 10 for our batch assignments.\\nModel hyperparameters and training settings\\nWe follow the same settings from §4.1, but only\\ncompare models at the 350M (medium) and 1.3B (xl) scales.\\n6.2\\nCompared Models\\nThe models in these experiments are intentionally not compute-matched, to demonstrate the efﬁciency\\ngains of our approach over an expensive baseline.\\nTRANSFORMER-LM\\nOur baseline is a large, randomly-initialized 1.3B parameter transformer\\nLM, trained for 6144 GPU hours (with 128 GPUs) on all 64 domains. We use the same training\\nsettings for this baseline as the TRANSFORMER-LM in §4.\\nELMFOREST\\nWe train on the 64 domains incrementally using 4 GPUs per ELM (see Figure 6\\nfor a diagram of the overall process). For each batch of domains, we follow the basic procedure of\\nBTM (§3): branch a new set of experts, train them on the batch, and merge trained ELMs back into\\nthe ELMFOREST. We start with ELMs from §5 trained with 75% seed training and 25% branched\\ntraining on the B1 domains, which achieves the best evaluation-domain performance in our initial\\nexperiments (Figure 4). To train on B2, we branch new ELMs with weighted averages of B1 experts,\\nthen train on B2 for 40 GPU hours. For B3, we branch new ELMs again, with weighted averages of\\nB1 and B2 experts, and train on B3 for 40 GPU hours. Finally, we scale into B4 by training for 20\\nGPU hours on each domain, initializing those ELMs with the weighted average of ELMs in B1, B2,\\nand B3.9 Our ﬁnal ELMFOREST contains 64 ELMs and is trained for a total of 2565 GPU hours.\\n9The choice of compute budget at each training stage is an open question for future work. This may be\\ninformed by how much data existing ELMs have already been exposed to and how much data exists in the group.\\n14\\n0\\n1K\\n2K\\n3K\\n4K\\n5K\\n6K\\nTraining cost (GPU Hours)\\n10\\n12\\n14\\n16\\n18\\n20\\nPerplexity\\nELMforest Ensemble \\n11.8 ppl\\n11.6 ppl\\n1.3B Transformer-LM\\n64 Training Domains\\n0\\n1K\\n2K\\n3K\\n4K\\n5K\\n6K\\nTraining cost (GPU Hours)\\n10\\n12\\n14\\n16\\n18\\n20\\nPerplexity\\nELMforest Ensemble \\n13.9 ppl\\n1.3B Transformer-LM\\n13.5 ppl\\n16 Evaluation Domains\\nFigure 7: ELMFOREST achieves performance comparable to a large TRANSFORMER-LM\\nbaseline at a much smaller computational budget (§6.3). Efﬁciency-performance trend for train-\\ning (left) and evaluation (right) domains when ELMFORESTs are incrementally trained on a 64\\ndomain corpus, versus a large TRANSFORMER-LM that is trained on 64 domains from scratch.\\nELMFORESTs achieve better perplexity at smaller compute budgets than TRANSFORMER-LMs.\\nELMFORESTs perform comparably to the 1.3B TRANSFORMER-LM, using only about 40% of the\\nTRANSFORMER-LM total budget.\\n6.3\\nResults\\nFigure 7 presents the performance of ELMFOREST and TRANSFORMER-LM as a function of the\\ntotal compute budget used for training on the 64 domains. We observe that ELMFOREST with\\nensembling achieves perplexities on training and evaluation domains that are comparable to a large\\nTRANSFORMER-LM, despite using only 40% of the total compute.10 ELMFOREST effectively\\nmatches training domain performance of the large TRANSFORMER-LM because the TRANSFORMER-\\nLM exhibits worse average performance on each individual training domain as it is exposed to\\nmore domains during training (see Appendix Figure 10 for more details).11 On the other hand, we\\nalways observe monotonically non-worsening performance on each training domain with BTM, since\\npreviously trained ELMs are unaffected when branching into new domains.\\nThe order and composition of the domain batches likely affects the performance-efﬁcency tradeoff\\ncurve for the ELMFOREST. We leave a careful analysis of these factors, and other optimal settings\\nfor scaling ELMFORESTs, to future work.\\n6.4\\nSparsity of the 64-expert domain posterior\\nA visualization of our resulting 64-expert ELMFOREST domain posteriors on held out validation\\ndata from the 80-domain corpus is displayed in Figure 8. We observe sparsity in the domain posterior\\nin both the training and evaluation domains. This sparsity suggests that only a few ELMs need to\\nbe active at test time, a hypothesis we test next. We also display the top 3 ELMs for a sample of\\nevaluation domains in Table 8. The most likely ELMs are relevant to the evaluation domain.\\n6.5\\nReducing inference costs\\nThe sparsity of the domain posterior in the 64-expert ELMFOREST suggests that fewer ELMs can be\\nused at test time with minimal performance degradation. We see in Figure 9 that this is indeed true;\\nas few as the top-8 ELMs can be ensembled together with negligible loss in performance relative\\n10ELMFOREST parameter averaging approaches the ensemble in performance; see §6.5 for details.\\n11Previous work has shown that phenomenon afﬂicts multilingual TRANSFORMER-LMs as well, as more\\nlanguages are added to the training corpus (Pfeiffer et al., 2022).\\n15\\n0\\n4\\n8\\n12 16 20 24 28 32 36 40 44 48 52 56 60\\nDomain\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\n36\\n39\\n42\\n45\\n48\\n51\\n54\\n57\\n60\\n63\\nELM\\nTraining Domains\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14 15\\nDomain\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\n36\\n39\\n42\\n45\\n48\\n51\\n54\\n57\\n60\\n63\\nELM\\nEvaluation Domains\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nprobability\\nFigure 8: Training and evaluation domain inference both use ELMs sparsely (§6.4). Domain\\nposterior visualization for 22.4B parameter ELMFOREST trained on 64 domains. ELM activation is\\nextremely sparse for both training and evaluation domains.\\nEvaluation Domain\\nTop-1\\nTop-2\\nTop-3\\nCovid Tweets\\nTwitter (0.48)\\n2021 Newscrawl (0.29)\\nHackerNews (0.05)\\nTeX\\nstackexchange (0.64)\\nMarkdown (0.22)\\nHTML (0.05)\\nCongressional Bills\\nSCOTUS Opinions (0.21)\\nOpenWebText (0.13)\\nStackexchange (0.11)\\nIMDB\\nMovie & TV Reviews (0.66)\\nBook Reviews (0.11)\\nRealNews (0.05)\\nLegal Contracts\\nC4 (0.30)\\nLegal (0.21)\\nSCOTUS Opinions (0.14)\\nTable 8: Top-3 ELMs (with probabilities in parentheses) under the domain posterior for a\\nsample of evaluation domains (§6.4). The most likely ELMs under the domain posterior are\\nusually highly relevant to the evaluation domain by inspection.\\nto the 64-expert ELMFOREST. Even with just the top-1 ELM, we see better performance for the\\ntraining cost than a TRANSFORMER-LM, at no additional inference cost.\\nOn the other hand, weighted parameter averaging of all 64 ELMs (with weights deﬁned by the\\ndomain posterior) provides the best performance at the same cost of the original TRANSFORMER-\\nLM, but ensembling at least two ELMs tends to outperform the ELMFOREST average12. The runtime\\nefﬁciency of averaged models may be desirable for resource-constrained applications, where using\\nlarge LMs (i.e., those that cannot ﬁt on a single GPU) is difﬁcult. With averaging, inference costs\\nstay constant regardless of the number of ELMs in the ELMFOREST.\\n6.6\\nSummary\\nOur results demonstrate that a large collection of smaller ELMs, each of which can be branched\\nindependently and on small compute budgets (e.g., at most four GPUs) with BTM, can perform\\ncomparably with a large TRANSFORMER-LM trained with at least 2.5× the compute. While the\\ncosts of using all ELMs in an ensemble at inference time is high, these costs can be reduced\\n(with minimal performance degradation) to that of the equivalent TRANSFORMER-LM through\\nparameter averaging. Future work may perform closer investigations of best practices for scaling and\\ncoordinating ELMFORESTs to improve on the results we report.\\n12This suggests that it may be possible to improve the algorithm to estimate weights for our parameter average.\\nWe leave this for future work.\\n16\\n350M\\n700M\\n1.4B\\n2.8B\\n11.2B\\n22.4B\\nNumber of inference parameters\\n14.0\\n14.5\\n15.0\\n15.5\\nPerplexity\\nTransformer-LM\\n64-ELM\\naverage\\ntopk=1\\ntopk=2\\ntopk=4\\ntopk=8\\ntopk=32\\ntopk=64\\n16 Evaluation Domains\\nFigure 9: Even using only the top-1 ELM outperforms the TRANSFORMER-LM baseline. We\\nvary the number of ELMs active at inference time, and observe performance changes in the evaluation\\ndomains. In general, using just the top 4 or 8 ELMs results in minimal performance loss relative to\\nusing 64 ELMs at test time, owing to sparsity in the domain posterior. ELM averaging results in\\na substantial performance improvement over TRANSFORMER-LMs at the same inference cost; but\\nlarger gains are observed with larger top-k values with output ensembling.\\n7\\nRelated Work\\nSparse Language Models\\nSparsely activated language models have been considered in a few\\nforms (Evci et al., 2020; Mostafa and Wang, 2019; Dettmers and Zettlemoyer, 2019), but the Mixture-\\nof-Experts (MoE) model is of particular note. Early versions (Jacobs et al., 1991) had independent\\nfeed-forward networks serving as experts. Recent MoE models (Shazeer et al., 2017) have been\\nstudied with token-based routing through backpropagation – notably, by Lepikhin et al. (2021), which\\nappplies this concept to machine translation, and Fedus et al. (2022), which simpliﬁes the architecture\\nto activation of only one expert per layer. Lewis et al. (2021), ﬁnd an alternative approach to routing\\nby formulating it as a linear assignment problem, and Roller et al. (2021) use a ﬁxed hash as the\\ngating function.\\nOf this line of work, ours is most closely related to Gururangan et al. (2022). In that work, DEMix\\nlayers – placed in the feedforward layers of the Transformer – contain experts which specialize\\non speciﬁc domains. Routing at train time is determined only by the domain label, but all experts\\nare activated at inference time and mixed according to weights estimated from a validation set.\\nSimilarly, Pfeiffer et al. (2022) develop a multilingual expert model with language-speciﬁc routing,\\nand Kudugunta et al. (2021) develop a multi-task expert model with task-speciﬁc routing.\\nAdapters\\nPrevious work has also explored extending the capacity of a model with additional\\nspecialized parameters (e.g., adapters; Houlsby et al., 2019; Pfeiffer et al., 2020; Ben Zaken et al.,\\n2022). However, unlike these existing approaches, our approach is signiﬁcantly simpliﬁed, as our\\nELMs each consist of an entire model which requires no additional parameters and no shared\\nparameters. Future work may explore combining ELMs with adapters to scale into smaller domains.\\nEnsembles\\nEnsemble methods are widely used in machine learning, for example in bagging,\\nboosting, and stacking (Breiman, 1996; Freund, 1995; Wolpert, 1992). In a setting where training\\ndata is streamed, Caccia et al. (2021) deﬁne a growing ensemble, in which new base models are\\ntrained sequentially on incoming batches. However, their growing ensemble, incrementally trained\\non the randomly created batches of their setting, underperforms non-incremental methods.\\nParameter Averaging\\nOur averaging mechanism is inspired by the discovery that averaging many\\nﬁne-tuned vision models improves out-of-domain generalization (Wortsman et al., 2022a; Izmailov\\n17\\net al., 2018). In Wortsman et al. 2022a, the authors propose a greedy mechanism for averaging experts\\nwith uniform weights. Here, we ﬁnd that uniform weighted averaging does not work for combining\\ndomain-speciﬁc models; instead we use a posterior weighted average, where the averaging weights\\nare estimated based on the relevance of the model to the target domain. Our posterior weighted\\naverage is highly related to Bayesian model averaging techniques used in classic ensembling methods\\n(Fragoso et al., 2018). Model averaging has also been explored for federated learning (McMahan\\net al., 2017), where different models are trained locally to ﬁt privacy-sensitive data on different\\ndevices and merged. However, these works have found success averaging models trained from the\\nsame random initialization, which we do not ﬁnd to hold in our setting. Matena and Raffel (2021)\\ncompute a parameter average of models, estimating the optimal weights via an approximation of the\\nFisher information. Future work may explore these (and other) variations of weighted averages with\\nELMs.\\nSeed training\\nOur discovery of the importance of the seed training as a critical warm-up phase for\\nBTM is in line with ﬁndings that parameter averaging only works when models share part of their\\noptimization trajectory (Frankle et al., 2020; Entezari et al., 2022). Future work may investigate what\\nis learned in the seed phase that makes it so useful for ELM specialization, regardless of the corpus\\nused for seeding. Similar to seed training, Nie et al. (2021) propose dense-to-sparse gating, where\\nmixture-of-experts routing mechanisms are gradually sparsiﬁed during the course of training.\\n8\\nLimitations\\nThe deﬁnition of a domain\\nThe nature of domains in NLP is a matter of active research. Textual\\ndomains reﬂect language variation that stems from factors such as vocabulary differences (Blitzer\\net al., 2006), sociolinguistic (Biber, 1988) or demographic (Rickford, 1985; Blodgett et al., 2016)\\nvariables, community membership (Lucy and Bamman, 2021), end-tasks (Gururangan et al., 2020),\\nor temporal shifts (Lazaridou et al., 2021; Luu et al., 2021). In this work, we follow Gururangan et al.\\n(2022) and deﬁne domains by provenance, or the source of the document. Provenance labels yield\\nsimple and interpretable segmentations of a corpus, which are useful for identifying ELMs in our\\nexperiments. However, other methods for discovering domains, including unsupervised techniques\\n(Aharoni and Goldberg, 2020; Chronopoulou et al., 2022), may yield better expert assignments. We\\nleave experimentation with other deﬁnitions of domain for future work.\\nDomain posterior data requirement\\nTo calculate the domain posteriors used for our ensembling\\nand parameter averaging weights, we assume access to a small additional sample of data to train\\nthe vector w. While it is easy to imagine that extra data may be available for most applications to\\nestimate the posterior, future work may explore the possibility of eliminating this requirement.\\nOther distributed training baselines\\nOur TRANSFORMER-LM baseline is implemented with\\ndistributed data-parallel. Model-parallel, fully sharded data-parallel, and other distributed training\\nstrategies (Artetxe et al., 2021) confer different scaling patterns that may change the conclusions that\\nwe report in this work. However, we expect that BTM will provide strong efﬁciency gains against\\nthese alternatives.\\nHarms of language models\\nBTM results in an LM whose test time behaviors can be controlled\\nwith much stronger guarantees after training due to the isolation of domains in ELMs. However,\\nELMFORESTs exposed to large datasets scraped from the Internet may contain toxic language (e.g.,\\nhatespeech) that are difﬁcult to identify with coarse provenance domain labels, and nevertheless\\nresult in harmful output from the ELMs (Gehman et al., 2020). Future work may explore recipes for\\ntraining and deploying ELMFORESTs to better support user safety.\\n9\\nConclusion\\nWe introduce BTM training, a new algorithm to train an ELMFOREST, which contains many EXPERT\\nLMs that can be added and removed, ensembled, or parameter averaged at any time for efﬁcient\\nscaling and rapid customization. Our extensive experiments show that ELMFOREST ensembles\\ntrained with BTM outperform baselines at no additional training cost. Additionally, parameter\\n18\\naveraged ELMFORESTs closely approach ELMFOREST ensemble performance while enabling\\nsubstantially cheaper inference. These results provide compelling evidence for the promise of scaling\\nlarge language models with many smaller, independently trained ELMs. We envision that this\\nwork lays the foundation for democratized model development at inclusive compute budgets — that\\ngroups with different resource constraints and research interests may combine efforts to build open-\\nsourced, community-authored large language models, comprised of continually-evolving repositories\\nof EXPERT LMs.\\nAcknowledgments and Disclosure of Funding\\nThis paper beneﬁted from thoughtful feedback from a number of people: Ari Holtzman, Candace\\nRoss, Colin Raffel, Gabriel Ilharco, Ishaan Gulrajani, Julian Michael, Mitchell Wortsman, Stephen\\nRoller, Swabha Swayamdipta and William Fedus.\\nAt UW, this work was partially supported by the Ofﬁce of Naval Research under MURI grant\\nN00014-18-1-2670.\\nReferences\\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\\n7747–7763, Online. Association for Computational Linguistics.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen,\\nHalil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeff\\nWang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2021. Efﬁcient large\\nscale language modeling with mixtures of experts.\\nLoïc Barrault, Ondˇ\\nrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette\\nGraham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias\\nMüller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on\\nmachine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation\\n(Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy. Association for Computational\\nLinguistics.\\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020.\\nThe pushshift reddit dataset.\\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efﬁcient\\nﬁne-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1–9,\\nDublin, Ireland. Association for Computational Linguistics.\\nDouglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press.\\nDaniel Blanchard, Joel R. Tetreault, Derrick Higgins, A. Cahill, and Martin Chodorow. 2013.\\nTOEFL11: A corpus of non-native English. ETS Research Report Series, 2013:15.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural\\ncorrespondence learning. In Proceedings of the 2006 Conference on Empirical Methods in\\nNatural Language Processing, pages 120–128, Sydney, Australia. Association for Computational\\nLinguistics.\\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal variation in social\\nmedia: A case study of African-American English. In Proceedings of the 2016 Conference on\\nEmpirical Methods in Natural Language Processing, pages 1119–1130, Austin, Texas. Association\\nfor Computational Linguistics.\\nLeo Breiman. 1996. Bagging predictors. Machine learning, 24(2):123–140.\\n19\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language models are few-shot learners.\\nLucas Caccia, Jing Xu, Myle Ott, Marc’Aurelio Ranzato, and Ludovic Denoyer. 2021. On anytime\\nlearning at macroscale. CoRR, abs/2106.09563.\\nCaselaw Access Project. Caselaw access project.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\\nRobinson. 2014. One billion word benchmark for measuring progress in statistical language\\nmodeling.\\nAlexandra Chronopoulou, Matthew Peters, and Jesse Dodge. 2022. Efﬁcient hierarchical domain\\nadaptation for pretrained language models. In Proceedings of the 2022 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\\ngies, pages 1336–1351, Seattle, United States. Association for Computational Linguistics.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A\\ndataset of information-seeking questions and answers anchored in research papers. In Proceedings\\nof the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 4599–4610, Online. Association for Computa-\\ntional Linguistics.\\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason\\nYosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled\\ntext generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\\nTim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster training without\\nlosing performance. CoRR, abs/1907.04840.\\nRahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. 2022. The role of permutation\\ninvariance in linear mode connectivity of neural networks. In International Conference on Learning\\nRepresentations.\\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. 2020. Rigging\\nthe lottery: Making all tickets winners. In Proceedings of the 37th International Conference on\\nMachine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2943–2952.\\nPMLR.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion\\nparameter models with simple and efﬁcient sparsity. Journal of Machine Learning Research,\\n23(120):1–39.\\nTiago Fragoso, Wesley Bertoli, and Francisco Louzada. 2018.\\nBayesian model averaging: A\\nsystematic review and conceptual classiﬁcation. International Statistical Review, 86:1–28.\\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear\\nmode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International\\nConference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,\\npages 3259–3269. PMLR.\\nYoav Freund. 1995. Boosting a weak learning algorithm by majority. Information and computation,\\n121(2):256–285.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An\\n800gb dataset of diverse text for language modeling.\\n20\\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Real-\\nToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association\\nfor Computational Linguistics.\\nGithub Archive Project. Github archive project.\\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.\\nRaphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. 2022. No one representation to\\nrule them all: Overlapping features of training methods. In International Conference on Learning\\nRepresentations.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022. DEMix\\nlayers: Disentangling domains for modular language modeling. In Proceedings of the 2022 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, pages 5557–5576, Seattle, United States. Association for Computational\\nLinguistics.\\nSuchin Gururangan, Ana Marasovi´\\nc, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and\\nNoah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\\n8342–8360, Online. Association for Computational Linguistics.\\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp\\ndataset for legal contract review.\\nAlexander Herzog and Slava Mikhaylov. 2017. Database of Parliamentary Speeches in Ireland,\\n1919-2013.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efﬁcient transfer learning\\nfor NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of\\nProceedings of Machine Learning Research, pages 2790–2799. PMLR.\\nHuggingface. Datasets.\\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.\\n2018. Averaging weights leads to wider optima and better generalization.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive\\nmixtures of local experts. Neural computation, 3(1):79–87.\\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019.\\nCtrl: A conditional transformer language model for controllable generation.\\nAnastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A corpus for automatic summarization\\nof US legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages\\n48–56, Hong Kong, China. Association for Computational Linguistics.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. 2021. Beyond distillation: Task-level mixture-of-experts for efﬁcient\\ninference. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages\\n3577–3599, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun\\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume, Tomáš Koˇ\\nciský, Sebastian Ruder, Dani\\nYogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal\\ngeneralization in neural language models. In Advances in Neural Information Processing Systems.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard: Scaling giant models\\nwith conditional computation and automatic sharding. In International Conference on Learning\\nRepresentations.\\n21\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. Base\\nlayers: Simplifying training of large, sparse models. In Proceedings of the 38th International\\nConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,\\npages 6265–6274. PMLR.\\nShen Li. 2021. Getting started with distributed data parallel.\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom\\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\\nde Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven\\nGowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet\\nKohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code\\ngeneration with alphacode. arXiv preprint arXiv:2203.07814.\\nPierre Lison and Jörg Tiedemann. 2016. OpenSubtitles2016: Extracting large parallel corpora from\\nmovie and TV subtitles. In Proceedings of the Tenth International Conference on Language\\nResources and Evaluation (LREC’16), pages 923–929, Portorož, Slovenia. European Language\\nResources Association (ELRA).\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The\\nsemantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Associ-\\nation for Computational Linguistics, pages 4969–4983, Online. Association for Computational\\nLinguistics.\\nLi Lucy and David Bamman. 2021. Characterizing English variation across social media communities\\nwith BERT. Transactions of the Association for Computational Linguistics, 9:538–556.\\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. 2021.\\nTime waits for no one! analysis and challenges of temporal misalignment.\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\\nPotts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual\\nMeeting of the Association for Computational Linguistics: Human Language Technologies, pages\\n142–150, Portland, Oregon, USA. Association for Computational Linguistics.\\nMichael Matena and Colin Raffel. 2021. Merging models with ﬁsher-weighted averaging. arXiv\\npreprint arXiv:2111.09832.\\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\\n2017. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial\\nintelligence and statistics, pages 1273–1282. PMLR.\\nHesham Mostafa and Xin Wang. 2019. Parameter efﬁcient training of deep convolutional neural\\nnetworks by dynamic sparse reparameterization. In Proceedings of the 36th International Con-\\nference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages\\n4646–4655. PMLR.\\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019a. Justifying recommendations using distantly-\\nlabeled reviews and ﬁne-grained aspects. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP), pages 188–197, Hong Kong, China. Association for\\nComputational Linguistics.\\nJianmo Ni, Chenguang Zhu, Weizhu Chen, and Julian McAuley. 2019b. Learning to attend on essential\\nterms: An enhanced retriever-reader model for open-domain question answering. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 335–344,\\nMinneapolis, Minnesota. Association for Computational Linguistics.\\nXiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi\\nYang, and Bin Cui. 2021. Dense-to-sparse gate for mixture-of-experts.\\n22\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\\nthe 2019 Conference of the North American Chapter of the Association for Computational Lin-\\nguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational\\nLinguistics.\\nOu-Yang, Lucas. Newspaper3k.\\nJonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel\\nArtetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers.\\nJonas Pfeiffer, Ivan Vuli´\\nc, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based\\nFramework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online.\\nAssociation for Computational Linguistics.\\nProject Gutenberg. Project gutenberg.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.\\nJohn R. Rickford. 1985. Ethnicity as a sociolinguistic boundary. American Speech, 60:99.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. 2021. Hash layers for\\nlarge sparse models. In Advances in Neural Information Processing Systems.\\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical\\nreasoning abilities of neural models. In International Conference on Learning Representations.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,\\nand Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,\\nApril 24-26, 2017, Conference Track Proceedings. OpenReview.net.\\nTrieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning.\\nTwitter Academic API. Twitter academic api.\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick,\\nDarrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kinney, Yunyao Li, Ziyang Liu, William\\nMerrill, Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon\\nStilson, Alex Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm, Boya Xie, Douglas\\nRaymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. Cord-19: The covid-19\\nopen research dataset.\\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\\n2019. Neural text generation with unlikelihood training. In International Conference on Learning\\nRepresentations.\\nWikimedia Foundation. Wikimedia downloads.\\nDavid H Wolpert. 1992. Stacked generalization. Neural networks, 5(2):241–259.\\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\\nAri S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig\\nSchmidt. 2022a. Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy\\nwithout increasing inference time.\\n23\\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig\\nSchmidt. 2022b. Robust ﬁne-tuning of zero-shot models. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR), pages 7959–7971.\\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021. Bot-adversarial\\ndialogue for safe conversational agents. In Proceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\\npages 2950–2968, Online. Association for Computational Linguistics.\\nYelp Reviews. Yelp reviews.\\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\\nYejin Choi. 2019. Defending against neural fake news. In NeurIPS.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\\n2022. Opt: Open pre-trained transformer language models.\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In The IEEE International Conference on Computer Vision\\n(ICCV).\\n24\\nA\\nAppendix\\nDomain\\nCorpus\\n# Train (Eval.) Tokens\\nTRAINING\\n1B\\n30M NewsWire sentences (Chelba et al., 2014)\\n700M (10M)\\nCS\\n1.89M full-text CS papers from S2ORC (Lo et al., 2020)\\n4.5B (10M)\\nLEGAL\\n2.22M U.S. court opinions (Caselaw Access Project)\\n10.5B (10M)\\nMED\\n3.2M full-text medical papers from S2ORC (Lo et al., 2020)\\n9.5B (10M)\\nWEBTEXT†\\n8M Web documents (Gokaslan and Cohen, 2019)\\n6.5B (10M)\\nREALNEWS†\\n35M articles from REALNEWS Zellers et al. (2019)\\n15B (10M)\\nREDDIT\\nReddit comments from pushshift.io (Baumgartner et al., 2020)\\n25B (10M)\\nREVIEWS†\\n30M Amazon product reviews (Ni et al., 2019a)\\n2.1B (10M)\\nTotal\\n73.8B (80M)\\nDomain\\nCorpus\\n# Train (Eval.) Tokens\\nEVALUATION\\nACL PAPERS\\n1.5K NLP papers from ACL (Dasigi et al., 2021)\\n1M (1M)\\nBREAKING NEWS†\\n20K English news articles, scraped using (Ou-Yang, Lucas)\\n11M (1M)\\nCONTRACTS†\\n500 commercial legal contracts (Hendrycks et al., 2021)\\n1.5M (1M)\\nCORD-19\\n400K COVID-19 research papers (Wang et al., 2020)\\n60M (10M)\\nGITHUB\\n230K public Github code (Github Archive Project)\\n200M (10M)\\nGUTENBERG\\n3.2M copyright-expired books (Project Gutenberg)\\n3B (10M)\\nTWEETS†\\n1M English tweets from 2013-2018\\n8M (1M)\\nYELP REVIEWS†\\n6M Yelp restaurant reviews (Yelp Reviews)\\n600M (10M)\\nTable 9: Multi-domain data corpus used in §4 and §5. Details of this corpus, both training and\\nevaluation domains, including the size of our training and evaluation (i.e. validation and test) data in\\nwhitespace-separated tokens. We borrow these datasets from Gururangan et al. (2022). † indicates\\ndatasets we de-identify with regexes in Table 12. REDDIT was de-identiﬁed by Xu et al. (2021); we\\nuse their version. Meta researchers did not collect any of the Reddit or Twitter data and the data was\\nnot collected on behalf of Meta.\\n25\\nDomain\\nCorpus\\nBatch\\nTRAINING DOMAINS\\n1B\\nNewsWire sentences (Chelba et al., 2014)\\nB1\\nCS\\nFull-text CS papers from S2ORC (Lo et al., 2020)\\nB1\\nLEGAL\\nU.S. court opinions (Caselaw Access Project)\\nB1\\nMED\\nFull-text medical papers from S2ORC (Lo et al., 2020)\\nB1\\nOPENWEBTEXT†\\nOpenWebText Corpus (Gokaslan and Cohen, 2019)\\nB1\\nREALNEWS†\\nRealnews Corpus Zellers et al. (2019)\\nB1\\nREDDIT\\nReddit comments from pushshift.io (Baumgartner et al., 2020)\\nB1\\nREVIEWS†\\nAmazon product reviews (Ni et al., 2019a)\\nB1\\nPSYCHOLOGY\\nFull-text Psychology papers from S2ORC (Lo et al., 2020)\\nB2\\nCHEMISTRY\\nFull-text Chemistry papers from S2ORC (Lo et al., 2020)\\nB2\\nECONOMICS\\nFull-text Economics papers from S2ORC (Lo et al., 2020)\\nB2\\nENGINEERING\\nFull-text Engineering papers from S2ORC (Lo et al., 2020)\\nB2\\nMATERIALS\\nFull-text Materials papers from S2ORC (Lo et al., 2020)\\nB2\\nGEOLOGY\\nFull-text Geology papers from S2ORC (Lo et al., 2020)\\nB2\\nSOCIOLOGY\\nFull-text Sociology papers from S2ORC (Lo et al., 2020)\\nB2\\nBUSINESS\\nFull-text Business papers from S2ORC (Lo et al., 2020)\\nB2\\nC4\\nColossal Cleaned Common Crawl snapshot (Raffel et al., 2020)\\nB3\\nWIKIPEDIA\\n2022.03.01 Wikipedia snapshot (Wikimedia Foundation)\\nB3\\nSTACKOVERFLOW†\\nStackoverﬂow posts from The Pile (Gao et al., 2021)\\nB3\\nTWITTER†\\nEnglish tweets from 2013-2018 (Twitter Academic API)\\nB3\\nBIOLOGY\\nFull-text Biology papers from S2ORC (Lo et al., 2020)\\nB3\\nJAVASCRIPT\\nJavaScript code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB3\\nHTML\\nHTML code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB3\\nGUTENBERG\\nCopyright-expired books (Project Gutenberg)\\nB3\\nPOLITICAL SCIENCE\\nFull-text Political Science papers from S2ORC (Lo et al., 2020)\\nB3\\nENVIRONMENTAL SCIENCE\\nFull-text Environmental Science papers from S2ORC (Lo et al., 2020)\\nB3\\nPHYSICS\\nFull-text Physics Papers from S2ORC (Lo et al., 2020)\\nB3\\nMATHEMATICS\\nFull-text Mathematics papers from S2ORC (Lo et al., 2020)\\nB3\\nJAVA\\nJava code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB3\\nC\\nC code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB3\\nC++\\nC++ code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB3\\nGEOGRAPHY\\nFull-text Geography papers from S2ORC (Lo et al., 2020)\\nB3\\nSTACKEXCHANGE†\\nStackexchange posts from The Pile (Gao et al., 2021)\\nB4\\nPHILOSOPHY\\nFull-text Philosophy papers from S2ORC (Lo et al., 2020)\\nB4\\nCORD19\\nCOVID-19 research papers (Wang et al., 2020)\\nB4\\nHISTORY\\nFull-text History papers from S2ORC (Lo et al., 2020)\\nB4\\nBOOKS REVIEWS†\\nBook review subset of Amazon reviews (Ni et al., 2019b)\\nB4\\nART\\nFull-text Art papers from S2ORC (Lo et al., 2020)\\nB4\\nPYTHON\\nPython code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nC#\\nC# code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nPHP\\nPHP code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nMARKDOWN\\nMarkdown code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nCODE CONTESTS\\nProgramming challenge questions and answers generated by AlphaCode (Li et al., 2022)\\nB4\\nMOVIE AND TV REVIEWS†\\nMovie and TV review subset of Amazon reviews (Ni et al., 2019b)\\nB4\\nSUPREME COURT OPINIONS\\nSupreme Court Opinions from the Pile (Gao et al., 2021)\\nB4\\nHACKER NEWS†\\nHacker news comments from The Pile (Gao et al., 2021)\\nB4\\n2021 WMT NEWSCRAWL\\n2021 newswire sentences (Barrault et al., 2019)\\nB4\\nN/A SEMANTIC SCHOLAR\\nFull-text papers marked N/A from S2ORC (Lo et al., 2020)\\nB4\\nOPENSUBTITLES\\nMovie subtitles (Lison and Tiedemann, 2016)\\nB4\\nSTORIES\\nFiltered \"story-like\" Common Crawl documents (Trinh and Le, 2018)\\nB4\\nBOOKCORPUS\\nSelf-published novels (Zhu et al., 2015)\\nB4\\nRUBY\\nRuby code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nSPORTS AND OUTDOOR REVIEWS†\\nSports and Outdoor Reviews subset of Amazon Reviews (Ni et al., 2019b)\\nB4\\nUS PATENT OFFICE\\nUS Patents from The Pile (Gao et al., 2021)\\nB4\\nACL PAPERS\\nFull-text ACL papers from S2ORC (Lo et al., 2020)\\nB4\\nYELP REVIEWS†\\n6M Yelp restaurant reviews (Yelp Reviews)\\nB4\\nDEEPMIND MATHEMATICS\\nSynthetically-generated mathematical question answer pairs (Saxton et al., 2019)\\nB4\\nCLOTHING REVIEWS†\\nClothing Reviews subset of Amazon Reviews (Ni et al., 2019b)\\nB4\\nHOME AND KITCHEN REVIEWS†\\nHome and Kitchen subset of Amazon reviews (Ni et al., 2019b)\\nB4\\nGAMING REDDIT COMMENTS\\nReddit comments from pushshift.io, gaming-topic (Baumgartner et al., 2020)\\nB4\\nELECTRONIC REVIEWS\\nElectronic Reviews subset of Amazon Reviews (Yelp Reviews)\\nB4\\nSPORTS COMMENTS\\nReddit comments from pushshift.io, sports-topic (Baumgartner et al., 2020)\\nB4\\nGO\\nGo code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nCSS\\nCSS code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nB4\\nTable 10: 64 training domains that make up our multi-domain training corpus, with the batch that\\nthey appear in for our scaling study in §6. † indicates datasets we de-identify with regexes in Table 12.\\nREDDIT was de-identiﬁed by Xu et al. (2021); we use their version. Meta researchers did not collect\\nany of the Reddit or Twitter data and the data was not collected on behalf of Meta.\\n26\\nDomain\\nCorpus\\nEVALUATION DOMAINS\\nIMDB\\nIMDB reviews (Maas et al., 2011)\\nLEGAL CONTRACTS\\n500 commercial legal contracts (Hendrycks et al., 2021)\\nCSCAREERQUESTIONS†\\nReddit comments from pushshift.io, restricted to /r/cscareerquestions (Baumgartner et al., 2020)\\nINDIA†\\nReddit comments from pushshift.io, restricted to /r/india (Baumgartner et al., 2020)\\nENRON†\\n6M Yelp restaurant reviews (Yelp Reviews)\\nHIPHOPHEADS†\\nReddit comments from pushshift.io, restricted to /r/hiphopheads (Baumgartner et al., 2020)\\nCONGRESSIONAL BILLS\\nCongressional bills from BillSum (Kornilova and Eidelman, 2019)\\nIRELAND SPEECHES\\nIrish parliamentary speeches, 1919-2013 (Herzog and Mikhaylov, 2017)\\nSQL\\nSQL code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nRUST\\nRust code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nPERL\\nPERL code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nTEX\\nTeX code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nFORTRAN\\nFORTRAN code from Github [MIT, BSD, Apache 2.0] (Huggingface)\\nCOVID19 TWEETS†\\nTweets with #COVID-19 hashtag (Twitter Academic API)\\nTOEFL EXAM RESPONSES\\nTOEFL exam responses (Blanchard et al., 2013)\\nBREAKING NEWS†\\n20K English news articles, scraped using Newspaper3K (Ou-Yang, Lucas)\\nTable 11: 32 domains that make up our novel domain corpus, including the size of our training and\\nevaluation (i.e. validation and test) data, in whitespace-separated tokens. We borrow these datasets\\nfrom Gururangan et al. (2022). † indicates datasets we de-identify with regexes in Table 12. Meta\\nresearchers did not collect any of the Reddit or Twitter data and the data was not collected on behalf\\nof Meta.\\nCategory\\nLink to Regex\\nDummy Token\\nEmail\\nhttps://regex101.com/r/ZqsF9x/1\\n<EMAIL>\\nDART\\nhttps://regex101.com/r/0tQ6EN/1\\n<DART>\\nFB User ID\\nhttps://regex101.com/r/GZl5EZ/1\\n<FB_USERID>\\nPhone Number\\nhttps://regex101.com/r/YrDpPD/1\\n<PHONE_NUMBER>\\nCredit Card Number\\nhttps://regex101.com/r/9NTO6W/1\\n<CREDIT_CARD_NUMBER>\\nSocial Security Number\\nhttps://regex101.com/r/V5GPNL/1\\n<SSN>\\nUser handles\\nhttps://regex101.com/r/vpey04/1\\n<USER>\\nTable 12: De-identiﬁcation schema. We de-identify text using the regexes provided in the above links\\nfor the categories listed.\\n27\\n125M – 16 GPUs – 80k updates\\nT-LM\\nT-LM\\nDEMIX\\nELMFOREST\\nRANDOM\\nELMFOREST\\nUNBALANCED\\n(random init)\\nENSEMBLE\\n(seed init)\\n125M\\n125M\\n512M\\n1B\\n1B\\n1B\\nTrain\\n19.8\\n20.7\\n17.7\\n18.0\\n23.0\\n17.2\\nNovel\\n25.6\\n26.4\\n23.1\\n24.1\\n26.0\\n22.4\\nAll\\n22.7\\n23.5\\n20.4\\n21.0\\n24.7\\n19.8\\n350M – 32 GPUs – 32k updates\\nT-LM\\nT-LM\\nDEMIX\\nELMFOREST\\nRANDOM\\nELMFOREST\\nUNBALANCED\\n(random init)\\nENSEMBLE\\n(seed init)\\n350M\\n350M\\n1.8B\\n2.8B\\n2.8B\\n2.8B\\nTrain\\n16.3\\n16.7\\n15.0\\n15.3\\n19.9\\n14.7\\nNovel\\n20.8\\n21.2\\n19.9\\n21.3\\n23.1\\n18.6\\nAll\\n18.5\\n19.0\\n17.5\\n18.3\\n21.5\\n16.7\\n750M – 64 GPUs – 24k updates\\nT-LM\\nT-LM\\nDEMIX\\nELMFOREST\\nRANDOM\\nELMFOREST\\nUNBALANCED\\n(random init)\\nENSEMBLE\\n(seed init)\\n750M\\n750M\\n3.8B\\n6B\\n6B\\n6B\\nTrain\\n14.7\\n14.9\\n13.5\\n14.4\\n17.4\\n13.4\\nNovel\\n19.3\\n19.8\\n17.7\\n19.3\\n20.9\\n16.7\\nAll\\n17.0\\n17.4\\n15.6\\n16.9\\n19.2\\n15.0\\n1.3B – 128 GPUs – 12k updates\\nT-LM\\nT-LM\\nDEMIX\\nELMFOREST\\nRANDOM\\nELMFOREST\\nUNBALANCED\\n(random init)\\nENSEMBLE\\n(seed init)\\n1.3B\\n1.3B\\n7B\\n10.4B\\n10.4B\\n10.4B\\nTrain\\n14.2\\n15.0\\n13.7\\n13.3\\n17.4\\n13.0\\nNovel\\n18.4\\n19.5\\n17.6\\n17.8\\n20.4\\n16.3\\nAll\\n16.3\\n17.3\\n15.6\\n15.6\\n18.9\\n14.6\\nTable 13: ELMFORESTs trained with BTM outperform all baselines and ensemble variations\\nacross multiple model scales. Average test-set perplexity (↓) for each model scale (125M, 350M,\\n750M, 1.3B parameters) across the 8 training, 8 novel, and all 16 domains described in §4.1. Total\\ncompute budget (in update numbers) and GPU usage are shown for each model size, and total\\nparameters are shown for each model type at each size. TRANSFORMER-LMs (here, abbreviated to\\nT-LM) trained without balancing between data domains performs worse than T-LM trained with\\ndata balancing; hence, we only compare against the balanced T-LM setting in §4. For ELMFOREST,\\nwe show results with 50% dense training.\\n28\\n8\\n16\\n32\\n64\\n# Domains\\n14.0\\n15.0\\n16.0\\n17.0\\nPerplexity\\nB1\\n8\\n16\\n32\\n64\\n# Domains\\n12.0\\n14.0\\n16.0\\n18.0\\nPerplexity\\nB2\\n8\\n16\\n32\\n64\\n# domains\\n10.0\\n12.0\\n14.0\\n16.0\\n18.0\\nPerplexity\\nB3\\n8\\n16\\n32\\n64\\n# Domains\\n10.0\\n12.0\\n14.0\\n16.0\\n18.0\\nperplexity\\nB4\\n350M/ELM ELMforest\\n1.3B Transformer-LM\\nFigure 10: As the number of training domains increase, ELMFORESTs retain or improve per-\\nformance, while compute-matched TRANSFORMER-LMs degrade. Average test set perplexity\\nover each batch of training domains (B1 - B4). For the TRANSFORMER-LM experiment, we train\\na new TRANSFORMER-LM from scratch on 8 (B1), 16 (B1 + B2), 32 (B1 + B2 + B3), and 64 (B1\\n+ B2 + B3 + B4) domains, for 6144 GPUs hours each. For the ELMFOREST experiment, we use\\nBTM to train on the batches incrementally. We observe that TRANSFORMER-LMs suffer as one\\ntrains on more domains while ELMFORESTs retain or improve performance as domains are added\\nincrementally. This reﬂects the \"curse of multilinguality\" phenomenon discovered for multilingual\\ntransformer LMs (Pfeiffer et al., 2022).\\n29\\n', 'source_name': 'Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models', 'source_url': 'https://arxiv.org/abs/2208.03306'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Benefits_of_ELMs.pdf #14\n",
      "{'content': 'Exploring the Beneﬁts of Training Expert Language Models\\nover Instruction Tuning\\nJoel Jang 1 2 Seungone Kim 1 Seonghyeon Ye 1 2 Doyoung Kim 1 Lajanugen Logeswaran 2 Moontae Lee 2 3\\nKyungjae Lee 2 Minjoon Seo 1\\nAbstract\\nRecently, Language Models (LMs) instruction-\\ntuned on multiple tasks, also known as multitask-\\nprompted ﬁne-tuning (MT), have shown the capa-\\nbility to generalize to unseen tasks. Previous work\\nhas shown that scaling the number of training\\ntasks is the key component in making stronger MT\\nLMs. In this work, we report an unexpected ﬁnd-\\ning that an expert LM ﬁne-tuned on just a single\\ntask can outperform an MT LM trained with 300+\\ndifferent tasks on 11 different unseen datasets and\\non 13 datasets of the BIG-bench benchmark by\\na mean accuracy of 3.20% and 1.29%, respec-\\ntively. This ﬁnding casts doubt on the previously\\nheld belief that simply scaling the number of tasks\\nmakes stronger MT LMs. Leveraging this ﬁnding,\\nwe further show that this distributed approach of\\ntraining a separate expert LM per training task in-\\nstead of a single MT LM for zero-shot inference\\npossesses many beneﬁts including (1) avoiding\\nnegative task transfer that often occurs during\\ninstruction tuning, (2) being able to continually\\nlearn new tasks without having to re-train on previ-\\nous tasks to avoid catastrophic forgetting, and (3)\\nshowing compositional capabilities when merging\\nindividual experts together. The code is available\\nat https://github.com/joeljang/ELM.\\n1. Introduction\\nRecent works show pretrained Language Models (LMs) that\\nhave been ﬁne-tuned on multiple tasks with instructions\\n(prompted instances), also known as multitask-prompted\\nﬁne-tuned LMs and referred to as MT LMs in this work,\\ncan generalize to unseen tasks without task-speciﬁc ﬁne-\\ntuning (Wei et al., 2021; Sanh et al., 2021; Chung et al.,\\nWork done while JJ and SY were interning at LG AI Research.\\n1KAIST 2LG AI Research 3University of Illinois Chicago. Corre-\\nspondence to: Joel Jang <joeljang@kaist.ac.kr>.\\ncosmos_qa\\nsocial_i_qa\\ndream\\nFigure 1. Mean accuracy performance of Expert LMs (each trained\\non a single task) on 11 unseen datasets compared to an instruction-\\ntuned LM, T0-3B. Results show some Expert LMs surpassing T0-\\n3B, challenging the commonly held belief that simply scaling the\\ntotal number of training tasks is the key component to enhancing\\nthe capability of MT LMs.\\n2022; Ye et al., 2022b; Ouyang et al., 2022; Wang et al.,\\n2022a; Muennighoff et al., 2022). This paper raises some\\nquestions regarding the current paradigm of training MT\\nLMs and is mainly divided into two parts. In Part 1, we\\nreport an unexpected ﬁnding regarding expert LMs (trained\\nonly on a single task) compared to MT LMs. In Part 2, we\\nleverage the ﬁnding to highlight some of the beneﬁts of\\nexpert LMs over MT LMs.\\nPart 1 (Section 5)\\nPreviously, the key component to en-\\nhancing the unseen task generalization performance of MT\\nLMs was thought to be scaling the total number of tasks\\narXiv:2302.03202v2  [cs.CL]  9 Feb 2023\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nT5\\nT5\\nT5\\nPhase 1: Training of Experts\\nPhase 2: Zero-shot Inference via Retrieval-of-Experts (RoE)\\nT5\\nSummarization Expert\\nThe picture appeared on the wall of a Poundland store on \\nWhymark Avenue [...] How would you rephrase that in a \\nfew words?\\nSummarization\\nGraffiti artist Banksy is believed to be \\nbehind [ ... ]\\nQuestion Answering\\nQA Expert\\nI know that the answer to \"What team did the Panthers \\ndefeat?\" is in \"The Panthers finished the regular season \\n[    ]\" Can you tell me what it is?\\nSentiment Analysis\\nSentiment Expert\\nReview: We can here on a Saturday night and luckily it \\nwasn\\'t as packed as I thought it would be [ ... ] On a \\nscale of 1 to 5, I would give this a \\nArizona Cardinals\\n4\\nNatural Language Inference\\nQA Expert\\nSuppose \"The banker contacted the professors and the \\nathlete\". Can we infer that \"The banker contacted the \\nprofessors\"?\\nYes\\nExpert n\\nExpert 2\\nExpert 1\\nInstance 2\\nInstance 1\\nInstance n\\nExpert Library\\nKeys\\nValues\\n❄\\n❄\\n❄\\n❄\\n🔥\\n🔥\\n🔥\\nDense \\nRetriever\\n❄\\n❄\\n🔥\\n❄\\n= Trainable\\n= Frozen\\nFigure 2. Independent training and Retrieval-of-Experts (RoE) for zero-shot task generalization. During training, only the additional\\nadapters (experts) are trained while the backbone LM is frozen. After training separate experts per training task, we construct an Expert\\nLibrary that stores samples of the training task as keys, and the speciﬁc expert id as values. During zero-shot inference, the most relevant\\nexpert is retrieved for an unseen task.\\nused in training (Wei et al., 2021; Chung et al., 2022; Wang\\net al., 2022a). However, in this work, we show that training\\na single expert LM on one1 out of the 300+ tasks used to\\ntrain an MT LM (T0-3B (Sanh et al., 2021)) can outperform\\nthe MT LM by a non-trivial margin on 24 unseen tasks on\\nmean accuracy.\\nSpeciﬁcally, following the same experimental setup (train-\\ning and evaluation) as T0-3B (Sanh et al., 2021), one of the\\nmost widely used MT LM, we ﬁrst train expert LMs for\\neach given training task (296) by freezing the underlying\\nLM and updating adapters (Houlsby et al., 2019). We report\\na ﬁnding that shows 7 out of the 296 experts surpass T0-\\n3B on the capability to generalize to unseen tasks on mean\\naccuracy (shown in Figure 1). Using the top performing\\nexpert for all of the unseen task evaluation tasks surpasses\\nT0-3B by a mean accuracy of 3.20% and 1.29% on 11 un-\\nseen datasets and 13 datasets of the BIG-Bench benchmark,\\nrespectively. We also show that applying a simple mecha-\\nnism to retrieve relevant experts for each individual unseen\\ntask results in comparable performance to T0-3B. Consider-\\ning the signiﬁcant room for improvement when retrieving\\n1Training task: cosmos qa, Prompt Name: no prompt text from\\nBach et al. (2022).\\nthe best-performing expert for each unseen task (+11.94%\\ncompared to T0-3B), these results imply that choosing the\\nright expert rather than na¨\\nıvely utilizing a single MT LM for\\nall of the unseen tasks can be a more efﬁcient and effective\\napproach.\\nPart 2 (Section 6)\\nLeveraging the ﬁnding of expert LMs\\nshowing improved unseen task generalization capability, we\\nhighlight three other advantages of training multiple expert\\nLMs for each task and retrieving the relevant expert during\\ninference (shown in Figure 2) compared to training MT\\nLMs.\\n#1. MT LMs do not show the optimal performance for seen\\ntasks because of negative task transfer, where learning mul-\\ntiple tasks at once hinders the learning of some speciﬁc\\ntasks (Aghajanyan et al., 2021; Asai et al., 2022a; Zhang\\net al., 2022). Expert LMs, on the other hand, are not subject\\nto negative task transfer (Levine et al., 2022) since each task\\nis learned independently; We show our approach of select-\\ning relevant experts during inference results in a +10.4%\\nmean accuracy improvement on validation datasets of the\\n36 training tasks compared to T0-3B.\\n#2. MT LMs are susceptible to catastrophic forgetting (Mc-\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nCloskey & Cohen, 1989) of previous tasks when learning\\nnew tasks and require re-training on previous tasks to miti-\\ngate forgetting (Chakrabarty et al., 2022). Results show our\\ndistributed (training individual tasks in an independent man-\\nner) approach results in absolutely no degradation of seen\\ntasks, even when adding the 8 new experts to the Expert\\nLibrary, without re-training on previous tasks when learning\\n8 new generative tasks.\\n#3. We show that MT LMs show poor ability in performing\\ncomposition of previously learned tasks given via concate-\\nnation of the corresponding instructions as a single com-\\npositional instruction. On the other hand, we show that\\nmerging the two experts trained on the individual tasks with\\nmT5-3B (Xue et al., 2021) as the underlying pre-trained\\nLM results in an expert that can outperform its MT LM\\ncounterpart, mT0-3B (Muennighoff et al., 2022), by a mean\\nROUGE-L score of +2.71 on 5 novel compositional tasks\\n(summarization & translation). Details of the merging mech-\\nanism are provided in Section 3.3.\\n2. Related Work\\n2.1. Multitask Prompted Fine-tuning of Language\\nModels\\nSeveral studies have demonstrated that multitask ﬁne-tuning\\nmoderately sized LMs with instructions, also referred to\\nas instruction tuning, enables zero-shot task generalization.\\nSpeciﬁcally, Sanh et al. (2021); Wang et al. (2022a) have\\nshown that scaling the number of training tasks, the number\\nof prompts per task, and the size of the LM helps boost\\nzero-shot task generalization performance. In addition to\\nscaling these aspects, Chung et al. (2022) include Chain-of-\\nThought (Wei et al., 2022) tasks during instruction tuning,\\nreaching state-of-the-art performance on zero-shot and few-\\nshot settings with PaLM 540B (Chowdhery et al., 2022)\\nas the underlying LM. Lin et al. (2022) improve MT LMs\\nby adapting MT LMs on subsets of the training data re-\\ntrieved given a few unlabeled examples of the unseen task.\\nOuyang et al. (2022) adapt MT LMs to align with human\\npreferences through reinforcement learning. Muennighoff\\net al. (2022) include multilingual tasks to show cross-lingual\\ngeneralization capability. Ye et al. (2022b) ﬂip the instruc-\\ntion and label space to enhance generalization capability to\\nnovel unseen labels. Asai et al. (2022b) utilize instruction\\ntuning to construct a general-purpose retrieval system. Simi-\\nlarly, Su et al. (2022) utilize instruction tuning to construct\\na general-purpose embedding model that can be used to\\nperform different unseen tasks requiring text embeddings.\\nWhile previous literature has mostly asserted that the pri-\\nmary key component of MT LMs is scaling the total number\\nof training tasks, in this paper, we propose an alternative per-\\nspective and instead show experimental results and ﬁndings\\nthat the feature of the tasks may be a more critical factor\\n(analysis provided in Section 5); Similar ﬁndings are shown\\nin the setting of few-shot adaptation (Chan et al., 2022) as\\nwell.\\n2.2. Retrieving task-speciﬁc embeddings\\nRetrieving task-speciﬁc parameters has the advantage of\\nrapid target task adaptation, especially for low-resource\\nscenarios (Vu et al., 2022; Asai et al., 2022a; Ye et al., 2022a;\\nQin & Eisner, 2021; Wang et al., 2022b; Bari et al., 2022).\\nVu et al. (2022) show that retrieving an optimal source\\nsoft prompt leads to better initialization for adapting to the\\ntarget task. Asai et al. (2022a) also focus on retrieval of soft\\nprompts for initialization for the target task but utilize the\\nidea of attention weights to effectively interpolate between\\nmultiple training soft prompts. Similarly, Ye et al. (2022a)\\nextend this idea of retrieving soft prompts, but utilize an MT\\nLM as the underlying LM and do not ﬁne-tune the LM to the\\ntarget task, performing the target task in a zero-shot manner.\\nOur work is motivated by Ye et al. (2022a), but proposes to\\nreplace the instruction tuning stage altogether, using vanilla\\npretrained LMs as the underlying LM instead of MT LMs.\\nWe accomplish this by training experts whereas previous\\nwork trained soft prompts on top of MT LMs.\\n2.3. Distributed Training of Language Models\\nRecent work has shown the possibilities and beneﬁts of\\ndistributed training of LMs. Li et al. (2022) have shown that\\nit is possible to merge individual LMs pretrained on different\\nsubsets (domains) of the training corpora to construct a\\nsingle LM that shows lower overall perplexity compared\\nto an LM trained on all of the corpora at once. Another\\nline of work that explores merging individually ﬁne-tuned\\nLMs is Wortsman et al. (2022b), where they merge LMs\\nﬁne-tuned on the same task with different conﬁgurations\\nto boost performance. Similarly, Wortsman et al. (2022a)\\nmerge LMs ﬁne-tuned on the same task, but with subsets\\nof the training data for efﬁciency. Don-Yehiya et al. (2022)\\nexplore merging LMs ﬁne-tuned on different tasks to make\\na multitask ﬁne-tuned LM in a distributed manner, which\\nhas many beneﬁts including federated learning (McMahan\\net al., 2017).\\nOther interesting extensions of distributed LM training in-\\nclude performing task arithmetic with task vectors (Ilharco\\net al., 2022), training and performing inference of several bil-\\nlion parameter LMs on distributed compute (Borzunov et al.,\\n2022), introducing language-speciﬁc modules for growing\\nthe total capacity of multilingual LMs (Pfeiffer et al., 2022),\\nﬁnding theoretical guarantees of why merging works (Fran-\\nkle et al., 2020; Ainsworth et al., 2022) and proposing novel\\nmethods of merging model weights (Matena & Raffel, 2021).\\nIn our work, we also show the beneﬁts of distributed LM\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nMCQA\\nSumm.\\nSocial\\nIQA\\nIMDB\\nContext: {context}\\nQuestion: {question}\\nPossible answers: \\nA: {a1} B: {a2} C: {a3}\\nWrite a summary of \\nthe following articles: \\n{article}\\nSummary: {summary}\\nCategory level\\nDataset level\\nPrompt level\\n…\\nAll MT\\ndataset\\nSenti.\\nDREAM\\nSocial_i\\n_qa\\nMulti \\nNews\\nIMDB\\n…\\n…\\nFigure 3. The hierarchy of all the training datasets used to train\\nMT LMs. In this work, we explore training Dataset level Experts\\n(DE) and Prompt level Experts (PE).\\ntraining by showing that the capability of expert LMs can\\nbe further ampliﬁed through merging individual experts.\\n3. Expert Language Models\\nIn this section, we describe the framework of our proposed\\nmethod. We train each expert by training adapters for each\\ntraining task (Section 3.1). During inference, we retrieve the\\nrelevant experts from the Expert Library (Section 3.2). We\\nadditionally explore the effect of merging experts to observe\\nthe beneﬁts of distributed training (Section 3.3).\\n3.1. Training Experts\\nFor training the experts, we mainly explore parameter-\\nefﬁcient ﬁne-tuning via adapters while freezing the under-\\nlying LM to train individual experts. We train experts for\\neach task with the corresponding prompts and denote the\\nresulting experts as Prompt Experts (PE).2 We also explore\\ntraining experts for each dataset, which consists of multi-\\nple training prompts, referred to as Dataset Experts (DE).\\nFor training DE, instead of utilizing a parameter-efﬁcient\\nﬁne-tuning approach (adapters), we instead simply train the\\nentire LM to observe the merging capability of expert LMs.3\\nFigure 3 shows the hierarchy of the training datasets and the\\nlevel at which PE and DE are trained on.\\nAdapters\\nWe apply a parameter-efﬁcient method of repre-\\nsenting experts by training additional adapters while freez-\\ning the original parameters (Houlsby et al., 2019). Speciﬁ-\\ncally, given a standard Transformer LM with l layers, input\\nsequence X containing T tokens, the output for a single\\n2Each prompt (instruction) is referred to as tasks, following\\nChung et al. (2022).\\n3Experimental results show that merging adapter experts does\\nnot lead to improved positive task transfer on mean accuracy\\n(shown in Section 5).\\nlayer hl\\n1:T is calculated by\\nhl\\nt = FFNd(ul\\nt) + ul\\nt,\\n(1)\\nul\\n1:T = SELF-ATT(hl−1\\n1:T ) + hl−1\\n1:T ,\\n(2)\\nwhere hl\\nt is the hidden state of t-token after the l-th layer,\\nSELF-ATT( · ) is the self-attention module, and FFNd( · ) is\\nthe feed-forward network with hidden dimensions d. When\\nﬁne-tuning the LM with an adapter expert, each layer be-\\nfore the self-attention layer (Equation 1) changes into the\\nfollowing format:\\nhl\\nt = FFNe(ul\\nt) + FFNd(ul\\nt) + ul\\nt,\\n(3)\\nwhere e represents the hidden dimension of the adapter feed-\\nforward network. When using adapters to represent experts,\\nparameters of FFNe are the only trainable parameters and\\nthe rest of the parameters in the LM are frozen.\\n3.2. Retrieval-of-Experts (RoE)\\nAfter independent (distributed) training of individual ex-\\nperts, we retrieve one of the experts to use during infer-\\nence (Ye et al., 2022a). To this end, we construct an Expert\\nLibrary and use dense retrieval to retrieve a relevant expert\\nfrom the library to use during inference.\\nExpert Library\\nWe ﬁrst construct the Expert Library.\\nThis library contains keys that are each embedding rep-\\nresentations of a single instance from the training tasks\\nand values that are unique ids of the corresponding trained\\nexperts. For each unique expert, S training instances are\\nrandomly sampled and stored in the library. This results in\\n[S × # of experts] entries in the Expert Library. To get the\\nembedding representation of the training instances, we em-\\nploy a simple Sentence Transformer (Reimers & Gurevych,\\n2019) as the dense retriever.4 For the text format of the\\ntraining instance that is given to the embedding model as in-\\nput, we simply concatenate the answer choice (e.g. Yes|No,\\nA|B|C|D) to the Prompted Input. The answer choice for gen-\\nerative tasks is given as ‘None’. We report ablation results of\\nvarying the text format given as the input to the embedding\\nmodel in Appendix B.\\nRetrieval\\nFollowing the approach of Lin et al. (2022); Ye\\net al. (2022a), given a target task during inference, we ﬁrst\\nrandomly select Q instances from the target task5. Next,\\nwe use the same text format (concatenation of Prompted\\nInput and Answer Choice) and the same embedding model\\n4We explore other text embedding models for the retriever\\nsuch as Sentence-T5, SimCSE, INSTRUCTOR, etc., in Appendix\\nB. Sentence Transformer shows the best performance among the\\nembedding models.\\n5We assume a scenario where we can perform batch-inference.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nused to construct the Expert Library to obtain embedding\\nrepresentations of each of the Q target queries. We then\\nuse MIPS (maximum inner product search) on our Expert\\nLibrary to identify the most similar training instance (key)\\nfor each query instance, resulting in a total of Q correspond-\\ning experts (value). We select the most frequently retrieved\\nexpert as the expert for solving the given target task.\\n3.3. Merging of Experts\\nPrevious work has shown the possibility of distributed\\nmultitask ﬁne-tuning by merging individually ﬁne-tuned\\nLMs (Don-Yehiya et al., 2022). Along with selecting the\\nmost retrieved expert, we observe how merging fully ﬁne-\\ntuned LMs (DE) affects the generalization performance on\\nthe unseen tasks.\\nA fully ﬁne-tuned LM can be represented in the form of a\\nvector τd = θd −θpre where θpre represent the full parame-\\nters of the vanilla pretrained LM and θd represents the full\\nparameters of the LM ﬁne-tuned on the training dataset d (Il-\\nharco et al., 2022). The formula for merging of N experts\\ncan be denoted as follows:\\nθnew = θpre + (\\nN\\nX\\ni\\nλiτi)\\n(4)\\nwhere λi =\\n1\\nN as default if not stated otherwise. Note\\nthat when λi = 1\\nN , it results in merging experts uniformly.\\nIn some cases, however, performance was optimal when\\nP\\ni λi > 1 and each λi (representing the importance to\\nplace on τi) and was set to a different value determined\\nusing a held-out validation dataset following Ilharco et al.\\n(2022). A concrete example is provided in Appendix C.\\n4. Experimental Setup\\nTraining Setup\\nFollowing the setting of Sanh et al.\\n(2021), we use a total of 36 training datasets of T0 for train-\\ning our experts.6 For each dataset, we use all of the prompts\\nused to train T0 from the Promptsource Library (Bach et al.,\\n2022) which results in a total of 296 prompts to train the\\ncorresponding experts (∼8 prompts per training dataset).\\nThis results in 36 Dataset Experts (DE) represented via fully\\nﬁne-tuned LMs, and 296 Prompt Experts (PE) via adapter\\ntraining. For each individual ﬁne-tuning, we randomly sam-\\nple K = 50, 000 training instances for each classiﬁcation\\n6The original T0 (Sanh et al., 2021) paper includes 38 training\\ndatasets. However, we could not load 4 datasets from the Hugging-\\nface Dataset library: adversarial qa/dbidaf, adversarial qa/dbert,\\nadversarial qa/droberta, and duorc/SelfRC. Instead, we utilize the\\nadversarial qa/adversarialQA dataset and also additionally train\\non commonsense qa dataset which is a variant of the cos e dataset,\\nresulting in a total of 36 training datasets.\\ntask and K = 10, 000 for each generative task.7 We use\\nthe LM-adapted T5 model (Lester et al., 2021) checkpoint\\nas our base model, and train for 5 epochs with a constant\\nlearning rate of 1e-4 for both adapter ﬁne-tuning and full\\nLM ﬁne-tuning. For the construction of the Expert Library,\\nmuch smaller S = 100 training instances are randomly\\nsampled for each expert following Ye et al. (2022a).\\nEvaluation Setup\\nWe evaluate the baseline MT LMs (T0-\\n3B, T0-11B) and our proposed method (T5-3B + DE/PE) on\\nthe same evaluation setting as the original T0 paper (Sanh\\net al., 2021): 11 unseen datasets that can be categorized\\ninto 4 task categories and on 13 datasets from BIG-Bench\\nbenchmark (Srivastava et al., 2022), which are diverse and\\nchallenging tasks that are not encountered during training.8\\nWe further evaluate the models on 8 new generative tasks9\\nthat were not included in the original T0 paper evaluation\\nsetting. We use a rank classiﬁcation evaluation by selecting\\nthe label option with higher log-likelihood following Brown\\net al. (2020); Sanh et al. (2021) for the classiﬁcation tasks.\\nFor the generative tasks, we use the ROUGE-L score as the\\ndefault metric if not stated otherwise. The details of each\\ntraining and evaluation dataset are provided in Appendix A.\\nDuring inference, we set Q = 32 for applying our Retrieval-\\nof-Expert (RoE) mechanism. We do not separately perform\\nablations of S and Q, simply following the optimal setting\\nof Ye et al. (2022a).\\n5. Expert LMs Can Generalize to Unseen\\nTasks\\nIn this section, we show experimental results of expert LMs\\nand show their potential for becoming a new paradigm over\\ninstruction tuning. Since this is a fairly novel approach of en-\\ndowing LMs the capability to generalize to unseen tasks, we\\nfocus on providing proof-of-concept of some core research\\nquestions instead of making head-to-head comparisons with\\nall of the baselines. We leave other extensive comparisons\\nand exhaustive ablations for future work.\\nMain Results\\nTable 1 shows the evaluation results on\\nthe 11 unseen datasets, Table 2 shows the results on the\\n13 unseen BIG-Bench tasks, and Table 3 shows the results\\non the 8 unseen generative tasks. Results from the three\\ntables show that (1) a single PE signiﬁcantly outperforms\\nT0-3B, (2) the ROE (ORC.) outperforms other baselines\\n7We train with less number of instances for the generative tasks\\nbecause the training generative tasks required longer max token\\nlength, and thus longer training time.\\n8We exclude NOVEL CONCEPTS task from the original T0\\nevaluation setting because it is a multi-label classiﬁcation task.\\nMultiple prompts are evaluated for each evaluation dataset.\\n9The dataset details of the 8 new generative tasks are provided\\nin Appendix A.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nMethod\\nNLI\\nSentence Completion\\nCoreference Resolut.\\nWSD\\nTotal Avg.\\nRTE\\nCB\\nAN. R1\\nAN. R2\\nAN. R3\\nCOPA\\nHellasw.\\nStoryC.\\nWinogr.\\nWSC\\nWiC\\nT0-11B\\n80.83\\n70.12\\n43.56\\n38.68\\n41.26\\n90.02\\n33.58\\n92.40\\n59.94\\n61.45\\n56.58\\n60.76\\nGPT-3(175B)\\n63.50\\n46.40\\n34.60\\n35.40\\n34.50\\n91.00\\n78.90\\n83.20\\n70.20\\n65.40\\n45.92\\n59.00\\nT0-3B\\n60.61\\n48.81\\n35.10\\n33.27\\n33.52\\n75.13\\n27.18\\n84.91\\n50.91\\n65.00\\n51.27\\n51.43\\nT5(3B) + COS PE\\n49.53\\n49.52\\n36.21\\n36.11\\n36.38\\n89.63\\n43.77\\n97.06\\n56.65\\n57.02\\n49.01\\n54.63\\nT5(3B) + PE W/ ROE\\n64.01\\n43.57\\n35.49\\n34.64\\n31.22\\n79.25\\n34.60\\n86.33\\n61.60\\n62.21\\n52.97\\n53.48\\nT5(3B) + PE W/ ROE (ORC.)\\n70.32\\n70.12\\n40.02\\n40.11\\n42.07\\n92.88\\n55.00\\n97.47\\n64.40\\n65.77\\n58.90\\n63.37\\nTable 1. Evaluation performance on 11 different unseen datasets categorized into 4 task categories. PE represents Prompt Experts. PE\\nW/ ROE (ORC.) represents retrieving the best-performing (oracle) expert for each evaluation task. COS PE represents the PE trained\\non COSMOS-QA with the prompt NO-PROMPT-TEXT which showed the highest mean accuracy on the 11 unseen tasks. PE W/ ROE\\nrepresents Retrieval-of-Expert (RoE) for each individual unseen task. Note that PE adds 100M additional parameters while freezing the\\n3B paramters of T5 during training. The best comparable performances are bolded and second best underlined.\\nDataset (metric)\\nT0\\nCOS PE\\nT0\\nGPT-3\\nPALM\\n3B\\n3B\\n11B\\n175B\\n540B\\nKnown Un.\\n47.83\\n58.70\\n65.22\\n60.87\\n56.52\\nLogic Grid\\n32.10\\n30.70\\n33.67\\n31.20\\n32.10\\nStrategy.\\n53.23\\n42.36\\n54.67\\n52.30\\n64.00\\nHindu Kn.\\n34.86\\n51.43\\n42.86\\n32.57\\n56.00\\nMovie D.\\n53.22\\n46.72\\n57.33\\n51.40\\n49.10\\nCode D.\\n53.33\\n66.67\\n51.67\\n31.67\\n25.00\\nConcept\\n67.25\\n72.92\\n71.72\\n26.78\\n59.26\\nLanguage\\n14.94\\n25.95\\n18.33\\n15.90\\n20.10\\nVitamin\\n58.18\\n46.55\\n57.33\\n12.30\\n14.10\\nSyllogism\\n52.27\\n50.00\\n48.33\\n50.50\\n49.90\\nMisconcept.\\n52.05\\n47.03\\n52.97\\n47.95\\n47.47\\nLogical\\n45.33\\n42.40\\n54.67\\n23.42\\n24.22\\nWinowhy\\n44.29\\n44.33\\n55.00\\n51.50\\n45.30\\nBIG-bench AVG\\n46.84\\n48.13\\n51.06\\n37.57\\n41.77\\nTable 2. Evaluation performance on 13 BIG-bench tasks. The best\\ncomparable performances are bolded.\\nby a non-trivial margin, and (3) our simple ROE approach\\noutperforms T0-3B on the classiﬁcation tasks, but not on\\ngenerative tasks. Details of each ﬁnding are provided in the\\nfollowing paragraphs.\\n#1. In Table 1, surprisingly, T5(3B) + COS PE, which is a\\nPrompt Expert (PE) that is only trained on a single prompt\\n(‘no prompt text’ prompt of COSMOS-QA dataset), outper-\\nforms its MT LM counterpart (T0-3B) on 8 out of 11 eval-\\nuation datasets and +3.20% on mean accuracy. Prior work\\nshows that scaling the total number of training tasks dur-\\ning instruction tuning leads to better generalization; in our\\ncase, training an expert on a single task outperforms an LM\\ntrained on 300+ tasks (T0-3B). This ﬁnding is bolstered in\\nTable 2 where the same COS PE that shows the highest mean\\naccuracy for the 11 unseen tasks outperforms T0-3B by\\n+1.29% on the mean accuracy performance on 13 datasets\\nof BIG-Bench Benchmark and in Table 3 where T5(3B)\\n+ SAM PE, which is a PE trained on (‘Given the above\\ndialogue write a summary’ prompt of SAMSUM dataset),\\noutperforms T0-3B by +6.83 mean score on the 8 generative\\ntasks.\\n#2. In Table 1, we can see that T5(3B) + PE W/ ROE\\n(ORC.), which is the upper-bound performance of choosing\\nthe best-performing expert based on the accuracy for each\\nunseen task, outperforms T0-3B, much larger GPT-3(175B)\\nand T0-11B by +11.94%, +4.37% and +2.61%, respectively,\\non the mean accuracy. T5(3B) + PE W/ ROE (ORC.) also\\noutperforms T0-3B by +13.69 mean score on the 8 unseen\\ngenerative tasks shown in Table 3. This means that ROE has\\na potential for strong unseen task generalization when the\\nproper expert is chosen.\\n#3. T5(3B) + PE W/ ROE, which is a simple method of\\nretrieving an expert for each unseen task leveraging an\\noff-the-shelf retriever (Sentence Transformer (Reimers &\\nGurevych, 2019)), outperforms T0-3B on 8 out of 11 evalu-\\nation datasets and by +2.05% on mean accuracy. However,\\nT5(3B) + PE W/ ROE underperforms T0-3B by -5.37 mean\\nscore on the 8 unseen generative tasks (Table 3). Consid-\\nering that T5(3B) + PE W/ ROE still shows a signiﬁcant\\nperformance gap compared to retrieving the best-performing\\nexpert (T5(3B) + PE W/ ROE (ORC.)), there is much room\\nfor improvement on the retriever side. One way to close the\\ngap is to train a supervised retrieval model, which we leave\\nfor future work.\\nMerging of Experts\\nTable 4 shows the merging capabil-\\nity of expert LMs. The ﬁrst three rows show the merging\\nresults of PE which are represented in the form of adapters.\\nWhile COS&SOC PE (MER.), which is an expert con-\\nstructed by performing uniform merging with COS PE and\\nSOC PE10 shows positive task transfers for some evaluation\\ndatasets (Copa & Story Cloze), not all of the results are the\\nbest or second best (RTE, Hellaswag, & Winogrande). This\\nmeans that there was a negative task transfer when merging\\nthe adapter experts.\\nThus, in order to further explore the merging capability of\\nexpert LMs, we train DE via full LM ﬁne-tuning, known\\nto be effective in previous literature (Ilharco et al., 2022),\\n10SOC PE is a PE that was trained on SOCIAL I QA with prompt\\n‘no prompt text’ that showed the second highest mean accuracy\\non the 11 unseen tasks other than PE trained with COSMOS-QA.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nMethod\\nwiki auto\\nHGen\\nhaiku\\ncovid qa\\neli5\\nemdg\\nesnli\\ntwitter\\nTotal\\n(BLEU)\\n(ROUGE)\\n(ROUGE)\\n(BS)\\n(BS)\\n(BS)\\n(BS)\\n(BS)\\nAvg.\\nT0-3B\\n21.76\\n33.29\\n19.93\\n50.00\\n59.86\\n47.76\\n42.80\\n28.40\\n37.98\\nT5(3B) + SAM PE\\n30.69\\n25.49\\n25.25\\n49.93\\n47.94\\n51.36\\n58.28\\n69.55\\n44.81\\nT5(3B) + PE W/ ROE\\n3.88\\n35.55\\n26.53\\n33.52\\n33.66\\n49.90\\n28.61\\n49.22\\n32.61\\nT5(3B) + PE W/ ROE (ORC.)\\n31.56\\n35.55\\n30.16\\n52.49\\n63.20\\n58.36\\n60.02\\n82.08\\n51.67\\nTable 3. Evaluation performance on 8 unseen generative tasks. SAM PE represents the PE trained on SAMSUM with the prompt GIVEN\\nTHE ABOVE DIALOGUE WRITE A SUMMARY which showed the highest mean score on the 8 unseen generative tasks. The best comparable\\nperformances are bolded and second best underlined.\\nMethod\\nNLI\\nSentence Completion\\nCoreference Resolut.\\nWSD\\nTotal Avg.\\nRTE\\nCB\\nAN. R1\\nAN. R2\\nAN. R3\\nCOPA\\nHellasw.\\nStoryC.\\nWinogr.\\nWSC\\nWiC\\nT5(3B) + COS PE\\n49.53\\n49.52\\n36.21\\n36.11\\n36.38\\n89.63\\n43.77\\n97.06\\n56.65\\n57.02\\n49.01\\n54.63\\nT5(3B) + SOC PE\\n61.26\\n38.81\\n33.16\\n33.63\\n33.46\\n90.50\\n37.21\\n97.09\\n55.28\\n50.00\\n50.11\\n52.77\\nT5(3B) + COS&SOC PE (MER.)\\n49.10\\n39.40\\n33.80\\n34.28\\n34.18\\n91.63\\n36.29\\n97.25\\n55.06\\n51.25\\n49.62\\n51.99\\nT5(3B) + COS DE\\n59.71\\n57.62\\n33.45\\n33.93\\n34.54\\n90.00\\n36.58\\n96.29\\n53.37\\n42.88\\n49.91\\n53.48\\nT5(3B) + SOC DE\\n65.52\\n48.69\\n35.20\\n35.39\\n37.11\\n83.25\\n30.38\\n87.18\\n54.27\\n54.62\\n51.39\\n53.00\\nT5(3B) + COS&SOC DE (MER.)\\n60.43\\n54.17\\n35.01\\n34.53\\n35.52\\n91.25\\n35.59\\n96.73\\n54.33\\n42.88\\n50.05\\n53.68\\nTable 4. Evaluation performance on 11 different unseen datasets categorized into 4 task categories. PE represents Prompt Experts. COS PE\\nrepresents the PE trained on COSMOS-QA dataset and NO PROMPT TEXT prompt and SOC PE represents the PE trained on SOCIAL-I-QA\\ndataset and SHOW CHOICES AND GENERATE ANSWER prompt. COS&SOC PE (MER.) represents expert constructed by performing\\nuniform merging with the COS PE and SOC PE. COS DE represents the DE trained on the COSMOS-QA dataset with all of the prompts\\nand SOC DE represents the DE trained on SOCIAL-I-QA on all of the propmts. COS&SOC DE (MER.) represents expert constructed by\\nperforming merging with the COS DE and SOC DE. The best comparable performances are bolded and second best underlined.\\nand merge them as shown in the last three rows in Table 4.\\nCOS DE (COSMOS-QA) and SOC DE (SOCIAL-I-QA) are\\nthe two highest performing DE based on the mean accuracy\\nperformance on the 11 unseen tasks. While COS&SOC DE\\n(MER.) shows only a +0.20% enhancement compared to\\nCOS DE on mean accuracy, it still shows either the best or\\nsecond best performance compared to the individual COS\\nand SOC DE. This implies that merging the two experts\\nresults in a composition of abilities. This opens up new\\npossibilities of leveraging the merging of experts to unlock\\nnew capabilities which are further explored in Section 6\\nwith the composition of instructions.\\nOverall, Table 4 shows that merging with adapters does not\\nalways result in positive task transfer while merging with\\nfull parameters seems to. Thus, future work should explore\\ndeveloping more parameter-efﬁcient methods of merging\\nexpert LMs since always training and utilizing the entire\\nLM weights is computationally demanding.\\nAnalysis of Experts\\nFigure 1 shows the mean accuracies\\nof all the PE and DE results on the 11 unseen datasets. We\\nhighlight three main analyses from the ﬁgure and from the\\ntables.\\nFirst, among the 8 training task categories, Multiple-Choice\\nQuestion Answering (MCQA) training tasks generally show\\nthe strongest generalization capability. We hypothesize this\\nto be the case because all of the 11 evaluation datasets\\nare classiﬁcation tasks and require some form of question\\nanswering via instructions. This extends the ﬁndings of\\nKhashabi et al. (2020) that Multiple-Choice Question An-\\nswering (MCQA) generalizes well to not only different\\nformat QA tasks, but also different types of tasks such as\\nnatural language inference, story completion, coreference\\nresolution, and word sense disambiguation as well.\\nSecond, among the 36 training datasets, 3 datasets con-\\nsistently ensure high performance for both PE and DE:\\nCOSMOS-QA (Huang et al., 2019), SOCIAL-I-QA (Sap et al.,\\n2019), and DREAM (Sun et al., 2019). All three datasets are\\ncommonsense reasoning datasets, which have been consid-\\nered to be crucial for generalization to unseen tasks (Lourie\\net al., 2021). We provide the full ranking of the PE and DE\\nfor the 11 unseen tasks shown in Figure 1 in Appendix D.\\nLastly, T5(3B) + SAM PE which is a PE trained on SAM-\\nSUM (Gliwa et al., 2019), a dataset with abstractive dialogue\\nsummaries, shows the best mean score on the 8 unseen\\ngenerative tasks in Table 3, outperforming T0-3B by +6.83\\nmean score. However, the same PE shows one of the low-\\nest ranks for the 11 unseen (classiﬁcation) tasks (shown in\\nAppendix D) underperforming T0-3B by -9.15% mean accu-\\nracy. This shows that there is no free lunch: a PE that shows\\nhigh mean performance for unseen generative tasks do not\\nshow high mean performance for unseen classiﬁcation tasks.\\nThis also implies that it is more-so important to retrieve the\\ncorrect expert dynamically depending on the given context\\n(target task).\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nMethod\\nMCQA (12)\\nSenti. (5)\\nTopic C. (3)\\nParaph. (3)\\nSTS (2)\\nSumm. (5)\\nEQA (4)\\nCBQA (2)\\nTotal Avg.\\n(ACC)\\n(ACC)\\n(ACC)\\n(ACC)\\n(ROUGE-L)\\n(ROUGE-L)\\n(ROUGE-L)\\n(ROUGE-L)\\nT0-3B\\n46.97\\n66.40\\n59.99\\n76.63\\n41.90\\n33.10\\n28.79\\n24.67\\n47.30\\nT0-11B\\n51.32\\n64.03\\n60.95\\n73.64\\n45.42\\n33.10\\n41.20\\n30.37\\n50.00\\nT5(3B)+ PE W/ ROE\\n58.95\\n70.18\\n96.52\\n72.97\\n47.57\\n33.14\\n30.36\\n51.89\\n57.70\\nT5(3B)+ PE W/ ROE (ORC.)\\n56.28\\n84.52\\n96.91\\n79.34\\n47.94\\n35.40\\n40.34\\n43.24\\n60.50\\nTable 5. Evaluation performance on 300 sample instances from each validation dataset of the 36 training tasks categorized into 8 task\\ncategories. The number in the () represents the # of datasets in the task category. The best comparable performances are bolded and\\nsecond best underlined.\\nMethod\\nSeen\\nUnseen\\nGen\\nAvg.\\nAvg.\\nAvg.\\nBefore Continual Learning\\nUnseen\\nT0-3B\\n47.30\\n51.43\\n37.98\\nT5(3B) + PE W/ ROE\\n57.70\\n53.48\\n32.61\\nAfter Continual Learning\\nSeen\\nCT0-3B\\n47.54\\n50.84\\n54.52 (↑)\\nT5(3B) + PE+ W/ ROE\\n57.70\\n53.33\\n55.60 (↑)\\nTable 6. Seen Avg. represents the mean accuracy of the 36 seen\\ntasks in Table 5. Unseen Avg. represents the mean accuracy of\\nthe 11 unseen tasks in Table 1. Gen Avg. represents the mean\\nscore of the 8 (unseen) generative tasks in Table 3. (BS) represents\\nBertScore. PE+ represents augmenting the Expert Library with 8\\nPE trained on the 8 generative tasks. We use the LM checkpoint\\nfrom Chakrabarty et al. (2022) for CT0-3B, T0-3B continually\\nﬁne-tuned on the 8 generative tasks is a sequential manner while\\nrehearsing previous tasks. The best comparable performances are\\nbolded.\\n6. Beneﬁts of Expert LMs over MT LMs\\nIn this section, we highlight the 3 main beneﬁts of expert\\nLMs and ROE over MT LMs.\\nSeen Task Performance\\nFirst, we show that expert LMs\\nare less susceptible to negative task transfer by comparing\\nthe performance of T5(3B) + PE W/ ROE on the validation\\ndatasets of the 36 training datasets with two MT LMs, T0-3B\\nand T0-11B. As shown in Table 5, our distributed approach\\noutperforms T0-3B and T0-11B by +10.40% and +7.70%\\non mean accuracy, respectively.\\nThis is because since evaluation is done with seen instruc-\\ntions, our simple retrieval mechanism is highly likely to\\nselect the best-performing expert from the Expert Library,\\nshowing comparable performance to T5(3B) + PE W/ ROE\\n(ORC.). In fact, T5(3B) + PE W/ ROE retrieves the PE\\nfrom the same training dataset on 280 out of 296 seen tasks,\\nand the PE trained with both the same prompt and dataset\\n(oracle) on 185 out of 296 seen tasks.\\nContinual Learning of New Tasks\\nIn some scenarios\\nwhen we want to additionally ﬁne-tune LMs on additional\\ndatasets after model deployment, making ﬁnetuned LMs\\ncontinual learners is important (Chakrabarty et al., 2022).\\nThis is because performing instruction tuning on the entire\\nset of original and additional tasks in each update would\\nlead to heavy computation. Previous work mitigates this is-\\nsue through a rehearsal-based method, continually training\\nthe instruction-tuned LM on samples of the original and\\nadditional tasks (Chakrabarty et al., 2022). However, this\\napproach (1) assumes that we have access to the original\\ndatasets and (2) still leads to additional computational over-\\nhead, especially when scaling the total number of seen tasks\\nduring instruction tuning.\\nWe show that we can accomplish the same feat through dis-\\ntributed training of experts without any access to original,\\nseen datasets by training separate experts for each additional\\ntask and simply adding them to the Expert Library. Speciﬁ-\\ncally, we show the comparison between continually training\\nan MT LM (T0-3B) which is referred to as CT0-3B through\\na rehearsal-based approach, and our distributed approach on\\n8 new generative tasks in Table 6. The 8 generative tasks\\nfor continual learning were chosen following the previous\\nwork (Chakrabarty et al., 2022).\\nThe table shows that our distributed approach results in ab-\\nsolutely no degradation of performance for the seen task, a\\nminor (-0.15%) degradation for unseen tasks, and superior\\nmean performance (+1.08) for the 8 target tasks compared\\nto the MT LM counterpart, outperforming CT0-3B on 7\\nout of the 8 target tasks. This shows that without any ac-\\ncess to original datasets or heavy computational cost, our\\ndistributed approach is mostly able to retain its original\\nability (seen & unseen) as well as outperform CT0-3B on\\nthe target tasks. We leave scaling the number of new target\\ntasks and how our distributed approach performs against its\\ninstruction-tuned counterpart for future work.\\nCompositional Instructions\\nPrior work has shown the\\nneed for performing compositional instructions (Lo-\\ngeswaran et al., 2021; Corona et al., 2021; Khot et al., 2022).\\nFor example, we can give the following instruction to the\\nLM: “Write a summary of the following English text and\\ntranslate the sentence into Korean.” where “Write a sum-\\nmary of the following English text.” and “Translate the sen-\\ntence into Korean.” are two separate instructions seen during\\ntraining. To test this compositional capability, especially in\\na multi-lingual setting, we utilize the mT0-3B (Muennighoff\\net al., 2022) as our MT LM and evaluate the composition of\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nMethod\\nxsum\\nxsum\\nxsum\\nxsum\\nxsum\\nTotal\\nen→ko\\nen→es\\nen→zh\\nen→fr\\nen→ja\\nAvg.\\nMT0-3B\\n1.84\\n16.14\\n6.74\\n20.37\\n3.44\\n9.71\\nMT5-3B + MER. EX.\\n8.23\\n16.97\\n2.40\\n20.55\\n13.98\\n12.43\\nTable 7. Comparison of compositional abilities of both summariza-\\ntion and translation task for MT LM (MT0-3B) and our distributed\\napproach (MT5-3B + MER. EX.) which involves merging the\\ncorresponding experts. ROUGE-L is used as the evaluation met-\\nric. ko, es, zh, fr, ja stand for Korean, Spanish, Chinese, French,\\nand Japanese, respectively. The best comparable performances are\\nbolded\\nTask\\nEXAMPLE\\nxsum\\nMT0: El asesinato de un ni˜\\nno de tres a˜\\nnos de edad en Francia fue atribuido a su hermano\\nen→es\\nmayor.\\nM.E.: La polic´\\nıa francesa arrest´\\no a cuatro miembros de la familia del ni˜\\nno por su presunta\\nimplicaci´\\non en el crimen, que ocurri´\\no el 17 de septiembre en la casa familiar en Mulhouse,\\nal oeste del pa´\\nıs, y de m´\\nas de 100.000 habitantes.\\nG.T.: La polic ıa que investiga el fallecimiento de un ni no de 9 a nos en Francia,\\nsupuestamente golpeado hasta la muerte, arrest o este martes a cuatro miembros de su\\nfamilia, seg un declaraciones de los ﬁscales a la agencia de noticias AFP.\\nxsum\\nMT0: Le pr´\\nesident de la R´\\nepublique d´\\nemocratique du Malawi a ´\\net´\\ne condamn´\\ne `\\na cinq ans\\nen→fr\\nde prison pour complicit´\\ne dans l’assassinat de Paul Mphwiyo.\\nM.E.: Le 8 novembre 2013, l’ancien ministre de la Justice du Malawi, M. Ralph\\nKasambara, a ´\\net´\\ne arrˆ\\net´\\ne apr`\\nes avoir commis le meurtre de Paul Paul MPHWIYO, le\\ndirecteur du budget du minist`\\nere des Finances.\\nG.T.: La Haute Cour de Lilongwe a condamn e mardi l’ancien ministre de la Justice,\\nRaphael Kasambara, ‘a 13 ans d’emprisonnement et de travaux forc es pour complicit\\ne de meurtre\\nxsum\\nMT0: 副裁Meng Ship 和副Meng Teng 被加拿大警方逮捕,被指控侵犯公民利。\\nen→ja\\nM.E.: カナダの最高裁判所(CFO)は、\\n12月に逮捕された創設者の息子であり、\\n副社長はカナダ政府とカナダ移民局(CBSA)と警察を告訴した。\\nG.T.: 中の通信機器最大手、\\n華技術(ファウェイ)の最高財務責任者(CFO)の孟晩\\n舟副長は、\\n昨年12月にカナダ局がアメリカの要請で自分を逮捕したことについて、\\nカナダを提訴した。\\nxsum\\nMT0: The Sierra Leonean nurse who was isolated for seven hours at the airport terminal\\nen→zh\\nhas said that the isolation experience is ”terrifying” and may make other medical workers\\nreluctant to go to West Africa.\\nM.E.: 一名感染埃博拉病毒的生Craig Spencer目前正在大都院接受隔治,但只得到了\\n一粮食棒的。\\nG.T.: 一位曾在塞拉利治埃博拉病人的美士返回美后被隔察,批了瓦克机待的方式\\nxsum\\nMT0: Korean peninsula has had its warmest winter since 1973, according to the\\nen→ko\\nMeteorological Administration.\\nM.E.: 지난해1월은국내에서가장따뜻한겨울이었다.\\nG.T.: 올겨울, 추위가실종됐다. 따뜻한날씨가이어지면서눈구경도어려워졌다.\\nTable 8. Example outputs from the 5 Compositional Tasks given\\nthe input “Write a summary of the following English text and trans-\\nlate the sentence into [Language]: [English Summary].”. M.E.\\nstands for Merged Experts. G.T. stands for Ground Truth. es, fr, ja,\\nzh, and ko stand for Spanish, French, Japanese, Chinese, and Ko-\\nrean, respectively. The actual input for the examples are provided\\nin Appendix C.\\nperforming 5 novel compositional tasks of summarization\\nand translation. To explore the beneﬁts of merging experts\\nfor performing compositional instructions, we perform 6\\nfull ﬁne-tuning with mT5-3B (Xue et al., 2021) as the under-\\nlying vanilla pretrained multilingual LLM: We use XSUM to\\ntrain one English Summarization expert and use ﬁve trans-\\nlation pairs in TATOEBA (en→es, en→fr, en→ja, en→zh,\\nen→ko) to train the corresponding ﬁve translation experts.\\nDuring inference, we merge the summarization expert with\\neach of the ﬁve translation experts11. Note that both XSUM\\n11We provide the speciﬁc conﬁgurations used for merging such\\nas the λi values for each task vector τi and the training and valida-\\ntion stats in Appendix C\\nand TATOEBA are part of the training tasks used during\\ninstruction tuning of mT0-3B.\\nEvaluation results on the ﬁve compositional tasks are shown\\nin Table 7. Our distributed approach, MT5-3B + MER. EX.,\\noutperforms its MT LM counterpart, MT0-3B on 4 out\\nof the 5 tasks and by a mean ROUGLE-L score of +2.71;\\nThis is due to a signiﬁcant performance gap for the tasks\\ninvolving low-resource languages (Korean and Japanese)\\nbecause the low-resource languages are protected from nega-\\ntive transfer when doing distributed training. Cherry-picked\\noutput examples of the MT LM and the merged experts are\\nprovided in Table 8.\\n7. Limitations and Discussions\\nWhile we highlight some of the major drawbacks of instruc-\\ntion tuning and propose an alternative approach of instead\\ntraining and retrieving experts in this paper, we do not per-\\nform experimental results over MT LMS that have more\\nthan >11B parameters. For example, MT LMs with >11B\\nparameters may be less susceptible to negative task trans-\\nfer because of increased model capacity. Also, during the\\ninference of unseen tasks, our retrieval mechanism assumes\\nbatch inference (i.e. having access to 32 samples of the target\\ntasks without labels). Finally, when showing the composi-\\ntional instruction experiments, we assume the two optimal\\nexperts could be retrieved from the compositional instruc-\\ntion (concatenation of the two seen instructions) given as\\nthe input along with the evaluation instance. This might not\\nnecessarily be the case with more complex, compositional\\ninstructions, which might require a separate decomposition\\nstage. We instead focus on showing the possibility merging\\nexperts can bring and leave developing novel methods of\\nretrieving the optimal experts during inference for future\\nwork.\\n8. Conclusion\\nIn this work, we provide an interesting ﬁnding that expert\\nLMs trained on single tasks show strong generalization capa-\\nbility to unseen tasks, even surpassing MT LMs trained on\\nmultiple tasks (300+) by a non-trivial margin. We leverage\\nthis capability and show three main beneﬁts of training and\\nretrieving experts for inference over MT LMs, demonstrat-\\ning that our proposed distributed approach is more robust\\nagainst negative task transfer, more adapt at learning new\\ntasks, and can perform compositional instructions. To this\\nend, we urge the research community to further explore\\ndistributed and collaborative training of experts which may\\nhave other future beneﬁts including efﬁciency, privacy, and\\npersonalization not explicitly explored in this paper.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nACKNOWLEDGMENTS\\nWe thank Colin Raffel, Sungdong Kim, Sejune Joo, Miy-\\noung Ko, Eunbi Choi, Hyunji Lee, Dongkeun Yoon, Yoon-\\njoo Lee, and Yujin Kim for the useful discussion and feed-\\nback on the paper.\\nReferences\\nAghajanyan, A., Gupta, A., Shrivastava, A., Chen, X.,\\nZettlemoyer, L., and Gupta, S. Muppet: Massive multi-\\ntask representations with pre-ﬁnetuning.\\nIn Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 5799–5811, On-\\nline and Punta Cana, Dominican Republic, November\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.emnlp-main.468. URL https://\\naclanthology.org/2021.emnlp-main.468.\\nAinsworth, S. K., Hayase, J., and Srinivasa, S. Git re-basin:\\nMerging models modulo permutation symmetries. arXiv\\npreprint arXiv:2209.04836, 2022.\\nAsai, A., Salehi, M., Peters, M. E., and Hajishirzi, H. At-\\ntempt: Parameter-efﬁcient multi-task tuning via atten-\\ntional mixtures of soft prompts. In Proceedings of the\\n2022 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 6655–6672, 2022a.\\nAsai, A., Schick, T., Lewis, P., Chen, X., Izacard, G., Riedel,\\nS., Hajishirzi, H., and Yih, W.-t. Task-aware retrieval with\\ninstructions. arXiv preprint arXiv:2211.09260, 2022b.\\nBach, S., Sanh, V., Yong, Z. X., Webson, A., Raffel, C.,\\nNayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T.,\\nAlyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-david, S.,\\nXu, C., Chhablani, G., Wang, H., Fries, J., Al-shaibani,\\nM., Sharma, S., Thakker, U., Almubarak, K., Tang, X.,\\nRadev, D., Jiang, M. T.-j., and Rush, A. PromptSource:\\nAn integrated development environment and repository\\nfor natural language prompts. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational Lin-\\nguistics: System Demonstrations, pp. 93–104, Dublin, Ire-\\nland, May 2022. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2022.acl-demo.9. URL https:\\n//aclanthology.org/2022.acl-demo.9.\\nBari, M. S., Zhang, A., Zheng, S., Shi, X., Zhu, Y., Joty, S.,\\nand Li, M. Spt: Semi-parametric prompt tuning for multi-\\ntask prompted learning. arXiv preprint arXiv:2212.10929,\\n2022.\\nBartolo, M., Roberts, A., Welbl, J., Riedel, S., and Stene-\\ntorp, P. Beat the AI: Investigating adversarial human\\nannotation for reading comprehension. Transactions of\\nthe Association for Computational Linguistics, 8:662–\\n678, 2020a. doi: 10.1162/tacl a 00338. URL https:\\n//aclanthology.org/2020.tacl-1.43.\\nBartolo, M., Roberts, A., Welbl, J., Riedel, S., and Stene-\\ntorp, P.\\nBeat the ai: Investigating adversarial human\\nannotation for reading comprehension.\\nTransactions\\nof the Association for Computational Linguistics, 8:\\n662–678, 2020b. doi: 10.1162/tacl\\\\ a\\\\ 00338. URL\\nhttps://doi.org/10.1162/tacl_a_00338.\\nBorzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M.,\\nBelkada, Y., Chumachenko, A., Samygin, P., and Raffel,\\nC. Petals: Collaborative inference and ﬁne-tuning of large\\nmodels. arXiv preprint arXiv:2209.01188, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nCamburu, O.-M., Rockt¨\\naschel, T., Lukasiewicz, T., and\\nBlunsom, P. e-snli: Natural language inference with nat-\\nural language explanations. In Bengio, S., Wallach, H.,\\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-\\nnett, R. (eds.), Advances in Neural Information Process-\\ning Systems 31, pp. 9539–9549. Curran Associates, Inc.,\\n2018.\\nChakrabarty, T., Scialom, T., and Muresan, S. Fine-tuned\\nlanguage models can be continual learners. In Challenges\\n& Perspectives in Creating Large Language Models,\\n2022. URL https://openreview.net/forum?\\nid=rbMH3zBIbc.\\nChan, J. S., Pieler, M., Jao, J., Scheurer, J., and Perez, E.\\nFew-shot adaptation works with unpredictable data. arXiv\\npreprint arXiv:2208.01009, 2022.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\\nChuang, Y.-S., Dangovski, R., Luo, H., Zhang, Y., Chang, S.,\\nSoljaˇ\\nci´\\nc, M., Li, S.-W., Yih, S., Kim, Y., and Glass, J. Dif-\\nfcse: Difference-based contrastive learning for sentence\\nembeddings. In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 4207–4218, 2022.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,\\nW., Li, E., Wang, X., Dehghani, M., Brahma, S., et al.\\nScaling instruction-ﬁnetuned language models. arXiv\\npreprint arXiv:2210.11416, 2022.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nCorona, R., Fried, D., Devin, C., Klein, D., and Darrell,\\nT. Modular networks for compositional instruction fol-\\nlowing. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies, pp.\\n1033–1040, Online, June 2021. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n81.\\nURL https://aclanthology.org/2021.\\nnaacl-main.81.\\nDagan, I., Glickman, O., and Magnini, B. The pascal recog-\\nnising textual entailment challenge. In Machine learning\\nchallenges workshop, pp. 177–190. Springer, 2005.\\nDe Marneffe, M.-C., Simons, M., and Tonhauser, J. The\\ncommitmentbank: Investigating projection in naturally\\noccurring discourse. In proceedings of Sinn und Bedeu-\\ntung, volume 23, pp. 107–124, 2019.\\nDon-Yehiya, S., Venezian, E., Raffel, C., Slonim, N., Katz,\\nY., and Choshen, L.\\nCold fusion: Collaborative de-\\nscent for distributed multitask ﬁnetuning. arXiv preprint\\narXiv:2212.01378, 2022.\\nFabbri, A., Li, I., She, T., Li, S., and Radev, D. Multi-news:\\nA large-scale multi-document summarization dataset and\\nabstractive hierarchical model. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Lin-\\nguistics, pp. 1074–1084, Florence, Italy, July 2019. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/\\nP19-1102.\\nURL https://aclanthology.org/\\nP19-1102.\\nFan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J.,\\nand Auli, M. ELI5: Long form question answering. In\\nProceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pp. 3558–3567,\\nFlorence, Italy, July 2019. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P19-1346. URL\\nhttps://aclanthology.org/P19-1346.\\nFrankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear\\nmode connectivity and the lottery ticket hypothesis. In\\nInternational Conference on Machine Learning, pp. 3259–\\n3269. PMLR, 2020.\\nGao, T., Yao, X., and Chen, D. Simcse: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the\\n2021 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 6894–6910, 2021.\\nGliwa, B., Mochol, I., Biesek, M., and Wawer, A. SAMSum\\ncorpus: A human-annotated dialogue dataset for abstrac-\\ntive summarization. In Proceedings of the 2nd Workshop\\non New Frontiers in Summarization, pp. 70–79, Hong\\nKong, China, November 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/D19-5409. URL\\nhttps://aclanthology.org/D19-5409.\\nGraff, D., Kong, J., Chen, K., and Maeda, K. English giga-\\nword. Linguistic Data Consortium, Philadelphia, 4(1):\\n34, 2003.\\nHasan, T., Bhattacharjee, A., Islam, M. S., Samin, K., Li,\\nY.-F., Kang, Y.-B., Rahman, M. S., and Shahriyar, R. Xl-\\nsum: Large-scale multilingual abstractive summarization\\nfor 44 languages, 2021.\\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\\nGelly, S. Parameter-efﬁcient transfer learning for NLP.\\nIn Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-\\nings of the 36th International Conference on Machine\\nLearning, volume 97 of Proceedings of Machine Learn-\\ning Research, pp. 2790–2799. PMLR, 09–15 Jun 2019.\\nURL https://proceedings.mlr.press/v97/\\nhoulsby19a.html.\\nHuang, L., Le Bras, R., Bhagavatula, C., and Choi, Y. Cos-\\nmos QA: Machine reading comprehension with contex-\\ntual commonsense reasoning. In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Conference\\non Natural Language Processing (EMNLP-IJCNLP), pp.\\n2391–2401, Hong Kong, China, November 2019. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/\\nD19-1243. URL https://aclanthology.org/\\nD19-1243.\\nIlharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S.,\\nSchmidt, L., Hajishirzi, H., and Farhadi, A. Editing mod-\\nels with task arithmetic. arXiv preprint arXiv:2212.04089,\\n2022.\\nJiang, C., Maddela, M., Lan, W., Zhong, Y., and Xu, W.\\nNeural CRF model for sentence alignment in text sim-\\npliﬁcation.\\nIn Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics, pp.\\n7943–7960, Online, July 2020. Association for Compu-\\ntational Linguistics.\\ndoi: 10.18653/v1/2020.acl-main.\\n709. URL https://aclanthology.org/2020.\\nacl-main.709.\\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O.,\\nClark, P., and Hajishirzi, H. Uniﬁedqa: Crossing format\\nboundaries with a single qa system. In Findings of the\\nAssociation for Computational Linguistics: EMNLP 2020,\\npp. 1896–1907, 2020.\\nKhot, T., Clark, P., Guerquin, M., Jansen, P., and Sabharwal,\\nA. Qasc: A dataset for question answering via sentence\\ncomposition. arXiv:1910.11473v2, 2020.\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,\\nClark, P., and Sabharwal, A. Decomposed prompting:\\nA modular approach for solving complex tasks. arXiv\\npreprint arXiv:2210.02406, 2022.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nLebret, R., Grangier, D., and Auli, M. Neural text generation\\nfrom structured data with application to the biography\\ndomain. In Proceedings of the 2016 Conference on Empir-\\nical Methods in Natural Language Processing, pp. 1203–\\n1213, Austin, Texas, November 2016. Association for\\nComputational Linguistics. doi: 10.18653/v1/D16-1128.\\nURL https://aclanthology.org/D16-1128.\\nLehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,\\nD., Mendes, P. N., Hellmann, S., Morsey, M., van Kleef,\\nP., Auer, S., and Bizer, C. Dbpedia - a large-scale, mul-\\ntilingual knowledge base extracted from wikipedia. Se-\\nmantic Web, 6:167–195, 2015.\\nLester, B., Al-Rfou, R., and Constant, N. The power of scale\\nfor parameter-efﬁcient prompt tuning. arXiv preprint\\narXiv:2104.08691, 2021.\\nLevesque, H., Davis, E., and Morgenstern, L. The winograd\\nschema challenge. In Thirteenth international confer-\\nence on the principles of knowledge representation and\\nreasoning, 2012.\\nLevine, Y., Dalmedigos, I., Ram, O., Zeldes, Y., Jan-\\nnai, D., Muhlgay, D., Osin, Y., Lieber, O., Lenz, B.,\\nShalev-Shwartz, S., et al.\\nStanding on the shoul-\\nders of giant frozen language models. arXiv preprint\\narXiv:2204.10019, 2022.\\nLi, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T.,\\nSmith, N. A., and Zettlemoyer, L. Branch-train-merge:\\nEmbarrassingly parallel training of expert language mod-\\nels. arXiv preprint arXiv:2208.03306, 2022.\\nLi, X. and Roth, D.\\nLearning question classiﬁers.\\nIn\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://www.\\naclweb.org/anthology/C02-1150.\\nLin, B. Y., Zhou, W., Shen, M., Zhou, P., Bhagavat-\\nula, C., Choi, Y., and Ren, X. CommonGen: A con-\\nstrained text generation challenge for generative com-\\nmonsense reasoning.\\nIn Findings of the Association\\nfor Computational Linguistics: EMNLP 2020, pp. 1823–\\n1840, Online, November 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.\\n165. URL https://aclanthology.org/2020.\\nfindings-emnlp.165.\\nLin, B. Y., Tan, K., Miller, C., Tian, B., and Ren, X. Unsuper-\\nvised cross-task generalization via retrieval augmentation.\\narXiv preprint arXiv:2204.07937, 2022.\\nLin, K., Tafjord, O., Clark, P., and Gardner, M.\\nRea-\\nsoning over paragraph effects in situations.\\nIn Pro-\\nceedings of the 2nd Workshop on Machine Reading for\\nQuestion Answering, pp. 58–62, Hong Kong, China,\\nNovember 2019. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/D19-5808. URL https:\\n//aclanthology.org/D19-5808.\\nLogeswaran, L., Carvalho, W. T., and Lee, H. Learning\\ncompositional tasks from language instructions. In Deep\\nRL Workshop NeurIPS 2021, 2021. URL https://\\nopenreview.net/forum?id=CoMFsP9Vs-k.\\nLourie, N., Le Bras, R., Bhagavatula, C., and Choi, Y. Uni-\\ncorn on rainbow: A universal commonsense reasoning\\nmodel on a new multitask benchmark. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence, vol-\\nume 35, pp. 13480–13488, 2021.\\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\\nA. Y., and Potts, C. Learning word vectors for sentiment\\nanalysis. In Proceedings of the 49th Annual Meeting\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies, pp. 142–150, Portland, Ore-\\ngon, USA, June 2011. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nP11-1015.\\nMatena, M. and Raffel, C. Merging models with ﬁsher-\\nweighted averaging. arXiv preprint arXiv:2111.09832,\\n2021.\\nMcAuley, J. J. and Leskovec, J. Hidden factors and hid-\\nden topics: understanding rating dimensions with re-\\nview text.\\nIn Yang, Q., King, I., Li, Q., Pu, P., and\\nKarypis, G. (eds.), Seventh ACM Conference on Rec-\\nommender Systems, RecSys ’13, Hong Kong, China, Oc-\\ntober 12-16, 2013, pp. 165–172. ACM, 2013. doi: 10.\\n1145/2507157.2507163. URL https://doi.org/\\n10.1145/2507157.2507163.\\nMcCloskey, M. and Cohen, N. J. Catastrophic interference\\nin connectionist networks: The sequential learning prob-\\nlem. Psychology of learning and motivation, 24:109–165,\\n1989.\\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and\\ny Arcas, B. A. Communication-efﬁcient learning of deep\\nnetworks from decentralized data. In Artiﬁcial intelli-\\ngence and statistics, pp. 1273–1282. PMLR, 2017.\\nM¨\\noller, T., Reina, A., Jayakumar, R., and Pietsch,\\nM.\\nCOVID-QA: A question answering dataset\\nfor COVID-19.\\nIn Proceedings of the 1st Work-\\nshop on NLP for COVID-19 at ACL 2020, Online,\\nJuly 2020. Association for Computational Linguis-\\ntics. URL https://aclanthology.org/2020.\\nnlpcovid19-acl.18.\\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Ba-\\ntra, D., Vanderwende, L., Kohli, P., and Allen, J.\\nA\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\ncorpus and cloze evaluation for deeper understanding\\nof commonsense stories. In Proceedings of the 2016\\nConference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Lan-\\nguage Technologies, pp. 839–849, San Diego, Califor-\\nnia, June 2016. Association for Computational Linguis-\\ntics.\\ndoi: 10.18653/v1/N16-1098.\\nURL https://\\naclanthology.org/N16-1098.\\nMuennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-\\nderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,\\nSchoelkopf, H., et al. Crosslingual generalization through\\nmultitask ﬁnetuning. arXiv preprint arXiv:2211.01786,\\n2022.\\nNarayan, S., Cohen, S. B., and Lapata, M. Don’t give me\\nthe details, just the summary! topic-aware convolutional\\nneural networks for extreme summarization. In Proceed-\\nings of the 2018 Conference on Empirical Methods in\\nNatural Language Processing, pp. 1797–1807, Brussels,\\nBelgium, October-November 2018. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/D18-1206. URL\\nhttps://aclanthology.org/D18-1206.\\nNi, J., Qu, C., Lu, J., Dai, Z., ´\\nAbrego, G. H., Ma, J., Zhao,\\nV. Y., Luan, Y., Hall, K. B., Chang, M.-W., et al. Large\\ndual encoders are generalizable retrievers. arXiv preprint\\narXiv:2112.07899, 2021.\\nNi, J., Abrego, G. H., Constant, N., Ma, J., Hall, K., Cer, D.,\\nand Yang, Y. Sentence-t5: Scalable sentence encoders\\nfrom pre-trained text-to-text models. In Findings of the\\nAssociation for Computational Linguistics: ACL 2022,\\npp. 1864–1874, 2022.\\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and\\nKiela, D. Adversarial NLI: A new benchmark for natu-\\nral language understanding. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational\\nLinguistics, pp. 4885–4901, Online, July 2020. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/\\n2020.acl-main.441. URL https://aclanthology.\\norg/2020.acl-main.441.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray,\\nA., et al. Training language models to follow instructions\\nwith human feedback. arXiv preprint arXiv:2203.02155,\\n2022.\\nPang, B. and Lee, L.\\nSeeing stars: Exploiting class re-\\nlationships for sentiment categorization with respect\\nto rating scales.\\nIn Proceedings of the 43rd Annual\\nMeeting of the Association for Computational Linguis-\\ntics (ACL’05), pp. 115–124, Ann Arbor, Michigan,\\nJune 2005. Association for Computational Linguistics.\\ndoi: 10.3115/1219840.1219855. URL https://www.\\naclweb.org/anthology/P05-1015.\\nPetroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M.,\\nDe Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Mail-\\nlard, J., Plachouras, V., Rockt¨\\naschel, T., and Riedel, S.\\nKILT: a benchmark for knowledge intensive language\\ntasks. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies, pp.\\n2523–2544, Online, June 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n200. URL https://aclanthology.org/2021.\\nnaacl-main.200.\\nPfeiffer, J., Goyal, N., Lin, X. V., Li, X., Cross, J., Riedel,\\nS., and Artetxe, M. Lifting the curse of multilinguality\\nby pre-training modular transformers. arXiv preprint\\narXiv:2205.06266, 2022.\\nPilehvar, M. T. and Camacho-Collados, J. WiC: the word-in-\\ncontext dataset for evaluating context-sensitive meaning\\nrepresentations. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers), pp. 1267–1273, Min-\\nneapolis, Minnesota, June 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/N19-1128. URL\\nhttps://aclanthology.org/N19-1128.\\nQin, G. and Eisner, J.\\nLearning how to ask: Querying\\nLMs with mixtures of soft prompts. In Proceedings of\\nthe 2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human\\nLanguage Technologies, pp. 5203–5212, Online, June\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.naacl-main.410.\\nURL https://\\naclanthology.org/2021.naacl-main.410.\\nRajani, N. F., McCann, B., Xiong, C., and Socher, R. Ex-\\nplain yourself! leveraging language models for common-\\nsense reasoning. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics, pp.\\n4932–4942, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1487.\\nURL https://aclanthology.org/P19-1487.\\nRashkin, H., Smith, E. M., Li, M., and Boureau, Y.-L. To-\\nwards empathetic open-domain conversation models: A\\nnew benchmark and dataset. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Lin-\\nguistics, pp. 5370–5381, Florence, Italy, July 2019. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/\\nP19-1534.\\nURL https://aclanthology.org/\\nP19-1534.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nReimers, N. and Gurevych, I. Sentence-bert: Sentence em-\\nbeddings using siamese bert-networks. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natu-\\nral Language Processing. Association for Computational\\nLinguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084.\\nRoemmele, M., Bejan, C. A., and Gordon, A. S. Choice\\nof plausible alternatives: An evaluation of commonsense\\ncausal reasoning. In AAAI spring symposium: logical\\nformalizations of commonsense reasoning, pp. 90–95,\\n2011.\\nRogers, A., Kovaleva, O., Downey, M., and Rumshisky,\\nA.\\nGetting closer to AI complete question answer-\\ning: A set of prerequisite real tasks.\\nIn The Thirty-\\nFourth AAAI Conference on Artiﬁcial Intelligence, AAAI\\n2020, The Thirty-Second Innovative Applications of Ar-\\ntiﬁcial Intelligence Conference, IAAI 2020, The Tenth\\nAAAI Symposium on Educational Advances in Arti-\\nﬁcial Intelligence, EAAI 2020, New York, NY, USA,\\nFebruary 7-12, 2020, pp. 8722–8731. AAAI Press,\\n2020a.\\nURL https://aaai.org/ojs/index.\\nphp/AAAI/article/view/6398.\\nRogers, A., Kovaleva, O., Downey, M., and Rumshisky,\\nA.\\nGetting closer to ai complete question answer-\\ning: A set of prerequisite real tasks.\\nProceedings\\nof the AAAI Conference on Artiﬁcial Intelligence, 34\\n(05):8722–8731, Apr. 2020b. doi: 10.1609/aaai.v34i05.\\n6398.\\nURL https://ojs.aaai.org/index.\\nphp/AAAI/article/view/6398.\\nSaha, A., Aralikatte, R., Khapra, M. M., and Sankara-\\nnarayanan, K.\\nDuoRC: Towards complex language\\nunderstanding with paraphrased reading comprehen-\\nsion.\\nIn Proceedings of the 56th Annual Meeting\\nof the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pp. 1683–1693, Melbourne, Aus-\\ntralia, July 2018. Association for Computational Lin-\\nguistics.\\ndoi: 10.18653/v1/P18-1156.\\nURL https:\\n//aclanthology.org/P18-1156.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\\nWinogrande: An adversarial winograd schema challenge\\nat scale. Communications of the ACM, 64(9):99–106,\\n2021.\\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\\nAlyafeai, Z., Chafﬁn, A., Stiegler, A., Scao, T. L., Raja,\\nA., et al. Multitask prompted training enables zero-shot\\ntask generalization. arXiv preprint arXiv:2110.08207,\\n2021.\\nSap, M., Rashkin, H., Chen, D., Le Bras, R., and Choi,\\nY. Social IQa: Commonsense reasoning about social in-\\nteractions. In Proceedings of the 2019 Conference on\\nEmpirical Methods in Natural Language Processing and\\nthe 9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP), pp. 4463–4473,\\nHong Kong, China, November 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/D19-1454.\\nURL https://aclanthology.org/D19-1454.\\nSee, A., Liu, P. J., and Manning, C. D. Get to the point:\\nSummarization with pointer-generator networks. In Pro-\\nceedings of the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\npp. 1073–1083, Vancouver, Canada, July 2017. Asso-\\nciation for Computational Linguistics. doi: 10.18653/\\nv1/P17-1099.\\nURL https://www.aclweb.org/\\nanthology/P17-1099.\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint arXiv:2206.04615, 2022.\\nSu, H., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., Yih, W.-t.,\\nSmith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder,\\nany task: Instruction-ﬁnetuned text embeddings. arXiv\\npreprint arXiv:2212.09741, 2022.\\nSun, K., Yu, D., Chen, J., Yu, D., Choi, Y., and Cardie,\\nC.\\nDREAM: A challenge data set and models for\\ndialogue-based reading comprehension. Transactions\\nof the Association for Computational Linguistics, 7:217–\\n231, 2019. doi: 10.1162/tacl a 00264. URL https:\\n//aclanthology.org/Q19-1014.\\nTafjord, O., Clark, P., Gardner, M., Yih, W.-t., and Sabhar-\\nwal, A. Quarel: A dataset and models for answering\\nquestions about qualitative relationships. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence,\\nvolume 33, pp. 7063–7071, 2019.\\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. Common-\\nsenseQA: A question answering challenge targeting com-\\nmonsense knowledge. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers), pp. 4149–4158,\\nMinneapolis, Minnesota, June 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/N19-1421.\\nURL https://aclanthology.org/N19-1421.\\nTandon, N., Dalvi, B., Sakaguchi, K., Clark, P., and Bosse-\\nlut, A. WIQA: A dataset for “what if...” reasoning over\\nprocedural text. In Proceedings of the 2019 Conference\\non Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP), pp. 6076–6085,\\nHong Kong, China, November 2019. Association for\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nComputational Linguistics. doi: 10.18653/v1/D19-1629.\\nURL https://aclanthology.org/D19-1629.\\nVu, T., Lester, B., Constant, N., Al-Rfou’, R., and Cer,\\nD. SPoT: Better frozen model adaptation through soft\\nprompt transfer. In Proceedings of the 60th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 5039–5059, Dublin, Ire-\\nland, May 2022. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2022.acl-long.346. URL https:\\n//aclanthology.org/2022.acl-long.346.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\\nman, S. GLUE: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: Analyz-\\ning and Interpreting Neural Networks for NLP, pp. 353–\\n355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446.\\nURL https://aclanthology.org/W18-5446.\\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,\\nMirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,\\nA. S., Naik, A., Stap, D., et al. Super-naturalinstructions:\\nGeneralization via declarative instructions on 1600+ nlp\\ntasks. URL https://arxiv. org/abs/2204.07705, 2022a.\\nWang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren,\\nX., Su, G., Perot, V., Dy, J., and Pﬁster, T. Learning\\nto prompt for continual learning. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pp. 139–149, 2022b.\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\\nguage models are zero-shot learners.\\narXiv preprint\\narXiv:2109.01652, 2021.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\\nLe, Q., and Zhou, D. Chain of thought prompting elic-\\nits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022.\\nWelbl, J., Liu, N. F., and Gardner, M. Crowdsourcing mul-\\ntiple choice science questions. In Proceedings of the\\n3rd Workshop on Noisy User-generated Text, pp. 94–106,\\nCopenhagen, Denmark, September 2017. Association for\\nComputational Linguistics. doi: 10.18653/v1/W17-4413.\\nURL https://aclanthology.org/W17-4413.\\nWelbl, J., Stenetorp, P., and Riedel, S. Constructing datasets\\nfor multi-hop reading comprehension across documents.\\nTransactions of the Association for Computational Lin-\\nguistics, 6:287–302, 2018. doi: 10.1162/tacl a 00021.\\nURL https://aclanthology.org/Q18-1021.\\nWortsman, M., Gururangan, S., Li, S., Farhadi, A., Schmidt,\\nL., Rabbat, M., and Morcos, A. S.\\nlo-ﬁ: distributed\\nﬁne-tuning without communication.\\narXiv preprint\\narXiv:2210.11948, 2022a.\\nWortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R.,\\nGontijo-Lopes, R., Morcos, A. S., Namkoong, H.,\\nFarhadi, A., Carmon, Y., Kornblith, S., et al.\\nModel\\nsoups: averaging weights of multiple ﬁne-tuned mod-\\nels improves accuracy without increasing inference time.\\nIn International Conference on Machine Learning, pp.\\n23965–23998. PMLR, 2022b.\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R.,\\nSiddhant, A., Barua, A., and Raffel, C. mT5: A massively\\nmultilingual pre-trained text-to-text transformer. In Pro-\\nceedings of the 2021 Conference of the North American\\nChapter of the Association for Computational Linguistics:\\nHuman Language Technologies, pp. 483–498, Online,\\nJune 2021. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2021.naacl-main.41.\\nURL https:\\n//aclanthology.org/2021.naacl-main.41.\\nYamada, K., Hitomi, Y., Tamori, H., Sasano, R., Okazaki,\\nN., Inui, K., and Takeda, K.\\nTransformer-based lex-\\nically constrained headline generation.\\nIn Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 4085–4090, On-\\nline and Punta Cana, Dominican Republic, November\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.emnlp-main.335. URL https://\\naclanthology.org/2021.emnlp-main.335.\\nYang, Y., Yih, W.-t., and Meek, C. WikiQA: A challenge\\ndataset for open-domain question answering. In Pro-\\nceedings of the 2015 Conference on Empirical Methods\\nin Natural Language Processing, pp. 2013–2018, Lis-\\nbon, Portugal, September 2015. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/D15-1237. URL\\nhttps://aclanthology.org/D15-1237.\\nYe, S., Jang, J., Kim, D., Jo, Y., and Seo, M. Retrieval of\\nsoft prompt enhances zero-shot task generalization. arXiv\\npreprint arXiv:2210.03029, 2022a.\\nYe, S., Kim, D., Jang, J., Shin, J., and Seo, M. Guess the\\ninstruction! making language models stronger zero-shot\\nlearners. arXiv preprint arXiv:2210.02969, 2022b.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\\nY. HellaSwag: Can a machine really ﬁnish your sen-\\ntence? In Proceedings of the 57th Annual Meeting of\\nthe Association for Computational Linguistics, pp. 4791–\\n4800, Florence, Italy, July 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/P19-1472. URL\\nhttps://aclanthology.org/P19-1472.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nZhang, W., Deng, L., Zhang, L., and Wu, D. A survey\\non negative transfer. IEEE/CAA Journal of Automatica\\nSinica, 2022.\\nZhang, X., Zhao, J., and LeCun, Y.\\nCharacter-level\\nconvolutional networks for text classiﬁcation.\\nIn\\nCortes, C., Lawrence, N., Lee, D., Sugiyama, M., and\\nGarnett, R. (eds.), Advances in Neural Information\\nProcessing Systems, volume 28. Curran Associates,\\nInc.,\\n2015a.\\nURL\\nhttps://proceedings.\\nneurips.cc/paper/2015/file/\\n250cf8b51c773f3f8dc8b4be867a9a02-Paper.\\npdf.\\nZhang, X., Zhao, J. J., and LeCun, Y.\\nCharacter-level\\nconvolutional networks for text classiﬁcation. In Cortes,\\nC., Lawrence, N. D., Lee, D. D., Sugiyama, M., and\\nGarnett, R. (eds.), Advances in Neural Information\\nProcessing Systems 28: Annual Conference on Neural\\nInformation\\nProcessing\\nSystems\\n2015,\\nDecember\\n7-12, 2015, Montreal, Quebec, Canada, pp. 649–\\n657,\\n2015b.\\nURL\\nhttps://proceedings.\\nneurips.cc/paper/2015/hash/\\n250cf8b51c773f3f8dc8b4be867a9a02-Abstract.\\nhtml.\\nZhang, Y., Baldridge, J., and He, L. PAWS: Paraphrase\\nadversaries from word scrambling. In Proceedings of the\\n2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Papers),\\npp. 1298–1308, Minneapolis, Minnesota, June 2019. As-\\nsociation for Computational Linguistics. doi: 10.18653/\\nv1/N19-1131. URL https://www.aclweb.org/\\nanthology/N19-1131.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nEmbedding Models\\nHellasw.\\nStoryC.\\nAN. R1\\nAN. R2\\nAN. R3\\nCOPA\\nCB\\nRTE\\nWSC\\nWiC\\nWinogr.\\nTotal Avg.\\nRANDOM\\n31.25\\n47.38\\n32.94\\n33.38\\n32.12\\n61.00\\n38.57\\n54.01\\n46.35\\n49.03\\n54.27\\n43.66\\nALL-MINILM-L6-V2\\n34.60\\n86.33\\n35.49\\n34.64\\n31.22\\n79.25\\n43.57\\n64.01\\n62.21\\n52.97\\n61.60\\n53.48\\nALL-MINILM-L12-V2\\n32.33\\n67.13\\n33.84\\n33.38\\n33.69\\n63.00\\n47.38\\n58.48\\n49.52\\n51.17\\n56.80\\n47.88\\nALL-MPNET-BASE-V2\\n31.53\\n59.33\\n33.71\\n33.02\\n31.73\\n61.38\\n46.43\\n53.97\\n44.62\\n52.33\\n54.93\\n47.73\\nNLI-MPNET-BASE-V2\\n22.60\\n50.87\\n34.02\\n33.69\\n34.53\\n58.75\\n38.57\\n48.59\\n52.21\\n49.77\\n51.07\\n43.15\\nSUP-SIMCSE-ROBERTA-LARGE\\n26.93\\n59.67\\n34.58\\n33.29\\n34.73\\n84.75\\n41.90\\n52.06\\n50.67\\n56.03\\n51.67\\n47.84\\nUNSUP-SIMCSE-ROBERTA-LARGE\\n24.27\\n71.93\\n33.98\\n32.22\\n33.78\\n69.75\\n43.33\\n50.72\\n55.38\\n50.33\\n50.93\\n46.97\\nHKUNLP/INSTRUCTOR-LARGE\\n19.80\\n57.33\\n33.16\\n33.78\\n32.93\\n54.50\\n39.64\\n47.80\\n55.96\\n49.20\\n51.20\\n43.21\\nHKUNLP/INSTRUCTOR-XL\\n19.60\\n44.53\\n32.62\\n32.82\\n32.31\\n57.88\\n44.52\\n47.83\\n60.77\\n48.77\\n51.80\\n43.04\\nGTR-T5-LARGE\\n29.60\\n70.47\\n33.04\\n31.64\\n32.31\\n58.38\\n50.95\\n54.69\\n57.79\\n51.50\\n50.80\\n47.38\\nGTR-T5-XL\\n37.20\\n84.80\\n33.24\\n33.27\\n33.58\\n83.00\\n43.69\\n58.59\\n45.00\\n50.73\\n51.07\\n50.38\\nSENTENCE-T5-LARGE\\n33.33\\n78.53\\n33.11\\n33.76\\n33.31\\n87.25\\n46.19\\n58.34\\n63.08\\n52.13\\n54.27\\n52.12\\nSENTENCE-T5-XL\\n25.67\\n87.13\\n35.27\\n33.38\\n32.98\\n68.63\\n46.19\\n59.10\\n61.63\\n52.33\\n51.67\\n50.36\\nVOIDISM/DIFFCSE-BERT-BASE-UNCASED-STS\\n21.93\\n46.53\\n33.07\\n32.91\\n32.47\\n58.75\\n45.60\\n49.71\\n60.77\\n49.70\\n50.33\\n43.80\\nT0-SMALL (YE ET AL., 2022A)\\n39.55\\n97.09\\n33.89\\n33.96\\n34.38\\n88.00\\n41.55\\n62.53\\n53.95\\n52.45\\n70.20\\n55.23\\nTable 9. Comparison of different embedding models, measured on 11 different unseen datasets using Prompt Experts(PE). For instance,\\nALL-MINILM-L6-V2 refers to T5(3B) + PE W/ ROE in Table 1. All the task format are ﬁxed to ‘Answer Choices: {answer choice},\\nInstance: {instance}’. The best comparable performances are bolded and second best underlined. Note that evaluation is performed with\\n300 samples from each evaluation dataset for efﬁciency.\\nA. Details of Training and Evaluation Datasets\\nDetails of Training Dataset\\nFollowing Sanh et al. (2021), we use 36 training datasets from the 8 task categories for\\ntraining our experts. We provide the ofﬁcial names given in Huggingface Datasets: Sentiment Classiﬁcation (Senti.) imdb\\n(Maas et al., 2011), amazon polarity (McAuley & Leskovec, 2013), rotten tomatoes (Pang & Lee, 2005), yelp review full\\n(Zhang et al., 2015b), and app reviews. Paraphrase Identiﬁcation (Para.) glue/qqp (Wang et al., 2018), glue/mrpc (Wang\\net al., 2018), and paws/labeled ﬁnal (Zhang et al., 2019). Topic Classiﬁcation (Topic C. ag news (Zhang et al., 2015a),\\ndbpedia 14 (Lehmann et al., 2015), and trec (Li & Roth, 2002). Summarization (Summ.) gigaword (Graff et al., 2003),\\nmulti news (Fabbri et al., 2019), samsum (Gliwa et al., 2019), xsum (Narayan et al., 2018), and cnn dailymail/3.0.0 (See\\net al., 2017). Structure-To-Text (STS) common gen (Lin et al., 2020) and wiki bio (Lebret et al., 2016). Multiple-Choice\\nQuestion Answering (MCQA) commonsense qa (Talmor et al., 2019), dream (Sun et al., 2019), quail (Rogers et al.,\\n2020a), qasc (Khot et al., 2020), quarel (Tafjord et al., 2019), cos e/v1.11 (Rajani et al., 2019), quail (Rogers et al., 2020b),\\nsocial i qa (Sap et al., 2019), wiqa (Tandon et al., 2019), cosmos qa (Huang et al., 2019), sciq (Welbl et al., 2017), and\\nwiki hop/original (Welbl et al., 2018) Extractive Question Answering (EQA) adversarial qa/adversarial qa (Bartolo et al.,\\n2020b), quoref (Bartolo et al., 2020a), ropes (Lin et al., 2019), and duorc/Paraphrase IdentiﬁcationRC (Saha et al., 2018)\\nClosed Book Question Answering (CBQA) kilt tasks/hotpotqa (Petroni et al., 2021) and wiki qa (Yang et al., 2015).\\nDetails of Evaluation Dataset\\nFollowing Sanh et al. (2021), we include 11 evaluation datasets as follows: RTE (Dagan\\net al., 2005), CB (De Marneffe et al., 2019), ANLI (Nie et al., 2020) for natural language inference task, COPA (Roemmele\\net al., 2011), Hellaswag (Zellers et al., 2019), Storycloze (Mostafazadeh et al., 2016) for sentence completion task,\\nWinogrande (Sakaguchi et al., 2021), WSC (Levesque et al., 2012) for coreference resolution task, and WiC (Pilehvar &\\nCamacho-Collados, 2019) for word sense disambiguation task.\\nFor BIG-bench tasks, we evaluate on 13 tasks, following Sanh et al. (2021): Known Unknown, Logic Grid, StrategyQA,\\nHindu Knowledge, Movie Dialog, Code Description, Conceptual, Language ID, Vitamin C, Syllogisms, Misconceptions,\\nLogical Deduction, and Winowhy.\\nFor the generative evaluation tasks, we follow Chakrabarty et al. (2022) and utilize 8 tasks: Text Simpliﬁcation (Wiki-\\nAuto) (Jiang et al., 2020), Headline Generation with constraint (HGen) (Yamada et al., 2021), Haiku Generation (Haiku),\\nCovid QA (M¨\\noller et al., 2020), Inquisitive Question Generation (ELI5) (Fan et al., 2019), Empathetic Dialogue Generation\\n(EmDg) (Rashkin et al., 2019), Explanation Generation (eSNLI) (Camburu et al., 2018), and Twitter Stylometry (Twitter)\\nB. Varying the Embedding Model and Text Format for Retrieval of Experts\\nPerformance of Different Embedding Models\\nWhile Ye et al. (2022a) used T0 (Sanh et al., 2021) as the base embedding\\nmodel to retrieve prompt embeddings, we explore 13 different sentence embedding models to waive the need of using\\ninstruction tuned models for retrieval of expert LMs.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nText Format\\nHellasw.\\nStoryC.\\nAN. R1\\nAN. R2\\nAN. R3\\nCOPA\\nCB\\nRTE\\nWSC\\nWiC\\nWinogr.\\nTotal Avg.\\n‘Instance: {instance}’\\n24.67\\n78.07\\n33.53\\n32.67\\n32.91\\n64.13\\n40.36\\n54.55\\n50.48\\n52.47\\n52.73\\n46.96\\n‘Answer Choices: {label list}’\\n24.93\\n51.47\\n33.80\\n34.29\\n33.20\\n58.38\\n42.38\\n50.83\\n51.54\\n53.47\\n51.13\\n44.13\\n‘Answer Choices: {answer choice}’\\n31.60\\n50.53\\n32.09\\n32.16\\n35.98\\n84.75\\n44.05\\n50.83\\n51.54\\n53.47\\n63.40\\n48.22\\n‘Answer Choices: {label list}, Instance: {instance}’\\n32.27\\n56.40\\n35.76\\n34.73\\n31.11\\n67.13\\n46.31\\n59.17\\n61.15\\n52.30\\n52.67\\n48.09\\n‘Answer Choices: {answer choice}, Instance: {instance}’\\n34.60\\n86.33\\n35.49\\n34.64\\n31.22\\n79.25\\n43.57\\n64.01\\n62.21\\n52.97\\n61.60\\n53.48\\n‘{instance}’\\n24.27\\n82.40\\n33.53\\n33.47\\n33.89\\n58.25\\n43.81\\n51.66\\n51.92\\n52.60\\n51.13\\n46.99\\n‘{label list}’\\n24.53\\n50.53\\n33.67\\n32.76\\n32.58\\n58.38\\n42.02\\n50.83\\n51.54\\n53.47\\n51.13\\n43.77\\n‘{answer choice}’\\n24.00\\n49.87\\n32.09\\n32.16\\n35.98\\n86.00\\n44.05\\n50.83\\n51.54\\n53.47\\n63.40\\n47.58\\n‘{label list}</s>{instance}’\\n25.53\\n65.60\\n35.76\\n33.91\\n31.07\\n62.38\\n46.90\\n60.14\\n62.69\\n53.70\\n50.73\\n48.04\\n‘{answer choice}</s>{instance}’\\n35.93\\n60.53\\n35.29\\n32.51\\n33.00\\n68.75\\n43.93\\n59.03\\n62.60\\n52.40\\n60.73\\n49.52\\nTable 10. Comparison of different text formats, measured on 11 different unseen datasets using Prompt Experts(PE). For instance, ‘Answer\\nChoices: {answer choice}, Instance: {instance}’ refers to T5(3B) + PE W/ ROE in Table 1. All the embedding model are ﬁxed to\\nALL-MINILM-L6-V2. The best comparable performances are bolded and second best underlined. Note that evaluation is performed with\\n300 samples from each evaluation dataset for efﬁciency.\\nMore speciﬁcally, we list of embedding models we use are as follows: (a) 4 different variants of SENTENCE TRANSFORMER\\nmodel (Reimers & Gurevych, 2019): all-MiniLM-L6-v2, all-MiniLM-L12-v2, all-mpnet-base-v2, nli-mpnet-base-v2, (b) 2\\ndifferent variants of SIMCSE model (Gao et al., 2021): sup-simcse-roberta-large, unsup-simcse-roberta-large, (c) 2 different\\nvariants of INSTRUCTOR model (Su et al., 2022): hkunlp/instructor-large, hkunlp/instructor-xl, (d) 2 different variants\\nof GTR model (Ni et al., 2021): gtr-t5-large, gtr-tr-xl, (e) 2 different variants of SENTENCET5 model (Ni et al., 2022):\\nsentence-t5-large, sentence-t5-xl, and (f) DIFFCSE model (Chuang et al., 2022): voidism/diffcse-bert-base-uncased-sts\\nwhich are all available on HuggingFace. Note that we try different embedding models in an unsupervised manner, i.e., not\\nrequiring any supervision to train the embedding model, but using it off-the-shelf. The results are shown in Table 9.\\nPerformance of Different Text Formats\\nWe also try different variants of text format given to the embedding model.\\nUsing Promptsource (Bach et al., 2022), we compare including the instance, label list, answer choice in 2 different formats.\\nSpeciﬁcally, the full list of text formats are as follows: (a) ‘Instance: {instance}’, (b) ‘Answer Choices: {label list}’, (c)\\n‘Answer Choices: {answer choice}’, (d) ‘Answer Choices: {label list}, Instance: {instance}’, (e) ‘Answer Choices: {answer\\nchoice}, Instance: {instance}’, (f) ‘{instance}’, (g) ‘{label list}’, (h) ‘{answer choice}’, (i) ‘{label list}</s>{instance}’,\\n(j) ‘{answer choice}</s>{instance}’. Label list and answer choice differ in that while label list uses the actual label options\\n(e.g., [‘swim’,‘ﬂy’,‘walk’,‘run’]), answer choice organizes them with a ‘—’ deliminator in the middle (e.g. A|B|C|D). The\\nresults are shown in Table 10.\\nResults\\nWhile we tried different variants, the oldest, yet most chosen model ALL-MINILM-L6-V2 outperforms other\\noptions. We conjecture that this is because most of the model variants we tested were trained as sentence embedding models,\\nnot for embedding prompted instances. Prompted instances are some how structural and formatted compared to natural\\nlanguage sentences used for training sentence embedding models. In terms of text format, using both the prompted instance\\nand the answer choice showed the best results. These results show that for the dense retriever to map instances, it should rely\\non both components, which are orthogonally important. Also, using the actual label option harms performance compared to\\nusing the answer choice, which indicates that the output format itself is important to retrieve well-matched expert LMs.\\nC. Details of Performing Compositional Instructions\\nOur compositional instruction setting consists of a total of 400 instances for each task (300 instances for the validation\\nset, and 100 instances for the test set.) per language that was obtained using google translate to change the input of the\\nXL-Sum (Hasan et al., 2021) dataset. We thus use the ground truth label in the speciﬁed language and the input is the\\nmachine-translated version. The reason for this is that we measure the λi values (the importance to place on each task\\nvector τi) by performing evaluation on the validation datasets. Empirically, setting 1.0 for each λi value resulted in the best\\nperformance. Thus, as mentioned in the method section, the total P λi results in 2.0, greater than 1.0.\\nWe also vary the decoding strategies to check the performance of merging two experts ﬁnetuned from MT5-3B compared\\nwith naive MT0-3B on XL-Sum dataset. The detailed optimal setting we found is as follows:\\n• LAMBDA1: 1.0\\n• LAMBDA2: 1.0\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\n• NO REPEAT NGRAM SIZE:2\\n• TEMPERATURE:1.0\\n• EARLY STOPPING:True\\n• DO SAMPLE:True\\n• TOP P:0.95\\nHere are the actual inputs for the LM generated & ground truth output examples shown in Table 8. The compositional\\ninstruction portion is shown in bold.\\nENGLISH →SPANISH: “Write a summary of the following English text and translate the sentence into Spanish: The\\nFrench police arrested four members of the child’s family for their alleged involvement on Tuesday. Police sources told\\nlocal media that the child refused to do his homework and that he was beaten with the stick of a broom. The 20 -year\\n-old sister, his older brother and his girlfriend were present at the time of the incident and were arrested. The three called\\nemergency services, which could not save the child. The alleged crime occurred on September 17 at the family’s home in\\nthe town of Mulhouse, in the east of the country, and of just over 100,000 inhabitants. Although the child’s mother was\\nnot at home because she was on a trip for work reasons, she was also arrested. The authorities say it will be questioned\\nto conﬁrm whether it encouraged the punishment. The four family members remain in police custody and must appear\\nbefore the Mulhouse Prosecutor’s Ofﬁce for a judicial investigation. Prosecutor Edwige Roux-Morizot will investigate the\\ncase. Moretones after the death of the child, victim of cardiac arrest, several neighbors celebrated a vigil in their honor and\\nmet with the child’s parents to offer them comfort. However, the results of the autopsy motivated the police to carry out\\nan investigation into what happened. The child’s body presented several bruises, especially at his feet, according to AFP.\\nDespite the conﬁrmation of cardiac arrest, pathologists said the cause of death was probably the blows he had suffered. A\\npolice source said the child was beaten with blunt objects. Although the main suspect of the murder is the older brother,\\nthe French authorities hope that the investigation will shed light on what happened. France is one of the 13 countries of\\nthe European Union where corporal punishment is legal. A legal practice The National Assembly of France is considering\\napproveing a law to prohibit corporal punishments for children. There are two new law proposals that would grant children a\\nviolence -free education, venting parents to use ”forms of humiliation such as physical or verbal...”\\nENGLISH →FRENCH: “Write a summary of the following English text and translate the sentence into French: The\\nformer Minister of Justice of Malawi, Ralph Kasambara, was arrested on November 8, 2013. Mr. Kasambara was found\\nguilty of conspiracy in the assassination in September 2013 of the former budget director at the Ministry of Finance, Paul\\nPaul MPHWIYO. The murder of Mr. Mphwiyo had led to the discovery of the scandal of ”cashgate”, the systematic looting\\nof public resources, during the administration of President Joyce Banda. Nearly 250 million had been fraudulently paid to\\nbusinessmen for services who have never been rendered. A few days before the tragedy, a subordinate ofﬁcial would have\\nbeen found with gold bars belonging to the cash, the equivalent of more than $ 300 million, in the trunk of his car. Money\\nwas also conﬁscated at the home of certain ofﬁcials and in chests from their vehicles. Immediately after his conviction last\\nmonth, Kasambara had suggested that he would not appeal the court verdict.”\\nENGLISH →JAPANESE: “Write a summary of the following English text and translate the sentence into Japanese:\\nVice Chairman Meng Ship, the highest ﬁnancial manager (CFO), was the daughter of the founder arrested in Vancouver,\\nCanada last December, and Vice President Meng Teng was sanctioned at Vancouver Airport last December. He was arrested\\nfor violating and associated scams and was charged at the end of January this year. The United States authorities are seeking\\nto hand over the vice chairman, but they deny the charges. Defendant Meng ﬁled an administrative lawsuit for the Canadian\\ngovernment, the immigration bureau, and the police for ”signiﬁcantly infringing” their citizenship. China has accused the\\ndefendant’s arrest and delivery procedure as a ”political project.” ¡Related article¿ Introduction is ”illegal” and ”Dandridy”\\nBritish Columbia Senior Court on the 1st, and Meng is the Canadian government and the Royal Canadian equestrian police\\n(RCMP), and the Canadian Immigration Bureau (CBSA). He is complaining of civil rights infringement. Before the arrest\\nof RCMP, CBSA complained that he had detained himself on unfair claims, investigated and interrogated his belongings.\\nThe vice chairman was bail and was at Vancouver’s home, and the authorities arrested Vice Chairman Meng on the spot. He\\ncomplained that it infringed on the rights based on the Canadian Characters of Human Rights. In addition, Vice -Chairman’s\\ndetention was ”illegal” and ”arbitrary”, and authorities pointed out that ”the reason for detention, the right to call lawyers, or\\nthe right to be paid to be silent.” What is the reaction of each country? The relationship between China, Canada and the\\nUnited States has deteriorated over the arrest of Vice Chairman Meng. In January, the U.S. Department was charged with 23\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\ncases of Huawei and Vice Chairman Meng. In addition to bank fraud, communication fraud, judicial obstruction, a major US\\ntelecommunications equipment T -mobile has been charged with trying to steal technology. China accused these movements\\nas ”abuse of the handover agreement” between the United States and Canada, and stated that they...”\\nENGLSIH →CHINESE: “Write a summary of the following English text and translate the sentence into Chinese: Dr.\\nCraig Spencer, who is infected with Ebola virus, is currently being hospitalized at the New York Metropolitan Hospital.\\nCaisyex said that the isolation experience is very scary and may also make other medical workers reluctant to go to West\\nAfrica to help curb the Ebola epidemic. Following New York and New Jersey, Illinois has also adopted a strict isolation\\npolicy. New measures means that those who have come into contact with any Ebola patient in West Africa will be forced to\\nisolate for 21 days. U.S. President Obama Obama said in a weekly radio speech on September (October 25) that Americans\\nshould believe in the facts rather than being dominated. He also reiterated that he can infect the virus only with direct body\\nﬂuids with Ebola patients. Higkos, who was ”criminals”, who was an isolated person, said that she had witnessed ”confusion,\\npanic, and the most terrifying isolation” when she returned from Sierra Leone on Friday (24th). Hekox wrote a newspaper in\\nthe United States: ”I don’t know how many medical workers who fought with Ebola virus in the West African epidemic area\\nwill have the same encounter.” She said, ”Will they feel like criminals like criminals like criminals? She also said that she\\nwas isolated for seven hours at the airport terminal, but she only got a grain rod to ﬁll her hunger. She denied that she had\\nhad a fever and said that she was just blushing at the time because she was not satisﬁed with the treatment at the airport.\\nEven though Hiccoks was negative in Ebola virus testing, she was still being isolated for three weeks and was monitored by\\nmedical ofﬁcials. Frontline medical staff was deeply inﬂuenced by the Ebola outbreak. After being diagnosed with Ebola\\npatients, a doctor of New York, who had worked in Guinea last week, was diagnosed with Ebola patients, New York State\\nand New Jersey have strengthened their isolation measures. Spencer is currently receiving isolation treatment in a hospital in\\nNew York. Mali has also recently appeared in Ebola, and President Ibrahim... ”\\nENGLISH →KOREAN: “Write a summary of the following English text and translate the sentence into Korean:\\nAccording to the Korea Meteorological Administration, January this year was the warmest winter since 1973, when the\\nweather observation began in the Korean peninsula. The average temperature in January last month was 2.8 degrees. This\\nis 3.8 degrees higher than the average of minus 1.0 degrees in January, 1981\\n2010. The previous average temperature\\nrecord was 1.6 degrees in 1979. Except for the ﬁrst day of the new year, the average temperature in the country was higher\\nthan normal. Due to the high temperature, the snowfall was the lowest. The Korea Meteorological Administration cited the\\nintroduction of warm southwestern air ﬂow into the Siberian region, and the fact that the ’pole whirl’, which traps cold\\nair in the Arctic, was strong as an abnormal temperature. It also analyzed that the warm south wind ﬂow was introduced\\nto the Korean peninsula due to the high sea level temperature of the Western Paciﬁc. Nationwide weather data in January,\\nthe average temperature in the coldest January of the year has continued to rise in recent years. According to the weather\\ndata released by the Korea Meteorological Administration in January 1973-2020, the average temperature in January in\\nKorea is steadily rising. Choi Jung -hee, the Korea Meteorological Agency, said that the warming of winter is ”global\\nwarming impact,” and ”most of the monthly weather data tends to be similar.” Detection of the ecosystem change is detected\\nthroughout the ecosystem. The ﬁrst spawning season of ’Bukbangsan Guri’, a climate change indicator, has been faster.\\nMudeungsan National Park Eastern Ofﬁce said on the 24th of last month that the ﬁrst spawning of the North Bangsan\\nGogi, a species designated by the Ministry of Environment, was observed. It was ﬁrst observed. It is 27 days earlier than\\nFebruary 19 last year. This is the ﬁrst time that spawning has been observed in January since 2010, when the survey began.\\nResearchers at the Park Industrial Complex believed that the spawning day was advanced due to the exceptionally warm... ”\\nD. Full List of PE and DE ranked on the 11 unseen datasets\\nTable 11 shows the full list of DE and Table 12 shows the full list of PE, both lists sorted in descending order with regards to\\nthe mean accuracy on 11 unseen tasks.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nAVG\\nCategories\\ncosmos qa\\n53.35229377\\nMCQA\\nsocial i qa\\n52.9111819\\nMCQA\\ndream\\n51.45885188\\nMCQA\\nquail\\n50.4459655\\nMCQA\\nqasc\\n48.05781887\\nMCQA\\npaws/labeled ﬁnal\\n47.65196514\\nParaph.\\ncommonsense qa\\n47.20113697\\nMCQA\\nsciq\\n47.07330356\\nMCQA\\ncos e/v1.11\\n46.66113821\\nMCQA\\nquartz\\n46.65265672\\nMCQA\\nadversarial qa/adversarialQA\\n45.62737167\\nEQA\\nwiki qa\\n45.36088559\\nCBQA\\nglue/qqp\\n44.0165991\\nParaph.\\ncnn dailymail/3.0.0\\n43.98887691\\nSumm.\\nhotpot qa/fullwiki\\n43.66845602\\nCBQA\\nxsum\\n43.62089761\\nSumm.\\namazon polarity\\n43.5926426\\nSenti.\\nropes\\n43.45845826\\nEQA\\nquoref\\n43.41009006\\nEQA\\nrotten tomatoes\\n43.35511468\\nSenti.\\ncommon gen\\n43.1382362\\nSTS\\napp reviews\\n43.05588093\\nSenti.\\nwiki bio\\n43.05367126\\nSTS\\nsamsum\\n42.7618847\\nSumm.\\nwiki hop/original\\n42.67778976\\nMCQA\\ngigaword\\n42.61971626\\nSumm.\\ntrec\\n42.46916224\\nTopic C.\\ndbpedia 14\\n42.21388133\\nTopic C.\\nmulti news\\n41.97036069\\nSumm.\\nag news\\n41.95621965\\nTopic C.\\nglue/mrpc\\n41.95418826\\nParaph.\\nduorc/ParaphraseRC\\n41.94062218\\nEQA\\nimdb\\n41.70437975\\nSenti.\\nwiqa\\n41.1534245\\nMCQA\\nyelp review full\\n40.85474309\\nSenti.\\nquarel\\n40.59043188\\nMCQA\\nTable 11. The full list of Dataset Experts (DE) ranked in the mean accuracy on the 11 unseen tasks. The evaluations are performed on 300\\nsample instances of each unseen task for efﬁciency.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\ncosmos qa\\nno prompt text\\n54.65821845\\nMCQA\\ncosmos qa\\ncontext question description answer text\\n54.3060466\\nMCQA\\ncosmos qa\\ncontext description question answer text\\n54.19701579\\nMCQA\\ncosmos qa\\ndescription context question answer text\\n53.15591518\\nMCQA\\nsocial i qa\\nShow choices and generate answer\\n53.06841536\\nMCQA\\ndream\\nbaseline\\n51.85164999\\nMCQA\\ndream\\nread the following conversation and answer the question\\n51.67431073\\nMCQA\\ncos e/v1.11\\ndescription question option text\\n50.65180447\\nMCQA\\ncosmos qa\\ncontext question description answer id\\n50.48691808\\nMCQA\\nsocial i qa\\nShow choices and generate index\\n50.43707145\\nMCQA\\ncos e/v1.11\\ndescription question option id\\n50.29845396\\nMCQA\\nsciq\\nMultiple Choice (Closed Book)\\n50.12860827\\nMCQA\\ncommonsense qa\\nmost suitable answer\\n50.06566011\\nMCQA\\ncommonsense qa\\nquestion answering\\n49.96578376\\nMCQA\\ncosmos qa\\ncontext description question answer id\\n49.89036173\\nMCQA\\nqasc\\nqa with separated facts 1\\n49.14814303\\nMCQA\\ncos e/v1.11\\nquestion option description text\\n49.08282529\\nMCQA\\nsciq\\nMultiple Choice\\n48.73448898\\nMCQA\\ncosmos qa\\nno prompt id\\n48.56936806\\nMCQA\\ncos e/v1.11\\nquestion option description id\\n48.55509469\\nMCQA\\nsciq\\nMultiple Choice Question First\\n48.50439309\\nMCQA\\ncosmos qa\\ndescription context question answer id\\n48.22390771\\nMCQA\\nqasc\\nqa with separated facts 2\\n48.2197083\\nMCQA\\nqasc\\nqa with combined facts 1\\n48.12678008\\nMCQA\\ncos e/v1.11\\nquestion description option text\\n47.23675042\\nMCQA\\npaws/labeled ﬁnal\\ntask description-no-label\\n47.23675042\\nParaph.\\ncos e/v1.11\\nquestion description option id\\n47.03021282\\nMCQA\\nsocial i qa\\nCheck if a random answer is valid or not\\n46.98766238\\nMCQA\\npaws/labeled ﬁnal\\nRewrite\\n46.90427355\\nParaph.\\nquartz\\nparagraph question plain concat\\n46.88892082\\nMCQA\\npaws/labeled ﬁnal\\nConcatenation\\n46.76229133\\nParaph.\\npaws/labeled ﬁnal\\ncontext-question\\n46.69767805\\nParaph.\\npaws/labeled ﬁnal\\nPAWS-ANLI GPT3-no-label\\n46.68362131\\nParaph.\\npaws/labeled ﬁnal\\nRewrite-no-label\\n46.66735722\\nParaph.\\nquartz\\ngiven the fact answer the q\\n46.65622609\\nMCQA\\ncommonsense qa\\nquestion to answer index\\n46.59109421\\nMCQA\\npaws/labeled ﬁnal\\nConcatenation-no-label\\n46.51096254\\nParaph.\\npaws/labeled ﬁnal\\nMeaning\\n46.15932052\\nParaph.\\npaws/labeled ﬁnal\\ncontext-question-no-label\\n46.06366702\\nParaph.\\nropes\\nprompt beginning\\n46.03684758\\nEQA\\nquartz\\nuse info from question paragraph\\n46.00687505\\nMCQA\\npaws/labeled ﬁnal\\nMeaning-no-label\\n45.89445599\\nParaph.\\nquartz\\nanswer question below\\n45.70112461\\nMCQA\\nqasc\\nqa with separated facts 4\\n45.63098518\\nMCQA\\nquartz\\nread passage below choose\\n45.45333529\\nMCQA\\ndream\\ngenerate-last-utterance\\n45.43172606\\nMCQA\\npaws/labeled ﬁnal\\nPAWS-ANLI GPT3\\n45.33228586\\nParaph.\\nquartz\\nuse info from paragraph question\\n45.29788178\\nMCQA\\nropes\\nplain bottom hint\\n45.21541083\\nEQA\\nwiki qa\\nDecide good answer\\n45.21394529\\nCBQA\\nwiki qa\\nautomatic system\\n45.21245106\\nCBQA\\nquartz\\nanswer question based on\\n45.18935019\\nMCQA\\nwiki qa\\nexercise\\n45.18589809\\nCBQA\\nropes\\nprompt mix\\n44.93591101\\nEQA\\nquartz\\nhaving read above passage\\n44.91798422\\nMCQA\\nrotten tomatoes\\nReviewer Opinion bad good choices\\n44.78559829\\nSenti.\\nropes\\nplain no background\\n44.53005571\\nEQA\\nwiki qa\\nGenerate Question from Topic\\n44.34881059\\nCBQA\\nropes\\nnew situation background answer\\n44.34412958\\nEQA\\nadversarial qa/adversarialQA\\nbased on\\n44.32984883\\nEQA\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\ncos e/v1.11\\nexplain why human\\n44.30354265\\nMCQA\\nropes\\nbackground situation middle\\n44.21042071\\nEQA\\nwiqa\\neffect with string answer\\n44.16834366\\nMCQA\\ncommonsense qa\\nanswer given question without options\\n44.051375\\nMCQA\\ntrec\\npick the best descriptor\\n44.04277181\\nTopic C.\\nsocial i qa\\nGenerate the question from the answer\\n44.04212031\\nMCQA\\nadversarial qa/adversarialQA\\nanswer the following q\\n44.03344043\\nEQA\\nropes\\nplain background situation\\n44.02607152\\nEQA\\nag news\\nclassify with choices\\n44.01140825\\nTopic C.\\nwiki qa\\nTopic Prediction - Question and Answer Pair\\n43.95941542\\nCBQA\\ntrec\\nﬁne grained DESC context ﬁrst\\n43.91506114\\nTopic C.\\nglue/qqp\\nquora\\n43.83700658\\nParaph.\\nqasc\\nis correct 1\\n43.81501204\\nMCQA\\nhotpot qa/fullwiki\\nclassify question type\\n43.80908285\\nCBQA\\ntrec\\nwhich category best describes\\n43.78976077\\nTopic C.\\nropes\\nprompt bottom no hint\\n43.66584294\\nEQA\\ncos e/v1.11\\naligned with common sense\\n43.6452535\\nMCQA\\napp reviews\\nconvert to rating\\n43.58299315\\nSenti.\\nwiki qa\\nIs This True?\\n43.57268207\\nCBQA\\ndbpedia 14\\ngiven list what category does the paragraph belong to\\n43.57149988\\nTopic C.\\ntrec\\nﬁne grained HUM context ﬁrst\\n43.53804635\\nTopic C.\\ncos e/v1.11\\ni think\\n43.52231837\\nMCQA\\nquarel\\nheres a story\\n43.5107948\\nMCQA\\nwiki qa\\nJeopardy style\\n43.46830805\\nCBQA\\nglue/qqp\\nanswer\\n43.44450543\\nParaph.\\nglue/qqp\\nduplicate or not\\n43.43977509\\nParaph.\\napp reviews\\nconvert to star rating\\n43.43198943\\nSenti.\\nquail\\ndescription context question answer text\\n43.42121948\\nMCQA\\ntrec\\ntrec1\\n43.41024144\\nTopic C.\\napp reviews\\ngenerate review\\n43.40556677\\nSenti.\\nglue/qqp\\nsame thing\\n43.39970221\\nParaph.\\nropes\\nprompt bottom hint beginning\\n43.37137146\\nEQA\\nyelp review full\\nso i would\\n43.35330514\\nSenti.\\nyelp review full\\nbased on that\\n43.35330514\\nSenti.\\nyelp review full\\nformat star\\n43.35330514\\nSenti.\\nyelp review full\\nthis place\\n43.35330514\\nSenti.\\nyelp review full\\nformat score\\n43.35330514\\nSenti.\\nyelp review full\\non a scale\\n43.35330514\\nSenti.\\nyelp review full\\nformat rating\\n43.35330514\\nSenti.\\nropes\\ngiven background situation\\n43.35288364\\nEQA\\nadversarial qa/adversarialQA\\ntell what it is\\n43.34066211\\nEQA\\nwiki qa\\nDirect Answer to Question\\n43.33163471\\nCBQA\\ncos e/v1.11\\nrationale\\n43.30650279\\nMCQA\\nglue/qqp\\nmeaning\\n43.28847032\\nParaph.\\nag news\\nwhich section choices\\n43.23247086\\nTopic C.\\nwiqa\\neffect with label answer\\n43.22751795\\nMCQA\\ntrec\\nﬁne grained NUM context ﬁrst\\n43.20388525\\nTopic C.\\nag news\\nwhich section\\n43.18354617\\nTopic C.\\ndbpedia 14\\npick one category for the following text\\n43.17307426\\nTopic C.\\ndbpedia 14\\ngiven a list of category what does the title belong to\\n43.15357419\\nTopic C.\\nqasc\\nis correct 2\\n43.13462473\\nMCQA\\nquail\\ncontext question answer description text\\n43.13447545\\nMCQA\\nquail\\ncontext question description answer text\\n43.13447545\\nMCQA\\nquail\\ncontext question description text\\n43.13447545\\nMCQA\\nquail\\ncontext description question text\\n43.13447545\\nMCQA\\nquail\\nno prompt text\\n43.13447545\\nMCQA\\nsocial i qa\\nGenerate answer\\n43.13447545\\nMCQA\\nquail\\ncontext description question answer text\\n43.1284649\\nMCQA\\nquail\\ncontext description question answer id\\n43.10641223\\nMCQA\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\nropes\\nread background situation\\n43.09034626\\nEQA\\nag news\\nclassify with choices question ﬁrst\\n43.07124399\\nTopic C.\\nquail\\ndescription context question text\\n43.06430021\\nMCQA\\nadversarial qa/adversarialQA\\nquestion context answer\\n43.04578872\\nEQA\\ntrec\\nﬁne grained open context ﬁrst\\n43.04356783\\nTopic C.\\ndream\\ngenerate-ﬁrst-utterance\\n43.04159372\\nMCQA\\nropes\\nbackground new situation answer\\n43.00629035\\nEQA\\nrotten tomatoes\\nReviewer Enjoyment Yes No\\n42.97922952\\nSenti.\\nquarel\\ndo not use\\n42.96312743\\nMCQA\\nwiki qa\\nTopic Prediction - Question Only\\n42.95471099\\nCBQA\\nquail\\ndescription context question answer id\\n42.91664826\\nMCQA\\nglue/qqp\\nduplicate\\n42.87048524\\nParaph.\\ntrec\\nﬁne grained ENTY\\n42.86820792\\nTopic C.\\ntrec\\nﬁne grained LOC context ﬁrst\\n42.8602283\\nTopic C.\\nglue/mrpc\\ngenerate sentence\\n42.84869263\\nParaph.\\ntrec\\nﬁne grained NUM\\n42.82283103\\nTopic C.\\nimdb\\nReviewer Expressed Sentiment\\n42.79593794\\nSenti.\\nsciq\\nDirect Question\\n42.79083732\\nMCQA\\ncos e/v1.11\\ngenerate explanation given text\\n42.74883356\\nMCQA\\namazon polarity\\nIs this review\\n42.74325079\\nSenti.\\namazon polarity\\nUser recommend this product\\n42.74325079\\nSenti.\\namazon polarity\\nIs this product review positive\\n42.74325079\\nSenti.\\namazon polarity\\nIs this review negative\\n42.74325079\\nSenti.\\namazon polarity\\nconvey negative or positive sentiment\\n42.74325079\\nSenti.\\namazon polarity\\nnegative or positive tone\\n42.74325079\\nSenti.\\namazon polarity\\nuser satisﬁed\\n42.74325079\\nSenti.\\namazon polarity\\nwould you buy\\n42.74325079\\nSenti.\\nglue/mrpc\\ngenerate paraphrase\\n42.74325079\\nParaph.\\namazon polarity\\nﬂattering or not\\n42.7424637\\nSenti.\\nwiki qa\\nfound on google\\n42.73480328\\nCBQA\\nquoref\\nGuess Title For Context\\n42.73108831\\nEQA\\ntrec\\ntrec2\\n42.67551711\\nTopic C.\\nwiqa\\nwhat is the ﬁnal step of the following process\\n42.66352026\\nMCQA\\nquarel\\nchoose between\\n42.63029283\\nMCQA\\ncommonsense qa\\nanswer to question\\n42.62117703\\nMCQA\\nquoref\\nGuess Answer\\n42.61963732\\nEQA\\nimdb\\nReviewer Enjoyment Yes No\\n42.59507536\\nSenti.\\nqasc\\nqa with separated facts 5\\n42.56217194\\nMCQA\\ncosmos qa\\ncontext question description text\\n42.53956247\\nMCQA\\nag news\\nclassify question ﬁrst\\n42.53946803\\nTopic C.\\nsocial i qa\\nI was wondering\\n42.52275144\\nMCQA\\nag news\\nrecommend\\n42.5213931\\nTopic C.\\nimdb\\nReviewer Opinion bad good choices\\n42.51140462\\nSenti.\\nwiki qa\\nTopic Prediction - Answer Only\\n42.46767372\\nCBQA\\nqasc\\nqa with separated facts 3\\n42.45034098\\nMCQA\\ntrec\\nﬁne grained HUM\\n42.43031616\\nTopic C.\\nquail\\ncontext question answer description id\\n42.42340357\\nMCQA\\nquail\\ncontext question description answer id\\n42.42340357\\nMCQA\\nquarel\\nlogic test\\n42.42340357\\nMCQA\\nquail\\nno prompt id\\n42.41294351\\nMCQA\\npaws/labeled ﬁnal\\nparaphrase-task\\n42.38669957\\nParaph.\\nxsum\\nDOC write summary of above\\n42.38486858\\nSumm.\\nxsum\\narticle DOC summary\\n42.38486858\\nSumm.\\nxsum\\nDOC how would you rephrase few words\\n42.38486858\\nSumm.\\nxsum\\ncollege roommate asked DOC so I recap\\n42.38486858\\nSumm.\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\nxsum\\nDOC boils down to simple idea that\\n42.38486858\\nSumm.\\nxsum\\nsummarize DOC\\n42.38486858\\nSumm.\\nxsum\\nsummarize this DOC summary\\n42.38486858\\nSumm.\\ncosmos qa\\ncontext description question text\\n42.3758318\\nMCQA\\nquoref\\nWhat Is The Answer\\n42.32137551\\nEQA\\nsamsum\\nGenerate a summary for this dialogue\\n42.31155911\\nSumm.\\nglue/mrpc\\nwant to know\\n42.29343352\\nParaph.\\nsamsum\\nGiven the above dialogue write a summary\\n42.27633128\\nSumm.\\nsciq\\nDirect Question (Closed Book)\\n42.26387475\\nMCQA\\nglue/mrpc\\nequivalent\\n42.26079671\\nParaph.\\nglue/mrpc\\nparaphrase\\n42.24289148\\nParaph.\\nglue/mrpc\\nreplace\\n42.2412404\\nParaph.\\nquoref\\nContext Contains Answer\\n42.23229654\\nEQA\\nquoref\\nGiven Context Answer Question\\n42.2152412\\nEQA\\nquoref\\nRead And Extract ’\\n42.21343959\\nEQA\\ncommon gen\\nsentence to concepts\\n42.14160703\\nSTS\\ntrec\\nﬁne grained open\\n42.13572199\\nTopic C.\\nquarel\\ntesting students\\n42.09377162\\nMCQA\\nhotpot qa/fullwiki\\ngenerate answer afﬁrmative\\n42.05311313\\nCBQA\\nhotpot qa/fullwiki\\ngenerate explanations afﬁrmative\\n42.05311313\\nCBQA\\nhotpot qa/fullwiki\\ngenerate answer interrogative\\n42.05311313\\nCBQA\\ncosmos qa\\nonly question answer\\n42.03758485\\nMCQA\\nquoref\\nFound Context Online\\n42.02555959\\nEQA\\ntrec\\nﬁne grained ABBR\\n42.01818176\\nTopic C.\\nsamsum\\nTo sum up this dialog\\n42.01224255\\nSumm.\\ncommon gen\\ntopics from the sentence\\n42.00149943\\nSTS\\ntrec\\nﬁne grained DESC\\n41.97978705\\nTopic C.\\ngigaword\\ngenerate summary for this\\n41.97889741\\nSumm.\\ngigaword\\nreverse writing\\n41.97889741\\nSumm.\\ngigaword\\nmake a title\\n41.97889741\\nSumm.\\ngigaword\\nﬁrst sentence title\\n41.97889741\\nSumm.\\ngigaword\\nTLDR\\n41.97889741\\nSumm.\\ngigaword\\nwrite its sentence\\n41.97889741\\nSumm.\\ngigaword\\nwrite a title for this sentence\\n41.97889741\\nSumm.\\ngigaword\\nin a nutshell\\n41.97889741\\nSumm.\\nsamsum\\nWrite a dialogue that match this summary\\n41.97889741\\nSumm.\\ngigaword\\nwrite an article\\n41.93445375\\nSumm.\\ntrec\\nﬁne grained ABBR context ﬁrst\\n41.91844542\\nTopic C.\\ncnn dailymail/3.0.0\\nwrite an outline\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\nnews summary\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\n2 or 3 sentences\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\ntldr summary\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\nnews card view\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\ngenerate story\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\nsum in brief\\n41.91841535\\nSumm.\\ncnn dailymail/3.0.0\\nnews stock\\n41.91841535\\nSumm.\\nquoref\\nAnswer Friend Question\\n41.91841535\\nEQA\\ncnn dailymail/3.0.0\\nspice up story\\n41.91295723\\nSumm.\\ntrec\\nwhat category best describe\\n41.89413219\\nTopic C.\\nwiqa\\nwhich of the following is the supposed perturbation\\n41.8674171\\nMCQA\\ncosmos qa\\ncontext answer to question\\n41.86422765\\nMCQA\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\nxsum\\nDOC given above write one sentence\\n41.81263384\\nSumm.\\nxsum\\nread below DOC write abstract\\n41.81263384\\nSumm.\\nxsum\\nDOC tldr\\n41.81263384\\nSumm.\\nrotten tomatoes\\nWriter Expressed Sentiment\\n41.80526158\\nSenti.\\nimdb\\nMovie Expressed Sentiment 2\\n41.80336688\\nSenti.\\nwiki hop/original\\nchoose best object interrogative 1\\n41.78715174\\nMCQA\\nwiki hop/original\\nexplain relation\\n41.78715174\\nMCQA\\nwiki hop/original\\ngenerate object\\n41.78715174\\nMCQA\\nwiki hop/original\\ngenerate subject\\n41.78715174\\nMCQA\\nwiki hop/original\\nchoose best object afﬁrmative 1\\n41.78715174\\nMCQA\\nwiki hop/original\\nchoose best object afﬁrmative 3\\n41.78715174\\nMCQA\\nwiki hop/original\\ngenerate subject and object\\n41.78715174\\nMCQA\\nwiki hop/original\\nchoose best object afﬁrmative 2\\n41.78715174\\nMCQA\\nwiki hop/original\\nchoose best object interrogative 2\\n41.78715174\\nMCQA\\napp reviews\\ncategorize rating using review\\n41.7833793\\nSenti.\\nsamsum\\nSummarize this dialogue:\\n41.78235121\\nSumm.\\nsamsum\\nSum up the following dialogue\\n41.75107511\\nSumm.\\ntrec\\nﬁne grained LOC\\n41.73465262\\nTopic C.\\nrotten tomatoes\\nReviewer Expressed Sentiment\\n41.72418821\\nSenti.\\nglue/mrpc\\nsame thing\\n41.72027244\\nParaph.\\nwiqa\\nwhat is the missing ﬁrst step\\n41.70884339\\nMCQA\\nwiqa\\nwhat might be the ﬁrst step of the process\\n41.6543053\\nMCQA\\nsamsum\\nSummarize:\\n41.6530481\\nSumm.\\nhotpot qa/fullwiki\\ngenerate title afﬁrmative\\n41.64987718\\nCBQA\\nhotpot qa/fullwiki\\ngenerate question\\n41.640237\\nCBQA\\nmulti news\\nsummary scenario\\n41.62718689\\nSumm.\\nimdb\\nWriter Expressed Sentiment\\n41.60178406\\nSenti.\\nrotten tomatoes\\nReviewer Enjoyment\\n41.58082141\\nSenti.\\ndream\\nanswer-to-dialogue\\n41.56118159\\nMCQA\\ncosmos qa\\ndescription context question text\\n41.5598663\\nMCQA\\nmulti news\\nwhat are the key points\\n41.55348468\\nSumm.\\nmulti news\\ndistill\\n41.55348468\\nSumm.\\nag news\\nclassify\\n41.52154068\\nTopic C.\\nrotten tomatoes\\nText Expressed Sentiment\\n41.51669372\\nSenti.\\nmulti news\\nexpand (reverse task)\\n41.49696057\\nSumm.\\nrotten tomatoes\\nSentiment with choices ’\\n41.49571862\\nSenti.\\nwiqa\\nwhat might be the last step of the process\\n41.4814863\\nMCQA\\nmulti news\\nsummarize\\n41.45677025\\nSumm.\\nmulti news\\nsynthesize\\n41.428813\\nSumm.\\ncommon gen\\nchoice in concept centric sentence generation\\n41.40527643\\nSTS\\ndbpedia 14\\ngiven a choice of categories ‘\\n41.39273021\\nTopic C.\\nrotten tomatoes\\nMovie Expressed Sentiment\\n41.35952481\\nSenti.\\nrotten tomatoes\\nReviewer Sentiment Feeling\\n41.29297692\\nSenti.\\nimdb\\nMovie Expressed Sentiment\\n41.29017\\nSenti.\\nduorc/ParaphraseRC\\nbuild story around qa\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\ndecide worth it\\n41.25012619\\nEQA\\nExploring the Beneﬁts of Training Expert Language Models over Instruction Tuning\\nDataset\\nPrompt\\nAVG\\nTask Category\\nduorc/ParaphraseRC\\nquestion answering\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\nmovie director\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\ngenerate question\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\nextract answer\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\ntitle generation\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\nanswer question\\n41.25012619\\nEQA\\nduorc/ParaphraseRC\\ngenerate question by answer\\n41.25012619\\nEQA\\ncommon gen\\nPut together\\n41.20526211\\nSTS\\nquoref\\nFind Answer\\n41.12144463\\nEQA\\nrotten tomatoes\\nMovie Expressed Sentiment 2\\n41.10981068\\nSenti.\\nquoref\\nAnswer Question Given Context\\n41.09694099\\nEQA\\nwiki bio\\nwho\\n41.07422576\\nSTS\\nimdb\\nReviewer Sentiment Feeling\\n41.04883277\\nSenti.\\nadversarial qa/adversarialQA\\ngenerate question\\n40.97089459\\nEQA\\nwiqa\\ndoes the supposed perturbation have an effect\\n40.94586331\\nMCQA\\nquoref\\nAnswer Test\\n40.88342121\\nEQA\\nimdb\\nNegation template for positive and negative\\n40.80008389\\nSenti.\\ncommon gen\\nGiven concepts - type 2\\n40.72623213\\nSTS\\nimdb\\nReviewer Enjoyment\\n40.70140793\\nSenti.\\nimdb\\nSentiment with choices ’\\n40.60427787\\nSenti.\\ncommon gen\\ntopic to sentence\\n40.54846736\\nSTS\\nimdb\\nText Expressed Sentiment\\n40.53260931\\nSenti.\\ncommon gen\\nGiven concepts type 1\\n40.52827679\\nSTS\\ncommon gen\\nrandom task template prompt\\n40.3974667\\nSTS\\ncommon gen\\nExample prompt\\n39.6913846\\nSTS\\nTable 12. The full list of Prompt Experts (PE) ranked in the mean accuracy on the 11 unseen tasks. The evaluations are performed on 300\\nsample instances of each unseen task for efﬁciency.\\n', 'source_name': 'Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES', 'source_url': 'https://arxiv.org/abs/2302.03202'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Expert_Gate_NOTES.pdf #15\n",
      "{'content': 'Expert Gate: Lifelong Learning with a Network of Experts \\nMain idea: this paper experiments with a novel approach for using MoE in a multi-task setting. \\nMore specifically, it focuses on the gating mechanism used. Expert Gate also focuses on \\nscalability, as it is a lifelong learning approach (can be scaled with time). A lifelong learning \\napproach means that: \\n- \\nModels are trained sequentially. \\n- \\nNo need to store the data used for training, only the models. \\nExpert Gate is trained on image classification and video prediction problems, but could \\ntechnically also be used in an NLP/LLM setting (but was not experimented with) \\n \\nAdvantages of Expert Gate \\nThe meat of this method is in the autoencoder gating mechanism used. This mechanism solves \\nproblems as: \\n- \\nData storage, since the models can be trained sequentially, so keeping all training data is \\nnot necessary. \\no Later the paper will show that storing training data used previously is not \\nnecessary. \\n- \\nCatastrophic forgetting, which is an issue other models suffer with. For example, \\ncontinuously training and fine-tuning the same model on new tasks will lead to this issue. \\n- \\nTask biases when fine-tuning which can lead to suboptimal local minima. \\no If a model is trained on a task and fine-tuned on a widely different task, it can lead \\nto suboptimal results due to the biases inferred in the initial task being different.  \\n- \\nMemory efficiency, as only one expert needs to be loaded into memory at a time. \\n- \\nTask relatedness, which can be measured by the autoencoder’s results and then be used \\nto figure out how to initialize the expert’s parameters for a new class and either to use \\nfine-tuning or learning-without-forgetting (LwF) for training the new expert. \\nLwF vs Fine-tuning \\nWhen two tasks are sufficiently related (above a certain task relatedness in threshold), it is \\nbeneficial to train a new expert with LwF based on an old task, otherwise the best approach is to \\nfine-tune the expert for the similar (existing) task on the training data from the new task. \\n- \\nFine-tuning \\no Based on an existing model, simply continue training using a new dataset \\no The result of this fine-tuning on an existing expert will be a brand-new expert, \\nwhile the existing expert that it was based on will remain unchanged. \\n▪ So, this process starts with 1 expert and ends up with 2 experts. \\n- \\nLwF \\no Technique used to prevent catastrophic forgetting when training an existing \\nmodel on new data. LwF uses soft targets (outputs of the old model) to help retain \\nknowledge from old tasks. \\no As with fine-tuning, this results in 2 experts. \\n \\nAutoencoder Mechanism – Expert Gate Inner Working \\nGoals: \\n- \\nTo select an expert based on input data. \\n- \\nTo measure task relatedness to figure out optimal parameters to initialize an expert \\n(based on most related task) and training strategy (fine-tuning or LwF – LwF in the case \\nof task relatedness being above a certain threshold). \\nThe Inner Workings of the Autoencoder \\n- \\nIt follows a regular encoder-decoder architecture. \\n- \\nEncoder 𝑓(ℎ(𝑥)), maps the input x to a code h(x). \\n- \\nDecoder 𝑟= 𝑔(ℎ(𝑥)), maps the encoder’s code (h(x)) to a reconstruction of the input. \\n- \\nThe autoencoder simply uses an encoder-decoder architecture to deconstruct the input \\n(done by the encoder) and reconstruct it (done by the decoder). \\n- \\nThe loss function 𝐿(𝑥, 𝑔(ℎ(𝑥))) is simply the reconstruction error. \\n- \\nThe encoder learns, through a hidden layer, a lower dimensional representation \\n(undercomplete autoencoder) or a higher dimensional representation (overcomplete \\nautoencoder) of the input data. \\n- \\nThe lower dimensional subspace learned by one of the undercomplete autoencoders will \\nbe maximally sensitive to variations observed in the task data but insensitive to changes \\northogonal to the manifold (it represents only the variations that are needed to \\nreconstruct relevant samples) \\no The autoencoder of a domain/task should be better at reconstructing the data of \\nthe task it was trained on better than the other autoencoders. \\n▪ The reconstruction error for each autoencoder then allows the input to be \\nrouted to the expert of the task of the autoencoder with the lowest \\nreconstruction error for that input (or multiple, in the case of multiple very \\ngood autoencoders for that input). \\n▪ The reconstruction error then acts like a score (all reconstruction errors \\nare passed through a SoftMax to determine a normalized score). \\n- \\nThe task relatedness between two tasks is also measured through the autoencoder’s \\nreconstruction error through the following formula: \\no 𝑅𝑒𝑙(𝑇𝑘, 𝑇\\n𝑎) = 1 −(\\n(𝐸𝑅𝑅𝑎−𝐸𝑅𝑅𝑘)\\n𝐸𝑅𝑅𝑘\\n) \\n▪ 𝑇𝑘 = new task. 𝑇\\n𝑎 = old task. \\n▪ 𝑅𝑒𝑙(𝑇𝑘, 𝑇\\n𝑎) is the relatedness between task k and task a. \\n▪ 𝐸𝑅𝑅𝑎 is the reconstruction error of the autoencoder for task a in the data \\nfor task k. \\n▪ 𝐸𝑅𝑅𝑘 is the reconstruction error of the autoencoder for task k on its own \\ndata. \\n▪ How can the reconstruction error of the autoencoder for task k on its own \\ndata be computed before the expert (and thus its autoencoder) is trained, \\nsince its initialization method relies on this task relatedness computation? \\nThis seems redundant. \\n \\nExperiments Results \\n- \\nExpert Gate was compared with and outperformed (on image classification): \\no Single fine-tuned model (sequentially fine-tuned on each task). \\n▪ One would think that this would result in severe catastrophic forgetting. \\no Single LwF model (sequentially trained on each task). \\n▪ One would think that you can’t train the same model with LwF forever on \\nmany different tasks without running into catastrophic forgetting issues. \\n- \\nExpert Gate performed on-par with: \\no Joint training (assumes all is always available for re-training). \\no Multiple fine-tuned models (fine-tuned on each task separately) \\n▪ This assumes an oracle gate, that is, a gate that knows perfectly how to \\nroute each input to the corresponding expert. \\no Multiple LwF models (trained on each task separately). \\n▪ Also assumes an oracle gate. \\n- \\nExpert Gate vs Discriminative Classifier (neural net trained on all the data available for \\ngating decisions – a routing mechanism). \\no Without ever having simultaneous access to the data of different tasks, Expert \\nGate based on autoencoders manages to assign test samples to the relevant tasks \\nequally accurately as a discriminative classifier (which assumes all training data is \\navailable). \\n- \\nTask relatedness analysis \\no Expert Gate succeeds in predicting when a task could help another in the LwF \\nframework and when it cannot (LwF vs fine-tuning decision). \\n \\n \\nMy takeaways: \\n- \\nThis is an interesting point to take note of when thinking of a problem related to fine-\\ntuning, especially when fine-tuning MoE. \\no Task biases when fine-tuning which can lead to suboptimal local minima. \\n▪ If a model is trained on a task and fine-tuned on a widely different task, it \\ncan lead to suboptimal results due to the biases inferred in the initial task \\nbeing different (think that the pre-training distribution shift can lead to \\nlocal minima that is optimal for that distribution, but distribution of new \\ntasks can be different and gain from other local minima that are \\nunreachable due to the pre-training local minima – imagine the gradient \\ndescent valley) \\n- \\nExpert Gate seems like DEMix.  \\no Expert Gate focuses on the LwF or fine-tuning decision when being presented a \\nnew task, DEMix focuses more on the modularity of each expert. \\no Expert Gate focuses on task-level experts while DEMix focuses on domain-level \\nexperts. \\no Expert Gate experiments on computer vision tasks while DEMix focuses on NLP \\ntasks. \\n- \\nBoth LwF and fine-tuning lead to the existing expert that was further trained with LwF or \\nfine-tuning remaining unchanged while also creating a new expert. So, 2 experts are a \\nresult of this process (one old, one new). \\n- \\nBoth the routing to determine the similarity of an input with the tasks reflected in the \\nexisting experts and the task relatedness are determined by an autoencoder mechanism \\nwhich is independent for each expert (it is trained as the expert is trained). \\n- \\nThe LwF method seems to be fine-tuning with a twist – instead of only fine-tuning with \\nhard targets from the new data, fine-tune is done by considering the new data and soft \\ntargets given by the existing expert. \\n- \\nRun through the methodology: \\no This method is a task-level MoE – it has the advantage of only routing the input \\nsequence once. Since this is done at the beginning of inference, the selected task \\nexperts can be pre-loaded to memory and the routing does not need to be \\nperformed again, saving on memory costs of loading different experts for every \\nnew token.  \\no Each task expert consists of the expert itself and an autoencoder, which is used \\nfor two things: \\n▪ Determine the similarity of an input sequence to the task (how well does \\nthe task expert fit into the input sequence). \\n▪ Determine the task relatedness between different tasks to help training of \\nnew experts. \\no Training new experts can be done in one of two ways: \\n▪ LwF, which uses soft targets of the existing/old model to train a new model \\nbased on the new task’s data. \\n▪ Fine-tuning, which fine-tunes an existing/old expert with new data, \\nresulting in a new expert. \\no Expert Gate also has the advantage of not all data needing to be stored on the \\nsame place at once for training. Since training can be done sequentially, training \\ndata can be used and sequentially discarded, saving on storage costs. \\n- \\nThe autoencoder is simply a function that deconstructs and attempts to reconstruct the \\ninput. The logic is that the closer the input is to the training data used to train that task’s \\nexpert, the better the autoencoder will be at reconstructing the input. \\n- \\nIn computing the relatedness between two tasks, how can the reconstruction error of the \\nautoencoder for task k on its own data be computed before the expert (and thus its \\nautoencoder) is trained, since its initialization method relies on this task relatedness \\ncomputation? This seems redundant. \\n \\n', 'source_name': 'Expert Gate: Lifelong Learning with a Network of Experts NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Expert_Gate_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BeyondDistillation_Task_Level_MoE.pdf #16\n",
      "{'content': 'Beyond Distillation: Task-level Mixture-of-Experts for Efﬁcient Inference\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun,\\nDmitry Lepikhin, Thang Luong, Orhan Firat\\nGoogle Research\\n{snehark,huangyp,ankurbpn,krikun,lepikhin,\\nthangluong,orhanf}@google.com\\nAbstract\\nSparse Mixture-of-Experts (MoE) has been\\na successful approach for scaling multilin-\\ngual translation models to billions of param-\\neters without a proportional increase in train-\\ning computation. However, MoE models are\\nprohibitively large and practitioners often re-\\nsort to methods such as distillation for serv-\\ning. In this work, we investigate routing strate-\\ngies at different granularity (token, sentence,\\ntask) in MoE models to bypass distillation. Ex-\\nperiments on WMT and a web-scale dataset\\nsuggest that task-level routing (task-MoE) en-\\nables us to extract smaller, ready-to-deploy\\nsub-networks from large sparse models.\\nOn WMT, our task-MoE with 32 experts\\n(533M parameters) outperforms the best per-\\nforming token-level MoE model (token-MoE)\\nby +1.0 BLEU on average across 30 language\\npairs. The peak inference throughput is also\\nimproved by a factor of 1.9x when we route\\nby tasks instead of tokens. While distilling a\\ntoken-MoE to a smaller dense model preserves\\nonly 32% of the BLEU gains, our sub-network\\ntask-MoE, by design, preserves all the gains\\nwith the same inference cost as the distilled\\nstudent model.\\nFinally, when scaling up to\\n200 language pairs, our 128-expert task-MoE\\n(13B parameters) performs competitively with\\na token-level counterpart, while improving the\\npeak inference throughput by a factor of 2.6x.\\n1\\nIntroduction\\nScaling up neural network models has recently re-\\nceived great attention, given the signiﬁcant quality\\nimprovements on a variety of tasks including nat-\\nural language understanding (Raffel et al., 2019;\\nBrown et al., 2020) and multilingual machine trans-\\nlation (Huang et al., 2019; Lepikhin et al., 2020).\\nWhile training massive models on large amounts\\nof data can almost guarantee improved quality,\\nthere are two factors affecting their practicality\\nand applicability: (1) training efﬁciency and (2)\\ninference efﬁciency. Large dense models are often\\nprohibitively compute-intensive to train, with some\\nmodels requiring TFlops-days of compute (Brown\\net al., 2020). A recent line of work has proposed\\nsparsely-gated Mixture-of-Experts (MoE) layers as\\nan efﬁcient alternative to dense models (Shazeer\\net al., 2017; Lepikhin et al., 2020; Riabinin and\\nGusev, 2020) in order to address training efﬁciency\\nlimitations. In a vanilla sparsely-gated MoE model\\neach token of the input sequence activates a differ-\\nent subset of the experts, hence the computation\\ncost per token becomes only proportional to the\\nsize of the activated sub-network. However, they\\nfail to meet requirements on inference efﬁciency.\\nConsider a long sequence where each token of\\nthe sequence activates a disjoint subset of available\\nexperts. From a practical standpoint, the inference\\ntrace of the full sequence spans several experts\\nindependently for every token, resulting in an inde-\\npendent pathway for each token. Although this is\\na desired property - adding ﬂexibility to the model\\nand increasing its capacity - it becomes prohibitive\\nfor inference for the following reasons: the model\\nparameters in these large models are beyond the\\nmemory limit of a single accelerator device, and\\nrequire model parallelism to shard them across a\\ncluster of devices during inference. For models\\nwith MoE Layers, the input token would be dy-\\nnamically routed to different experts allocated to\\ndifferent devices. This further adds communica-\\ntion cost across devices to the overall serving cost.\\nMoreover, due to the sequential nature of the auto-\\nregressive decoding (Kasai et al., 2020; Chen et al.,\\n2018), the added communication cost from model\\nparallel decoders gets multiplied by the number\\nof decoding steps. To add to this, serving MoE\\nmodels efﬁciently requires batching a large number\\nof input tokens together, otherwise only a subset\\nof the MoE network will be activated leading to\\nsevere device under-utilization.\\nIn this work, we study the inference efﬁciency of\\nsparsely gated MoE models while taking into ac-\\narXiv:2110.03742v1  [cs.CL]  24 Sep 2021\\ncount the characteristics of the intended application,\\nMultilingual Neural Machine Translation (MNMT).\\nMNMT is an inherently multi-task learning prob-\\nlem, aimed at building a single neural network for\\ntranslating multiple language pairs simultaneously.\\nIn a MNMT model, the extent to which parameters\\nare shared across languages determines the magni-\\ntude of positive transfer (Baldwin and Ford, 1988)\\nand conversely task interference due to the capacity\\nbottleneck (Arivazhagan et al., 2019). In an ideal\\nscenario, we would want to efﬁciently train a sin-\\ngle large MNMT model maximizing transfer while\\nexpanding the capacity bottleneck; meanwhile, we\\nwould like to enjoy the beneﬁts of sparsely acti-\\nvated sub-networks per-task at inference time, i.e.\\nextracting out a sub-network to decode for a partic-\\nular language pair to actualize inference efﬁciency.\\nAn alternative way to enjoy high inference ef-\\nﬁciency from a large model is knowledge distilla-\\ntion (Hinton et al., 2015). However, (Fedus et al.,\\n2021) found that only a small fraction of quality\\ngains from a large sparse model can be preserved\\nin the student models. Instead;\\n• We propose routing algorithms for MoE mod-\\nels with affordable serving costs (Section 3).\\nWhile vanilla MoEs route each sub-word to-\\nken in the input to its preferred experts, we\\nexplore alternative routing strategies that are\\ntrained to leverage global task level informa-\\ntion to route all tokens corresponding to a\\nparticular task collectively to the same set of\\nexperts. We decode different tasks separately\\nand only load the subset of experts associated\\nwith the corresponding task during inference.\\n• We report the advantages of our task-level\\nrouting method in translation quality and in-\\nference cost on a multilingual WMT task (Sec-\\ntion 4). With the comparable inference cost,\\nthe task-level routing achieved +3.6 BLEU\\ngain over the multilingual model training from\\nscratch, and +2.1 BLEU gain over the dense\\nstudent model distilled from the large token-\\nlevel /position-wise MoE (token-MoE) model.\\n• The observed quality gains from our approach\\nare comparable with the token-MoE models\\nwhile achieving 1.9x peak throughput and\\n6.3% of the decoder size.\\n• We scaled up the token-MoE model on a large\\nscale in-house dataset and saw similar quality\\ngains (+3.6 BLEU) against the dense baseline\\n(Section 5.2). Compared to the token-level\\nrouting approach, our method achieves com-\\nparable quality gain, with 2.6x higher peak\\nthroughput and 1.6% of the decoder size.\\n• Finally, we analyze the routing decisions\\nmade in MoE models and motivate our\\nmethod (Section 5.4).\\n2\\nScaling Transformers with\\nMixture-of-Experts\\nThe Transformer (Vaswani et al., 2017) architec-\\nture is a popular model used for neural machine\\ntranslation and other natural language understand-\\ning/generation problems. In sequence-to-sequence\\nproblems, the model consists of an encoder and de-\\ncoder, each of which contains multiple Transformer\\nlayers. For further details, we refer the reader to\\nthe original paper (Vaswani et al., 2017).\\nWe use the Mixture-of-Experts Transformer\\nmodels proposed by (Lepikhin et al., 2020), where\\nthe MoE layers for the Transformers consist\\nof E feed-forward networks (FFN), such that\\n(FFN1 . . . FFNE).\\nFFNe(xs) = woe · ReLU(wie · xs)\\nys =\\nE\\nX\\ne=1\\nGs,e · FFNe(xs)\\nHere, xs is the input token at position s to the MoE\\nlayer and each FFNe is a two layer neural network\\nusing a ReLU activation function. wie and woe\\nare the input and output projection weights of the\\ne-th expert. Finally, Gs,E is vector computed by\\nthe gating network (also referred as router). For\\neach expert, most values of this vector are zeros,\\none value being positive. We use this vector to\\nroute the token to a select few experts. The entries\\nchosen from Gs,E determine how much the expert\\ncontributes to the ﬁnal output ys. Note that, in this\\nwork we choose the top 2 weight experts for each\\nexample to be comparable with the prior work.\\nThe gating network Gs,E must be considered\\ncarefully for efﬁciency purposes: (1) the utilization\\nof experts must be balanced and (2) the function\\nmust be efﬁcient to implement at scale. For a more\\nthorough discussion of MoE transformers, we di-\\nrect the reader to (Lepikhin et al., 2020).\\n3\\nMethods\\nIn this section we describe our candidate routing\\nstrategies in the context of MNMT and discuss\\n(a) Task MoE\\n(b) Token MoE\\nFigure 1: Tokens are routed to the same expert based on task or some other prior in (a) task-based MoE whereas\\ndifferent tokens are routed to different experts in (b) token-based MoE models.\\ntheir trade-offs from the perspective of the train-\\ning and inference efﬁciency. Multilingual models\\nlearn joint representations across languages to the\\nextent of the parameters being shared (Wu and\\nDredze, 2019; Tiedemann, 2018; Tan et al., 2019;\\nZhang et al., 2020; Östling and Tiedemann, 2016;\\nKudugunta et al., 2019). While being beneﬁcial for\\ntransfer, extreme sharing of the parameters exacer-\\nbates interference. Allowing dedicated (unshared)\\nparameters are known to be effective at mitigat-\\ning interference (Zhang et al., 2021; Kong et al.,\\n2021) and MoE variants are inherently learn such\\npartitioning across languages/tasks. Therefore we\\nstudy the routing algorithm GATE(xs) of MoEs to\\nmitigate interference, while enabling transfer and\\neffective at inference.\\n3.1\\nRouting Strategies\\nGiven the sequential nature of the multilingual ma-\\nchine translation task, the routing decisions can\\nbe made at three different granularities, from bot-\\ntom up (i) token-level, (ii) sentence-level and (iii)\\ntask-level, as detailed below.\\nToken-level Routing:\\nThis is the baseline dis-\\ncussed in Section 2 where each token is routed\\nindependently.\\nSentence-level Routing:\\nEach sequence (sen-\\ntence), and all tokens that form the sequence, are\\nrouted to the same expert. We change the rout-\\ning algorithm to select experts by sentence repre-\\nsentation, calculated by taking the average token\\nrepresentations in a given sentence.\\nTask-level Routing:\\nWe select experts by task\\nboundaries as opposed to making input-level deci-\\nsions. In the context of MNMT, these task bound-\\naries can either be deﬁned by the target language\\n(French-to-English and German-to-English are the\\nsame task) or the language pair (French-to-English\\nand German-to-English are different tasks). Sen-\\ntence and task level routing are formulated as fol-\\nlows:\\nGs,E = GATE( 1\\nS\\nS\\nX\\ns=1\\nxs)\\n(Sentence-level),\\nGs,E = GATE(task_ids)\\n(Task-level).\\nWe illustrate the difference in Figure 1, in token-\\nbased MoE models (Figure 1b), tokens from each\\ndatapoint are routed to different experts, whereas\\nin task-level MoE models (Figure 1a), tokens may\\nbe routed to the same expert based on task.\\n3.2\\nInference Implications of Routing\\nStrategies\\nWhile the MoE models discussed in (Shazeer et al.,\\n2017; Lepikhin et al., 2020) train quickly relative\\nto the number of parameters in terms of the wall-\\nclock time, they are expensive to serve. Consider\\na MoE with 512 experts and 50B parameters (Lep-\\nikhin et al., 2020). When employing token-level\\nrouting, each token can be independently routed to\\na different set of experts during inference. Given\\nthat the entire model is too large to load into mem-\\nory on a single accelerator, the two potential so-\\nlutions to utilize this model for inference are: (i)\\nLoading experts dynamically from host to device\\ndepending on routing decisions, or (ii) Utilizing\\nmodel-parallelism over multiple accelerators for\\nserving. While the ﬁrst solution incurs heavy host-\\ndevice communication costs, the second introduces\\nsigniﬁcantly inter-device communication overhead.\\nOther practical approaches to serve a large MoE\\ninclude model quantization, pruning and knowl-\\nedge distillation (Cheng et al., 2017). While the\\nﬁrst two strategies haven’t been explored in the con-\\ntext of conditional computation, distillation (Hin-\\nton et al., 2015; Kim and Rush, 2016) has been\\nfound to introduce undesirable artifacts into the\\nstudent model (Freitag et al., 2019; Bogoychev and\\nSennrich, 2019) in the context of NMT. Moreover,\\nsome studies have found that distilling large sparse\\nmodels preserves only a small fraction of the gains\\nachieved by scaling. On the other hand, if we limit\\nthe number of experts available to every task in\\nthe model to a small fraction of the total available\\ncapacity, it is possible to extract task-speciﬁc mod-\\nels for serving, alleviating the need for complex\\nserving strategies or compression. Since decod-\\ning time complexity for auto-regressive encoder-\\ndecoder models is dominated by the decoder (Kasai\\net al., 2020), we can also pursue a hybrid strategy\\nwhere the encoder utilizes more expensive routing\\nstrategies while the decoder of the model utilizes\\nsimpler and efﬁcient routing.\\nSummarizing the effective decoding cost of the\\nMoE models utilizing different routing strategies:\\n• Token/Sentence level routing: The routing\\ndecisions are made dynamically. Assuming\\neach token/sentence makes disjoint choices,\\nthe server needs to load all E experts.\\n• Task-level routing: Tokens corresponding to\\neach input sentence are routed to the same\\nexperts statically. The server only needs to\\npre-load K experts (assuming top-K routing).\\n4\\nExperiments on 30 Language Pairs\\nWe compare routing strategies at multiple levels in\\nboth, the encoder and the decoder, by conducting\\nextensive experiments on two benchmarks: the pub-\\nlic WMT dataset with 30 language pairs (Section\\n4.1) and an in-house web-scale dataset with 200\\nlanguage pairs (Section 5). We start with WMT\\nsetup.\\n4.1\\nExperimental Setup\\nFor our experiments, we use parallel training and\\nevaluation data from the WMT corpus and adopt\\nthe setup used by (Siddhant et al., 2020) with 15\\nlanguages, to and from English. Full training data\\ndetails may be found in Table 3 in the Appendix.\\nThe amount of data ranges from more than 60 mil-\\nlion sentence pairs in en-cs translation direction\\n(en-cs) to roughly 150k sentence pairs for en-gu.\\nWe use a temperature based data sampling strat-\\negy to train our models, similar to the strategy used\\nto train the multilingual models in (Arivazhagan\\net al., 2019): if pL is the probability that a sen-\\ntence in the corpus belongs to language pair L, we\\nsample from a distribution where the probability\\nof sampling from L is proportional to pL\\n1\\nT . All\\nthe experiments in this paper are performed on a\\nmodel trained with a sampling temperature T = 5.\\nWe use the 142M Transformer Base (Vaswani\\net al., 2017) architecture (or enhanced versions of\\nit with MoE layers) for all of our experiments with\\nWMT. Our models are optimized using Adafactor\\n(Shazeer and Stern, 2018) with momentum factor-\\nization and a per-parameter norm clipping thresh-\\nold of 1.0. We followed a learning rate of 3.0,\\nwith 40K warm-up steps for the schedule, which\\nis decayed with the inverse square root of the num-\\nber of training steps after warm-up. BLEU scores\\npresented in this paper are calculated using Sacre-\\nBLEU (Post, 2018) on the WMT test sets.\\nMultilingual baseline:\\nWe train a Transformer\\nBase model on this dataset as our multilingual\\ndense baseline. We share all parameters across\\nlanguage pairs, including the softmax layer and in-\\nput/output word embeddings. We use a 64k token\\nSentence Piece vocabulary (Kudo and Richardson,\\n2018). The vocabulary is shared on both the en-\\ncoder and decoder side. Each sentence pair has\\na <2xx> token pre-pended to the source sentence\\nto indicate the target language, following Johnson\\net al. (2017).\\nMixture of Experts Models:\\nFor MoE models,\\nwe replace the feed forward network (FFN) of al-\\nternate layers of the Transformer with a set of iden-\\ntical FFN experts as depicted in Figure 1b. For\\nbrevity, we provide aggregate BLEU scores in Sec-\\ntion 4.2 . We provide the full individual BLEU\\nSystem\\nRouting Granularity\\nThroughput\\nBLEU\\nEncoder\\nDecoder\\nPeak tokens/s\\nAverage\\nxx2en\\nen2xx\\nHigh\\nLow\\nBilingual Baselines\\n-\\n-\\n2.3 × 105\\n21.0\\n21.8\\n18.9\\n28.2\\n11.8\\nMultilingual Transformer-Base\\n-\\n-\\n20.0\\n23.7\\n17.5\\n23.3\\n15.9\\nStatic MoE – 32 experts\\n-\\n-\\n2.3 × 105\\n17.6\\n25.0\\n10.2\\n20.9\\n13.5\\nToken-level MoE – 32 experts\\nToken\\nToken\\n1.3 × 105\\n22.6\\n24.9\\n20.4\\n27.5\\n16.3\\nSentence-level MoE – 32 expert\\nSentence\\nSentence\\n1.3 × 105\\n19.9\\n24.1\\n16.8\\n22.6\\n16.1\\nTask-level MoE – 32 experts\\nLanguage Pair\\nLanguage Pair\\n2.3 × 105\\n21.4\\n25.2\\n16.9\\n23.4\\n17.3\\nTarget\\nTarget\\n22.9\\n25.6\\n20.2\\n27.2\\n17.3\\nLanguage Pair\\nToken\\n22.4\\n25.6\\n20.3\\n26.9\\n16.8\\nTarget\\nToken\\n22.3\\n24.5\\n20.4\\n26.8\\n16.6\\nToken\\nLanguage Pair\\n23.0\\n26.2\\n20.3\\n27.2\\n17.6\\nToken\\nTarget\\n23.6\\n26.0\\n21.1\\n28.5\\n17.4\\nTable 1: Routing strategies for Mixture-of-Experts (MoE) models – We compare routing experts by either\\ntokens, sentence representations, or tasks (using either language pairs or target languages). For task-level MoE,\\nrouting can also be different between encoder and decoder. For results, Average is the average results of all\\nlanguage pairs, whereas xx2en and en2xx are the averages of translations into and from English respectively. High\\nindicates high-resource language pairs (> 1 million sentence pairs) while Low is for low-resource language pairs\\n(< 1 million sentence pairs).\\nscores in the Appendix A.3, along with bilingual\\nbaselines. In addition, we provide the number of\\nparameters for different components of our models\\nin Appendix A.4.\\n4.2\\nComparison of different Routing\\nStrategies on WMT\\nWe compare the token-level, sentence-level and\\ntask-level routing strategies discussed in Section\\n3 at identical network size (32 experts, 533M pa-\\nrameters). The results are presented in Table 1. In\\ngeneral, we ﬁnd that all types of task-level routing\\nperform better than token-level routing. We see\\nthat using sentence representations to route exam-\\nples (Sentence-level MoE - 32 experts) performs\\nmuch worse, so we do not conduct further exper-\\niments on this setting. In addition, we trained an\\nMoE baseline where the experts are deterministi-\\ncally allocated to tasks (Static MoE - 32 Experts) -\\nthis too, did not perform well in our experiments.\\nWhen we use Task MoE on both the encoder\\nand the decoder (Task-level MoE - 32 experts:\\nTarget/Target), we see consistent gains across the\\nboard. To investigate this further, we trained a\\nmodel that has (a) Token MoE on the encoder and\\nTask MoE on the decoder (Task-level MoE - 32 ex-\\nperts: Token/Target or Token/Language Pair) and\\n(b) Task MoE on the encoder and Token MoE on\\nthe decoder (Task-level MoE - 32 experts: Tar-\\nget/Token or Language Pair/Token). In Table 1 we\\nsee that using strategy (a) works the best, whether\\nwe choose to route by the target language or the\\nlanguage pair. In Section 5.4, we discuss these\\nobservations further.\\nOverall we ﬁnd that using Task MoE only on the\\ndecoder (Task-level MoE 32 experts: Token/Target)\\nworks the best, with gains of 1 BLEU over Token\\nMoE. These gains are consistent across xx2en lan-\\nguage pairs, en2xx language pairs, high resource\\nlanguages (more than 1 million sentence pairs), low\\nresource languages and the 2 zero shot pairs.\\n4.3\\nComparison of Throughput of Sparse\\nModels\\nBatch Size\\nThroughput (tok/sec)\\n0.00E+0\\n5.00E+4\\n1.00E+5\\n1.50E+5\\n2.00E+5\\n2.50E+5\\n0\\n1000\\n2000\\n3000\\n4000\\nTaskMoE (12L, 32E)\\nMoE (12L, 32E)\\nDense Baseline (12L)\\nFigure 2:\\nInference cost analysis:\\nWe measure\\nthe throughput of our Task-MoE model, baseline\\nTransformer-Base model and baseline Token-MoE\\nmodel across batch sizes and see that the peak through-\\nput of Task-MoE (and Transformer-Base) is 1.87 times\\nhigher than that of Token-MoE.\\nWe further compare Task-level MoEs with\\nToken-level MoEs in terms of throughput across\\ndifferent batch sizes in Figure 2. We measure this\\nby decoding the WMT14 English-German test set\\nwith our TaskMoE model and with the baseline\\nTokenMoE model on 32 Cloud TPU V3 cores.\\nSystem\\nRouting Granularity\\nThroughput\\nBLEU\\nEncoder\\nDecoder\\nPeak tokens/s\\nAverage\\nEnFr\\nFrEn\\nEnDe\\nDeEn\\nEnRo\\nRoEn\\nEnHi\\nHiEn\\nBilingual Baselines\\n-\\n-\\n2.3 × 105\\n24.3\\n38.1\\n35.5\\n26.4\\n27.4\\n23.7\\n30.1\\n4.5\\n8.5\\nMultilingual Transformer-Base\\n-\\n-\\n2.3 × 105\\n25.9\\n36.1\\n34.1\\n22.0\\n28.6\\n23.9\\n33.4\\n10.4\\n19.2\\nTask-level MoE – 32 experts\\nToken\\nTarget\\n2.3 × 105\\n29.0\\n39.9\\n37.1\\n27.1\\n32.0\\n26.6\\n36.2\\n13.3\\n20.1\\nToken-level MoE – 32 experts\\nToken\\nToken\\n1.3 × 105\\n28.2\\n40.1\\n36.4\\n26.7\\n31.2\\n26.5\\n33.7\\n11.5\\n19.8\\nDistillation (from Token MoE)\\n-\\n-\\n2.3 × 105\\n26.9\\n37.3\\n33.2\\n25.1\\n29.3\\n24.6\\n34.6\\n13.9\\n17.6\\nTable 2: Comparing Distillation to Task-MoE: We compare our best performing Task-MoE model to Distilling\\na Token MoE model to Transformer-Base and a version with 2x the width for several language pairs. We see\\nthat distillation consistently underperforms our best-performing Task MoE model - distillation from Token MoE\\nachieves an average BLEU score of 26.9, while our best-performing Task MoE model has an average BLEU score\\nof 29.0 (+2.1 BLEU) for these language pairs.\\nWe ﬁnd that our Task-MoE model has 1.87 times\\nhigher peak throughput while using 3.75 times less\\ndecoder parameters (142M vs 533M). Moreover,\\nour Task-MoE model has minimal communication\\noverhead compared to decoding with Token-MoE\\n(0.0% versus 26.9% of step time).\\nWe note that the inference time of the token-\\nbased MoE model is dominated by the decoder,\\nwith the decoders taking 200x the time per step than\\nthe encoders at peak throughput. Therefore, the\\ninference cost of task-level routing on decoder only\\nis roughly equivalent to that on both the encoder\\nand decoder.\\n4.4\\nComparison of Extracting Task MoE\\nModels to Distillation\\nWhile in Section 4.3 we compared the throughput\\nof task-level MoE and token-level MoE models, it\\nis common practice for large models to be distilled\\nto smaller student models suitable for deployment.\\nWe distill our token-level MoE baseline to\\nTransformer-Base student models with the same\\narchitecture as the multlingual dense baseline dis-\\ncussed in 4.1. As done in (Fedus et al., 2021),\\nwe initialize the student model with non-expert\\nweights of the teacher model. We distill the model\\nwith the source sides of the WMT parallel data\\nused while training the original teacher model. We\\ndo this for several language pairs across different\\nlanguage families and resource sizes - EnFr, FrEn,\\nDeEn, EnDe, EnRo, RoEn, EnHi and HiEn. Addi-\\ntional training details are provided in the Appendix\\nA.1.\\nIn Table 2, we compare the BLEU scores of our\\nbest performing Task MoE models to distillation of\\nour Token MoE baseline into models with similar\\ninference cost (shown in Figure 2). We see that\\ndistillation consistently underperforms our best-\\nperforming Task MoE model - distillation from\\nToken MoE achieves an average BLEU score of\\n26.9, while our best-performing Task MoE model\\nhas an average BLEU score of 29.0 (+2.1 BLEU)\\nfor these language pairs. We note that while distill-\\ning our sparse MoE model, only 32.25% of gains\\nover dense multilingual baselines are preserved.\\nThis is in line with the distillation results discussed\\nin (Fedus et al., 2021).\\n5\\nScaling up to 200 Language Pairs\\nWe now scale our results up to a larger internal\\ndataset with over 200 language pairs, while also\\nscaling the number of parameters to beyond 10\\nbillion weights. In addition, we look more closely\\nat the gating decisions made by these sparse models\\nand discuss their implications.\\n5.1\\nExperimental Setup\\nData:\\nWe use an in-house training corpus gener-\\nated by crawling and extracting parallel sentences\\nfrom the web (Uszkoreit et al., 2010). This dataset\\nhas 204 direct language pairs (102 languages to\\nand from English), with a total of 25 billion sen-\\ntence pairs. This dataset covers a diverse range of\\ndomains and languages, and is quite noisy. There\\nis also a heavy imbalance when it comes to the\\nnumber of examples available per language pair,\\nranging between 104 and 109 sentence pairs. In\\norder to record gating decisions while controlling\\nfor semantics, we created a multi-way aligned eval-\\nuation set containing nearly 3k sentence pairs for\\nall languages.1\\nModel:\\nWe use the 473M Transformer Big\\n(Vaswani et al., 2017) architecture (or modiﬁed ver-\\nsions of it in the case of sparse models) as described\\nby (Chen et al., 2018) for this set of experiments.\\nSimilar to Section 4.1, we (1) share all parame-\\nters across language pairs including softmax layer\\n1Each sentence in our evaluation set is semantically identi-\\ncal across all other languages.\\n(a) Performance of different routing strategies on Xx-En language\\npairs.\\n(b) Performance of different routing strategies on En-Xx language\\npairs.\\nFigure 3:\\nComparing the performance of differ-\\nent routing strategies for Mixture-of-Experts (MoE)\\nmodels on a massively multilingual dataset – We\\ncompare routing experts by tokens, and tasks (using\\neither language pairs or target languages). Given that\\nrouting by token on the encoder and routing by task on\\nthe decoder performed the best on WMT (Table 1), we\\nuse those settings for the scaled up 128 expert models\\nwe compare. We split the comparison of results into\\n(a) Xx-En language pairs and (b) En-Xx language pairs.\\nThe languages on the x-axis are sorted left-to-right in\\ndescending order of resource size. Best seen in color.\\nNote that the token-level MoE has 6.5B parameters in\\nthe decoders while our task-level MoE has only 200M.\\nand input/output word embeddings, (2) pre-pend a\\n<2xx> token to the source sentence to indicate the\\ntarget language and (3) use a Sentence Piece Model\\n(Kudo and Richardson, 2018) with 64k tokens vo-\\ncabulary shared on both the encoder and decoder\\nside.We followed the training and architecture as\\nshown in Lepikhin et al. (2020).2\\n2As opposed to displaying BLEU scores for each language\\npair, we place the baselines on the x-axis at zero and report the\\n∆BLEU trendline of each model we consider. In order to set\\nthese bilingual baselines, we train Neural Machine Translation\\nmodels for each language pair (e.g. a single model for German-\\nto-English), tuned depending on the available training data for\\n5.2\\nResults\\nWe compare Task-level MoEs and Token-level\\nMoEs to their bilingual and multilingual baselines\\nin Figure 2. We train 128 expert MoE models with\\nrouting in these settings: (1) Routing by token on\\nboth the encoder and decoder, (2) Routing by to-\\nken on the encoder and by target language on the\\ndecoder and (3) Routing by token on the encoder\\nand by language pair on the decoder.\\nWe ﬁnd that these scaled up sparse models per-\\nform better than their dense baselines, with hybrid\\ntask-level routing performing slightly better on En-\\nXx language pairs and pure token-level routing per-\\nforming slightly better on Xx-En language pairs.\\nWe hypothesize that for the Xx-En tasks, not explic-\\nitly dividing expert parameters by tasks on the de-\\ncoder results in better transfer, thus explaining the\\nbetter performance of token-level routing. This sug-\\ngests that a hybrid strategy that partially restricts\\naccess to experts based on task-boundaries, while\\nstill permitting routing by tokens, might provide\\nthe right balance between efﬁciency and quality.\\nWe also note that while both forms of routing\\nhave 13B parameters (6.5B on decoder) at train\\ntime, token level routing only on the decoder uses\\nonly 200M parameters at inference time, in addi-\\ntion to the practical considerations discussed in\\nSection 3.1. We provide aggregate BLEU scores in\\nAppendix A.6 and parameter count breakdowns in\\nAppendix A.5. In addition, we take a closer look at\\nrouting decisions made for different languages by\\nthe model in Section 5.4.\\n5.3\\nComparison of Throughput on Massive\\nModels\\nSimilar to Section 4.3, we compare Task-level\\nMoEs with Token-level MoEs in terms of through-\\nput across different batch sizes in Figure 4. We\\ndecode the WMT14 English-German test set with\\nour TaskMoE model and with the baseline Token-\\nMoE model on 128 Cloud TPU V3 cores. We ﬁnd\\nthat our Task-MoE model has 2.6 times higher peak\\nthroughput while using 32.34 times less decoder\\nparameters (201M vs 6.5B). Moreover, our Task-\\nMoE model has minimal communication overhead\\ncompared to decoding with Token-MoE (0.2% ver-\\nsus 36% of step time).\\nthat given language We tuned batch-size and different values\\nof regularization methods (e.g. dropout) in a Transformer-\\nBig or Transformer-Base layout, for high or low-resourced\\nlanguages respectively.\\nFigure 4:\\nInference cost analysis:\\nWe measure\\nthe throughput of our Task-MoE model and baseline\\nToken-MoE model across batch sizes and see that the\\npeak throughput of Task-MoE is 2.6 times higher.\\n5.4\\nA Closer Look at the Routing Decisions\\nNow, we analyze the routing decisions made in\\ntoken-level MoE models to further motivate our\\ninvestigation. We take a token-level MoE model\\ntrained on the massively multilingual dataset and\\ndecode these models on the multiway test-sets,\\nwhile logging routing decisions for every token.\\nWe plot the top expert distributions of several tasks\\nwith different scripts and language families in Fig-\\nure 5. For clarity, and because these two groups\\nof languages behave differently in a multilingual\\nsetting, we split the gating decisions into those for\\nXx-En and En-Xx language pairs. In the encoder\\n(Figure 5a), tokens from all tasks (Xx-En) seem to\\nprefer the same set of few experts slightly over the\\nothers. On the other hand, in the decoder (Figure\\n5b) each task seems to have a slight preference for\\na few experts over the others. Moreover, the set of\\nexperts appears to be similar for related languages.\\nFor example, English-Spanish and English-Catalan\\n(two Romance Languages) have similar expert dis-\\ntributions and so do English-Russian and English-\\nUkranian (two Slavic Languages). In the Appendix\\nA.7, we provide expert distribution plots for other\\nlayers of this model. In addition, we provide expert\\ndistributions of the MoE model that routes tokens\\nby target language discussed in Section 3.\\nOur analysis suggest that, when using token-\\nlevel routing, task-level decisions emerge naturally\\nin the decoder, providing additional motivation for\\nour proposed routing strategies.\\n6\\nRelated Work\\nConditional Computation:\\nConditional compu-\\ntation (Bengio et al., 2015), or routing examples\\nthrough the neural network by activating only a\\nsub-network of the network depending on the input\\nhas seen success in large scale natural language\\nprocessing (NLP) ((Shazeer et al., 2017; Lepikhin\\net al., 2020; Bapna et al., 2019)) and computer vi-\\nsion ((Yang et al., 2019)) tasks. A variety of strate-\\ngies can be used to route examples such as learning\\na function on the input (Shazeer et al., 2017; Lep-\\nikhin et al., 2020), computational budget (Bapna\\net al., 2019; Elbayad et al., 2019) or simplifying\\nthe expert allocation and training regimen (Lewis\\net al., 2021; Fedus et al., 2021).\\nMulti-task Learning\\nMulti-task Learning im-\\nproves model performance across all tasks trained\\non due to regularization and positive transfer be-\\ntween related tasks (Caruana, 1997). Here, sub-\\nnetworks are be activated depending on the task to\\nwhich the input belongs - some of these parameters\\nmay be shared. This approach has seen success in\\na variety of domains such as classiﬁcation, recom-\\nmender systems and NLP ((Ma et al., 2019, 2018;\\nClark et al., 2019; Collobert and Weston, 2008;\\nRuder et al., 2019; Tan et al., 2019)). Like our\\nwork, some of these models have been designed\\nwith inference beneﬁts in mind ((Ma et al., 2019)).\\nIn this work we focus on multi-task learning in the\\ncase of Multilingual NMT.\\nMulti-task learning for Multilingual NMT\\nModels:\\nMulti-task learning in multilingual mod-\\nels has been well-studied: while complete param-\\neter sharing is simple and works well ((Johnson\\net al., 2017)), an optimal strategy for sharing pa-\\nrameters and possibly having languages-speciﬁc\\nparameters would maximize transfer while mini-\\nmizing interference (Hokamp et al., 2019). Strate-\\ngies involve allocating language speciﬁc hidden\\nstates, attention modules, decoders or additional\\nspecialized layers ((Hokamp et al., 2019; Wang\\net al., 2018; Gu et al., 2018; Bapna et al., 2019)).\\nIn addition some strategies involve grouping param-\\neters by language group (Fan et al., 2020; Tan et al.,\\n2019). Compared to these works, our approach\\nto parameter sharing is designed to scale models\\nwithout impacting inference efﬁciency (as opposed\\nto simply adding language-speciﬁc capacity) while\\nstill enjoying the beneﬁts of scaling. Most sim-\\nilar to our work in terms of the inference utility\\nis proposed by (Li et al., 2020) where discrete la-\\ntent variables used to learn language speciﬁc layer\\ncombinations, whereas in our study we focus on\\nimproving inference efﬁciency of mixture of expert\\n(a) Gating decisions of the last layer of the encoder for Xx-En language pairs.\\n(b) Gating decisions of the last layer of the decoder for En-Xx language pairs.\\nFigure 5: We record the gating decisions of our MoE model trained on internal data on a multiway parallel dataset.\\nThe darker a cell, corresponding to, say en-sr and the 37th expert, the more the expert is used. In (a) the encoder,\\ntokens from all tasks (Xx-En) seem to prefer the same set of few experts slightly over the others; while in (b) the\\ndecoder each task (En-Xx) seems to slightly prefer a few experts over the other. Moreover, the set of experts appears\\nto be similar for related languages. For example, English-Spanish and English-Catalan (two Romance Languages)\\nhave similar expert distributions and so do English-Russian and English-Ukranian (two Slavic Languages).\\nmodels at scale.\\n7\\nConclusions\\nIn this work we discussed more inference friendly\\nalgorithms for routing examples in multilingual\\nSparse Mixture-of-Experts models by making use\\nof task boundaries. We empirically demonstrated\\nthat this new algorithm performs as well as, or\\nbetter than, conventional token-based routing al-\\ngorithms on two different datasets: a multilingual\\nWMT setup covering 30 language pairs and a large\\ninternal dataset covering 200 language pairs, in\\nterms of machine translation quality evaluated with\\nBLEU. By carefully comparing inference through-\\nput across different routing approaches and dis-\\ntilled models, we demonstrated the superiority of\\ntask-based routing algorithms over either serving\\na token-based MoE model as-is (in terms of peak\\nthroughput) and over distilling a large MoE model\\ninto a smaller dense model (in terms of BLEU).\\nWe conclude by highlighting that algorithms that\\nare more inference friendly while retaining the qual-\\nity gains of MoE models are a promising direction\\nfor future exploration, motivating research on in-\\nference efﬁciency for large models. Although we\\nstudied some hybrid routing strategies where en-\\ncoder and decoder networks utilize different rout-\\ning schemes, we believe that future research on\\nmore granular routing hybrids or hierarchical vari-\\nants will deliver more gains and advance our under-\\nstanding of large scale, sparsely gated, massively\\nmulti-task networks.\\n8\\nAcknowledgements\\nWe would like to thank Wolfgang Macherey,\\nYuanzhong Xu and Macduff Richard Hughes for\\ntheir helpful feedback on the draft. We would\\nalso like to thank the Google Translate and Google\\nBrain teams for their useful input and discussions,\\nand the entire GShard development team for their\\nfoundational contributions to this project. In addi-\\ntion, we thank the anonymous reviewers for their\\ninsightful comments.\\nReferences\\nNaveen Arivazhagan,\\nAnkur Bapna,\\nOrhan Firat,\\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\\nMia Xu Chen, Yuan Cao, George Foster, Colin\\nCherry, Wolfgang Macherey, Zhifeng Chen, and\\nYonghui Wu. 2019. Massively multilingual neural\\nmachine translation in the wild: Findings and chal-\\nlenges.\\nTimothy T Baldwin and J Kevin Ford. 1988. Trans-\\nfer of training: A review and directions for future\\nresearch. Personnel psychology, 41(1):63–105.\\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat.\\n2019.\\nSimple, scalable adaptation for neural ma-\\nchine translation. arXiv preprint arXiv:1909.08478.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau,\\nand Doina Precup. 2015. Conditional computation\\nin neural networks for faster models. arXiv preprint\\narXiv:1511.06297.\\nNikolay Bogoychev and Rico Sennrich. 2019. Domain,\\ntranslationese and noise in synthetic data for neural\\nmachine translation.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165.\\nRich Caruana. 1997.\\nMultitask learning.\\nMachine\\nlearning, 28(1):41–75.\\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\\nJohnson, Wolfgang Macherey, George Foster, Llion\\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\\n2018. The best of both worlds: Combining recent\\nadvances in neural machine translation. In Proceed-\\nings of the 56th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers), pages 76–86, Melbourne, Australia. Associa-\\ntion for Computational Linguistics.\\nYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang.\\n2017.\\nA survey of model compression and accel-\\neration for deep neural networks.\\narXiv preprint\\narXiv:1710.09282.\\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\\nwal, Christopher D Manning, and Quoc V Le.\\n2019.\\nBam!\\nborn-again multi-task networks for\\nnatural language understanding.\\narXiv preprint\\narXiv:1907.04829.\\nRonan Collobert and Jason Weston. 2008. A uniﬁed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning.\\nIn Pro-\\nceedings of the 25th international conference on Ma-\\nchine learning, pages 160–167.\\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\\nAuli. 2019.\\nDepth-adaptive transformer.\\narXiv\\npreprint arXiv:1910.10073.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\\nChaudhary, et al. 2020.\\nBeyond english-centric\\nmultilingual machine translation.\\narXiv preprint\\narXiv:2010.11125.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\\nSwitch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity.\\nCoRR,\\nabs/2101.03961.\\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\\nAPE at scale and its implications on MT evaluation\\nbiases. In Proceedings of the Fourth Conference on\\nMachine Translation (Volume 1: Research Papers),\\npages 34–44, Florence, Italy. Association for Com-\\nputational Linguistics.\\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK\\nLi. 2018. Universal neural machine translation for\\nextremely low resource languages. arXiv preprint\\narXiv:1802.05368.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\\nDistilling the knowledge in a neural network. arXiv\\npreprint arXiv:1503.02531.\\nChris Hokamp, John Glover, and Demian Gholipour.\\n2019. Evaluating the supervised and zero-shot per-\\nformance of multi-lingual translation models. arXiv\\npreprint arXiv:1906.09675.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji-\\nquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\\nGpipe: Efﬁcient training of giant neural networks\\nusing pipeline parallelism. In Advances in neural\\ninformation processing systems, pages 103–112.\\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\\net al. 2017. Google’s multilingual neural machine\\ntranslation system: Enabling zero-shot translation.\\nTransactions of the Association for Computational\\nLinguistics, 5:339–351.\\nJungo Kasai, Nikolaos Pappas, Hao Peng, James\\nCross, and Noah A Smith. 2020.\\nDeep encoder,\\nshallow decoder:\\nReevaluating the speed-quality\\ntradeoff in machine translation.\\narXiv preprint\\narXiv:2006.10369.\\nYoon Kim and Alexander M. Rush. 2016. Sequence-\\nlevel knowledge distillation. In Proceedings of the\\n2016 Conference on Empirical Methods in Natu-\\nral Language Processing, pages 1317–1327, Austin,\\nTexas. Association for Computational Linguistics.\\nXiang Kong, Adithya Renduchintala, James Cross,\\nYuqing Tang, Jiatao Gu, and Xian Li. 2021. Mul-\\ntilingual neural machine translation with deep en-\\ncoder and multiple shallow decoders. In Proceed-\\nings of the 16th Conference of the European Chap-\\nter of the Association for Computational Linguistics:\\nMain Volume, pages 1613–1624.\\nTaku Kudo and John Richardson. 2018. Sentencepiece:\\nA simple and language independent subword tok-\\nenizer and detokenizer for neural text processing.\\narXiv preprint arXiv:1808.06226.\\nSneha Reddy Kudugunta, Ankur Bapna, Isaac Caswell,\\nNaveen Arivazhagan, and Orhan Firat. 2019.\\nIn-\\nvestigating multilingual nmt representations at scale.\\narXiv preprint arXiv:1909.02197.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\\nGshard: Scaling giant models with conditional com-\\nputation and automatic sharding.\\narXiv preprint\\narXiv:2006.16668.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\\nSimplifying training of large, sparse models. CoRR,\\nabs/2103.16716.\\nXian Li, Asa Cooper Stickland, Yuqing Tang, and Xi-\\nang Kong. 2020.\\nDeep transformers with latent\\ndepth. arXiv preprint arXiv:2009.13102.\\nJiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong,\\nand Ed H Chi. 2019. Snr: Sub-network routing for\\nﬂexible parameter sharing in multi-task learning. In\\nProceedings of the AAAI Conference on Artiﬁcial In-\\ntelligence, volume 33, pages 216–223.\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan\\nHong, and Ed H Chi. 2018.\\nModeling task re-\\nlationships in multi-task learning with multi-gate\\nmixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge\\nDiscovery & Data Mining, pages 1930–1939.\\nRobert Östling and Jörg Tiedemann. 2016.\\nContinu-\\nous multilinguality with language vectors.\\narXiv\\npreprint arXiv:1612.07486.\\nMatt Post. 2018. A call for clarity in reporting BLEU\\nscores. In Proceedings of the Third Conference on\\nMachine Translation: Research Papers, pages 186–\\n191, Belgium, Brussels. Association for Computa-\\ntional Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2019. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. arXiv preprint arXiv:1910.10683.\\nMaksim Riabinin and Anton Gusev. 2020. Learning@\\nhome: Crowdsourced training of large neural net-\\nworks using decentralized mixture-of-experts. arXiv\\npreprint arXiv:2002.04013.\\nSebastian Ruder, Joachim Bingel, Isabelle Augenstein,\\nand Anders Søgaard. 2019. Latent multi-task archi-\\ntecture learning. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence, volume 33, pages\\n4822–4829.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\\nDean. 2017.\\nOutrageously large neural networks:\\nThe sparsely-gated mixture-of-experts layer. arXiv\\npreprint arXiv:1701.06538.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\narXiv preprint arXiv:1804.04235.\\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan\\nFirat, Mia Chen, Sneha Kudugunta, Naveen Ari-\\nvazhagan, and Yonghui Wu. 2020.\\nLeveraging\\nmonolingual data with self-supervision for multi-\\nlingual neural machine translation. arXiv preprint\\narXiv:2005.04816.\\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\\nYan Liu. 2019. Multilingual neural machine trans-\\nlation with knowledge distillation.\\narXiv preprint\\narXiv:1902.10461.\\nJörg Tiedemann. 2018.\\nEmerging language spaces\\nlearned from massively multilingual corpora. arXiv\\npreprint arXiv:1802.00273.\\nJakob Uszkoreit, Jay M Ponte, Ashok C Popat, and\\nMoshe Dubiner. 2010.\\nLarge scale parallel docu-\\nment mining for machine translation. In Proceed-\\nings of the 23rd International Conference on Compu-\\ntational Linguistics, pages 1101–1109. Association\\nfor Computational Linguistics.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, pages 5998–6008.\\nYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,\\nand Chengqing Zong. 2018. Three strategies to im-\\nprove one-to-many multilingual translation. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 2955–\\n2960.\\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\\ncas: The surprising cross-lingual effectiveness of\\nbert. arXiv preprint arXiv:1904.09077.\\nBrandon Yang, Gabriel Bender, Quoc V Le, and Ji-\\nquan Ngiam. 2019.\\nCondconv: Conditionally pa-\\nrameterized convolutions for efﬁcient inference. In\\nAdvances in Neural Information Processing Systems,\\npages 1307–1318.\\nBiao Zhang, Ankur Bapna, Rico Sennrich, and Orhan\\nFirat. 2021.\\nShare or not?\\nlearning to schedule\\nlanguage-speciﬁc capacity for multilingual transla-\\ntion. In International Conference on Learning Rep-\\nresentations.\\nBiao Zhang, Philip Williams, Ivan Titov, and Rico\\nSennrich. 2020. Improving massively multilingual\\nneural machine translation and zero-shot translation.\\narXiv preprint arXiv:2004.11867.\\nA\\nAppendix\\nA.1\\nWMT Model and Training Details\\nFor our experiments, we use the Transformer Base\\nmodel in (Chen et al., 2018), The sole difference\\nis that we use a 64k vocabulary: our model there-\\nfore contains 142M parameters. For multilingual\\nmodels, we share all parameters across language\\npairs including softmax layer in input/output word\\nembeddings.\\nWe use a 64k token vocabulary formed using\\na Sentence Piece Model (Kudo and Richardson,\\n2018). The vocabulary is shared on both the en-\\ncoder and decoder side. To learn a joint SPM model\\ngiven our imbalanced dataset, we followed the tem-\\nperature based sampling strategy with a tempera-\\nture of T = 5.\\nFinally, our models are optimized using the\\nAdafactor optimizer (Shazeer and Stern, 2018) with\\nmomentum factorization and a per-parameter norm\\nclipping threshold of 1.0. We followed a learn-\\ning rate of of 3.0, with 40K warm-up steps for the\\nschedule, which is decayed with the inverse square\\nroot of the number of training steps after warm-up.\\nBLEU scores presented in this paper are calculated\\nusing SacreBLEU (Post, 2018) on the WMT test\\nsets. 3\\nFor distillation, training and model details are\\nidentical apart from a reduced learning rate of 0.2.\\nA.2\\nWMT Dataset Details\\nIn Table 3 we provide the training set details for\\nthe WMT 4 setup we use (Siddhant et al., 2020).\\nWe provide the data sizes and WMT years of the\\nTrain, Dev and Test sets we use.\\nA.3\\nIndividual WMT BLEU Scores\\nBilingual baselines: We ﬁrst train Transformer\\nBase and Big models on each language pair. The\\nresults are in Table 4.\\nIn Tables 5 and 6 we provide individual BLEU\\nscores of the models discussed in Table 1.\\nA.4\\nDetailed Breakdown of Parameter\\nCounts on WMT\\nTable 7 describes the parameter counts of different\\nparts of the Transformers compared in Table 1.\\n3 BLEU+case.mixed+lang.<sl>-<tl>+\\nnumrefs.1+smooth.exp+tok.<tok>+version\\n.1.3.0 , where sl is the source language, tl is the target\\nlanguage and tok = zh if tl = zh and intl otherwise.\\n4http://www.statmt.org/wmt20/\\nA.5\\nDetailed Breakdown of Parameter\\nCounts\\nIn Table 8 we describe the parameter counts of\\ndifferent parts of the Transformers discussed in\\nSection 5.\\nA.6\\nResults on Large MoE Model\\nIn Table 9 we provide aggregate BLEU scores for\\nthe results in Figure 3.\\nA.7\\nGating Decisions for task-level and\\ntoken-level MoEs\\nIn this section, we show the top expert distributions\\nof different layers of the position-wise MoE model\\ndiscussed in Section 5.4 in Figures 6, 7, 8 and 9.\\nWe also show expert distributions on MoE model\\nrouting by target language from EnX that was intro-\\nduced in Section 5.2 in Figures 10 and 11. We omit\\nresults on XEn language pairs because they belong\\nto the same task in the context of this model.\\n(a) Gating decisions of the ﬁrst layer of the encoder for Xx-En language pairs.\\n(b) Gating decisions of the last layer of the encoder for Xx-En language pairs.\\nFigure 6: Gating decisions of the encoder of the position-wise MoE model on Xx-En language pairs, trained on\\ninternal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say en-sr and\\nthe 37th expert, the more the expert is used. In both the last layer of the encoder and decoder, the tokens from\\neach language are fairly well distributed across experts. In (a) the ﬁrst layer of the encoder, there does not seem\\nto be any major pattern in the expert distribution whereas in (b) the last layer of the encoder, tokens from all tasks\\n(Xx-En) seem to prefer the same set of few experts slightly over the others.\\n(a) Gating decisions of the ﬁrst layer of the decoder for Xx-En language pairs.\\n(b) Gating decisions of the last layer of the decoder for Xx-En language pairs.\\nFigure 7: Gating decisions of the decoder of the position-wise MoE model on Xx-En language pairs, trained on\\ninternal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say en-sr and\\nthe 37th expert, the more the expert is used. In both the ﬁrst and last layer of the decoder, the tokens from each\\nlanguage are fairly well distributed across experts. In fact, tokens from all tasks (Xx-En) seem to prefer the same\\nset of few experts slightly over the others.\\n(a) Gating decisions of the ﬁrst layer of the encoder for En-Xx language pairs.\\n(b) Gating decisions of the last layer of the encoder for En-Xx language pairs.\\nFigure 8: Gating decisions of the encoder of the position-wise MoE model on En-Xx language pairs, trained on\\ninternal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say en-sr and\\nthe 37th expert, the more the expert is used. In both the ﬁrst and last layer of the encoder, the tokens from each\\nlanguage are fairly well distributed across experts. Each task (En-Xx) seems to slightly prefer a few experts over\\nthe other.\\n(a) Gating decisions of the ﬁrst layer of the decoder for En-Xx language pairs.\\n(b) Gating decisions of the last layer of the decoder for En-Xx language pairs.\\nFigure 9: Gating decisions of the decoder of the position-wise MoE model on En-Xx language pairs, trained on\\ninternal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say en-sr and\\nthe 37th expert, the more the expert is used. In both the ﬁrst and last layer of the decoder, the tokens from each\\nlanguage are fairly well distributed across experts. Each task (En-Xx) seems to slightly prefer a few experts over\\nthe other. Moreover, the set of experts appears to be similar for related languages. For example, English-Spanish\\nand English-Catalan (two Romance Languages) have similar expert distributions and so do English-Russian and\\nEnglish-Ukranian (two Slavic Languages).\\n(a) Gating decisions of the ﬁrst layer of the encoder for En-Xx language pairs.\\n(b) Gating decisions of the last layer of the encoder for En-Xx language pairs.\\nFigure 10: Gating decisions of the encoder of the target language-wise MoE model on En-Xx language pairs,\\ntrained on internal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say\\nen-sr and the 37th expert, the more the expert is used. The encoder behaves similarly to that of the position-wise\\nmodel: in both the ﬁrst and last layer of the encoder, the tokens from each language are fairly well distributed\\nacross experts. Each task (En-Xx) seems to slightly prefer a few experts over the other.\\n(a) Gating decisions of the ﬁrst layer of the decoder for En-Xx language pairs.\\n(b) Gating decisions of the last layer of the decoder for En-Xx language pairs.\\nFigure 11: Gating decisions of the decoder of the target language-wise MoE model on En-Xx language pairs,\\ntrained on internal data on a multiway parallel dataset. In this diagram, the darker a cell, corresponding to, say\\nen-sr and the 37th expert, the more the expert is used. There seems to be some amount of expert sharing on a\\nlinguistic basis: en-ur, en-te and en-ta (two Dravidian Languages and an Indo-Iranian language) and en-tr, en-uz\\nand en-uk (two Turkic languages and a Slavic language) share an expert. On the other hand, en-es and en-ca (two\\nRomance languages) have different experts.\\nLanguage\\nPair\\nData Sources\\n# Samples\\nTrain\\nDev\\nTest\\nTrain\\nDev\\nTest\\ncs→en\\nWMT’19\\nWMT’17\\nWMT’18\\n64336053\\n3005\\n2983\\nfr→en\\nWMT’15\\nWMT’13\\nWMT’14\\n40449146\\n3000\\n3003\\nru→en\\nWMT’19\\nWMT’18\\nWMT’19\\n38492126\\n3000\\n2000\\nzh→en\\nWMT’19\\nWMT’18\\nWMT’19\\n25986436\\n3981\\n2000\\nes→en\\nWMT’13\\nWMT’13\\nWMT’13\\n15182374\\n3004\\n3000\\nﬁ→en\\nWMT’19\\nWMT’18\\nWMT’19\\n6587448\\n3000\\n1996\\nde→en\\nWMT’14\\nWMT’13\\nWMT’14\\n4508785\\n3000\\n3003\\net→en\\nWMT’18\\nWMT’18\\nWMT’18\\n2175873\\n2000\\n2000\\nlv→en\\nWMT’17\\nWMT’17\\nWMT’17\\n637599\\n2003\\n2001\\nlt→en\\nWMT’19\\nWMT’19\\nWMT’19\\n635146\\n2000\\n1000\\nro→en\\nWMT’16\\nWMT’16\\nWMT’16\\n610320\\n1999\\n1999\\nhi→en\\nWMT’14\\nWMT’14\\nWMT’14\\n313748\\n520\\n2507\\nkk→en\\nWMT’19\\nWMT’19\\nWMT’19\\n222424\\n2066\\n1000\\ntr→en\\nWMT’18\\nWMT’17\\nWMT’18\\n205756\\n3007\\n3000\\ngu→en\\nWMT’19\\nWMT’19\\nWMT’19\\n155798\\n1998\\n1016\\nen→cs\\nWMT’19\\nWMT’17\\nWMT’18\\n64336053\\n3005\\n2983\\nen→fr\\nWMT’15\\nWMT’13\\nWMT’14\\n40449146\\n3000\\n3003\\nen→ru\\nWMT’19\\nWMT’18\\nWMT’19\\n38492126\\n3000\\n2000\\nen→zh\\nWMT’19\\nWMT’18\\nWMT’19\\n25986436\\n3981\\n2000\\nen→es\\nWMT’13\\nWMT’13\\nWMT’13\\n15182374\\n3004\\n3000\\nen→ﬁ\\nWMT’19\\nWMT’18\\nWMT’19\\n6587448\\n3000\\n1996\\nen→de\\nWMT’14\\nWMT’13\\nWMT’14\\n4508785\\n3000\\n3003\\nen→et\\nWMT’18\\nWMT’18\\nWMT’18\\n2175873\\n2000\\n2000\\nen→lv\\nWMT’17\\nWMT’17\\nWMT’17\\n637599\\n2003\\n2001\\nen→lt\\nWMT’19\\nWMT’19\\nWMT’19\\n635146\\n2000\\n1000\\nen→ro\\nWMT’16\\nWMT’16\\nWMT’16\\n610320\\n1999\\n1999\\nen→hi\\nWMT’14\\nWMT’14\\nWMT’14\\n313748\\n520\\n2507\\nen→kk\\nWMT’19\\nWMT’19\\nWMT’19\\n222424\\n2066\\n1000\\nen→tr\\nWMT’18\\nWMT’17\\nWMT’18\\n205756\\n3007\\n3000\\nen→gu\\nWMT’19\\nWMT’19\\nWMT’19\\n155798\\n1998\\n1016\\nfr→de\\nWMT’19\\nWMT’13\\nWMT’13\\n9824476\\n1512\\n1701\\nde→fr\\nWMT’19\\nWMT’13\\nWMT’13\\n9824476\\n1512\\n1701\\nTable 3: Data sources and number of samples for the parallel data in our corpus. Please note that we don’t use\\nparallel data in Fr-De for any of the experiments in the paper.\\nxx\\ncs\\nfr\\nru\\nzh\\nes\\nﬁ\\nde\\net\\nlv\\nlt\\nro\\nhi\\nkk\\ntr\\ngu\\nAny-to-English (xx→en)\\n31.3\\n37.2\\n36.0\\n21.7\\n32.7\\n27.3\\n31.7\\n23.1\\n15.0\\n21.3\\n30.1\\n8.5\\n11.5\\n15.9\\n1.0\\nEnglish-to-Any (en→xx)\\n23.8\\n41.3\\n26.4\\n31.3\\n31.1\\n18.1\\n29.9\\n18.2\\n14.2\\n11.5\\n23.4\\n4.5\\n1.9\\n13.6\\n0.6\\nTable 4: Bilingual baselines. xx refers to language in the column header. (Siddhant et al., 2020)\\nSystem\\nRouting Granularity\\nBLEU\\nAVG\\nxx2en\\nen2xx\\nHRL\\nLRL\\ncs_en\\nen_cs\\nfr_en\\nen_fr\\nru_en\\nen_ru\\nzh_en\\nen_zh\\nes_en\\nen_es\\nde_fr\\nfr_de\\nMultilingual Transformer-Base\\n-\\n-\\n20.03\\n23.69\\n17.5\\n23.25\\n15.88\\n27.2\\n18.1\\n34.1\\n36.1\\n31.7\\n21.1\\n18.9\\n17.2\\n31.3\\n29.2\\n17.4\\n5.5\\nMultilingual Transformer-Big\\n-\\n-\\n23.84\\n26.10\\n22.03\\n27.69\\n18.89\\n31.03\\n23.24\\n37.75\\n40.43\\n35.2\\n25.09\\n20.02\\n25.99\\n33.45\\n32.27\\n20.07\\n20.98\\nSentence-level MoE – 32 expert\\nSentence\\nSentence\\n19.88\\n24.05\\n16.83\\n22.56\\n14.14\\n27.6\\n18.7\\n34.4\\n36.5\\n32.7\\n15.1\\n20.4\\n7.2\\n31.3\\n30.1\\n13.6\\n9.1\\nToken-level MoE – 32 experts\\nToken\\nToken\\n22.58\\n24.91\\n20.35\\n27.49\\n16.28\\n29.8\\n21.8\\n36.4\\n40.1\\n34.6\\n25.7\\n19.9\\n23.7\\n33.9\\n32.8\\n23.9\\n19.9\\nTask-level MoE – 32 experts\\nLanguage Pair\\nLanguage Pair\\n22.04\\n25.43\\n19.5\\n25.57\\n17.5\\n26.8\\n21.7\\n35.4\\n39.2\\n33\\n21\\n22.1\\n17.9\\n32.4\\n32.1\\n12.2\\n19.1\\nTarget\\nTarget\\n22.88\\n25.63\\n20.19\\n27.21\\n17.3\\n29.1\\n21.7\\n36.1\\n40.2\\n33.8\\n24.7\\n21.9\\n24.8\\n32.6\\n33.1\\n25.8\\n18.8\\nLanguage Pair\\nToken\\n22.45\\n25.58\\n20.34\\n26.85\\n16.79\\n30.3\\n21.5\\n36.7\\n40.3\\n34.8\\n25.1\\n21\\n25.9\\n33.6\\n32.4\\n12.9\\n16.6\\nTarget\\nToken\\n22.33\\n24.47\\n20.44\\n26.82\\n16.55\\n29.4\\n22\\n35.3\\n39.7\\n33.8\\n25.2\\n21\\n26.2\\n32.4\\n32.7\\n22.2\\n18.6\\nToken\\nLanguage Pair\\n23.03\\n26.16\\n20.28\\n27.23\\n17.62\\n30.1\\n23.2\\n37.5\\n39.5\\n35.5\\n21.9\\n21.7\\n15.7\\n34.5\\n33.5\\n20.1\\n20.1\\nToken\\nTarget\\n23.62\\n25.95\\n21.09\\n28.48\\n17.37\\n30.5\\n22.5\\n37.1\\n39.9\\n35.4\\n25.6\\n21.4\\n27\\n34.3\\n33.5\\n27.7\\n22.4\\nTable 5: Part 1 of the table with individual BLEU scores for Table1\\nSystem\\nRouting Granularity\\nBLEU\\nﬁ_en\\nen_ﬁ\\nde_en\\nen_de\\net_en\\nen_et\\nlv_en\\nen_lv\\nlt_en\\nen_lt\\nro_en\\nen_ro\\nhi_en\\nen_hi\\nkk_en\\nen_kk\\ntr_en\\nen_tr\\ngu_en\\nen_gu\\nMultilingual Transformer-Base\\n-\\n-\\n23.9\\n17\\n28.6\\n22\\n23.1\\n16.1\\n17.2\\n14.9\\n24.6\\n11.4\\n33.4\\n23.9\\n19.2\\n10.4\\n13.5\\n2.5\\n20.9\\n17.5\\n7.8\\n5.1\\nMultilingual Transformer-Big\\n-\\n-\\n27.89\\n20.83\\n30.72\\n27.37\\n28.49\\n17.59\\n20.32\\n17.76\\n26.1\\n26.1\\n35.84\\n26.83\\n20.87\\n14.61\\n10.4\\n5.23\\n22.69\\n19.44\\n10.68\\n7.67\\nSentence-level MoE – 32 expert\\nSentence\\nSentence 23.5\\n17.2\\n29.4\\n21.8\\n22\\n15.4\\n17.9\\n14.7\\n24.6\\n11.6\\n33.6\\n24.8\\n20.5\\n12.2\\n14\\n2.9\\n21.4\\n17.9\\n7.4\\n6.3\\nToken-level MoE – 32 experts\\nToken\\nToken\\n27.3\\n20.2\\n31.2\\n26.7\\n27\\n19.9\\n18.7\\n17\\n23.7\\n13.9\\n33.7\\n26.5\\n19.8\\n11.5\\n8.5\\n2.4\\n20.3\\n18\\n8.8\\n5.1\\nTask-level MoE – 32 experts\\nLanguage Pair\\nLanguage Pair\\n25.2\\n20.1\\n31.3\\n26.9\\n24.7\\n19.2\\n18.4\\n16.3\\n25.1\\n13.6\\n34.8\\n25.7\\n22.5\\n13.1\\n15\\n2.4\\n23.4\\n18.2\\n11.4\\n5.1\\nTarget\\nTarget\\n25.6\\n19.5\\n30.7\\n26.8\\n24.8\\n19.8\\n18.4\\n15.7\\n25.9\\n13.6\\n34.9\\n25.8\\n21.7\\n12.3\\n15.5\\n2.4\\n22.5\\n17.7\\n11\\n4.8\\nLanguage Pair\\nToken\\n26.7\\n20\\n32.2\\n26.9\\n26.8\\n19.6\\n18.9\\n16.3\\n25.1\\n13.3\\n34.2\\n25.8\\n21.1\\n12.6\\n12.6\\n2.3\\n21.7\\n18.4\\n8\\n4.7\\nTarget\\nToken\\n23.7\\n19.8\\n30.7\\n26.1\\n24.1\\n19.9\\n18\\n16.5\\n24.4\\n13.6\\n33.1\\n26.1\\n20\\n12.7\\n12.7\\n2.9\\n21.1\\n18.2\\n7.4\\n5\\nToken\\nLanguage Pair\\n27.8\\n21.1\\n32.3\\n27\\n27.6\\n21\\n19.8\\n17.2\\n26\\n14.6\\n36.4\\n26.8\\n20.4\\n14.2\\n12.3\\n3.3\\n21.5\\n19.4\\n9\\n5.8\\nToken\\nTarget\\n27.9\\n20.5\\n32\\n27.1\\n27.3\\n20.5\\n19.4\\n17.6\\n25.9\\n14.4\\n36.2\\n26.6\\n20.1\\n13.3\\n11.6\\n3\\n21.2\\n19.2\\n9\\n5.7\\nTable 6: Part 2 of the table with individual BLEU scores for Table1\\nSystem\\nRouting Granularity\\nNo. of Parameters\\nEffective n(params) at inference time\\nEncoder\\nDecoder\\nVocabulary\\nEncoder\\nDecoder\\nSoftmax\\nTotal\\nEncoder\\nDecoder\\nTotal\\nMultilingual Transformer-Base\\n-\\n-\\n33M\\n19M\\n25M\\n65M\\n142M\\n19M\\n25M\\n142M\\nToken-level MoE – 32 experts\\nToken\\nToken\\n33M\\n214M\\n221M\\n65M\\n533M\\n214M\\n221M\\n533M\\nSentence-level MoE – 32 expert\\nSentence\\nSentence\\n214M\\n221M\\n533M\\nTask-level MoE – 32 experts\\nLanguage Pair\\nLanguage Pair\\n25M\\n32M\\n155M\\nTarget\\nTarget\\n25M\\n32M\\n155M\\nLanguage Pair\\nToken\\n214M\\n25M\\n338M\\nTarget\\nToken\\n214M\\n25M\\n338M\\nToken\\nLanguage Pair\\n19M\\n221M\\n338M\\nToken\\nTarget\\n19M\\n221M\\n338M\\nTable 7: We break down the parameter counts of the models we compare in Section 4.2 by components.\\nSystem\\nRouting Granularity\\nNo. of Parameters\\nEffective n(params) at inference time\\nEncoder\\nDecoder\\nVocabulary\\nEncoder\\nDecoder\\nSoftmax\\nTotal\\nEncoder\\nDecoder\\nTotal\\nMultilingual Transformer-Big\\n-\\n-\\n65M\\n126M\\n151M\\n131M\\n473M\\n126M\\n151M\\n473M\\nToken-level MoE – 128 experts\\nToken\\nToken\\n6.5B\\n6.5B\\n13B\\n6.5B\\n6.5B\\n13.3B\\nTask-level MoE – 128 experts\\nToken\\nLanguage\\n6.5B\\n6.5B\\n13B\\n6.5B\\n201M\\n6.9B\\nTask-level MoE – 128 experts\\nToken\\nTarget\\n6.5B\\n6.5B\\n13B\\n6.5B\\n201M\\n6.9B\\nTable 8: We break down the parameter counts of the models we compare in Section 5.2 by components.\\nSystem\\nRouting Granularity\\nBLEU\\nEncoder\\nDecoder\\nAVG\\nEn-X\\nX-En\\nHigh-25 (EnX)\\nMid 52 (EnX)\\nLow 25 (Enx)\\nHigh-25 (XEn)\\nMid 52 (XEn)\\nLow 25 (XEn)\\nMultilingual Transformer-Big\\n-\\n-\\n24.49\\n18.61\\n30.37\\n28.03\\n16.9\\n12.75\\n33.84\\n30.23\\n26.96\\nToken-level MoE – 128 experts\\nToken\\nToken\\n28.37\\n20.51\\n36.26\\n30.99\\n18.94\\n13.33\\n40.14\\n36.74\\n31.03\\nTask-level MoE – 128 experts\\nToken\\nLanguage\\n28.09\\n20.66\\n35.52\\n31.21\\n19.17\\n13.28\\n39.69\\n36.42\\n29.16\\nTask-level MoE – 128 experts\\nToken\\nTarget\\n27.83\\n20.76\\n34.90\\n31.05\\n19.23\\n13.68\\n38.88\\n35.28\\n29.93\\nTable 9: We summarize the results in Figure 3 on scaled up 128 expert MoE models. Here, High-25 means the average BLEU of the 25 highest resource languages, Low-25\\nmeans the average BLEU of the 25 lowest resource languages while Mid-52 is the average BLEU of the remaining 52 languages.\\n.\\n', 'source_name': 'Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference', 'source_url': 'https://arxiv.org/abs/2110.03742'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DEMix_NOTES.pdf #17\n",
      "{'content': 'DEMix Layers: Disentangling Domains for Modular Language Modeling \\nMain Idea: DEMix, which stands for domain-expert mixture, is a type of architecture that \\nencourages domain specialization. It looks to train multiple feedforward networks that are each \\nspecialized in a specific domain, and similarly to MoE, pick one to run during inference, depending \\non the input space. DEMix layers are modular, meaning they can be mixed, added, removed or \\nused to initialize other layers after initial training. DEMix aims to achieve domain specialization \\nin the sparse layer, while retaining generalization knowledge with shared parameters. \\n \\nMotivation \\nDense training consists of updating all parameters to minimize loss on all the data. This means \\nthat it assumes that the model will be able to learn/fit different domains equally. In practice, \\ndomains are skewed to domains that are more prevalent in the training data, so models have a \\nhard time generalizing to other domains. Fine-tuning these large dense models can also be \\nexpensive and lead to catastrophic forgetting – worsening performance on pre-training domains \\nnot represented in the fine-tuning data – since all weights need to be updated. Finally, managing \\nunwanted behavior in dense models is also a challenge. \\nTo help with these issues, a DEMix (modular) architecture is proposed. That is, an LLM with \\ndifferent components that can be modified during inference. \\n \\nSome Characteristics/Highlights of DEMix \\n- \\nDEMix is a substitute for an FF layer in the Transformers architecture (every FF layer is \\nreplaced by a DEMix layer) and can be conditioned on the input text in cases where the \\ndomain is previously known, as well as when the input domain is not known. \\n- \\nThe router used for DEMix is parameter-free and thus not learned, depending on the \\nnatural segmentation of the data. \\no Parameter-free probabilistic approach to dynamically estimate a weighted \\nmixture of domains during inference, which is used for novel domains (when it is \\nnot clear/known in advance where the input is from, or it is from a brand-new \\ndomain). \\n- \\nMixing (like using top-k > 1) experts is shown to improve performance in novel domains \\nas well as training domains during test time (probably due to overlap between domains \\nthat the shared parameters are not enough to capture). \\n- \\nThe modularity of DEMix offers flexibility by enabling the removal or addition of new \\ndomains at inference, thus allowing the ability of choosing what is forgotten. Catastrophic \\nforgetting is also not an issue since a new domain expert can be initialized or an existing \\none can be further specialized without modifying the model’s behavior on other domains. \\n \\nData \\n- \\n8 training domains \\n- \\n8 testing domains \\no Used to test robustness of mixing experts to data distribution shifts not seen \\nduring training. \\n \\nDEMix vs Traditional MoE \\n- \\nWhile in traditional MoE the routing function is learned through training at a token-level, \\nDEMix routing is done at the document (sequence) level and only needs to be performed \\nonce per input (all tokens in an input sequence are routed the same way). \\no Token-level routing has been shown to specialize experts in token-level areas, \\nsuch as semantics and syntax. Document-level routing should enable experts to \\nspecialize in specific tasks/domains. \\n- \\nBecause of this characteristic in specializing in domains compared to semantics, the \\nexperts are more flexible in terms of addiction and subtraction to the network and \\nprovide an ease of interpretation that traditional MoEs don’t have (they are more of a \\nblack box). \\n \\nTraining \\n- \\nDuring training, each domain expert is assigned to a single GPU (similarly to how it is done \\nin traditional MoE). \\n- \\nEach mini batch sends k domain examples to each expert (a balanced load is easy to \\nachieve since we know each input’s domain for training). \\n- \\nDistributed data parallelism is used (expert is replicated through the number of GPUs \\navailable for that specific expert, since there were more GPUs available than experts) \\no This is efficient because only globally shared parameters are synced through all \\nGPUs, while domain expert parameters are only synced between the GPUs \\nassigned to that expert. \\n▪ Reduced communication costs due to a decrease in alltoall computations. \\n \\nEvaluation \\n- \\nIn-domain performance \\no 4 variations used: \\n▪ DENSE – regular dense model with no conditioning on domain. \\n▪ DENSE (balanced) – dense model with equal amount of data used for each \\ndomain. \\n▪ +DOMAIN-TOKEN – variation of DENSE (balanced) with a prepended token \\non every input sequence to indicate its domain. \\n• The motivation behind this is to add info about the domain of the \\ninput to the context to try to create a dense oracle gate. \\n▪ DEMix – DEMix architecture with known domain for each input. \\n• Uses top-1 routing for in-domain experts based on the already \\nknown domain of the input. \\no Adding domain info (DENSE (balanced) and +DOMAIN-TOKEN) is shown to help \\nthe dense baseline. \\n▪ The smaller the model, the more helpful this is. \\no Heterogeneous domains (diverse domains like WEBTEXT and REDDIT) have more \\noverlap with other training domains, and thus don’t really benefit from DEMix vs \\na dense baseline. \\n- \\nUnknown domain performance – mixing experts at inference time. \\no Routing approach \\n▪ In practice, the domain of an input is not always known. In this case, it \\nmakes more sense to use a soft choice for routing (top-2 routing), as it was \\nproposed for cases where the domain was known. \\n▪ To not increase training costs with a learned routing approach (more \\ncommunication costs), a probabilistic routing score based on Bayes’ Rule \\nwas used (this is parameter-free). \\n▪ Probabilistic Routing Score: \\n• The main part of this is calculating the domain posterior – the \\nprobability that the input is from a certain domain d. \\n• This approach is very inefficient (the input needs to go through \\neach expert, so the routing is useless in practice) and is improved \\nin future work. \\n▪ They propose 3 variations on the posterior calculation: \\n• Uniform - each domain is estimated to be equally likely. \\n• Updating - weighted moving average of the posteriors from the \\nprevious timesteps. \\n• Cached – fixed prior estimated from the test data (100 test \\nsequences used) \\no The estimates of posteriors for both the training and the novel domains is shown \\nto be sparse, justifying the top-1 and top-2 routing selections (so not all experts \\nneed to be used, aka sparsity is justified). \\no Ensembling DEMix experts (mixing) using the cached approach performs better \\nthan all models analyzed. \\n▪ Compared to DENSE, this is beneficial at smaller scales, while the dense \\nmodels can catch up as the parameter count increases. \\n• Perhaps more data is needed when increasing the DEMix \\nparameters? \\n▪ Ensembling/mixing is also shown to lead to improvements on training \\ndomains, especially more heterogeneous ones (more diverse domains). \\n \\nDEMix-DAPT \\nDEMix-DAPT consists of adopting existing experts to new domains. \\n- \\nPreviously, experiments were made to evaluate the performance of DEMix in novel \\ndomains (domains not seen during training). DEMix-DAPT is different in the sense that it \\napplies new domain data to existing domain experts to create a new expert. \\n- \\nThe new expert is initialized with the parameters of the closest existing domain expert. \\nSo, the new expert is a fine-tuned version of an existing domain expert. \\no How close each domain expert is from each other is calculated from the router’s \\ndomain posterior. \\n- \\nIn DEMix-DAPT, only the expert parameters are trained. Shared parameters are kept \\nfrozen. \\n- \\nFor inference, the cached posterior approach is taken. \\nResults (DEMix-DAPT) \\n- \\nDEMix-DAPT is compared to Dense-DAPT, which is a dense version of adapting to a new \\ndomain. \\n- \\nAs expected, it is shown that Dense-DAPT suffers from the issue of catastrophic \\nforgetting. This is apparent when seeing how training a Dense-DAPT model in a novel \\ndomain leads to degraded performance on (original) training domains. \\n- \\nAs expected, adding experts through DEMix-DAPT significantly improves performance on \\nthose novel domains. \\n \\nIn this paper, it was also shown how removing an expert from an unwanted domain (for example, \\ndue to hate speech or leaking of private data), leads to similar performance on that domain \\ncompared to DEMix models not trained on that domain. This shows that expert domains can be \\nremoved from DEMix, if desirable. This also shows that most domain specialization comes from \\nthe DEMix layers. \\n \\n', 'source_name': 'DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/DEMix_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BTM_NOTES.pdf #18\n",
      "{'content': 'Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models \\nMain Idea: This paper serves as a continuation to DEMix, focusing more on the aspect of \\nemploying techniques to train these modular models more efficiently. Due to the modularity of \\nDEMix, Branch-Train-Merge (BTM) shows that it is possible to train these domain experts \\nindependently, saving on multi-node synchronization costs commonly required in the training of \\nLLMs. BTM also explores scaling up the number of experts to 64 (DEMix only trained up to 16). \\nBTM trains an ELMForest (Expert Language Models for Efficient Sparse Training), which are \\nembarrassingly parallel, that is, different parts of the model are independently trained on \\ndifferent subsets of the data, with no need for multi-node training or inference. \\nEach ELM is specialized in a different domain with no shared parameters (contrary to DEMix). \\nELMs can be added or removed to the model at any time, or parameter-averaged to collapse \\nback into a single LM. \\n \\nBranch-Train-Merge Algorithm \\n- \\nThe BTM algorithm consists of repeatedly expanding the ELMForest (combination of \\nexperts) by adding experts in an embarrassingly parallel manner. There are two possible \\nscenarios: when we are first building the forest (creating the first expert) and when we \\nalready have at least one expert created, which makes the process of initializing other \\nexperts easier. \\n- \\nThe addition of a new expert is done by: \\no Branch – initializing a new LM with an average of the parameters of the most \\nrelevant of the currently existing experts. \\no Train – train this recently initialized expert on new domain data. \\no Merge – merge the trained expert into the ELMForest. \\n- \\nThe first step (branch) needs to be done in a different manner when training the first \\nexpert since there are no experts to initialize this expert to. The training of the initial \\nexpert is done by training on heterogeneous (diverse) data. \\nThis approach is shown to outperform dense and DEMix when used as an ensemble or when \\nparameter-averaging the weights of the experts. This shows that there are inherent gains from \\ntraining using the BTM approach. \\nOverall, BTM shows an efficient way of scaling LLMs without having to train extremely large \\nmodels. Instead, an ensemble of domain experts, or even a parameter-average, outperforms the \\ndense version. (the models were compared based on GPU training time; the parameter-averaged \\nmodel is also compute-matched to dense). \\nIn this work, the domains are defined by provenance (source). This is suboptimal and improved \\nin later work. \\nLike DEMix, BTM has the advantages of fully adding and removing experts, if desired. Since each \\nexpert is trained on their own specific data split and there are no parameters shared, this means \\nthat removing an expert will lead to complete removal of that domain from the model. The only \\ncaveat is if other domain experts were initialized from an undesired domain. In this scenario, \\nsimply removing the undesired domain may not be sufficient. \\n \\nEnsembling and Averaging ELMs \\n- \\nEnsembling and averaging ELMs keeps the inference cost constant regardless of the \\nnumber of experts added. \\n- \\nEnsembling leads to higher inference costs (due to multiple expert results needed), \\nhowever, results show that top-k routing should be possible. \\no The expert routing (for top-k) or score for parameter-averaging are done through \\nthe same domain posterior method from DEMix (with a cached prior, more \\nspecifically). \\n \\nBTM Approach (in more detail) \\n- \\nBTM can be done asynchronously, that is, multiple new ELMs can be trained in parallel. \\nThis can be thought of as having multiple BTM training rounds, each initializing its new \\nexperts based on the existing experts at the beginning of the training round. \\n- \\nStep 0 \\no The first ELM needs to be initialized differently, since there are no existing ELMs \\nyet to obtain parameters to initialize an expert from. \\no For this, an initial ELM is trained on heterogeneous (diverse) data. \\no Once this initial ELM is trained, its parameters can be used to initialize the weights \\nof the first batch of the ELMs. \\n- \\nBranch \\no Refers to adding a new ELM (Expert Language Model). \\no Idea is to initialize the new ELM to be a parameter-average of the current \\nELMForest (all existing domain experts). \\no The best approach for initialization was to perform a weighted average of existing \\nELM parameters based on their domain posterior or the new domain data (finding \\nthe closest domains to the new domain and only use the parameters of the most \\nrelevant experts for this new domain). \\n- \\nTrain \\no After initializing the weights of the new ELM (branching), the ELM is trained \\nindependently on its domain data. \\n- \\nMerge \\no Once the new ELM is fully trained on its domain data, it can be added to the \\nELMForest. \\n- \\nIt would make sense that the more ELMs exist, the less time new ELMs need to be trained \\nfor, since more ELMs means more specialized ELMs, and that the data distribution of the \\nnew domain will probably be closer to the distribution of existing domains (since there \\nare more domains to pick from). \\n \\nInitial Results \\n- \\nSetup \\no ELMForest trained on 8 domains, one trained at step 0 and the remaining 7 were \\ntrained in parallel from the initial domain (only one BTM cycle done). \\no Models compared at a compute-matched basis at training. \\no 3 models used: \\n▪ Dense Transformer - where the data from each domain is balanced. \\n▪ DEMix – domain specialized layer (domain-level MoE). \\n▪ ELMForest – full domain models (ELMs). \\n- \\nELMForest provides the best performance on all sizes (up to 1.3B dense), and these hold \\nwith scale. \\no However, a full ELMForest ensemble has an increased inference cost. \\n- \\nELMForest provides speedups during training (more updates per second). \\no This is justified by the reduction in cross-GPU communication for parameter \\nsynchronization (no alltoall operations needed). \\n- \\nTo match inference costs with dense, the ELMForest weights can be averaged. This is \\nexperimented through 3 strategies: \\no Uniform – each ELM is given the same weight. \\no Argmax – use only the ELM that is closer to the target data, equivalent to top-k \\nwith k=1. \\no Posterior – weighted average between all domains based on the domain posterior \\nscore. \\no Uniform performs worse than all other strategies, even dense. \\no Argmax performs better than dense in training domains, but worse in evaluation \\ndomains. \\n▪ This is expected since evaluation domains (out-of-domain performance) \\nbenefit more from using shared knowledge/parameters. \\no Posterior performs better than all strategies (including dense) except for the \\nsmallest model (dense is the best in that scenario). \\n▪ With enough training, Posterior top-k can outperform dense at the 125M \\nscale. \\n- \\nEven though Posterior parameter-averaging is promising due to improved performance \\nover dense at the same training and inference cost, a full ensemble still provides the best \\nresults. \\no The significantly reduced inference cost from Posterior parameter-averaging \\nmakes this much more practical. \\n \\nFurther Analysis \\n- \\nAblations are made to compare the traditional BTM model with: \\no A random ensemble - same setup but each ELM is trained on a random data split, \\nnot on a specific domain. This results in an ensemble of general experts instead of \\nan ensemble of specialized experts. \\no An ELMForest where all ELMs are randomly initialized. This should take away the \\neffect of optimizing the initialization of new experts. \\no These 2 variations led to worse performances, so the ELMForest performance is \\nnot simply the result of ensembling parameters. \\n- \\nAblations were done to decide on how much compute should be given to the seed training \\n(step 0) – these ablations explain and fix the underperformance of ELMForest compared \\nto dense at the 125M scale: \\no In the initial setup used (8 training domains), the optimal amount of deed training, \\nin relation to the total training budget, was from 40%-60%. \\n▪ For the parameter-averaging approach, the ideal is 60%-70%, and \\nrandomly initialized ELMs (0% seed training) do not work well at all (they \\nperform very poorly) in this setup. \\no Although not optimal, reducing seed training down to 10% of the total budget \\nresults in gains over dense and randomly initialized ELMs. \\n▪ This shows that ELMForest performance is robust to a wide range of seed \\nLM training compute allocations. \\no More seed training is especially useful for evaluation domains (out-of-domain \\nperformance). \\n- \\nFurther ablations were done using different datasets for seed training (using a 50% \\ncompute allocation to seed training). \\no The more heterogeneous (diverse) the seed data is, the better. \\no However, performance is robust to the choice of seed training corpus. \\n▪ Even using only JavaScript code for seed training led to better performance \\nthan dense. \\no Removal of unwanted ELM domains is also robust to the seed training corpus. \\n▪ Performance on removed domains degrades significantly when such \\ndomain is removed. \\n \\nScaling the ELMForest to 64 Domains \\n- \\n64 domains used for training and 16 for evaluation (80 total). \\no 4 BTM cycles are done, 16 training domains for each cycle/batch. \\n- \\nThe dense Transformer used for comparison: \\no 1.3B parameter model. \\no Trained for 6144 total GPU hours (using 128 GPUs). \\n- \\nThe 64-domain ELMForest: \\no Uses seed training of 75%. \\no 4 GPUs per ELM (4x16 = 64 GPUs used concurrently). \\no For BTM cycles/batches 2 and 3, 40 GPU hours were used for each domain, and \\nfor batch 4 20 GPU hours per domain were used. \\no The total training cost of training this 64-domain ELMForest was 2565 hours, \\nsignificantly lower than training the dense model. \\n- \\nUsing only 40% of the dense Transformer’s computational budget for training, the ELM \\nfull ensemble (not parameter-averaged) performs comparably to the Transformer \\n(although at an increased inference cost). \\no ELMForest is especially better for training domains since the parameters for each \\ntraining domain is not updated and thus not forgotten. \\n- \\nAnalysis shows that sparsity in the ELMForest posterior, suggesting a top-k approach can \\nbe taken to reduce inference costs: \\no Top-8 routing performs similarly to a full ensemble.  \\n▪ This means that the ensemble can be reduced to 8 experts chosen at \\ninference without a loss in quality. \\no Top-1 routing still performs better than the dense Transformer if the Transformer \\nwas given the same amount of training. \\n▪ Parameter-averaging performs significantly better than top-1, and almost \\nas well as top-2 (top-2 has double the inference costs). \\n▪ Anywhere from averaging the parameters to condense them into a single \\nLM or using top-2 to top-8 routing seems optimal, depending on the \\ncompute available. \\n \\nMy takeaways: \\n- \\nFuture research can include: \\no How to improve the weights taken for parameter averaging of the ELMForest? \\n▪ There is a hot area of research, so different techniques exist. \\no Best practices for scaling and coordinating the training of ELMForests. \\no Combining ELMForests with adapters to scale into smaller domains. \\no Unsupervised domain assignment. \\n▪  \\n▪ A small sampling of training data is required for calculating the domain \\nposterior. Unsupervised assignment would get rid of this. \\n▪ Topic of the next research paper that gives continuation from the research \\nwork by the University of Washington – “Scaling Expert Language Models \\nwith Unsupervised Domain Discovery”. \\no Recipes for leveraging ELMForests for user safety. \\n \\n', 'source_name': 'Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BTM_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DEMix.pdf #19\n",
      "{'content': 'DEMIX Layers: Disentangling Domains for Modular Language Modeling\\nSuchin Gururangan†}\\nMike Lewis}\\nAri Holtzman†\\nNoah A. Smith†♠\\nLuke Zettlemoyer†}\\n†Paul G. Allen School of Computer Science & Engineering, University of Washington\\n♠Allen Institute for AI\\n}Facebook AI Research\\nSeattle, WA, USA\\nsg01@cs.washington.edu\\nAbstract\\nWe introduce a new domain expert mixture\\n(DEMIX) layer that enables conditioning a lan-\\nguage model (LM) on the domain of the input\\ntext. A DEMIX layer is a collection of expert\\nfeedforward networks, each specialized to a\\ndomain, that makes the LM modular: experts\\ncan be mixed, added or removed after initial\\ntraining. Extensive experiments with autore-\\ngressive transformer LMs (up to 1.3B parame-\\nters) show that DEMIX layers reduce test-time\\nperplexity, increase training efﬁciency, and en-\\nable rapid adaptation with little overhead. We\\nshow that mixing experts during inference, us-\\ning a parameter-free weighted ensemble, al-\\nlows the model to better generalize to hetero-\\ngeneous or unseen domains.\\nWe also show\\nthat experts can be added to iteratively incor-\\nporate new domains without forgetting older\\nones, and that experts can be removed to re-\\nstrict access to unwanted domains, without ad-\\nditional training. Overall, these results demon-\\nstrate beneﬁts of explicitly conditioning on tex-\\ntual domains during language modeling.\\n1\\nIntroduction\\nConventional language model (LM) training algo-\\nrithms assume data homogeneity: all parameters\\nare updated to minimize the loss on all of the data.\\nWe refer to this approach as dense training. Yet hu-\\nman language is as varied as human experience, a\\nfact researchers often refer to obliquely when they\\nuse the term domain to describe distinct underly-\\ning subpopulations of the corpus. Dense training\\nleaves variation in the data to be implicitly discov-\\nered (Aharoni and Goldberg, 2020), assuming that\\nmodels will be able to ﬁt all domains equally well.\\nWhile dense training is convenient, and densely\\ntrained LMs achieve impressive results (Brown\\net al., 2020), the approach has drawbacks with re-\\nspect to generalization, efﬁciency, and ﬂexibility.\\nEven if training data is sourced from many do-\\nmains, dense training can in practice emphasize\\nsubsets of the data in proportion to their ease of\\naccess (Oren et al., 2019; Fan et al., 2020), limiting\\ngeneralization to less prevalent domains. Updat-\\ning all parameters of the network gets substantially\\nmore expensive as model size grows (Strubell et al.,\\n2019), making ﬁne-tuning or domain-adaptive pre-\\ntraining (DAPT; Gururangan et al., 2020) harder\\nto perform with smaller computational budgets. It\\nis also difﬁcult to adapt to new domains without\\nforgetting the original data (McCloskey and Cohen,\\n1989; Aghajanyan et al., 2021) or restrict access to\\ncertain domains the LM has been exposed to dur-\\ning training (e.g., those that contain hate speech;\\nBender et al. 2021), leading to risks of unwanted\\nbehavior (Gehman et al., 2020).\\nTo address these limitations of dense training, we\\nargue that LMs should be designed with modularity.\\nWe propose a modular LM that has components\\nspecialized to distinct domains in the training data,\\nand can be customized at inference-time by mixing,\\nadding, or removing these separated components\\nas needed. This design principle emphasizes the\\nability to rapidly adapt the LM after training, a\\nneed that has been broadly advocated for language\\nsystems (Dinan et al., 2021; Lazaridou et al., 2021).\\nWe introduce modularity into an LM with a new\\ndomain expert (DEMIX) layer that explicitly condi-\\ntions the LM on the domain of the input text (when\\nit is known), or estimates the input domain during\\ninference (when it is not known). A DEMIX layer\\nis a drop-in substitute for a feedforward layer in a\\ntransformer LM (e.g., GPT-3), creating a special-\\nized version of the layer (or expert) per domain (see\\nFigure 1; §3).1 We ﬁnd that replacing every feed-\\n1This is an example of conditional computation (Fedus\\net al., 2021; Lepikhin et al., 2020; Lewis et al., 2021; Roller\\net al., 2021), which follow prior literature on mixture of ex-\\nperts (Jacobs et al., 1991; Shazeer et al., 2017). Unlike dense\\ntraining, conditional computation activates different parame-\\nters for different inputs. Instead of learning how to route data\\nto experts, the DEMIX layer routing mechanism follows from\\na natural, observable segmentation of the data.\\n1\\narXiv:2108.05036v2  [cs.CL]  20 Aug 2021\\nTraining \\nInference \\nSelf-ATTN\\nx0\\nh0\\nDEmix \\nLayer\\nMedical \\nPapers\\n Mix Experts \\nCOVID-19 \\nPapers\\n Add Experts \\nU.S. Court \\nOpinions\\nGithub  \\nCode\\nRemove Experts  \\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nSocial \\nMedia\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 5\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nNews  \\nArticles\\nFigure 1: Illustration of a DEMIX layer in a single transformer block. During training, expert feedforward net-\\nworks are conditionally activated based on the domain (here, document provenance) of the input sequence (i.e.,\\nscientiﬁc papers or court opinions). At inference time, the language model has new modular functions: domain\\nexperts can be mixed to handle heterogeneous domains, added to adapt to novel domains, or removed to “forget”\\nunwanted domains. Image attribution: news icon from emojipedia.org; all other icons from istockphoto.com.\\nforward layer in the transformer with a DEMIX\\nlayer offers new affordances for modularity, ad-\\ndressing the challenges above, while improving\\nperformance in both training domains and novel\\ntest-time domains.\\nAlthough the concept of a domain lacks a rigor-\\nous deﬁnition in NLP, we use coarse provenance\\ncategories (e.g., whether a document is a medical\\nresearch paper or a Reddit post) as a conditioning\\nvariable when training an LM with DEMIX layers\\n(§2). Training on data from eight different domains,\\nwe ﬁnd that DEMIX layers consistently improve\\nin-domain performance (§4). However, because\\nthese categories may not be an optimal segmenta-\\ntion of the training data, or may lack coverage of\\ntest-time domains, naively selecting a single do-\\nmain expert at test time can hurt generalization.\\nInstead, we introduce a parameter-free probabilis-\\ntic approach to dynamically estimate a weighted\\nmixture of domains during inference (§5). Mixing\\nexperts improves DEMIX performance not only on\\nnovel test-time domains, but also on test data from\\nthe training domains, which may themselves be\\nheterogeneous. Our results suggest that introduc-\\ning modularity into an LM need not come at a cost\\nto generalization performance.\\nBecause DEMIX forces experts to specialize to\\ndomains, the overall model can be (partially) disen-\\ntangled after training. Beyond mixing, we can add\\n(§6) or remove (§7) domain experts, resulting in\\npredictable changes in model behavior at inference\\ntime: adding experts allows for model adaptation\\nwithout updating all parameters (hence avoiding\\nforgetting), and removing experts allows for sim-\\nulating the removal of training domains without\\nadditional training. Overall, DEMIX layers demon-\\nstrate beneﬁts of explicitly conditioning on textual\\ndomains during language modeling, and our results\\nsuggest that these beneﬁts persist at scale. Our\\ncode is publicly available.2\\n2\\nMulti-Domain Corpus\\nWe center this study around a large, multi-domain\\ncorpus we constructed with explicit provenance\\nmetadata (Table 1). While other multi-domain cor-\\npora (Koh et al., 2021; Gao et al., 2020) cover many\\nmore domains and tasks, the corpus we introduce\\ncontains substantial metadata-tagged text for lan-\\nguage modeling, as well as datasets with friendly\\nlicensing to support reproducibility.\\n2http://github.com/kernelmachine/demix\\n2\\nroom for \\nimprovemen\\nt\\nDomain\\nCorpus\\n# Train (Eval.) Tokens\\nTRAINING\\n1B\\n30M NewsWire sentences (Chelba et al., 2014)\\n700M (10M)\\nCS\\n1.89M full-text CS papers from S2ORC (Lo et al., 2020)\\n4.5B (10M)\\nLEGAL\\n2.22M U.S. court opinions, 1658 to 2018 (Caselaw Access Project, 2018)\\n10.5B (10M)\\nMED\\n3.2M full-text medical papers from S2ORC (Lo et al., 2020)\\n9.5B (10M)\\nWEBTEXT†\\n8M Web documents (Gokaslan and Cohen, 2019)\\n6.5B (10M)\\nREALNEWS†\\n35M articles from REALNEWS (Zellers et al., 2019)\\n15B (10M)\\nREDDIT\\nReddit comments from pushshift.io (Baumgartner et al., 2020)\\n25B (10M)\\nREVIEWS†\\n30M Amazon product reviews (Ni et al., 2019)\\n2.1B (10M)\\nTotal\\n73.8B (80M)\\nDomain\\nCorpus\\n# Train (Eval.) Tokens\\nNOVEL\\nACL PAPERS\\n1.5K NLP papers from ACL (Dasigi et al., 2021)\\n1M (1M)\\nBREAKING NEWS†\\n20K latest articles from 400 English news sites (Baly et al., 2018)\\n11M (1M)\\nCONTRACTS†\\n500 commercial legal contracts (Hendrycks et al., 2021)\\n1.5M (1M)\\nCORD-19\\n400K excerpts from COVID-19 research papers (Wang et al., 2020)\\n60M (10M)\\nGITHUB\\n230K public Github repository contents (Github Archive Project)\\n200M (10M)\\nGUTENBERG\\n3.2M copyright-expired books (Project Gutenberg)\\n3B (10M)\\nTWEETS†\\n1M English tweets from 2013-2018\\n8M (1M)\\nYELP REVIEWS†\\n6M Yelp restaurant reviews (Yelp Reviews)\\n600M (10M)\\nTable 1: Domains that make up our multi-domain training corpus, including the size of our training and eval-\\nuation (i.e. validation and test) data, in whitespace-separated tokens. † indicates datasets that we (partially)\\nanonymize (§2). REDDIT was extracted and obtained by a third party and made available on pushshift.io,\\nand was anonymized by Xu et al. (2020); we use their version. See Appendix §A.1 for more details on how these\\ndata were collected.\\n2.1\\nDocument Provenance as a Domain Label\\nWhile a growing body of work has attempted to\\naddress the structure and composition of language\\ndomains (Eisenstein et al., 2014; Plank, 2016; Aha-\\nroni and Goldberg, 2020; Gururangan et al., 2020),\\nfundamentally what a domain is remains a matter\\nof debate. In this work, we focus on the prove-\\nnance of a document, operationalized coarsely by\\nthe dataset we used to access it, which approxi-\\nmates a social process that produced it. Deﬁning\\ndomains this way is easy and intuitive, conveys a\\ngreat deal about the variation in a document’s lan-\\nguage, and aligns with common practice in NLP\\nresearch. However, other accounts of variation\\nin language (e.g., Lucy and Bamman, 2021), and\\nricher notions of relationships among domains (e.g.,\\nhierarchies; Gururangan et al., 2020), may be stud-\\nied in future work.\\n2.2\\nCorpus Description\\nThe multi-domain corpus we use in this study con-\\nsists of two parts. The ﬁrst is a collection of train-\\ning domains: text from eight domains of largely\\nEnglish text, listed at the top of Table 1, each of\\nwhich vary in complexity and coverage and has\\nbeen the subject of study in NLP.3\\n3The metadata for each document includes at least its\\nprovenance, and in some cases more information (e.g., URLs,\\nThe second part is a collection of novel domains:\\ntext from eight domains also of largely English\\ntext, listed at the bottom of Table 1, which may\\nor may not align with the training domains. The\\nnovel domains allow us to measure how models\\ngeneralize to a more challenging data distribution\\nshift, where domain boundaries may be less clear.\\nSee Appendix §A.1 for more details on how\\nthese data were collected. To support future work\\nwith the data, we also release a standard API to\\ndownload and preprocess it into a format compat-\\nible with Fairseq (Ott et al., 2019).4 We replace\\nuser identiﬁable information (e.g., email addresses,\\nuser handles, social security numbers, credit card\\nnumbers, phone numbers) with dummy tokens.5\\n3\\nDEMIX Layer\\n3.1\\nBackground: Mixture-of-Experts\\nTransformers\\nThe transformer architecture is comprised of inter-\\nleaved multi-head self-attention, layer-norms, and\\npublication venue, or legal jurisdiction). Future work might\\nexplore more ﬁne-grained notions of domain.\\n4https://github.com/kernelmachine/\\ndemix-data\\n5While it is difﬁcult to anonymize data perfectly, especially\\nat scale, we use a suite of regexes to identify commonly oc-\\ncurring identiﬁable information on the Internet. See Appendix\\n§A.2 for more details.\\n3\\nScaling Expert Language \\nModels with Unsupervised \\nDomain Discovery\\nfeedforward networks (Vaswani et al., 2017). Each\\nof these layers produces a vector representation\\nfor each of the input tokens. Our focus is on the\\nfeedforward component:\\nht,` = FFN(ht,`−1),\\n(1)\\nwhere ht,` is the vector for the tth token produced\\nby layer `.\\nShazeer et al. (2017) propose a formulation of\\none or more feedforward layers as an ensemble\\nof n experts FFN1, . . . , FFNn, assigned weights\\nrespectively by functions g1, . . . , gn:\\nFFN(ht,`−1) =\\nn\\nX\\nj=1\\ngj(ht,`−1) · FFNj(ht,`−1)\\n(2)\\nThe g function routes tokens to different experts,\\nusually each a separate instance of the original\\nfeedforward network. If g routes to a single ex-\\npert, then the computational cost (in ﬂoating-point\\noperations; FLOPs) will be same as the original\\nfeedforward network, even though it has slightly\\nmore than n times as many parameters.\\n3.2\\nDEMIX Routing\\nPrevious approaches learn the weighting functions\\ng at a token-level, and either assign at most one\\n(Fedus et al., 2021) or two (Lepikhin et al., 2020)\\nexperts per token. This necessitates load balancing\\nand other techniques to encourage the model to use\\nall experts instead of relying on just a few (Fedus\\net al., 2021; Lewis et al., 2021).\\nWe instead use domain metadata provided with\\ntraining documents to route data to experts at the\\ndocument (i.e., sequence) level. During training,\\nevery token in the same sequence is assigned to the\\nsame expert based on the domain label.\\nLet D denote the set of domain labels (i.e., the\\neight labels in Table 1). If we index the experts by\\nD and d 2 D is the domain label for the current\\ntraining instance, then\\ngj(ht,`) =\\n⇢1\\nif j = d\\n0\\notherwise\\n(3)\\nWhile we assume that each training document is\\nassociated with a single domain label, we relax this\\nrequirement at inference time (§5), which improves\\nmodel performance in mixed and unknown domain\\nscenarios.\\n3.3\\nDEMIX Architecture\\nOur design results in one expert in a DEMIX layer\\nper domain (i.e., eight experts for eight training\\ndomains in our multi-domain corpus).\\nWe replace every feedforward layer in the trans-\\nformer with a DEMIX layer, in contrast to previous\\nwork (Fedus et al., 2021; Lepikhin et al., 2020) that\\ninterleaves shared and expert layers. Preliminary\\nexperiments showed that interleaving led to worse\\nin-domain performance with DEMIX layers. We\\nhypothesize that shared layers may serve as a bot-\\ntleneck to ﬁnd shared features between domains,\\nand may impact performance adversely when train-\\ning domains are highly different from one another.6\\nFuture work might perform careful comparisons of\\ndifferent architectural choices.\\nIn this study, each expert FFNj is a two-layer\\nMLP with the same dimensions as the original\\nFFN layer of the transformer. As with other con-\\nditional computation models (Fedus et al., 2021;\\nLepikhin et al., 2020), this means that the effec-\\ntive number of parameters in the overall DEMIX\\nLM increases (Table 2). While this incurs memory\\ncosts, the computational budget we consider in this\\nstudy centers around runtime costs. DEMIX layers\\ndecrease the runtime costs of training the LM.\\n3.4\\nDEMIX Training\\nDEMIX layers increase the total parameters of the\\nLM while also reducing GPU latency costs dur-\\ning training, effectively reducing runtime costs of\\ntraining the LM.\\nDENSE training (also referred to as data-\\nparallel) is usually implemented by copying model\\nparameters to every GPU, feeding a different mini-\\nbatch of shufﬂed data to each GPU, computing a\\nstochastic gradient for each mini-batch, and updat-\\ning all parameters synchronously with the average\\nstochastic gradient from across all GPUs.\\nTo train an LM with DEMIX layers, we instead\\npartition the GPUs among the domains, so that\\neach GPU is assigned a single domain (along with\\nits corresponding expert). During training, we ﬁll a\\nmini-batch with k sequences, where each sequence\\nrepresents data from a particular domain, and we\\nsend each mini-batch to its dedicated domain ex-\\npert.\\nWe use larger batch sizes by performing\\ndata-parallel training between expert parameters\\n6Indeed, preliminary experiments suggest that interleaving\\nexpert layers causes large performance hits in the most distinct\\ndomains, i.e., those with lower vocabulary overlap with other\\ndomains in the corpus.\\n4\\nDomain-level \\nrouting. Has \\nimplications on \\nexpert’s \\nspecialization\\nMemory costs, \\n# of total \\nparameters\\nTraining costs\\nInference \\ncost, \\n# of active \\nparameter\\ns\\non GPUs assigned to the same domain; we assign\\nn/8 GPUs to each domain (Table 2). To reduce\\noverﬁtting, we ensure that each of these n/8 GPUs\\nis assigned to different shards of their domain’s\\ntraining data.\\nWe compare the training efﬁciency of DENSE\\nand DEMIX models up to 1.3B parameters per\\nGPU in Table 2.\\nCompared to DENSE LMs,\\nDEMIX layers achieve the same or slightly higher\\nthroughput (measured in TFLOPs/GPU) for the\\nsame total FLOPs per update, despite adding sig-\\nniﬁcantly more parameters.\\nDEMIX achieves higher throughput because we\\nonly synchronize expert parameters allocated to the\\nsame domain.7 As we increase model size, this re-\\nsults in a reduction of latency costs between GPUs,\\nand hence, faster training; instead of synchroniz-\\ning parameters over n GPUs, we perform eight\\nsynchronizations over n/8 GPUs.8\\nIn this work, we assume that there is sufﬁcient\\ndata for each training domain that each expert can\\nbe exposed to the same amount of data, and load\\nbalancing between experts is not necessary. Future\\nwork may consider how varying the amount of\\ndata per domain inﬂuences absolute and relative\\nperformance across domains, especially in the long\\ntail of rare domains.\\nWhile the total number of parameters of DEMIX\\nLMs are substantially larger than their DENSE\\ncounterparts, since the practical training costs are\\nessentially the same, we compare baselines in all\\nsubsequent experiments based on parameters per\\nGPU, as we do in Table 2.\\n4\\nIn-Domain Performance\\nThe ﬁrst set of experiments in this study considers\\nthe impact of replacing the conventional feedfor-\\nward layers in a transformer LM with DEMIX lay-\\ners. We run all experiments in this section with the\\ntraining domains (Table 1).\\n4.1\\nExperimental Setup\\nArchitecture and Input\\nThe model architecture\\nis a randomly-initialized LM with the GPT-3\\n(Brown et al., 2020) architecture implemented in\\nFairseq (Ott et al., 2019). We experiment with\\nmultiple architectures (i.e., those of GPT-3 small,\\nmedium, large, and XL), at a maximum size of\\n7Shared parameters are synchronized across all GPUs.\\n8While this technique reduces latency costs, the bandwidth\\ncosts are the same between DEMIX and DENSE models.\\nParameters per GPU\\n125M\\n350M\\n760M\\n1.3B\\nDENSE\\nGPUs\\n32\\n64\\n128\\n128\\nTotal Experts\\n0\\n0\\n0\\n0\\nGPUs/expert\\n0\\n0\\n0\\n0\\nTotal params\\n125M\\n350M\\n760M\\n1.3B\\nTFLOPs/update\\n556\\n3279\\n13,637\\n23,250\\nTFLOPs/GPU\\n31\\n37\\n45\\n51\\nDEMIX\\nGPUs\\n32\\n64\\n128\\n128\\nTotal Experts\\n8\\n8\\n8\\n8\\nGPUs/expert\\n4\\n8\\n16\\n16\\nTotal params\\n512M\\n1.8B\\n3.8B\\n7.0B\\nTFLOPs/update\\n556\\n3279\\n13,637\\n23,250\\nTFLOPs/GPU\\n31\\n37\\n48\\n55\\nTable 2: Our speciﬁcations for training DENSE and\\nDEMIX LMs.\\nAll models are trained for about 48\\nhours on V100 GPUs. DEMIX layers increase the total\\nparameters of the LM while maintaining (or increasing)\\nthroughput, measured in TFLOPs/GPU. We use the for-\\nmula described in Narayanan et al. (2021) to calculate\\nthese metrics. See Appendix §A.3 for more details.\\nabout 1.3B parameters per GPU. We use the GPT-2\\n(Radford et al., 2019) vocabulary of 50,264 BPE\\ntypes, and train with 1,024-token sequences, with\\ncross-document boundaries. Each document has a\\nbeginning-of-sentence token prepended to it.\\nHyperparameters\\nWe set the total number of\\ntraining steps based on this allocated runtime, set\\n8% of these steps to be warm-up, and use the Adam\\noptimizer (Kingma and Ba, 2017) with a polyno-\\nmial learning rate decay. Learning rates are tuned\\nfor each model separately over {0.0001, 0.0003,\\n0.0005}, taking the fastest learning rate that avoids\\ndivergence. Each worker processes two sequences\\nof length 1,024, and gradients are accumulated over\\n8 updates. We clip gradients if their L2 norm ex-\\nceeds 0.1. See Appendix §A.4 for more details.\\nThese settings are inspired by Lewis et al. (2021).\\nComputational Budget\\nWe follow previous\\nwork in using runtime as the primary computational\\nbudget, which provides a better comparison of the\\npractical costs of training conditional compute and\\ndense models (Lewis et al., 2021). We assume a\\nﬁxed budget of about 48 hours on NVIDIA V100\\n32GB GPUs. We display the number of GPUs used\\nfor each model size in Table 2; we chose these GPU\\nbudgets because larger models require more com-\\npute to train properly (Lewis et al., 2021; Kaplan\\net al., 2020), and found these GPU budgets to result\\nin stable training for each model size given mostly\\nﬁxed hyperparameters.\\n5\\nAssumes enough data is \\navailable and that loads \\nare balanced\\nParameters per GPU\\n125M\\n350M\\n760M\\n1.3B\\nDENSE\\n20.6\\n16.5\\n14.5\\n13.8\\nDENSE (Balanced)\\n19.9\\n15.8\\n14.3\\n13.6\\n+DOMAIN-TOKEN\\n19.2\\n15.9\\n14.3\\n13.4\\nDEMIX (naive)\\n18.4\\n15.5\\n14.2\\n13.8\\nDEMIX (cached; §5.4)\\n17.8\\n14.7\\n13.9\\n13.4\\nTable 3: Average of in-domain test-set perplexity. We\\ndiscuss the last row in §5.4.\\nEvaluation\\nWe report test-set perplexities after\\nabout 48 hours of training. In all tables, we report\\neach result with respect to a set number of parame-\\nters per GPU, as in Table 2. As mentioned in §3.4,\\nDEMIX LM will have a larger effective size than\\nthe DENSE LM at the same increased throughput.\\n4.2\\nCompared Models\\nDENSE\\nThe ﬁrst baseline is a DENSE model that\\ntreats the data as homogeneous, i.e., it shares all pa-\\nrameters across all domains. Under this setup, the\\nlanguage model parameters are copied across all\\nGPUs, and gradients computed during training are\\nall-reduced across every GPU. There is no explicit\\nconditioning on domain.\\nDENSE (Balanced)\\nUnder this setting, we train\\ndensely but ensure that the model is exposed to an\\nequal amount of data from each domain. While\\nthere is still no explicit conditioning on domain,\\nthe gradient updates that the model makes during\\ntraining are an average of those computed across\\nall domains represented in a batch.\\n+DOMAIN-TOKEN\\nThis model is trained iden-\\ntically to DENSE (Balanced), but we prepend a\\ntoken indicating the sequence’s domain to every\\nsequence block (during training and test time). A\\nvariant of this domain token is explored in some\\nprevious studies (Zellers et al., 2019; Keskar et al.,\\n2019). This baseline provides domain information\\nto the language model in the form of input supervi-\\nsion. We ignore the domain token when computing\\nperplexity during evaluation.\\nDEMIX (naive)\\nWe replace every feedforward\\nlayer in the transformer with a DEMIX layer, as\\ndetailed in §3. Under this setting, the domain of\\nthe test data is known and revealed to the model\\n(e.g., the CS expert is used for CS test data), which\\nwe refer to as naive. We also ensure that the model\\nis exposed to an equal amount of data from each\\ndomain.\\n1.3B parameters per GPU\\nDomain\\nDENSE\\nDEMIX\\nDEMIX\\n(naive)\\n(cached prior; §5.4)\\n1B\\n11.8\\n11.5\\n11.3\\nCS\\n13.5\\n12.2\\n12.1\\nLEGAL\\n6.8\\n6.7\\n6.7\\nMED\\n9.5\\n9.2\\n9.1\\nWEBTEXT\\n13.8\\n14.6\\n14.3\\nREALNEWS\\n12.5\\n13.3\\n13.1\\nREDDIT\\n28.4\\n30.6\\n28.1\\nREVIEWS\\n14.0\\n12.6\\n12.5\\nAverage\\n13.8\\n13.8\\n13.4\\nTable 4: Test-set perplexity by domain, for an LM with\\n1.3B parameters per GPU. We discuss the last column\\nin §5.4.\\n4.3\\nResults\\nTable 3 shows test-set perplexities, averaged across\\nthe eight training domains.\\nFirst, we observe\\nthat domain balancing is consistently helpful for\\nDENSE training. We ﬁnd that balancing is espe-\\ncially important in cases in which there is an im-\\nbalance of domain prevalence, conﬁrming similar\\nobservations from previous studies (Arivazhagan\\net al., 2019).9\\nNext, we observe that the beneﬁts of additional\\ndomain information (i.e, domain tokens or DEMIX\\nlayers) are clearest for the smallest model; for\\nlarger models, the beneﬁts are smaller but consis-\\ntent. This result suggests that domain-speciﬁc in-\\nformation enables the model to better specialize to\\ndifferent domains in its training data. However, as\\nthe model size grows, the DENSE baseline becomes\\nincreasingly better at ﬁtting the training domains,\\ncatching up to models with additional domain in-\\nformation, in the average case.\\n4.4\\nDomain Hetereogeneity\\nA more complete view of the experiments with the\\nlargest model is shown in Table 4. We see that\\neven at scale, most training domains beneﬁt from\\nDEMIX layers in a naive setting (where the domain\\nlabel is revealed at test time), but some do not;\\nWEBTEXT, REALNEWS, and REDDIT fare worse\\nthan the DENSE baseline. We believe that this\\nvariation can be explained by heterogeneity within\\ndomains and varying degrees of similarity between\\nthem. DENSE training may be advantageous for\\n9Balancing improves performance on most domains, but\\nhurts performance relative to a DENSE baseline on the REDDIT\\ndomain (Appendix §A.5). In the multi-domain corpus, there\\nis far more REDDIT text than anything else; see Table 1.\\n6\\nOracle that \\nprovides the \\ndomain of the \\ninput\\nAs the model grows, \\nit gains the ability to \\nidentify the input’s \\ndomain\\nFigure 2: Domain experts in DEMIX specialize to\\ntheir domain. We compute the above heatmap with a\\nDEMIX LM with 1.3B parameters per GPU. Each cell\\nof the heatmap is a ratio between an expert’s test per-\\nplexity on a domain to that of the expert trained on that\\ndomain. The diagonal indicates that each expert has\\nthe best performance on its assigned domain. While\\nsome experts (e.g., 1B, MED) do not transfer well to\\nmost domains in the training corpus, WEBTEXT and\\nREALNEWS experts transfer much better, conﬁrming\\ntheir heterogeneity. Key: LG ! LEGAL, MD ! Med,\\nWT ! WEBTEXT, RN ! REALNEWS, RD ! RED-\\nDIT, RV ! REVIEWS.\\ndomains that have a higher degree of overlap with\\nother domains in the corpus (and therefore, beneﬁt\\nfrom parameter sharing).\\nTo provide further evidence for this explanation,\\nwe measure the hetereogeneity of domains in the\\nmulti-domain corpus, according to a DEMIX LM.\\nWe plot a matrix of the perplexity changes across\\nall domain experts in Figure 2, comparing all ex-\\nperts against the expert explicitly trained for each\\ndomain. As the perplexity change tends lower, the\\ncorresponding expert has higher afﬁnity to the tar-\\nget domain.\\nFirst, we observe that domain experts have the\\nhighest afﬁnity to their assigned domain, indicat-\\ning that they do specialize. We also observe that\\nsome experts, e.g., WEBTEXT, REALNEWS, and\\nREDDIT, have relatively high afﬁnities to many\\ndomains, suggesting that these domains are hetere-\\nogeneous. Separately we observe that an expert’s\\nafﬁnity to a domain correlates positively with bi-\\ngram overlap between the expert domain and target\\ndomain (r=0.40, t=3.45, p=0.001). This further\\nsuggests that similar domains have more closely\\naligned domain experts.\\nThese ﬁndings suggest that a discrete notion of\\ndomain, while usually helpful on average (in our\\nartiﬁcially constructed population of eight training\\ndomains), is too rigid. In the next section, we in-\\ntroduce new ways of softening Equation 3 into a\\nmixture over domain experts, to improve perfor-\\nmance on heterogeneous domains.\\n5\\nMixing Experts at Inference Time\\nThe previous section establishes that incorporating\\nDEMIX layers improves LM performance on test\\ndata from known training domains. At inference\\ntime, the domain label was revealed to the model\\nand used to select an expert within each DEMIX\\nlayer. In practice, however, text may not come with\\na domain label, may straddle multiple domains, or\\nmay not belong to any of the domains constructed\\nat training time; the provenance of the data may\\neven be unknown.\\nIn these cases, rather than a hard choice among\\nexperts (Equation 3), we propose to treat g1, . . . , gn\\nas mixture coefﬁcients, transforming the domain\\nmembership of an input text into a matter of proba-\\nbilistic belief. Unlike previously proposed mixture-\\nof-experts formulations (Shazeer et al., 2017; Lep-\\nikhin et al., 2020), this approach introduces no new\\nparameters and the weights are computed only at\\ntest time.10\\nTo analyze inference-time behavior in mixed or\\nunknown domain scenarios, we turn to the corpus\\nof novel domains in the multi-domain corpus (Table\\n1). As mentioned in §2, these domains have fuzzier\\nboundaries, compared to the training domains.\\n5.1\\nDynamically Estimating Domain\\nMembership\\nConsider the probabilistic view of language model-\\ning, where we estimate p(Xt | x<t). We introduce\\na domain variable, Dt, alongside each word. We\\nassume that this hidden variable depends on the\\nhistory, x<t, so that:\\np(Xt | x<t)=\\nn\\nX\\nj=1\\np(Xt | x<t, Dt = j) · p(Dt = j | xt)\\n|\\n{z\\n}\\ngj\\n(4)\\nThis model is reminiscent of class-based n-gram\\nLMs (Brown et al., 1992) and their derivatives\\n(Saul and Pereira, 1997).\\n10We choose to explore inference-time mechanisms instead\\nof training mechanisms to mix experts because 1) we want to\\navoid substantially increasing training costs, i.e., GPU commu-\\nnication between domain experts and 2) we want to maintain\\nthe modularity of experts. Exploring mechanisms for training\\nexpert mixtures while satisfying these desiderata is a rich area\\nfor future work.\\n7\\nx<t\\n“ The COVID-19 pandemic is \\ncaused by severe acute  \\nrespiratory syndrome \\ncoronavirus-2 (SARS-CoV-2)  \\nand has spread worldwide…”\\nht\\nP(Dt|x<t)\\nDt\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nFigure 3: Illustration of inference with domain expert\\nmixing. For a given input text x<t from CORD-19, we\\nestimate a posterior domain probabilities p(Dt | x<t),\\ninformed by a prior that is either iteratively updated dur-\\ning inference, or is precomputed and cached on held-\\nout data. In this example, the model assigns highest\\ndomain probabilities to the medical and news domains.\\nWe use these probabilities in a weighted mixture of ex-\\npert outputs to compute the hidden representation ht.\\nWe have already designed the DEMIX LM to\\ncondition on a domain label, giving a form for\\np(Xt | x<t, Dt = j). The modiﬁcation is to treat\\ng1, . . . , gn as a posterior probability over domains,\\ncalculated at each timestep, given the history so far.\\nTo do this, we apply Bayes’ rule:\\np(Dt = j | xt)= p(x<t | Dt = j) · p(Dt = j)\\np(x<t)\\n(5)\\n=\\np(x<t | Dt = j) · p(Dt = j)\\nPn\\nj0=1 p(x<t | Dt = j0) · p(Dt = j0)\\n(6)\\nThe conditional probabilities of word sequences\\ngiven a domain label, as noted above, are already\\ndeﬁned by the DEMIX LM. For the prior over\\ndomain labels, we consider three alternatives:\\nUniform\\nFix the prior to be uniform across the\\nknown domains.\\nUpdating\\nSet the prior at timestep t to be an\\nexponentially-weighted moving average of the pos-\\nteriors from previous timesteps:\\np(Dt = j) /\\nt−1\\nX\\nt0=1\\nλt−t0 · p(Dt0 = j | xt0)\\n(7)\\nFigure 4: Estimates of posteriors p(Dt | x<t) with\\na DEMIX LM with 1.3B parameters per GPU, after\\n100 sequences (i.e., 102,400 tokens) of data in train-\\ning domains (top heatmap) and new domains (bottom\\nheatmap). Key: LG ! LEGAL, MD ! Med, WT !\\nWEBTEXT, RN ! REALNEWS, RD ! REDDIT, RV\\n! REVIEWS, CD ! CORD-19, GH ! GITHUB, GT\\n! GUTENBERG, BN ! BREAKING NEWS, LC !\\nCONTRACTS, AP ! ACL PAPERS, TW ! TWEETS,\\nYR ! YELP REVIEWS.\\nDuring evaluation, this moving average is calcu-\\nlated over the posterior at the end of each sequence\\nblock. The decay factor avoids putting too much\\nweight on calculations made early in the dataset,\\nwhen posterior calculations are noisier (Appendix\\n§A.6). We performed a small grid search over {0.1,\\n0.3, 0.5, 1.0} to set the value λ, and found that 0.3\\nworked well for most settings.\\nCached\\nIf, prior to testing, some data from the\\ntest distribution is available, we calculate the pos-\\nterior over domain labels from that data, and ﬁx\\nthe prior to that estimate. Under this setting, we\\nuse 100 sequences (i.e., 102,400 tokens) from the\\ndevelopment set to estimate the prior, which we\\nfound to result in stable posterior probabilities (see\\nAppendix §A.6 for more details).\\n8\\nAssumes \\ndata from \\ntesting \\ndomains is \\navailable\\nJustiﬁes mixing \\non unseen \\ndomains vs not \\nmixing in \\ntraining \\ndomains\\nParameters per GPU\\n125M\\n350M\\n760M\\n1.3B\\nDENSE\\n25.9\\n21.4\\n18.4\\n17.8\\nDENSE (B)\\n25.3\\n19.6\\n18.3\\n17.1\\n+DOMAIN-TOKEN\\n24.8\\n20.4\\n18.4\\n18.0\\nDEMIX (naive)\\n28.8\\n23.8\\n21.8\\n21.1\\nDEMIX (average)\\n27.2\\n22.4\\n21.5\\n20.1\\nDEMIX (uniform)\\n24.5\\n20.5\\n19.6\\n18.7\\nDEMIX (updating)\\n21.9\\n18.7\\n17.6\\n17.1\\nDEMIX (cached)\\n21.4\\n18.3\\n17.4\\n17.0\\nTable 5: Average perplexity on domains unseen during\\ntraining. Mixing domain experts with a prior estimated\\nusing a small amount of data in the target domain out-\\nperforms all other baselines.\\nWe display an illustration of the mixture tech-\\nnique in Figure 3.\\n5.2\\nVisualizing Domain Membership\\nIn Figure 4, we plot the posteriors, calculated using\\nthe updating method above after 100 sequences of\\ndevelopment data, each from training and novel\\ndomains. This evaluation is carried out using the\\nDEMIX LM with 1.3B parameters per GPU from\\n§4, with no modiﬁcations.\\nFor known domains (top heatmap of Figure 4),\\nthe correct label has the highest posterior, but these\\ndatasets do not appear to be as distinct or mutu-\\nally exclusive as we assume. For example, Red-\\ndit data is estimated to be around 80% REDDIT,\\n11% WEBTEXT, and 8% REALNEWS. More varia-\\ntion in the estimates is expected and observed for\\nthe new domains (bottom heatmap of Figure 4).\\nWhile ACL PAPERS is mostly associated with the\\nCS domain, and BREAKING NEWS mostly with\\nthe WEBTEXT and REALNEWS domains, CORD-\\n19 is spread across MED, REALNEWS, and 1B;\\nYELP REVIEWS across REVIEWS, WEBTEXT, and\\nREDDIT. The alignment of multiple domains like\\nGITHUB and CONTRACTS primarily to WEBTEXT\\nsuggests the beneﬁt of including a relatively hetero-\\ngeneous domain in training.\\n5.3\\nExperimental Setup\\nWe experiment with the corpus of novel do-\\nmains (Table 1) to test out-of-distribution perfor-\\nmance. We evaluate the three mixture treatments of\\nDEMIX layers (i.e., uniform, updating, and cached\\npriors) against ﬁve baselines. Note that no new\\nmodels are trained for this experiment beyond those\\nused in §4.\\nDENSE and DENSE (Balanced)\\nThese are the\\nbasic baselines trained as in §4; there is no explicit\\nreasoning about domain.\\n+DOMAIN-TOKEN\\nHere test data is evaluated\\nusing each domain label token, and we choose the\\nlowest among these perplexity values per test set.\\nDEMIX (naive)\\nSimilar to +DOMAIN-TOKEN,\\nwe evaluate the data separately with each of the\\neight experts, and report the lowest among these\\nperplexity values per test set.\\nDEMIX (average)\\nAt every timestep, we take a\\nsimple average of the eight experts’ predictions.\\n5.4\\nResults\\nNovel Domain Performance\\nResults averaged\\nacross the eight novel domains are summarized\\nin Table 5. Ensembling DEMIX experts outper-\\nforms DENSE baselines and using experts individ-\\nually (i.e., the “naive” baseline), and caching a\\nprior prior to evaluation results in the best average\\nperformance. While +DOMAIN-TOKEN is compet-\\nitive with naively using DEMIX layers in-domain\\n(Table 3), it consistently underperforms DEMIX\\nwith a weighted mixture on the novel domains. We\\nobserve that ensembling DEMIX experts with a\\ncached prior allows smaller models to match or\\noutperform much larger DENSE models. We also\\nﬁnd that weighted ensembling outperforms simple\\naveraging, conﬁrming the importance of sparsity in\\nthe expert mixture.\\nExamining per-domain performance (Appendix\\n§A.5), we ﬁnd that DEMIX LMs with a cached\\nprior either outperform DENSE baselines or closely\\nmatch them.\\nThe largest improvement against\\nDENSE baselines comes from the TWEETS domain,\\nwhich are on average 67% better across all model\\nsizes. This domain is heterogeneous according\\nto the DEMIX model (Figure 4), conﬁrming the\\nimportance of mixing experts for heterogeneous\\ndomains. These results demonstrate that condition-\\ning the LM on domains during training need not\\ncome at a large cost to generalization to new do-\\nmains, and in many cases can provide large boosts\\nin performance over DENSE baselines.\\nIn-Domain Performance\\nWe can also apply the\\nexpert mixture variant of inference (using a cached\\nprior) to the training domains. We ﬁnd that doing\\nso is beneﬁcial; see the last line of Table 3.\\nWe see improvements in performance across all\\ndomains for every scale, though the largest im-\\n9\\n3. Adapt new expert, freezing all other parameters\\nx<t\\n1. Calculate Domain Posteriors\\n2. Copy “closest” expert\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 5\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 5\\nP(Dt|x<t)\\nDt\\nCOVID-19 \\nPapers\\nCOVID-19 \\nPapers\\nFigure 5: Illustration of DEMIX-DAPT. First, we esti-\\nmate domain posteriors on a held out sample of the tar-\\nget domain (in this case, CORD-19). We then initial-\\nize a new expert with the parameters of the most prob-\\nable expert under the domain posterior distribution. Fi-\\nnally, we adapt the parameters of the newly initialized\\nexpert to the target domain, keeping all other parame-\\nters in the LM frozen.\\nprovements seem to come from hetereogeneous\\ndomains (across all model sizes, REDDIT improves\\non average 10.7%, WEBTEXT 2.4%, REALNEWS\\n1.9%), again conﬁrming that our intuition that do-\\nmain metadata may not perfectly align with the\\nmost effective domain boundaries.\\n6\\nAdaptive Pretraining with New\\nExperts\\nDomain-adaptive, continued pretraining11 of a lan-\\nguage model (DAPT) is a way to use unannotated,\\nin-domain text to improve task performance (Gu-\\nrurangan et al., 2020). However, for a large model,\\nDAPT with DENSE training (which we refer to\\nas DENSE-DAPT) is expensive and may not be\\nfeasible on some computational budgets. Further-\\n11This approach typically precedes supervised ﬁne-tuning\\non task data, hence pretraining.\\nmore, DENSE-DAPT may result in forgetting what\\nwas learned during earlier training phases, limiting\\nreusability.\\nThe modular approach of DEMIX LMs allows\\nthe model to avoid forgetting training domains and\\nadapt cheaply: we can train a new expert and add\\nit to the DEMIX layers of the network without\\nupdating the other experts or the shared parame-\\nters. Because the original model is not changed,\\nforgetting is impossible. We refer to this method\\nof adaptation as DEMIX-DAPT.12\\nWe display an illustration of DEMIX-DAPT\\nin Figure 5. We instantiate a new expert in each\\nDEMIX feedforward layer, initialize it with the pa-\\nrameters of the pretrained expert nearest to the new\\ndomain. We use the posterior calculations from §5\\non a held-out sample to choose the most probable\\nexpert. We then train the added expert on target\\ndata, updating only the new expert parameters. For\\ninference, we use the weighted mixture of domain\\nexperts with a cached prior (§5).\\n6.1\\nExperimental Setup\\nWe compare DEMIX-DAPT to DENSE-DAPT on\\nall novel domains. We report ﬁnal test-set perplex-\\nity after adapting to each domain for 1 hour with\\n8 NVIDIA V100 32GB GPUs, tracking validation\\nperplexity every 10 minutes for early stopping. We\\nadapt to each novel domain with the same hyper-\\nparameters as the original phase of training (§4),\\nexcept for a 10x smaller learning rate.\\n6.2\\nResults\\nAdding one expert\\nWe display examples of\\nDEMIX-DAPT and DENSE-DAPT on a single ad-\\nditional domain in Figure 6. We observe that while\\nDENSE-DAPT reduces perplexity on the novel do-\\nmain, its performance on the training domains pro-\\ngressively worsens, displaying the forgetting effect\\n(we show similar results in larger models in Ap-\\npendix §A.7). In contrast, DEMIX-DAPT reduces\\nperplexity on the novel domain without forgetting.\\nWe generally observe that DEMIX-DAPT out-\\nperforms DENSE-DAPT for some domains (e.g.,\\nCORD-19 and ACL PAPERS), while it closely ap-\\nproaches DENSE-DAPT for others (e.g., GUTEN-\\nBERG; Appendix §A.5). Overall, the parameters\\nfor the additional expert comprise about 10% of the\\ntotal parameters in the DEMIX model, and DENSE-\\nDAPT involves updating all the parameters of the\\n12Our proposed technique is reminiscent of Progressive\\nNeural Networks (Rusu et al., 2016).\\n10\\nFigure 6:\\nAdapting LMs with 125M parameters\\nper GPU to CORD-19 or GUTENBERG.\\nTop row:\\nwhen performing DENSE-DAPT on a new domain\\n(TARGET), average perplexity on all pretraining do-\\nmains degrades. Bottom row: DEMIX-DAPT avoids\\nthat degradation while achieving close (in the case of\\nGUTENBERG) or better (in the case of CORD-19) per-\\nformance. The new CORD-19 expert was initialized\\nwith the MED expert, and the new GUTENBERG expert\\nwas initialized with a WEBTEXT expert.\\nParameters per GPU\\nDomains\\n# Experts\\n125M\\n350M\\n760M\\n1.3B\\nTRAINING\\n8\\n17.8\\n14.7\\n13.9\\n13.4\\n16\\n17.7\\n14.6\\n13.7\\n13.4\\nNOVEL\\n8\\n21.4\\n18.3\\n17.4\\n17.0\\n16\\n16.0\\n14.0\\n13.5\\n12.5\\nTable 6: Average perplexity in training and novel do-\\nmains before and after adding 8 experts adapted to the\\nnovel domains (via DEMIX-DAPT). Adding experts\\nreduces perplexity on all domains, even those previ-\\nously seen.\\nmodel towards in the target domain, so we would\\nexpect that DENSE-DAPT outperforms DEMIX-\\nDAPT in some cases. The strong performance of\\nDEMIX-DAPT on domains like CORD-19 and\\nACL PAPERS suggests that DEMIX-DAPT is es-\\npecially helpful when the target domain strongly\\naligns with one of the experts (Figure 4).\\nAdding eight experts\\nWith expert mixing (§5),\\nnewly added experts can be combined with existing\\nones in the model at test time. To more thoroughly\\nunderstand the effect of adding more experts to the\\nsystem, we add all experts adapted to novel do-\\nmains to the DEMIX model from §4. We display\\nthe performance of a DEMIX LM with 16 experts\\n(8 experts trained on training domains, 8 additional\\nexperts adapted to novel domains) in Table 6. We\\ngenerally observe that DEMIX-DAPT reduces per-\\nplexity on all domains for all model sizes, again\\nwithout forgetting.\\nAdding the eight additional experts in fact re-\\nduces perplexity on previously seen domains. For\\nexample, across all model sizes, on average, we see\\nan 2.4% reduction on MED, 1.8% reduction on RE-\\nALNEWS, and 2% reduction on REDDIT (Appendix\\n§A.5). These improvements are small, which is\\nexpected given that we only performed DEMIX-\\nDAPT for at most one hour with eight GPUs. Even\\nso, these results suggest that DEMIX layers can en-\\nable the LM to incorporate knowledge from novel\\ndomains to improve its performance on previously\\nseen domains.\\n7\\nLanguage Models with Removable\\nParts\\nCurrent LM pretraining datasets are rife with un-\\ndesirable content, from hatespeech to extremism\\n(Gehman et al., 2020; Bender et al., 2021). Another\\nconsequence of DENSE training is that it is difﬁcult\\nto restrict the model’s access to these problematic\\ndomains after training, as might be desirable for\\nmany user-facing tasks (Xu et al., 2020; Dinan\\net al., 2021).\\nDEMIX layers offer new capabilities for\\nlightweight control over the domains in the training\\ndata that LMs use to make predictions at inference\\ntime. In particular, since DEMIX layer experts spe-\\ncialize to their domain (Figure 2), experts that are\\nassigned to domains that are unwanted at test-time\\ncan be simply disabled and unused.\\nA key question is whether disabling an expert\\ncan simulate a model that has not been exposed to\\nthat domain, which we study in this section. How-\\never, since the self-attention and input embedding\\nparameters in the DEMIX LM are shared across\\ndomains, removing an expert offers no guarantee\\nof having fully forgotten content from the removed\\ndomain. Establishing such bounds is an important\\navenue for future work.\\n7.1\\nExperimental Setup\\nTo evaluate whether we can simulate models that\\nhave not been exposed to a particular domain, we\\ncompare three settings:\\n+EXPERT\\nA DEMIX LM with all experts active.\\n–EXPERT\\nA DEMIX LM with a domain expert\\ndeactivated.\\n11\\n125M Parameters per GPU\\nDomain\\n+EXPERT\\n–EXPERT\\n–DOMAIN\\n1B\\n13.7\\n25.5\\n30.4\\nCS\\n15.7\\n22.4\\n25.4\\nLEGAL\\n8.9\\n20.9\\n22.7\\nMED\\n12.4\\n18.6\\n21.9\\nWEBTEXT\\n20.9\\n27.3\\n25.4\\nREALNEWS\\n18.9\\n26.7\\n25.0\\nREDDIT\\n34.4\\n47.8\\n51.3\\nREVIEWS\\n20.5\\n39.0\\n43.0\\nAverage\\n18.2\\n28.5\\n30.6\\nTable 7: In a 125M parameter model, removing a do-\\nmain expert (–EXPERT) results in perplexity degrada-\\ntion on the corresponding domain, approaching the per-\\nformance of an LM that has not been exposed to that\\ndomain (–DOMAIN). Here we bold the worst perform-\\ning model for each domain, i.e. the one that gets the\\nhighest perplexity.\\n–DOMAIN\\nA DEMIX LM retrained from scratch\\nwithout a particular domain. We replace the re-\\nmoved domain with GUTENBERG.13\\nWe evaluate expert removal (+EXPERT and –\\nEXPERT) with the DEMIX LM with 125M param-\\neters per GPU from §4, with no modiﬁcations. For\\nall baselines,we evaluate use expert mixing with a\\ncached prior (§5).\\n7.2\\nResults\\nRemoving a domain expert harms model perfor-\\nmance on the associated domain, in most cases ap-\\nproaching the performance of a model that has not\\nbeen exposed to data from that domain (Table 7).\\nIn some cases (e.g., WEBTEXT and REALNEWS),\\n–EXPERT even underperforms –DOMAIN. This\\nleads us to conjecture that most domain-speciﬁc\\nlearning happens within the DEMIX layer, despite\\nthe fact that other parts of the model are affected\\nby all training domains.\\n8\\nRelated Work\\nIncorporating Metadata\\nDocument metadata\\nhas been commonly used to improve the quality\\nof topic models (Mimno and McCallum, 2012; Ra-\\nmage et al., 2009; Zhu et al., 2012), and previous\\nworks have used metadata for adapting RNN-based\\nlanguage models (Jaech and Ostendorf, 2018) or\\nlearning better document representations (Card\\net al., 2018).\\nZellers et al. (2019) and Keskar\\net al. (2019) prepend document metadata in the\\n13Our cluster requires that jobs are allocated with eight\\nGPUs, necessitating eight experts — hence the substitution.\\ninput text (similar to our +DOMAIN-TOKEN set-\\nting) while training transformer LMs to provide\\nbetter inference-time control of text generation.\\nInference-time Control\\nDEMIX layers provide\\na simple mechanism for inference-time control of\\nlanguage model behavior. Previously proposed\\nmethods for inference-time control are either ex-\\npensive to use (Dathathri et al., 2020), or rely on\\ndensely trained models (e.g., Keskar et al., 2019).\\nLiu et al. (2021) use multiple experts for inference-\\ntime text generation control. This method may be\\napplied to DEMIX layers to steer text generation\\nwith experts trained on different domains.\\nMultilinguality\\nRelated to variation across do-\\nmains is crosslingual variation.\\nPast work has\\nsuggested that multilingual models beneﬁt from\\nlanguage-speciﬁc parameters (Fan et al., 2020;\\nPfeiffer et al., 2020; Chau et al., 2020).\\nHere,\\nwe investigate the effect of incorporating domain-\\nspeciﬁc parameters into the LM. Though the bound-\\naries between languages are (often) more clear than\\nthose among domains, DEMIX layers draw inspi-\\nration from multilingual research, and future work\\nmight explore a compositional approach with both\\nlanguage experts and domain experts.\\nContinual Learning\\nDEMIX-DAPT is a type\\nof continual learning, in which the model learns\\nincrementally on new data (Chen et al., 2018). Pre-\\nviously proposed techniques to support continual\\nlearning include regularization (Kirkpatrick et al.,\\n2017), meta-learning (Munkhdalai and Yu, 2017),\\nepisodic memory modules (Lopez-Paz and Ran-\\nzato, 2017; de Masson d’Autume et al., 2019), and\\ndata replay (Sun et al., 2019), all of which may be\\ncombined with DEMIX layers. Model expansion\\ntechniques to incorporate new reinforcement learn-\\ning or visual tasks (Rusu et al., 2016; Draelos et al.,\\n2017) is especially related to DEMIX-DAPT. Our\\nresults suggest that continual learning in LMs is nat-\\nurally enabled with modular domain experts; this\\nmay be further explored using temporally-relevant\\ndomains (Lazaridou et al., 2021).\\nLM Adapters\\nAlso related to DEMIX-DAPT is\\nthe line of work into adapter modules for pretrained\\nLMs (Houlsby et al., 2019; Pfeiffer et al., 2020).\\nSimilar to the setting in which we add experts for\\nnew domains, adapter modules involve freezing the\\npretrained language model and updating a small\\nnumber of additional parameters that are appended\\n12\\nto certain parts of the network. This study conﬁrms\\nprevious ﬁndings that only a subset of LM parame-\\nters need to be ﬁne-tuned to a target dataset (Zaken\\net al., 2021). Expert addition may be performed\\nwith adapter modules to further improve efﬁciency.\\nMulti-Domain Models\\nMulti-domain models\\nhave been studied extensively in the context of\\nmachine translation, ﬁrst with statistical systems\\n(Banerjee et al., 2010; Sennrich et al., 2013), and\\nmore recently with neural networks (Pham et al.,\\n2021). Other works have explored multi-domain\\nsettings with smaller models and explicit domain\\nlabels, using supervision (e.g., Wright and Augen-\\nstein, 2020; Guo et al., 2018; Zeng et al., 2018)\\nor dense training (e.g., Maronikolakis and Schütze,\\n2021). Previous studies have shown the importance\\nconsidering domains when adapting LMs (Ram-\\nponi and Plank, 2020; Gururangan et al., 2020).\\nOur study establishes the importance of consider-\\ning domains when training LMs from scratch.\\n9\\nConclusion\\nWe introduce DEMIX layers for language models,\\nwhich provide modularity at inference time, ad-\\ndressing limitations of dense training by providing\\na rapidly adaptable system. DEMIX layers experts\\ncan be mixed to handle heterogeneous or unseen\\ndomains, added to iteratively incorporate new do-\\nmains, and removed to restrict unwanted domains.\\nThere are many exciting directions for future\\nwork, in addition to those described throughout\\nthe paper. They include combining domain and\\ntoken-level routing, to realize the beneﬁts of mod-\\nularity while scaling models efﬁciently. The de-\\nsign of DEMIX layers assumes access to coarse\\nprovenance labels (or other metadata) to identify\\ndomains in pretraining data; an alternative option\\nis to use unsupervised learning to discover do-\\nmains in the corpus, which, in concert with domain\\nmetadata, may lead to better DEMIX expert as-\\nsignments. Furthermore, in this work, we study\\nDEMIX layers with a dataset that has a few large\\ndomains. In practice, textual domains usually con-\\ntain many diverse subdomains of varying preva-\\nlence. Training DEMIX layers on dataset with a\\nlong tail of domains may require automatic mea-\\nsures to cluster smaller domains, or hierarchical\\nexperts that are specialized to progressively nar-\\nrower data distributions.\\nAcknowledgments\\nThe authors thank members of UWNLP, FAIR, and\\nAI2, speciﬁcally Shruti Bhosale, Tim Dettmers,\\nEmily Dinan, Doug Downey, Margaret Li, Myle\\nOtt, Oﬁr Press, and Swabha Swayamdipta, for help-\\nful comments. At UW, this work was partially sup-\\nported by NSF grant 1562364, the Ofﬁce of Naval\\nResearch under MURI grant N00014-18-1-2670,\\nand an Amazon research award.\\nReferences\\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\\n2021.\\nBetter ﬁne-tuning by reducing representa-\\ntional collapse. In 9th International Conference on\\nLearning Representations, ICLR 2021, Virtual Event,\\nAustria, May 3-7, 2021. OpenReview.net.\\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\\ndomain clusters in pretrained language models. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 7747–\\n7763, Online. Association for Computational Lin-\\nguistics.\\nNaveen Arivazhagan,\\nAnkur Bapna,\\nOrhan Firat,\\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\\nMia Xu Chen, Yuan Cao, George Foster, Colin\\nCherry, Wolfgang Macherey, Zhifeng Chen, and\\nYonghui Wu. 2019. Massively multilingual neural\\nmachine translation in the wild: Findings and chal-\\nlenges.\\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\\nJames Glass, and Preslav Nakov. 2018.\\nPredict-\\ning factuality of reporting and bias of news media\\nsources. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 3528–3539, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nPratyush Banerjee, Jinhua Du, Baoli Li, Sudip Naskar,\\nAndy Way, and Josef van Genabith. 2010.\\nCom-\\nbining multi-domain statistical machine translation\\nmodels using automatic classiﬁers. In Proceedings\\nof the 9th Conference of the Association for Machine\\nTranslation in the Americas: Research Papers, Den-\\nver, Colorado, USA. Association for Machine Trans-\\nlation in the Americas.\\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\\nMegan Squire, and Jeremy Blackburn. 2020. The\\npushshift reddit dataset.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\\nMajor, and Shmargaret Shmitchell. 2021.\\nOn the\\ndangers of stochastic parrots: Can language models\\nbe too big? In Proceedings of the 2021 ACM Confer-\\nence on Fairness, Accountability, and Transparency,\\nFAccT ’21, page 610–623, New York, NY, USA. As-\\nsociation for Computing Machinery.\\n13\\nPeter F. Brown, Vincent J. Della Pietra, Peter V. deS-\\nouza, Jenifer C. Lai, and Robert L. Mercer. 1992.\\nClass-based n-gram models of natural language.\\nComputational Linguistics, 18(4):467–480.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell,\\nSandhini Agarwal,\\nAriel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot learn-\\ners.\\nDallas Card, Chenhao Tan, and Noah A. Smith. 2018.\\nNeural models for documents with metadata. Pro-\\nceedings of the 56th Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long\\nPapers).\\nCaselaw Access Project. 2018. Caselaw access project.\\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\\nParsing with multilingual BERT, a small corpus, and\\na small treebank.\\nIn Findings of the Association\\nfor Computational Linguistics: EMNLP 2020, pages\\n1324–1334, Online. Association for Computational\\nLinguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2014. One billion word benchmark for measur-\\ning progress in statistical language modeling.\\nZhiyuan Chen, Bing Liu, Ronald Brachman, Peter\\nStone, and Francesca Rossi. 2018.\\nLifelong Ma-\\nchine Learning: Second Edition.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\\nNoah A. Smith, and Matt Gardner. 2021. A dataset\\nof information-seeking questions and answers an-\\nchored in research papers.\\nIn Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 4599–4610, On-\\nline. Association for Computational Linguistics.\\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\\nRosanne Liu. 2020. Plug and play language models:\\nA simple approach to controlled text generation. In\\n8th International Conference on Learning Represen-\\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\\n26-30, 2020. OpenReview.net.\\nCyprien de Masson d’Autume, Sebastian Ruder, Ling-\\npeng Kong, and Dani Yogatama. 2019.\\nEpisodic\\nmemory in lifelong language learning.\\nEmily Dinan, Gavin Abercrombie, A. Stevie Bergman,\\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\\nVerena Rieser. 2021. Anticipating safety issues in\\ne2e conversational ai: Framework and tooling.\\nT. Draelos, N. Miner, Christopher C. Lamb, Jonathan A.\\nCox, Craig M. Vineyard, Kristofor D. Carlson,\\nWilliam M. Severa, C. James, and J. Aimone. 2017.\\nNeurogenesis deep learning: Extending deep net-\\nworks to accommodate new classes.\\n2017 In-\\nternational Joint Conference on Neural Networks\\n(IJCNN), pages 526–533.\\nJacob Eisenstein, Brendan O’Connor, Noah A. Smith,\\nand Eric P. Xing. 2014. Diffusion of lexical change\\nin social media. PLoS ONE, 9(11):e113114.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\\ndeep Baines, Onur Celebi, Guillaume Wenzek,\\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\\nMichael Auli, and Armand Joulin. 2020. Beyond\\nenglish-centric multilingual machine translation.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\\nSwitch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\\ning, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn\\nPresser, and Connor Leahy. 2020.\\nThe pile: An\\n800gb dataset of diverse text for language modeling.\\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\\nYejin Choi, and Noah A. Smith. 2020.\\nRealToxi-\\ncityPrompts: Evaluating neural toxic degeneration\\nin language models. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020, pages\\n3356–3369, Online. Association for Computational\\nLinguistics.\\nGithub Archive Project. Github archive project.\\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\\ntext corpus.\\nJiang Guo, Darsh Shah, and Regina Barzilay. 2018.\\nMulti-source domain adaptation with mixture of ex-\\nperts.\\nIn Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 4694–4703, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nSuchin\\nGururangan,\\nAna\\nMarasovi´\\nc,\\nSwabha\\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\\nand Noah A. Smith. 2020. Don’t stop pretraining:\\nAdapt language models to domains and tasks.\\nIn\\nProceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics, pages\\n8342–8360, Online. Association for Computational\\nLinguistics.\\nDan Hendrycks,\\nCollin Burns,\\nAnya Chen,\\nand\\nSpencer Ball. 2021. Cuad: An expert-annotated nlp\\ndataset for legal contract review.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin de Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly.\\n2019. Parameter-efﬁcient transfer learning for nlp.\\n14\\nRobert Jacobs, Michael Jordan, Steven Nowlan, and\\nGeoffrey Hinton. 1991. Adaptive mixture of local\\nexpert. Neural Computation, 3:78–88.\\nAaron Jaech and Mari Ostendorf. 2018. Low-rank rnn\\nadaptation for context-aware language modeling.\\nJared Kaplan,\\nSam McCandlish,\\nTom Henighan,\\nTom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\\nAmodei. 2020.\\nScaling laws for neural language\\nmodels.\\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\\nney, Caiming Xiong, and Richard Socher. 2019.\\nCtrl: A conditional transformer language model for\\ncontrollable generation.\\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\\nmethod for stochastic optimization.\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\\nKieran Milan, John Quan, Tiago Ramalho, Ag-\\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\\n2017. Overcoming catastrophic forgetting in neural\\nnetworks.\\nPang\\nWei\\nKoh,\\nShiori\\nSagawa,\\nHenrik\\nMark-\\nlund, Sang Michael Xie, Marvin Zhang, Akshay\\nBalsubramani, Weihua Hu, Michihiro Yasunaga,\\nRichard Lanas Phillips, Irena Gao, Tony Lee, Eti-\\nenne David, Ian Stavness, Wei Guo, Berton Earn-\\nshaw, Imran Haque, Sara M Beery, Jure Leskovec,\\nAnshul Kundaje, Emma Pierson, Sergey Levine,\\nChelsea Finn, and Percy Liang. 2021.\\nWILDS:\\nA benchmark of in-the-wild distribution shifts. In\\nProceedings of the 38th International Conference\\non Machine Learning, volume 139 of Proceedings\\nof Machine Learning Research, pages 5637–5664.\\nPMLR.\\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\\nSebastian Ruder, Dani Yogatama, Kris Cao, Tomas\\nKocisky, Susannah Young, and Phil Blunsom. 2021.\\nPitfalls of static language modelling.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\\nGshard: Scaling giant models with conditional com-\\nputation and automatic sharding.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\\nGoyal, and Luke Zettlemoyer. 2021.\\nBase layers:\\nSimplifying training of large, sparse models.\\nAlisa\\nLiu,\\nMaarten\\nSap,\\nXiming\\nLu,\\nSwabha\\nSwayamdipta,\\nChandra\\nBhagavatula,\\nNoah\\nA.\\nSmith, and Yejin Choi. 2021. Dexperts: Decoding-\\ntime controlled text generation with experts and anti-\\nexperts.\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\\nney, and Daniel Weld. 2020. S2ORC: The semantic\\nscholar open research corpus. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics, pages 4969–4983, Online. As-\\nsociation for Computational Linguistics.\\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\\nGradient episodic memory for continual learning.\\nLi Lucy and David Bamman. 2021.\\nCharacterizing\\nenglish variation across social media communities\\nwith bert.\\nAntonios Maronikolakis and Hinrich Schütze. 2021.\\nMultidomain pretrained language models for green\\nNLP.\\nIn Proceedings of the Second Workshop\\non Domain Adaptation for NLP, pages 1–8, Kyiv,\\nUkraine. Association for Computational Linguistics.\\nM. McCloskey and N. Cohen. 1989. Catastrophic in-\\nterference in connectionist networks: The sequential\\nlearning problem. Psychology of Learning and Mo-\\ntivation, 24:109–165.\\nDavid Mimno and Andrew McCallum. 2012.\\nTopic\\nmodels conditioned on arbitrary features with\\ndirichlet-multinomial regression.\\nTsendsuren Munkhdalai and Hong Yu. 2017. Meta net-\\nworks. Proceedings of machine learning research,\\n70:2554–2563.\\nDeepak\\nNarayanan,\\nMohammad\\nShoeybi,\\nJared\\nCasper, Patrick LeGresley, Mostofa Patwary, Vi-\\njay Anand Korthikanti, Dmitri Vainbrand, Prethvi\\nKashinkunti,\\nJulie Bernauer,\\nBryan Catanzaro,\\nAmar Phanishayee, and Matei Zaharia. 2021.\\nEf-\\nﬁcient large-scale language model training on gpu\\nclusters using megatron-lm.\\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\\nJustifying recommendations using distantly-labeled\\nreviews and ﬁne-grained aspects.\\nIn Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 188–197, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nYonatan Oren, Shiori Sagawa, Tatsunori Hashimoto,\\nand Percy Liang. 2019. Distributionally robust lan-\\nguage modeling. In Proceedings of the 2019 Con-\\nference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-\\nIJCNLP), pages 4227–4237, Hong Kong, China. As-\\nsociation for Computational Linguistics.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019.\\nfairseq: A fast, extensible\\ntoolkit for sequence modeling.\\nIn Proceedings of\\nthe 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics\\n15\\n(Demonstrations), pages 48–53, Minneapolis, Min-\\nnesota. Association for Computational Linguistics.\\nJonas Pfeiffer, Ivan Vuli´\\nc, Iryna Gurevych, and Se-\\nbastian Ruder. 2020. MAD-X: An Adapter-Based\\nFramework for Multi-Task Cross-Lingual Transfer.\\nIn Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 7654–7673, Online. Association for Computa-\\ntional Linguistics.\\nMinhQuang Pham, Josep Maria Crego, and François\\nYvon. 2021. Revisiting multi-domain machine trans-\\nlation. Transactions of the Association for Computa-\\ntional Linguistics, 9:17–35.\\nBarbara Plank. 2016. What to do about non-standard\\n(or non-canonical) language in nlp.\\nProject Gutenberg. Project gutenberg.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners.\\nDaniel Ramage, David Hall, Ramesh Nallapati, and\\nChristopher D. Manning. 2009. Labeled LDA: A su-\\npervised topic model for credit attribution in multi-\\nlabeled corpora. In Proceedings of the 2009 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, pages 248–256, Singapore. Association\\nfor Computational Linguistics.\\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\\npervised domain adaptation in NLP—A survey. In\\nProceedings of the 28th International Conference\\non Computational Linguistics, pages 6838–6855,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\\nand Jason Weston. 2021.\\nHash layers for large\\nsparse models.\\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Des-\\njardins, Hubert Soyer, James Kirkpatrick, Koray\\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell.\\n2016. Progressive neural networks.\\nLawrence Saul and Fernando Pereira. 1997. Aggregate\\nand mixed-order Markov models for statistical lan-\\nguage processing. In Second Conference on Empiri-\\ncal Methods in Natural Language Processing.\\nRico Sennrich, Holger Schwenk, and Walid Aransa.\\n2013.\\nA multi-domain translation model frame-\\nwork for statistical machine translation. In Proceed-\\nings of the 51st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers), pages 832–840, Soﬁa, Bulgaria. Association\\nfor Computational Linguistics.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\\nAndy Davis, Quoc V. Le, Geoffrey E. Hinton, and\\nJeff Dean. 2017.\\nOutrageously large neural net-\\nworks: The sparsely-gated mixture-of-experts layer.\\nIn 5th International Conference on Learning Rep-\\nresentations, ICLR 2017, Toulon, France, April 24-\\n26, 2017, Conference Track Proceedings. OpenRe-\\nview.net.\\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\\nlum. 2019.\\nEnergy and policy considerations for\\ndeep learning in NLP. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics, pages 3645–3650, Florence, Italy.\\nAssociation for Computational Linguistics.\\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.\\n2019. Lamol: Language modeling for lifelong lan-\\nguage learning.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need.\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\\nRussell Reas, Jiangjiang Yang, Doug Burdick, Dar-\\nrin Eide, Kathryn Funk, Yannis Katsis, Rodney\\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\\nPaul Mooney, Dewey Murdick, Devvret Rishi, Jerry\\nSheehan, Zhihong Shen, Brandon Stilson, Alex\\nWade, Kuansan Wang, Nancy Xin Ru Wang, Chris\\nWilhelm, Boya Xie, Douglas Raymond, Daniel S.\\nWeld, Oren Etzioni, and Sebastian Kohlmeier. 2020.\\nCord-19: The covid-19 open research dataset.\\nDustin Wright and Isabelle Augenstein. 2020. Trans-\\nformer based multi-source domain adaptation.\\nIn\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 7963–7974, Online. Association for Computa-\\ntional Linguistics.\\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason\\nWeston, and Emily Dinan. 2020. Recipes for safety\\nin open-domain chatbots.\\nYelp Reviews. Yelp reviews.\\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.\\n2021. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\\nfor transformer-based masked language-models.\\nRowan Zellers,\\nAri Holtzman,\\nHannah Rashkin,\\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\\nYejin Choi. 2019.\\nDefending against neural fake\\nnews. In NeurIPS.\\nJiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun\\nXie, Yongjing Yin, and Jianqiang Zhao. 2018. Multi-\\ndomain neural machine translation with word-level\\ndomain context discrimination. In Proceedings of\\nthe 2018 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 447–457, Brus-\\nsels, Belgium. Association for Computational Lin-\\nguistics.\\nJun Zhu, Amr Ahmed, and Eric P. Xing. 2012. Medlda:\\nMaximum margin supervised topic models. Journal\\nof Machine Learning Research, 13(74):2237–2278.\\n16\\nA\\nAppendix\\nA.1\\nCollecting Domains\\nFor most domains,\\nwe use the associated\\nsources, listed in Table 1, without modiﬁca-\\ntion.\\nFor TWEETS, we use the Twitter Aca-\\ndemic API. For GUTENBERG,\\nwe use the\\nscraping tool provided in https://github.com/\\naparrish/gutenberg-dammit. For BREAKING\\nNEWS,\\nwe identify a list of factually reli-\\nable English news sources, using the list cu-\\nrated by Baly et al. (2018).\\nSpeciﬁcally,\\nwe\\nﬁlter\\non\\n\"high\"\\nfactuality\\nin\\nthe\\ndata\\nprovided in this repository:\\nhttps://github.\\ncom/ramybaly/News-Media-Reliability.\\nWe\\nthen use Newspaper3K (https://newspaper.\\nreadthedocs.io/en/latest/) to scrape the lat-\\nest 1000 articles from each site.\\nAfter drop-\\nping duplicates, we arrive at about 20K ar-\\nticles\\nfrom\\n400\\nnews\\nsources.\\nWe\\npro-\\nvide downloading links and general instruc-\\ntions at https://github.com/kernelmachine/\\ndemix-data/blob/main/DOWNLOAD_DATA.md.\\nA.2\\nDataset Anonymization\\nTo anonymize certain datasets, we apply a suite\\nof regexes that aim to identify common patterns\\nof user-identiﬁable data and substitute them with\\ndummy tokens. We display anonymization regexes\\nand associated dummy tokens in Table 8.\\nA.3\\nCalculating TFLOPs/GPU\\nWe use the formula presented in Narayanan\\net\\nal.\\n(2021)\\nto\\ncalculate\\nTFLOPs/GPU\\nand\\nTFLOPs/update.\\nThe\\nspreadsheet\\nthat\\ncontains\\nthe\\ncalculations\\nand\\nfor-\\nmula\\ncan\\nbe\\naccessed\\nhere:\\nhttps:\\n//docs.google.com/spreadsheets/d/1NO-Lz_\\nVqZGF2fpJTFxtXyjhmaoYi6qnz50Xr8W8hgGw/\\nedit?usp=sharing.\\nA.4\\nHyperparameter Assignments\\nWe display hyperparameter assignments for LM\\npretraining in Tables 9, 10,11, and 12.\\nA.5\\nPer-Domain Results\\nWe\\ndisplay\\nper-domain\\ntest\\nresults\\nin\\nthe\\nspreadsheets\\nat\\nthe\\nfollowing\\nlink:\\nhttps://docs.google.com/spreadsheets/d/\\n1yNMZGSPAvhTi3JttLamiCULaOIGTJ4QGEOajO3b5kt8/\\nedit?usp=sharing\\nA.6\\nDomain Posterior Calculations\\nWe track calculated domain posteriors over blocks\\nof development data in Figure 7 (training domains)\\nand Figure 8 (novel domains). The calculate do-\\nmain posteriors are noisier for earlier blocks, sta-\\nbilizing usually after around 50 blocks. For all\\nexperiments, we conservatively use 100 blocks of\\ndata to compute the domain posterior, though one\\nmay be able to accurately calcuate the domain pos-\\nterior for some domains with less data.\\nA.7\\nPerplexity changes after DENSE-DAPT\\nIn Table 13, we display the average perplexity\\nchange after performing DENSE-DAPT on a new\\ndomain. We observe that across all model sizes,\\nDENSE-DAPT improves performance in the novel\\ndomain, at the cost of a large performance hit in\\nthe training domains.\\n17\\nFigure 7: Calculated domain posteriors for 8 training domains.\\nFigure 8: Calculated domain posteriors for 8 novel domains.\\n18\\nCategory\\nLink to Regex\\nDummy Token\\nEmail\\nhttps://regex101.com/r/ZqsF9x/1\\n<EMAIL>\\nDART\\nhttps://regex101.com/r/0tQ6EN/1\\n<DART>\\nFB User ID\\nhttps://regex101.com/r/GZl5EZ/1\\n<FB_USERID>\\nPhone Number\\nhttps://regex101.com/r/YrDpPD/1\\n<PHONE_NUMBER>\\nCredit Card Number\\nhttps://regex101.com/r/9NTO6W/1\\n<CREDIT_CARD_NUMBER>\\nSocial Security Number\\nhttps://regex101.com/r/V5GPNL/1\\n<SSN>\\nUser handles\\nhttps://regex101.com/r/vpey04/1\\n<USER>\\nTable 8: Anonymization schema. We anonymize text using the regexes provided in the above links for the cate-\\ngories listed.\\nComputing Infrastructure\\n32 Volta 32GB GPUs\\nHyperparameter\\nAssignment\\narchitecture\\nGPT-3 small\\ntokens per sample\\n1024\\nbatch size\\n2\\nnumber of workers\\n2\\nlearning rate\\n[5e–4, 3e–4, 1e–4]\\nclip norm\\n0.1\\ngradient acculumation steps\\n8\\nnumber of steps\\n300,000\\nsave interval updates\\n6,000\\nvalidation interval\\n3,000\\nnumber of warmup steps\\n24,000\\nlearning rate scheduler\\npolynomial decay\\nlearning rate optimizer\\nAdam\\nAdam beta weights\\n(0.9, 0.95)\\nAdam epsilon\\n10e-8\\nweight decay\\n0.1\\nTable 9: Hyperparameters for pretraining the LM with 125M parameters per GPU. All hyperparameters are the\\nsame for DEMIX and DENSE training.\\n19\\nComputing Infrastructure\\n64 Volta 32GB GPUs\\nHyperparameter\\nAssignment\\narchitecture\\nGPT-3 medium\\ntokens per sample\\n1024\\nbatch size\\n2\\nnumber of workers\\n2\\nlearning rate\\n[5e–4, 3e–4, 1e–4]\\nclip norm\\n0.1\\ngradient acculumation steps\\n8\\nnumber of steps\\n120,000\\nsave interval updates\\n3,000\\nvalidation interval\\n2,000\\nnumber of warmup steps\\n9,600\\nlearning rate scheduler\\npolynomial decay\\nlearning rate optimizer\\nAdam\\nAdam beta weights\\n(0.9, 0.95)\\nAdam epsilon\\n10e-8\\nweight decay\\n0.1\\nTable 10: Hyperparameters for pretraining the LM with 350M parameters per GPU. All hyperparameters are the\\nsame for DEMIX and DENSE training.\\nComputing Infrastructure\\n128 Volta 32GB GPUs\\nHyperparameter\\nAssignment\\narchitecture\\nGPT-3 large\\ntokens per sample\\n1024\\nbatch size\\n2\\nnumber of workers\\n2\\nlearning rate\\n[5e–4, 3e–4, 1e–4]\\nclip norm\\n0.1\\ngradient acculumation steps\\n8\\nnumber of steps\\n65,000\\nsave interval updates\\n2,000\\nvalidation interval\\n1,000\\nnumber of warmup steps\\n5,200\\nlearning rate scheduler\\npolynomial decay\\nlearning rate optimizer\\nAdam\\nAdam beta weights\\n(0.9, 0.95)\\nAdam epsilon\\n10e-8\\nweight decay\\n0.1\\nTable 11: Hyperparameters for pretraining the LM with 760M parameters per GPU. All hyperparameters are the\\nsame for DEMIX and DENSE training.\\n20\\nComputing Infrastructure\\n128 Volta 32GB GPUs\\nHyperparameter\\nAssignment\\narchitecture\\nGPT-3 XL\\ntokens per sample\\n1024\\nbatch size\\n2\\nnumber of workers\\n2\\nlearning rate\\n[5e–4, 3e–4, 1e–4]\\nclip norm\\n0.1\\ngradient acculumation steps\\n8\\nnumber of steps\\n50000\\nsave interval updates\\n2,000\\nvalidation interval\\n500\\nnumber of warmup steps\\n4000\\nlearning rate scheduler\\npolynomial decay\\nlearning rate optimizer\\nAdam\\nAdam beta weights\\n(0.9, 0.95)\\nAdam epsilon\\n10e-8\\nweight decay\\n0.1\\nTable 12: Hyperparameters for pretraining the LM with 1.3B parameters per GPU. All hyperparameters are the\\nsame for DEMIX and DENSE training.\\nParameters\\n125M\\n350M\\n760M\\n1.3B\\nDENSE-\\nDAPT\\nT\\n+70.1%\\n+21.4%\\n+16.7%\\n+20.6%\\nN\\n–55.1%\\n–46.6%\\n–38.3%\\n-44.4%\\nTable 13: Average change in perplexity in training (T) and novel (N) domains after DENSE-DAPT. Negative values\\nindicate better performance relative to the original DENSE LM. While average perplexity in the novel domains\\ndecreases more for DENSE-DAPT, this comes at the cost of a signiﬁcant deterioration in performance in training\\ndomains.\\n21\\n', 'source_name': 'DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES', 'source_url': 'https://arxiv.org/abs/2108.05036'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "cBTM_NOTES.pdf #20\n",
      "{'content': 'Scaling Expert Language Models with Unsupervised Domain Discovery \\nMain Idea: this research work picks up where BTM left off, adding a caveat to the framework – \\ninstead of classifying domains based on provenance (source), this is done in an unsupervised \\nmanner, assigning domain data based on clusters. The new framework is named c-BTM (cluster \\nBranch-Train-Merge), and it still holds the embarrassingly parallel characteristic of the original \\nBTM. \\n \\nPros of Unsupervised vs Provenance-based Domain Classification \\n- \\nNot all datasets are able to be grouped based on provenance (like internet crawls). \\n- \\nGroups created by provenance cannot be easily merged or divided, so one ELM is needed \\nfor each group. This is not flexible in terms of adjusting the size and number of ELMs. \\n- \\nInstead of a domain posterior routing approach, which comes with many disadvantages, \\nrouting in c-BTM is done based on the distance of a document’s vector to a cluster’s \\ncenter, a simpler and more effective approach for routing. \\n \\nc-BTM vs MoE \\n- \\nc-BTM routes sequences instead of tokens. This allows for different specialization in \\ndomains/tasks instead of specializing in semantics/syntax because of the routing being \\ndone at a sentence/document level, not at a token level. \\n- \\nc-BTM uses offline balanced clustering (size of the clusters can be adjusted to achieve \\nload balancing) compared to online load balancing. \\n- \\nc-BTM has no shared parameters, which leads to savings in communication costs and \\nresults in a highly efficient framework for training domain experts. \\n- \\nc-BTM has more interpretable experts. \\nOBS: c-BTM is directly compared to sparse upcycling, which mirrors how c-BTM initializes experts \\nbut instead of training ELMs, sparse upcycling substitutes the feedforward networks in the dense \\ncheckpoints by MoE layers. \\n \\nc-BTM Algorithm \\n- \\nOBS: this paper only focuses on using 1 iteration/cycle for c-BTM, meaning training all \\nELMs from the seed ELM (no cycles trained based on existing specialized ELMs. \\n- \\nStep 0: Cluster \\no K-means clustering, with enforced balanced clusters (during training, not \\ninference), is used during training. \\no Tf-idf is used since it was the best performing embedding approach. \\n- \\nStep 1: Branch (from seed LM) \\no The seed LM is trained on a diverse corpus – experiments can be found at the BTM \\npaper. \\no Done the same way as in BTM. \\n- \\nStep 2: Train \\no Done the same way as in BTM. \\n- \\nStep 3: Merge \\no Done the same way as in BTM. \\n \\nInference \\n- \\nSparse ensemble of outputs of existing ELMs - router chooses top-k ELMs and a weighted \\naverage of those ELMs is used. \\n- \\nThe router is fixed at inference and does not need to be updated after training. \\n \\nExperimental Setup \\n- \\nOPT is used as the seed LM (the dense checkpoint). \\no Both the 1.3B and the 6.7B versions of OPT were experimented with. \\n- \\nK between 2 and 128 for k-means clustering was experimented with to evaluate the \\noptimal number of ELMs. \\n- \\nDropout of 0.1 is used for all layers except the embedding layer. \\n- \\nUsing only the second half of a document from the embedding-based routing is shown to \\nnot result in a performance drop while leading to faster inference. \\n- \\nModels are compared against each other in a compute basis. Using total training \\nparameters is said to be misleading for sparse models, so it is not used. \\no Total GPU-time is used to evaluate training efficiency. \\no GPU-time needed for inference and latency for end-users are used to evaluate \\ninference efficiency. \\n \\nResults \\n- \\nControlling for total training tokens: \\no Using a single cluster (dense) is always the worst setup. \\no Increasing cluster count in c-BTM improves language modeling performance for a \\nfixed compute budget (up to 16 clusters experimented with). \\no The advantage of c-BTM only increases with an increase in the number of total \\ntraining tokens. \\no There is a range of optimum number of training clusters and this increases with \\nan increase in total training tokens. \\n▪ It is cheaper to train on more clusters in parallel, so there could be a \\ntradeoff there. \\n▪ This is consistent to scaling up the size of each ELM. \\n- \\nComparing training time: \\no Due to c-BTM models with higher cluster counts using fewer GPUs per expert \\nunder a fixed budget and having no communication costs between experts, c-BTM \\nmodels with more clusters can be exposed to more data for the same amount of \\ntime as dense models. \\n▪ The more clusters, the faster the training updates. \\no Training on more clusters, due to the small number of GPUs per ELM and the fact \\nthat no communication is needed between ELMs, results in a much more robust \\ntraining setup to GPU failure. \\no Models trained with more clusters have faster updates as we increase the total \\ncompute. \\n▪ Opposite is true for MoE due to communication costs between experts. \\n- \\nControlling for inference costs via parameter count: \\no The largest training budget used was 168B tokens. \\no c-BTM with top-1 routing (same inference cost as dense) outperforms dense. \\n▪ The more clusters, the better the top-1 routing performance. \\no Top-2 to top-4 routing (of c-BTM) pretty much matches the performance of a full \\nc-BTM ensemble (better parameter efficiency than regular c-BTM, which was top-\\n2 to top-8). \\n▪ Top-2 to top-4 routing sometimes even perform better than a full \\nensemble. \\no These conclusions hold true even when scaling the ELM count to large values \\n(128). \\n- \\nComparing to a larger dense model: \\no 6.7B parameter dense model vs 1.3B parameter ELM c-BTM with 16 clusters and \\ntop-4 routing (5.2B inference cost) (1.3B latency cost – since the parameters of \\nthe ELMs can be run in embarrassingly parallel fashion). \\no c-BTM has a 3.5x speedup in training (using 168B training tokens). \\nDownstream Task Results (few-shot results) \\n- \\n6 classification tasks experimented with; 8-shot evaluations used. c-BTM trained on the \\nC4 dataset. \\n- \\n16-cluster c-BTM always outperforms its 1-cluster version (1.3B ELM size). \\nComparing to MoE (sparse upcycling aka MoE from a dense checkpoint) \\n- \\nSparse upcycling was shown to be unstable with a high number of experts (64 and 128). \\nWith 32 experts, it was shown to have stable training and perform better than the higher \\nexpert-count stable runs. \\no Based on this, the sparse upcycling model used here was a 32-expert MoE with \\ntop-2 routing. \\n- \\nMoE has more expensive training due to top-2 routing. \\no Shared parameters need to communicate with each other, resulting in slower \\ntraining. \\n- \\nSparse upcycling performs better at small compute budgets but performs much worse at \\nlarger budgets, even performing worse than dense models. \\no Authors speculate this could be due to distribution shifts after pretraining, which \\nmight increase the instability of sparse upcycling. \\n \\nFinal Analysis \\n- \\nClustering is important as random clustering underperforms it significantly. \\n- \\nThe load balancing constraint in k-means is shown to be useful. \\no This becomes more important with an increase in the number of clusters. \\n- \\nUsing the tf-idf approach, an analysis of important terms in clusters point to cluster \\nspecialization. Further analysis also shows that ELMs successfully specialize in their own \\ncluster. \\n- \\nIt is shown that metadata may not correspond to the most optimal segmentation of the \\ncorpus (although it is more interpretable). \\no Since c-BTM performs better than regular BTM, with the only significant change \\nbeing how the segmentation of data is done. \\n- \\nFuture research may investigate improving the technique to merge model weights (as this \\nis a hot area of research). \\n \\nMy takeaways: \\n- \\nRegarding the relatively low dropout used for the training of ELMs, I believe that these \\nare more robust to overfitting than traditional MoEs due to ELMs being full networks, and \\nthus having more parameters than a single expert FF.  \\no The fact that k-means has a constraint to ensure the loads are balanced between \\nELMs at training time might also help with overfitting. \\no On a similar note, ELMs seem to benefit from larger batch sizes. This is also a sign \\nthat ELMs would be more robust to overfitting since the opposite is true for MoEs. \\n▪ Larger batch sizes = more accurate updates (less noise) = less regularization \\neffect. \\n \\n', 'source_name': 'Scaling Expert Language Models with Unsupervised Domain Discovery NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/cBTM_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BeyondDistillation_Task_Level_MoE_NOTES.pdf #21\n",
      "{'content': 'Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference \\nMain idea: the goal of this work is to find an alternative method to distillation to store MoE \\nmodels. It experimented with token-level, task-level and sentence-level routing. MoE solves the \\nissue of training efficiency when compared to dense models (since only a subset of the network \\nis activated at a time) (tradeoff of a few more communication costs due to experts’ \\ncommunication and routing but less parameters needing to be updated per forward pass \\ncompared to a dense model of the same size in terms of total parameters) but still leaves room \\nfor improvement in inference efficiency due to the requirement of storing the model across many \\ndevices, adding to communication costs and idle resources for calling small batches (since in small \\nbatches, most machines will not be used since the respective expert is not needed). This paper’s \\nmain goal is to improve inference efficiency for sparse MoE models. Distillation is a possible \\nsolution but tends to lead to loss in quality. The task used for experiments was a multilingual \\nmachine translation task. \\n \\nApproach \\n- \\nTrained a routing strategy to leverage global task-level information to route all tokens \\ncorresponding to a particular task collectively to the same set of experts. \\no Decode different tasks separately and only load the subset of experts associated \\nwith the corresponding task during inference. \\n- \\nTask-level routing strategy showed gains over a dense model trained from scratch and a \\ndistilled model (student) trained from learning through a token-level MoE teacher model. \\n- \\nComparable quality to token-MoE model (not distilled) while achieving significant \\ninference gains (1.9x peak throughput and 6.3% of the decoder size). \\n- \\nTop-2 routing mechanism used. \\n \\nRouting Strategies Experimented With \\n- \\nToken-level. \\no Traditional MoE where each token is routed independently. \\n- \\nSentence-level. \\no Route tokens by sentence, determined by the expert with the highest average \\ntoken weight in the sentence. \\no First thought is that this won’t work well due to the average token weight per \\nexpert is used (this is proven to be correct by experiments done later in the paper). \\nA better sentence-level approach could be to use sentence embeddings, which \\nwould also only need to call the router once per sentence. \\n- \\nTask-level. \\no Route tokens based on a task. In the multilingual translation task, this can be \\ndetermined by either the target language or the language pair. \\n \\nInference Implications \\n- \\nThe token-level and sentence-level approach makes inference costly. To help with the \\nchallenge of needing to have all experts ready and loaded to the server at inference, these \\napproaches can have experts be dynamically loaded based on the routing decision or \\nmodel parallelism can be employed (the server often needs to load all experts). Both incur \\nhigh communication costs. \\no This needs to be done for every then, hence the high cost. \\n- \\nTask-level routing only need to pre-load the top-k experts for the given input sequence. \\nThis is done by determining which task most resembles the input sequence and using the \\ntop-k experts for that task only for all tokens. \\no Loading experts only needs to be done once for each input sequence. \\n \\nResults \\n- \\nSentence-level MoE did not perform well. \\n- \\nThe best encoder-decoder model used had a token-MoE in the encoder and a task-MoE \\nin the decoder. \\n- \\nThe best decoder-only model was the task-MoE decoder. \\n- \\nStatically determining the task through a deterministic approach did not work very well \\n(experts are deterministically allocated to tasks). \\n- \\nTask-level MoE has higher throughput (tokens/sec), uses less decoder parameters and \\nhas less communication overhead (or none) compared to token-level MoE. \\n- \\nTask-level MoE performs better than models distilled from token-level MoE. \\nAdditionally, analysis of the routing decisions shows that at a task level, the experts called in the \\nencoder do not change much, but experts in the decoder seem to naturally specialize in tasks, \\ngiving a possible explanation why the decoder-only task MoE performed well. \\n \\nMy takeaways: \\n- \\nIn MoE, there is a tradeoff in training costs compared to dense models. MoE provides less \\ncommunication costs overall: \\no There is a partial increase in communication costs due to the communication that \\nneeds to be done between activated experts and between these activated experts \\nand the router. \\no Overall, however, MoE is more efficient at training due to only a subset of \\nparameters needing to be updated per forward pass (on the MoE layers, where \\nthe bulk of parameters are located). This allows MoE to scale the total number of \\nparameters in an easier way. \\n- \\nThe inspiration for the approach used comes from trying to decrease the cost of storing \\nexperts during inference.  \\no This is a necessary step as all experts need to be ready to be called during \\ninference, which leads to idle resources (no experts being used for some batches \\nbut needing to be stored and ready). \\no Distillation is (was) the most common approach for this, but distilling experts \\ntends to lead to significant loss in quality. \\n▪ Distillation consists of training a small dense model (student) from a large \\nMoE expert (teacher). \\n- \\nThe gains obtained from inference efficiency do not come from calling less parameters at \\ninference (number of active parameters), but from the number of experts being loaded \\n(number of total parameters available). \\no The idea seems to be to predict the most relevant experts that will be needed on \\na task level, so only those need to be loaded and ready during inference. \\no The meat of this approach is to correctly predict the experts needed. If this \\nprediction is correct, the model will have good quality, otherwise it won’t. \\n▪ The approach seemed to work since the quality of the resulting model was \\ncomparable to token-MoE. \\no This ends up reducing the latency costs since the experts used only need to be \\nloaded once per input sequence, and not for every token. \\n- \\nThe task-level approach seems to be useful in some scenarios but not possible in others. \\nFor example, if an out-of-domain task is shown at testing that is different than the training \\ntasks, my intuition tells me that the router won’t be able to select the most relevant \\nexperts very well (and the experts won’t be prepared for this situation), thus leading to \\nthe model underperforming a token-level approach, which I believe would be more \\nrobust to these situations. \\no This approach sounds interesting in a scenario where there are predefined tasks \\nthat we want the model to perform well on, and it does not necessarily need to \\nperform so well on out-of-domain tasks. \\no This should be considered when choosing between the task-level MoE and a \\ndistilled student model (the student model, in theory, would perform better in \\nterms of generalization – not as good in a few tasks, but good in everything -, while \\ntask-level MoE would probably perform better in specific tasks scenarios – \\nespecially good at a few tasks (depends on training)). \\n \\n', 'source_name': 'Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BeyondDistillation_Task_Level_MoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Expert_Gate.pdf #22\n",
      "{'content': 'Expert Gate: Lifelong Learning with a Network of Experts\\nRahaf Aljundi\\nPunarjay Chakravarty\\n{rahaf.aljundi, Punarjay.Chakravarty, Tinne.Tuytelaars}@esat.kuleuven.be\\nKU Leuven, ESAT-PSI, IMEC, Belgium\\nTinne Tuytelaars\\nAbstract\\nIn this paper we introduce a model of lifelong learning,\\nbased on a Network of Experts. New tasks / experts are\\nlearned and added to the model sequentially, building on\\nwhat was learned before. To ensure scalability of this pro-\\ncess, data from previous tasks cannot be stored and hence is\\nnot available when learning a new task. A critical issue in\\nsuch context, not addressed in the literature so far, relates\\nto the decision which expert to deploy at test time. We in-\\ntroduce a set of gating autoencoders that learn a represen-\\ntation for the task at hand, and, at test time, automatically\\nforward the test sample to the relevant expert. This also\\nbrings memory efﬁciency as only one expert network has to\\nbe loaded into memory at any given time. Further, the au-\\ntoencoders inherently capture the relatedness of one task to\\nanother, based on which the most relevant prior model to be\\nused for training a new expert, with ﬁne-tuning or learning-\\nwithout-forgetting, can be selected. We evaluate our method\\non image classiﬁcation and video prediction problems.\\n1. Introduction\\nIn the age of deep learning and big data, we face a sit-\\nuation where we train ever more complicated models with\\never increasing amounts of data. We have different mod-\\nels for different tasks trained on different datasets, each of\\nwhich is an expert on its own domain, but not on others. In\\na typical setting, each new task comes with its own dataset.\\nLearning a new task, say scene classiﬁcation based on a pre-\\nexisting object recognition network trained on ImageNet,\\nrequires adapting the model to the new set of classes and\\nﬁne-tuning it with the new data. The newly trained network\\nperforms well on the new task, but has a degraded perfor-\\nmance on the old ones. This is called catastrophic forget-\\nting [12], and is a major problem facing life long learning\\ntechniques [32, 33, 31], where new tasks and datasets are\\nadded in a sequential manner.\\nIdeally, a system should be able to operate on different\\ntasks and domains and give the best performance on each\\nof them. For example, an image classiﬁcation system that\\nFeatures Extraction\\nInput\\nT1\\nT2\\nTk\\n...\\nSoft max\\nExpert k\\nExpert 2\\nExpert 1\\nThe Gate\\n...\\nFigure 1. The architecture of our Expert Gate system.\\nis able to operate on generic as well as ﬁne-grained classes,\\nand in addition performs action and scene classiﬁcation. If\\nall previous training data were available, a direct solution\\nwould be to jointly train a model on all the different tasks\\nor domains. Each time a new task arrives along with its\\nown training data, new layers/neurons are added, if needed,\\nand the model is retrained on all the tasks. Such a solu-\\ntion has three main drawbacks. The ﬁrst is the risk of the\\nnegative inductive bias when the tasks are not related or\\nsimply adversarial. Second, a shared model might fail to\\ncapture specialist information for particular tasks as joint\\ntraining will encourage a hidden representation beneﬁcial\\nfor all tasks. Third, each time a new task is to be learned,\\nthe whole network needs to be re-trained. Apart from the\\nabove drawbacks, the biggest constraint with joint training\\nis that of keeping all the data from the previous tasks. This\\nis a difﬁcult requirement to be met, especially in the era of\\nbig data. For example, ILSVRC [30] has 1000 classes, with\\nover a million images, amounting to 200 GB of data. Yet\\nthe AlexNet model trained on the same dataset, is only 200\\nMB, a difference in size of three orders of magnitude. With\\nincreasing amounts of data collected, it becomes less and\\nless feasible to store all the training data, and more practi-\\ncal to just store the models learned from the data.\\nWithout storing the data, one can consider strategies like\\nusing the previous model to generate virtual samples (i.e.\\n1\\n3366\\nuse the soft outputs of the old model on new task data to\\ngenerate virtual labels) and use them in the retraining phase\\n[5, 21, 32]. This works to some extent, but is unlikely to\\nscale as repeating this scheme a number of times causes a\\nbias towards the new tasks and an exponential buildup of\\nerrors on the older ones, as we show in our experiments.\\nMoreover, it suffers from the same drawbacks as the joint\\ntraining described above. Instead of having a network that\\nis jack of all trades and master of none, we stress the need\\nfor having different specialist or expert models for different\\ntasks, as also advocated in [13, 16, 31]. Therefore we build\\na Network of Experts, where a new expert model is added\\nwhenever a new task arrives and knowledge is transferred\\nfrom previous models.\\nWith an increasing number of task specializations, the\\nnumber of expert models increases. Modern GPUs, used\\nto speed up training and testing of neural nets, have limited\\nmemory (compared to CPUs), and can only load a relatively\\nsmall number of models at a time. We obviate the need for\\nloading all the models by learning a gating mechanism that\\nuses the test sample to decide which expert to activate ( see\\nFigure 1 ). For this reason, we call our method Expert Gate.\\nUnlike [17], who train one Uber network for performing\\nvision tasks as diverse as semantic segmentation, object de-\\ntection and human body part detection, our work focuses on\\ntasks with a similar objective. For example, imagine a drone\\ntrained to ﬂy through an environment using its frontal cam-\\nera. For optimal performance, it needs to deploy different\\nmodels for different environments such as indoor, outdoor\\nor forest. Our gating mechanism then selects a model on the\\nﬂy based on the input video. Another application could be\\na visual question answering system, that has multiple mod-\\nels trained using images from different domains. Here too,\\nour gating mechanism could use the data itself to select the\\nassociated task model.\\nEven if we could deploy all the models simultaneously,\\nselecting the right expert model is not straightforward. Just\\nusing the output of the highest scoring expert is no guaran-\\ntee for success as neural networks can erroneously give high\\nconﬁdence scores, as shown in [27]. We also demonstrate\\nthis in our experiments. Training a discriminative classiﬁer\\nto distinguish between tasks is also not an option since that\\nwould again require storing all training data. What we need\\nis a task recognizer that can tell the relevance of its associ-\\nated task model for a given test sample. This is exactly what\\nour gating mechanism provides. In fact, also the prefrontal\\ncortex of the primate brain is considered to have neural rep-\\nresentations of task context that act as a gating in different\\nbrain functions [23].\\nWe propose to implement such task recognizer using an\\nundercomplete autoencoder as a gating mechanism.\\nWe\\nlearn for each new task or domain, a gating function that\\ncaptures the shared characteristics among the training sam-\\nples and can recognize similar samples at test time. We\\ndo so using a one layer under-complete autoencoder. Each\\nautoencoder is trained along with the corresponding ex-\\npert model and maps the training data to its own lower di-\\nmensional subspace. At test time, each task autoencoder\\nprojects the sample to its learned subspace and measures the\\nreconstruction error due to the projection. The autoencoder\\nwith the lowest reconstruction error is used like a switch,\\nselecting the corresponding expert model (see Figure 1).\\nInterestingly, such autoencoders can also be used to eval-\\nuate task relatedness at training time, which in turn can be\\nused to determine which prior model is more relevant to a\\nnew task. We show how, based on this information, Expert\\nGate can decide which specialist model to transfer knowl-\\nedge from when learning a new task and whether to use\\nﬁne-tuning or learning-without-forgetting [21].\\nTo summarize, our contributions are the following. We\\ndevelop Expert Gate, a lifelong learning system that can se-\\nquentially deal with new tasks without storing all previous\\ndata. It automatically selects the most related prior task to\\naid learning of the new task. At test time, the appropriate\\nmodel is loaded automatically to deal with the task at hand.\\nWe evaluate our gating network on image classiﬁcation and\\nvideo prediction problems.\\nThe rest of the paper is organized as follows. We dis-\\ncuss related work in Section 2. Expert Gate is detailed in\\nSection 3, followed by experiments in Section 4. We ﬁnish\\nwith concluding remarks and future work in Section 5.\\n2. Related Work\\nMulti-task learning Our end goal is to develop a system\\nthat can reach expert level performance on multiple tasks,\\nwith tasks learned sequentially. As such, it lies at the inter-\\nsection between multi-task learning and lifelong learning.\\nStandard multi-task learning [5] aims at learning multiple\\ntasks in a joint manner. The objective is to use knowledge\\nfrom different tasks, the so called inductive bias [25], in or-\\nder to improve performance on individual tasks. Often one\\nshared model is used for all tasks. This has the beniﬁt of\\nrelaxing the number of required samples per task but could\\nlead to suboptimal performance on the individual tasks. On\\nthe other hand, multiple models can be learned, that are each\\noptimal for their own task, but utilize inductive bias / knowl-\\nedge from other models [5].\\nTo determine which related tasks to utilize, [35] cluster\\nthe tasks based on the mutual information gain when using\\nthe information from one task while learning another. This\\nis an exhaustive process. As an alternative, [15, 38, 19] as-\\nsume that the parameters of related task models lie close\\nby in the original space or in a lower dimensional subspace\\nand thus cluster the tasks’ parameters. They ﬁrst learn task\\nmodels independently, then use the tasks within the same\\ncluster to help improving or relearning their models. This\\n3367\\nrequires learning individual task models ﬁrst. Alternatively,\\nwe use our tasks autoencoders, that are fast to train, to iden-\\ntify related tasks.\\nMultiple models for multiple tasks One of the ﬁrst ex-\\namples of using multiple models, each one handling a sub-\\nset of tasks, was by Jacobs et al. [16]. They trained an adap-\\ntive mixture of experts (each a neural network) for multi-\\nspeaker vowel recognition and used a separate gating net-\\nwork to determine which network to use for each sample.\\nThey showed that this setup outperformed a single shared\\nmodel. A downside, however, was that each training sam-\\nple needed to pass through each expert, for the gating func-\\ntion to be learned. To avoid this issue, a mixture of one\\ngeneralist model and many specialist models has been pro-\\nposed [1, 13]. At test time, the generalist model acts as a\\ngate, forwarding the sample to the correct network. How-\\never, unlike our model, these approaches require all the\\ndata to be available for learning the generalist model, which\\nneeds to be retrained each time a new task arrives.\\nLifelong learning without catastrophic forgetting In\\nsequential lifelong learning, knowledge from previous tasks\\nis leveraged to improve the training of new tasks, while tak-\\ning care not to forget old tasks, i.e. preventing catastrophic\\nforgetting [12]. Our system obviates the need for storing all\\nthe training data collected during the lifetime of an agent, by\\nlearning task autoencoders that learn the distribution of the\\ntask data, and hence, also capture the meta-knowledge of\\nthe task. This is one of the desired characteristics of a life-\\nlong learning system, as outlined by Silver et al. [33]. The\\nconstraint of not storing all previous training data has been\\nlooked at previously by Silver and Mercer [32]. They use\\nthe output of the previous task networks given new training\\ndata, called virtual samples, to regularize the training of the\\nnetworks for new tasks. This improves the new task perfor-\\nmance by using the knowledge of the previous tasks. More\\nrecently, the Learning without Forgetting framework of [21]\\nuses a similar regularization strategy, but learns a single net-\\nwork for all tasks: they ﬁnetune a previously trained net-\\nwork (with additional task outputs) for new tasks. The con-\\ntribution of previous tasks/networks in the training of new\\nnetworks is determined by task relatedness metrics in [32],\\nwhile in [21], all previous knowledge is used, regardless of\\ntask relatedness. [21] demonstrates sequential training of a\\nnetwork for only two tasks. In our experiments, we show\\nthat the shared model gets worse when extended to more\\nthan two tasks, especially when task relatedness is low.\\nLike us, two recent architectures, namely the progressive\\nnetwork [31] and the modular block network [34], also use\\nmultiple networks, a new one for each new task. They add\\nnew networks as additional columns with lateral connec-\\ntions to the previous nets. These lateral connections mean\\nthat each layer in the new network is connected to not only\\nits previous layer in the same column, but also to previous\\nlayers from all previous columns. This allows the networks\\nto transfer knowledge from older to newer tasks. However,\\nin these works, choosing which column to use for a particu-\\nlar task at test time is done manually, and the authors leave\\nits automation as future work. Here, we propose to use an\\nautoencoder to determine which model, and consequently\\ncolumn, is to be selected for a particular test sample.\\n3. Our Method\\nWe consider the case of lifelong learning or sequential\\nlearning where tasks and their corresponding data come one\\nafter another. For each task, we learn a specialized model\\n(expert) by transferring knowledge from previous tasks –\\nin particular, we build on the most related previous task.\\nSimultaneously we learn a gating function that captures the\\ncharacteristics of each task. This gate forwards the test data\\nto the corresponding expert resulting in a high performance\\nover all learned tasks.\\nThe question then is: how to learn such a gate function\\nto differentiate between tasks, without having access to the\\ntraining data of previous tasks? To this end, we learn a low\\ndimensional subspace for each task/domain. At test time\\nwe then select the representation (subspace) that best ﬁts\\nthe test sample. We do that using an undercomplete autoen-\\ncoder per task. Below, we ﬁrst describe this autoencoder in\\nmore detail (Section 3.1). Next, we explain how to use them\\nfor selecting the most relevant expert (Section 3.2) and for\\nestimating task relatedness (Section 3.3).\\n3.1. The Autoencoder Gate\\nAn autoencoder [4] is a neural network that learns to\\nproduce an output similar to its input [11]. The network is\\ncomposed of two parts, an encoder f = h(x), which maps\\nthe input x to a code h(x) and a decoder r = g(h(x)),\\nthat maps the code to a reconstruction of the input. The\\nloss function L(x, g(h(x))) is simply the reconstruction er-\\nror. The encoder learns, through a hidden layer, a lower\\ndimensional representation (undercomplete autoencoder) or\\na higher dimensional representation (overcomplete autoen-\\ncoder) of the input data, guided by regularization criteria to\\nprevent the autoencoder from copying its input. A linear\\nautoencoder with a Euclidean loss function learns the same\\nsubspace as PCA. However, autoencoders with non-linear\\nfunctions yield better dimensionality reduction compared to\\nPCA [14]. This motivates our choice for this model.\\nAutoencoders are usually used to learn feature represen-\\ntations in an unsupervised manner or for dimensionality re-\\nduction. Here, we use them for a different goal. The lower\\ndimensional subspace learned by one of our undercomplete\\nautoencoders will be maximally sensitive to variations ob-\\nserved in the task data but insensitive to changes orthogonal\\nto the manifold. In other words, it represents only the vari-\\nations that are needed to reconstruct relevant samples. Our\\n3368\\nStandardization\\nSigmoid\\nRelu\\nSigmoid\\nPreprocessing\\n Step\\nDecoding\\nEncoding\\nLoss:cross-entropy \\nFigure 2. Our autoencoder gate structure.\\nmain hypothesis is that the autoencoder of one domain/task\\nshould thus be better at reconstructing the data of that task\\nthan the other autoencoders. Comparing the reconstruction\\nerrors of the different tasks’ autoencoders then allows to\\nsuccessfully forward a test sample to the most relevant ex-\\npert network. It has been stated by [2] that in regularized\\n(over-complete) autoencoders, the opposing forces between\\nthe risk and the regularization term result in a score like be-\\nhavior for the reconstruction error. As a result, a zero recon-\\nstruction loss means a zero derivative which could be a local\\nminimum or a local maximum. However, we use an unregu-\\nlarized one-layer under-complete autoencoder and for these,\\nit has been shown [3, 24] that the mean squared error cri-\\nterion we use as reconstruction loss estimates the negative\\nlog-likelihood. There is no need in such a one-layer autoen-\\ncoder to add a regularization term to pull up the energy on\\nunseen data because the narrowness of the code already acts\\nas an implicit regularizer.\\nPreprocessing We start from a robust image represen-\\ntation x, namely the activations of the last convolutional\\nlayer of AlexNet pretrained on ImageNet. Before the en-\\ncoding layer, we pass this input through a preprocessing\\nstep, where the input data is standardized, followed by a\\nsigmoid function. The standardization of the data, i.e. sub-\\ntracting the mean and dividing the result by the standard\\ndeviation, is essential as it increases the robustness of the\\nhidden representation to input variations. Normally, stan-\\ndardization is done using the statistics of the data that a net-\\nwork is trained on, but in this case, this is not a good strat-\\negy. This is because, at test time, we compare the relative\\nreconstruction errors of the different autoencoders. Differ-\\nent standardization regimes lead to non-comparable recon-\\nstruction errors. Instead, we use the statistics of Imagenet\\nfor the standardization of each autoencoder. Since this is a\\nlarge dataset it gives a good approximation of the distribu-\\ntion of natural images. After standardization, we apply the\\nsigmoid function to map the input to a range of [0 1].\\nNetwork architecture We design a simple autoencoder\\nthat is no more complex than one layer in a deep model,\\nwith a one layer encoder/decoder (see Figure 2). The en-\\ncoding step consists of one fully connected layer followed\\nby ReLU [39]. We make use of ReLU activation units as\\nthey are fast and easy to optimize. ReLU also introduces\\nsparsity in the hidden units which leads to better generaliza-\\ntion. For decoding, we use again one fully connected layer,\\nbut now followed by a sigmoid. The sigmoid yields values\\nbetween [0 1], which allows us to use cross entropy as the\\nloss function. At test time, we use the Euclidean distance to\\ncompute the reconstruction error.\\n3.2. Selecting the most relevant expert\\nAt test time, and after learning the autoencoders for the\\ndifferent tasks, we add a softmax layer that takes as input\\nthe reconstruction errors eri from the different tasks autoen-\\ncoders given a test sample x. The reconstruction error eri of\\nthe i-th autoencoder is the output of the loss function given\\nthe input sample x. The softmax layer gives a probability\\npi for each task autoencoder indicating its conﬁdence:\\npi =\\nexp(−eri/t)\\nP\\nj exp(−erj/t)\\n(1)\\nwhere t is the temperature. We use a temperature value of\\n2 as in [13, 21] leading to soft probability values. Given\\nthese conﬁdence values, we load the expert model associ-\\nated with the most conﬁdent autoencoder. For tasks that\\nhave some overlap, it may be convenient to activate more\\nthan one expert model instead of taking the max score only.\\nThis can be done by setting a threshold on the conﬁdence\\nvalues, see section 4.2.\\n3.3. Measuring task relatedness\\nGiven a new task Tk associated with its data Dk, we ﬁrst\\nlearn an autoencoder for this task Ak. Let Ta be a previous\\ntask with associated autoencoder Aa. We want to measure\\nthe task relatedness between task Tk and task Ta. Since\\nwe do not have access to the data of task Ta, we use the\\nvalidation data from the current task Tk. We compute the\\naverage reconstruction error Erk on the current task data\\nmade by the current task autoencoder Ak and, likewise, the\\naverage reconstruction error Era made by the previous task\\nautoencoder Aa on the current task data. The relatedness\\nbetween the two tasks is then computed:\\nRel(Tk, Ta) = 1 −(Era −Erk\\nErk\\n)\\n(2)\\nNote that the relatedness value is not symmetric. Applying\\nthis to every previous task, we get a relatedness value to\\neach previous task.\\nWe exploit task relatedness in two ways. First, we use it\\nto select the most related task to be used as prior model for\\nlearning the new task. Second, we exploit the level of task\\nrelatedness to determine which transfer method to use: ﬁne-\\ntuning or learning-without-forgetting (LwF) [21]. We found\\nin our experiments that LwF only outperforms ﬁne-tuning\\n3369\\nAlgorithm 1 Expert Gate\\nTraining Phase input:\\nexpert-models (E1, ., Ej),\\ntasks-autoencoders (A1, ., Aj), new task (Tk), data\\n(Dk) ; output: Ek\\n1: Ak =train-task-autoencoder (Dk)\\n2: (rel,rel-val)=select-most-related-task(Dk,Ak,{A})\\n3: if rel-val >rel-th then\\n4:\\nEk=LwF(Erel, Dk)\\n5: else\\n6:\\nEk=ﬁne-tune(Erel, Dk)\\n7: end if\\nTest Phase input: x ; output: prediction\\n8: i=select-expert({A}, x)\\n9: prediction = activate-expert(Ei, x)\\nwhen the two tasks are sufﬁciently related. When this is not\\nthe case, enforcing the new model to give similar outputs for\\nthe old task may actually hurt performance. Fine-tuning,\\non the other hand, only uses the previous task parameters\\nas a starting point and is less sensitive to the level of task\\nrelatedness. Therefore, we apply a threshold on the task\\nrelatedness value to decide when to use LwF and when to\\nﬁne-tune. Algorithm 1 shows the main steps of our Expert\\nGate in both training and test phase.\\n4. Experiments\\nFirst, we compare our method against various baselines\\non a set of three image classiﬁcation tasks (Section 4.1).\\nNext, we analyze our gate behavior in more detail on a big-\\nger set of tasks (Section 4.2), followed by an analysis of\\nour task relatedness measure (Section 4.3). Finally, we test\\nExpert Gate on a video prediction problem (Section 4.4).\\nImplementation details We use the activations of the last\\nconvolutional layer of an AlexNet pre-trained with Ima-\\ngeNet as image representation for our autoencoders. We\\nexperimented with the size of the hidden layer in the au-\\ntoencoder, trying sizes of 10, 50, 100, 200 and 500, and\\nfound an optimal value of 100 neurons. This is a good com-\\npromise between complexity and performance. If the task\\nrelatedness is higher than 0.85, we use LwF; otherwise, we\\nuse ﬁne-tuning. We use the MatConvNet framework [36]\\nfor all our experiments.\\n4.1. Comparison with baselines\\nWe start with the sequential learning of three image clas-\\nsiﬁcation tasks: in order, we train on MIT Scenes [29] for\\nscene classiﬁcation, Caltech-UCSD\\nBirds [37] for ﬁne-\\ngrained bird classiﬁcation and Oxford Flowers [28] for ﬁne-\\ngrained ﬂower classiﬁcation.\\nTo simulate a scenario in\\nwhich an agent or robot has some prior knowledge, and is\\nthen exposed to datasets in a sequential manner, we start off\\nTable 1. Classiﬁcation accuracy for the sequential learning of 3\\nimage classiﬁcation tasks. Methods with * assume all previous\\ntraining data is still available, while methods with ** use an oracle\\ngate to select the proper model at test time.\\nMethod\\nScenes\\nBirds\\nFlowers\\navg\\nJoint Training*\\n63.1\\n58.5\\n85.3\\n68.9\\nMultiple ﬁne-tuned models**\\n63.4\\n56.8\\n85.4\\n68.5\\nMultiple LwF models**\\n63.9\\n58.0\\n84.4\\n68.7\\nSingle ﬁne-tuned model\\n63.4\\n-\\n-\\n-\\n50.3\\n57.3\\n-\\n-\\n46.0\\n43.9\\n84.9\\n58.2\\nSingle LwF model\\n63.9\\n-\\n-\\n-\\n61.8\\n53.9\\n-\\n-\\n61.2\\n53.5\\n83.8\\n66.1\\nExpert Gate (ours)\\n63.5\\n57.6\\n84.8\\n68.6\\nwith an AlexNet model pre-trained on ImageNet. We com-\\npare against the following baselines:\\n1. A single jointly-trained model: Assuming all training\\ndata is always available, this model is jointly trained (by\\nﬁnetuning an AlexNet model pretrained on ImageNet) for\\nall three tasks together.\\n2. Multiple ﬁne-tuned models: Distinct AlexNet models\\n(pretrained on ImageNet) are ﬁnetuned separately, one for\\neach task. At test time, an oracle gate is used, i.e. a test\\nsample is always evaluated by the correct model.\\n3. Multiple LwF models: Distinct models are learned with\\nlearning-without-forgetting [21], one model per new task,\\nalways using AlexNet pre-trained on ImageNet as previous\\ntask. This is again combined with an oracle gate.\\n4.\\nA single ﬁne-tuned model: one AlexNet model (pre-\\ntrained on ImageNet) sequentially ﬁne-tuned on each task.\\n5. A single LwF model: LwF sequentially applied to mul-\\ntiple tasks. Each new task is learned with all the outputs\\nof the previous network as soft targets for the new train-\\ning samples. So, a network (pre-trained on ImageNet) is\\nﬁrst trained for Task 1 data without forgetting ImageNet\\n(i.e. using the pretrained AlexNet predictions as soft tar-\\ngets). Then, this network is trained with Task 2 data, now\\nusing ImageNet and Task 1 speciﬁc layers outputs as soft\\ntargets; and so on.\\nFor baselines with multiple models (2 and 3), we rely on\\nan oracle gate to select the right model at test time. So re-\\nported numbers for these are upper bounds of what can be\\nachieved in practice. The same holds for baseline 1, as it\\nassumes all previous training data is stored and available.\\nTable 1 shows the classiﬁcation accuracy achieved on the\\ntest sets of the different tasks. For our Expert Gate system\\nand for each new task, we ﬁrst select the most related previ-\\nous task (including ImageNet) and then learn the new task\\nexpert model by transferring knowledge from the most re-\\nlated task model, using LwF or ﬁnetuning.\\nFor the Single ﬁne-tuned model and Single LwF\\nmodel, we also report intermediate results in the sequen-\\ntial learning.\\nWhen learning multiple models (one per\\nnew task), LwF improves over vanilla ﬁne-tuning for\\n3370\\nScenes and Birds, as also reported by [21]1.\\nHow-\\never, for Flowers, performance degrades compared to\\nﬁne-tuning.\\nWe measure a lower degree of task re-\\nlatedness to ImageNet for Flowers than for Birds or\\nScenes (see Figure 3) which might explain this effect.\\nI\\nS\\nF\\nB\\nF\\nB\\nS\\nI\\n0.92\\n0.87\\n0.83\\n0.81\\n0.82\\n0.79\\nFigure 3. Task relatedness.\\nFirst letters indicate tasks.\\nComparing\\nthe\\nSingle\\nﬁne-tuned\\nmodel\\n(learned\\nsequentially) with the Mul-\\ntiple\\nﬁne-tuned\\nmodels,\\nwe observe an increasing\\ndrop\\nin\\nperformance\\non\\nolder\\ntasks:\\nsequentially\\nﬁne-tuning a single model\\nfor new tasks shows catas-\\ntrophic forgetting and is not\\na good strategy for lifelong learning. The Single LwF model\\nis less sensitive to forgetting on previous tasks. However,\\nit is still inferior to training exclusive models for those\\ntasks (Multiple ﬁne-tuned / LwF models), both for older\\nas well as newer tasks. Lower performance on previous\\ntasks is because of a buildup of errors and degradation\\nof the soft targets of the older tasks. This results in LwF\\nfailing to compensate for forgetting in a sequence involving\\nmore than 2 tasks.\\nThis also adds noise in the learning\\nprocess of the new task. Further, the previous tasks have\\nvarying degree of task relatedness. On these datasets, we\\nsystematically observed the largest task relatedness values\\nfor ImageNet (see Figure 3). Treating all the tasks equally\\nprevents the new task from getting the same beneﬁt of\\nImageNet as in the Multiple LwF models setting.\\nOur\\nExpert Gate always correctly identiﬁes the most related\\ntask, i.e. ImageNet. Based on the relatedness degree, it\\nused LwF for Birds and Scenes, while ﬁne-tuning was\\nused for Flowers. As a result, the best expert models were\\nlearned for each task. At test time, our gate mechanism\\nsucceeds to select the correct model for 99.2% of the test\\nsamples. This leads to superior results to those achieved\\nby the other two sequential learning strategies (Single\\nﬁne-tuned model and Single LwF model).\\nWe achieve\\ncomparable performance on average to the Joint Training\\nthat has access to all the tasks data. Also, performance is\\non par with Multiple ﬁne-tuned models or Multiple LwF\\nmodels that both assume having the task label for activating\\nthe associated model.\\n4.2. Gate Analysis\\nThe goal of this experiment is to further evaluate our Ex-\\npert Gate’s ability in successfully selecting the relevant net-\\nwork(s) for a given test image. For this experiment, we add\\n3 more tasks: Stanford Cars dataset [18] for ﬁne-grained car\\n1Note these numbers are not identical to [21] but show similar trends.\\nAt the time of experimentation, the code for LwF was not available, so we\\nimplemented this ourselves in consultation with the authors of [21], and\\nused parameters provided by them.\\nclassiﬁcation, FGVC-Aircraft dataset [22] for ﬁne-grained\\nclassiﬁcation of aircraft, and VOC Actions, the human ac-\\ntion classiﬁcation subset of VOC challenge 2012 [9]. This\\nlast dataset has multi-label annotations. For sake of consis-\\ntency, we only use the actions with single label. For these\\nnewly added datasets, we use the bounding boxes instead of\\nthe full images as the images might contain more than one\\nobject. So in total we deal with 6 different tasks: Scenes,\\nBirds, Flowers, Cars, Aircrafts, and Actions, along with Im-\\nageNet that is considered as a generalist model or initial\\npre-existing model.\\nWe compare again with Joint Training, where we ﬁne-\\ntune the ImageNet pre-trained AlexNet jointly on the six\\ntasks assuming all the data is available. We also compare\\nwith a setting with multiple ﬁne-tuned models where the\\nmodel with the maximum score is selected (Most conﬁdent\\nmodel). For our Expert Gate, we follow the same regime as\\nin the previous experiment. The most related task is always\\nImageNet. Based on our task relatedness threshold, LwF\\nwas selected for Actions, while Aircrafts and Cars were\\nﬁne-tuned. Table 2 shows the results.\\nEven though the jointly trained model has been trained\\non all the previous tasks data simultaneously, its average\\nperformance is inferior to our Expert Gate system. This\\ncan be explained by the negative inductive bias where some\\ntasks negatively affect others, as is the case for Scenes and\\nCars.\\nAs we explained in the Introduction, deploying all mod-\\nels and taking the max score (Most conﬁdent model) is not\\nan option: for many test samples the most conﬁdent model\\nis not the correct one, resulting in poor performance. Ad-\\nditionally, with the size of each expert model around 220\\nMB and the size of each autoencoder around 28 MB, there\\nis almost an order of magnitude difference in memory re-\\nquirements.\\nComparison with a discriminative classiﬁer Finally,\\nwe compare with a discriminative classiﬁer trained to pre-\\ndict the task. For this classiﬁer, we ﬁrst assume that all data\\nfrom the previous tasks are stored, even though this is not in\\nline with a lifelong learning setup. Thus, it serves as an up-\\nper bound. For this classiﬁer (Discriminative Task Classi-\\nﬁer) we use a neural net with one hidden layer composed of\\n100 neurons (same as our autoencoder code size). It takes as\\ninput the same data representation as our autoencoder gate\\nand its output is the different tasks labels. Table 3 compares\\nthe performance of our gate on recognizing each task data to\\nthat of the discriminative classiﬁer. Further, we test the sce-\\nnario of a discriminative classiﬁer with the number of stored\\nsamples per task varying from 10-2000 (Figure 5). It ap-\\nproaches the accuracy of our gate with 2000 samples. Note\\nthat this is 1\\n2 to 1\\n3 of the size of the used datasets. For larger\\ndatasets, an even higher number of samples would proba-\\nbly be needed to match performance. In spite of not having\\n3371\\nTable 2. Classiﬁcation accuracy for the sequential learning of 6 tasks. Method with * assumes all the training data is available.\\nMethod\\nScenes\\nBirds\\nFlowers\\nCars\\nAircrafts\\nActions\\navg\\nJoint Training*\\n59.5\\n56.0\\n85.2\\n77.4\\n73.4\\n47.6\\n66.5\\nMost conﬁdent model\\n40.4\\n43.0\\n69.2\\n78.2\\n54.2\\n8.2\\n48.7\\nExpert Gate\\n60.4\\n57.0\\n84.4\\n80.3\\n72.2\\n49.5\\n67.3\\nTable 3. Results on discriminating between the 6 tasks (classiﬁcation accuracy)\\nMethod\\nScenes\\nBirds\\nFlowers\\nCars\\nAircrafts\\nActions\\navg\\nDiscriminative Task Classiﬁer - using all the tasks data\\n97.0\\n98.6\\n97.9\\n99.3\\n98.8\\n95.5\\n97.8\\nExpert Gate (ours) - no access to the previous tasks data\\n94.6\\n97.9\\n98.6\\n99.3\\n97.6\\n98.1\\n97.6\\naccess to any of the previous tasks data, our Expert Gate\\nachieves similar performance to the discriminative classi-\\nﬁer. In fact, our Expert Gate can be seen as a sequential\\nclassiﬁer with new classes arriving one after another. This\\nis one of the most important results from this paper: with-\\nout ever having simultaneous access to the data of different\\ntasks, our Expert Gate based on autoencoders manages to\\nassign test samples to the relevant tasks equally accurately\\nas a discriminative classiﬁer.\\nFigure 4 shows some of the few confusion cases for our\\nExpert Gate. For some test samples even humans have a\\nhard time telling which expert should be activated. For ex-\\nample, Scenes images containing humans can also be clas-\\nsiﬁed as Actions. To deal with such cases, it may be prefer-\\nable in some settings to allow more than one expert to be\\nactivated. This can be done by setting a threshold on the\\nprobabilities for the different tasks. We tested this scenario\\nwith a threshold of 0.1 and observed 3.7% of the test sam-\\nples being analyzed by multiple expert models. Note that\\nin this case we can only evaluate the label given by the cor-\\nresponding task as we are missing the ground truth for the\\nother possible tasks appearing in the image. This leads to an\\naverage accuracy of 68.2%, i.e. a further increase of 0.9%.\\n4.3. Task Relatedness Analysis\\nIn the previous cases, the most related task was always\\nImagenet. This is due to the similarity between the im-\\nages of these different tasks and those of Imagenet. Also,\\nthe wide diversity of Imagenet classes enables it to cover\\na good range of these tasks.\\nDoes this mean that Ima-\\ngenet should be the only task to transfer knowledge from,\\nregardless of the current task nature? To answer this ques-\\ntion, we add three more different tasks to our previous bas-\\nket: the Google Street View House Numbers SVHN [26]\\nfor digit recognition, the Chars74K dataset [8] for charac-\\nter recognition in natural images (Letters), and the Mnist\\ndataset [20] for handwritten digits. For Chars74K, we use\\nthe English set and exclude the digits, considering only the\\ncharacters. From the previous set, we pick the two most\\nrelated tasks, Actions and Scenes, and the two most unre-\\nlated tasks, Cars and Flowers. We focus on LwF [21] as a\\nmethod for knowledge transfer. We also consider ImageNet\\nas a possible source.\\nWe consider the following knowl-\\nedge transfer cases: Scenes →Actions, ImageNet →Ac-\\ntions, SVHN →Letters, ImageNet →Letters, SVHN →\\nMnist, ImageNet →Mnist, Flowers →Cars and Imagenet\\n→Cars. Figure 6 shows the performance of LwF compared\\nto ﬁne-tuning the tasks with pre-trained AlexNet (indicated\\nby ”Only X”) along with the degree of task relatedness. The\\nred line indicates the threshold of 0.85 task relatedness used\\nin our previous experiments.\\nIn the case of a high score for task relatedness, the LwF\\nuses the knowledge from the previous task and improves\\nperformance on the target task – see e.g. (SVHN→Letter,\\nScenes→Actions, ImageNet→Actions). When the tasks are\\nless related, the method fails to improve and starts to de-\\ngrade its performance, as in (Imagenet→Letters, SVHN →\\nMnist). When the tasks are highly unrelated, LwF can even\\nfail to reach a good performance for the new task, as in the\\ncase of (Imagenet→Cars, Flowers→Cars). This can be\\nexplained by the fact that each task is pushing the shared\\nparameters in a different direction and thus the model fails\\nto reach a good local minimum. We conclude that our gate\\nautoencoder succeeds to predict when a task could help an-\\nother in the LwF framework and when it cannot.\\n4.4. Video Prediction\\nNext, we evaluate our Expert Gate for video prediction\\nin the context of autonomous driving. We use a state of\\nthe art system for video prediction, the Dynamic Filter Net-\\nwork (DFN) [7]. Given a sequence of 3 images, the task for\\nthe network is to predict the next 3 images. This is quite\\na structured task, where the task environment and training\\ndata affect the prediction results quite signiﬁcantly. An au-\\ntonomous vehicle that uses video prediction needs to be able\\nto load the correct model for the current environment. It\\nmight not have all the data from the beginning, and so it\\nbecomes important to learn specialists for each type of en-\\nvironment, without the need for storing all the training data.\\nEven when all data is available, joint training does not give\\nthe best results on each domain, as we show below.\\nWe show experiments conducted on three domains/tasks:\\nfor Highway, we use the data from DFN [7], with the same\\ntrain/test split; for Residential data, we use the two longest\\nsequences from the KITTI dataset [10]; and for City data,\\nwe use the Stuttgart sequence from the CityScapes dataset\\n3372\\nScenes as Flowers\\nBirds as Scenes\\nFlowers as Birds\\nCars as Aircrafts\\nAircrafts as Cars\\nActions as Birds\\nFigure 4. Examples of confusion cases made by our Expert Gate.\\n100%\\n95%\\n90%\\n85%\\n80%\\n75%\\n70%\\n10\\n30\\n50\\n100\\n500\\n1000\\n2000\\nNumber of samples\\nAccuracy\\nDiscriminative classifier\\nOur gate\\nFigure 5. Comparison between our gate and the discriminative\\nclassiﬁer with varying number of stored samples per task.\\nFigure 6. Relatedness analysis. The relatedness values are normal-\\nized for the sake of better visualization. The red line indicates our\\nrelatedness threshold value.\\n[6], i.e. the only sequence in that dataset with densely sam-\\npled frames. We use a 90/10 train/test split on both residen-\\ntial and city datasets. We train the 3 tasks using 3 differ-\\nent regimes: sequential training using a Single Fine-tuned\\nModel, Joint Training and Expert Gate. For video predic-\\ntion, LwF does not seem applicable. In this experiment, we\\nuse the autoencoders only as gating function. We do not use\\ntask relatedness. Video prediction results are expressed as\\nthe average pixel-wise L1-distance between predicted and\\nground truth images (lower is better), and shown in table 4.\\nSimilar trends are observed as for the image classiﬁca-\\ntion problem: sequential ﬁne-tuning results in catastrophic\\nforgetting, where a model ﬁne-tuned on a new dataset de-\\nteriorates on the original dataset after ﬁne-tuning.\\nJoint\\ntraining leads to better results on each domain, but requires\\nall the data for training.\\nOur Expert Gate system gives\\nbetter results compared to both sequential and joint train-\\ning. These numbers are supported by qualitative results as\\nwell (Figure 7). Please refer to the supplementary materi-\\nals for more ﬁgures. These experiments show the potential\\nof our Expert Gate system for video prediction tasks in au-\\ntonomous driving applications.\\nTable 4. Video prediction results (average pixel L1 distance). For\\nmethods with * all the previous data needs to be available.\\nMethod\\nHighway\\nResidential\\nCity\\navg\\nSingle Fine-tuned Model\\n13.4\\n-\\n-\\n-\\n25.7\\n45.2\\n-\\n-\\n26.2\\n50.0\\n17.3\\n31.1\\nJoint Training*\\n14.0\\n40.7\\n16.9\\n23.8\\nExpert Gate (ours)\\n13.4\\n40.3\\n16.5\\n23.4\\nFigure 7. Qualitative results for video prediction. From left to\\nright: last ground truth image (in a sequence of 3); predicted image\\nusing sequential ﬁne-tuning and using Expert Gate. Examining the\\nlane markers, we see that Expert Gate is visually superior.\\n5. Conclusions and Future Work\\nIn the context of lifelong learning, most work has fo-\\ncused on how to exploit knowledge from previous tasks and\\ntransfer it to a new task. Little attention has gone to the\\nrelated and equally important problem of how to select the\\nproper (i.e. most relevant) model at test time. This is the\\ntopic we tackle in this paper. To the best of our knowl-\\nedge, we are the ﬁrst to propose a solution that does not re-\\nquire storing data from previous tasks. Surprisingly, Expert\\nGate’s autoencoders can distinguish different tasks equally\\nwell as a discriminative classiﬁer trained on all data. More-\\nover, they can be used to select the most related task and\\nthe most appropriate transfer method during training. Com-\\nbined, this gives us a powerful method for lifelong learning,\\nthat outperforms not only the state-of-the-art but also joint\\ntraining of all tasks simultaneously.\\nOur current system uses only the most related model for\\nknowledge transfer. As future work, we will explore the\\npossibility of leveraging multiple related models for the\\ntraining of new tasks – for instance, by exploring new strate-\\ngies for balancing the contribution of the different tasks by\\ntheir relatedness degree rather than just varying the learn-\\ning rates. Also a mechanism to decide when to merge tasks\\nwith high relatedness degree rather than adding a new ex-\\npert model, seems an interesting research direction.\\nAcknowledgment: The ﬁrst author’s PhD is funded by an FWO\\nscholarship. We are grateful for support from KU Leuven GOA\\nproject CAMETRON. The authors would like to thank Matthew\\nB. Blaschko and Amal Rannen Triki for valuable discussions.\\n3373\\nReferences\\n[1] K. Ahmed, M. H. Baig, and L. Torresani. Network of ex-\\nperts for large-scale image categorization.\\narXiv preprint\\narXiv:1604.06119, 2016.\\n[2] G. Alain and Y. Bengio.\\nWhat regularized auto-encoders\\nlearn from the data-generating distribution. Journal of Ma-\\nchine Learning Research, 15(1):3563–3593, 2014.\\n[3] Y. Bengio et al. Learning deep architectures for ai. Founda-\\ntions and trends R\\n⃝in Machine Learning, 2(1):1–127, 2009.\\n[4] H. Bourlard and Y. Kamp. Auto-association by multilayer\\nperceptrons and singular value decomposition.\\nBiological\\ncybernetics, 59(4-5):291–294, 1988.\\n[5] R. Caruana. Multitask learning. In Learning to learn, pages\\n95–133. Springer, 1998.\\n[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\\nR. Benenson, U. Franke, S. Roth, and B. Schiele.\\nThe\\ncityscapes dataset for semantic urban scene understanding.\\nIn Proc. of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), 2016.\\n[7] B. De Brabandere, X. Jia, T. Tuytelaars, and L. Van Gool.\\nDynamic ﬁlter networks. arXiv preprint arXiv:1605.09673,\\n2016.\\n[8] T. E. de Campos, B. R. Babu, and M. Varma.\\nCharacter\\nrecognition in natural images. In Proceedings of the Interna-\\ntional Conference on Computer Vision Theory and Applica-\\ntions, Lisbon, Portugal, February 2009.\\n[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman.\\nThe PASCAL Visual Object Classes\\nChallenge 2012 (VOC2012) Results.\\nhttp://www.pascal-\\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n[10] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets\\nrobotics: The kitti dataset.\\nThe International Journal of\\nRobotics Research, page 0278364913491297, 2013.\\n[11] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning.\\nBook in preparation for MIT Press, 2016.\\n[12] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and\\nY. Bengio. An empirical investigation of catastrophic for-\\ngetting in gradient-based neural networks.\\narXiv preprint\\narXiv:1312.6211, 2013.\\n[13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n[14] G. E. Hinton and R. R. Salakhutdinov.\\nReducing the\\ndimensionality of data with neural networks.\\nScience,\\n313(5786):504–507, 2006.\\n[15] L. Jacob, J.-p. Vert, and F. R. Bach. Clustered multi-task\\nlearning: A convex formulation. In Advances in neural in-\\nformation processing systems, pages 745–752, 2009.\\n[16] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hin-\\nton. Adaptive mixtures of local experts. Neural computation,\\n3(1):79–87, 1991.\\n[17] I. Kokkinos.\\nUbernet: Training auniversal’convolutional\\nneural network for low-, mid-, and high-level vision us-\\ning diverse datasets and limited memory.\\narXiv preprint\\narXiv:1609.02132, 2016.\\n[18] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-\\nresentations for ﬁne-grained categorization. In Proceedings\\nof the IEEE International Conference on Computer Vision\\nWorkshops, pages 554–561, 2013.\\n[19] A. Kumar and H. Daume III.\\nLearning task group-\\ning and overlap in multi-task learning.\\narXiv preprint\\narXiv:1206.6417, 2012.\\n[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE, 86(11):2278–2324, 1998.\\n[21] Z. Li and D. Hoiem. Learning without forgetting. In Eu-\\nropean Conference on Computer Vision, pages 614–629.\\nSpringer, 2016.\\n[22] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.\\nFine-grained visual classiﬁcation of aircraft. Technical re-\\nport, 2013.\\n[23] V. Mante, D. Sussillo, K. V. Shenoy, and W. T. Newsome.\\nContext-dependent computation by recurrent dynamics in\\nprefrontal cortex. Nature, 503(7474):78–84, 2013.\\n[24] Y. MarcAurelio Ranzato and L. B. S. C. Y. LeCun. A uniﬁed\\nenergy-based framework for unsupervised learning. In Proc.\\nConference on AI and Statistics (AI-Stats), volume 24, 2007.\\n[25] T. M. Mitchell.\\nThe need for biases in learning gener-\\nalizations.\\nDepartment of Computer Science, Laboratory\\nfor Computer Science Research, Rutgers Univ. New Jersey,\\n1980.\\n[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\\nNg. Reading digits in natural images with unsupervised fea-\\nture learning. 2011.\\n[27] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks\\nare easily fooled: High conﬁdence predictions for unrecog-\\nnizable images. In 2015 IEEE Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), pages 427–436. IEEE,\\n2015.\\n[28] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-\\nﬁcation over a large number of classes. In Proceedings of the\\nIndian Conference on Computer Vision, Graphics and Image\\nProcessing, Dec 2008.\\n[29] A. Quattoni and A. Torralba. Recognizing indoor scenes.\\nIn Computer Vision and Pattern Recognition, 2009. CVPR\\n2009. IEEE Conference on, pages 413–420. IEEE, 2009.\\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge. International Journal of Computer\\nVision (IJCV), 115(3):211–252, 2015.\\n[31] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,\\nJ. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Had-\\nsell.\\nProgressive neural networks.\\narXiv preprint\\narXiv:1606.04671, 2016.\\n[32] D. L. Silver and R. E. Mercer. The task rehearsal method of\\nlife-long learning: Overcoming impoverished data. In Con-\\nference of the Canadian Society for Computational Studies\\nof Intelligence, pages 90–101. Springer, 2002.\\n[33] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning\\nsystems: Beyond learning algorithms. In AAAI Spring Sym-\\nposium: Lifelong Machine Learning, pages 49–55. Citeseer,\\n2013.\\n3374\\n[34] A. V. Terekhov, G. Montone, and J. K. ORegan. Knowledge\\ntransfer in deep block-modular neural networks. In Confer-\\nence on Biomimetic and Biohybrid Systems, pages 268–279.\\nSpringer, 2015.\\n[35] S. Thrun and J. OSullivan. Clustering learning tasks and the\\nselective cross-task transfer of knowledge. In Learning to\\nlearn, pages 235–257. Springer, 1998.\\n[36] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural\\nnetworks for matlab. In Proceedings of the 23rd ACM inter-\\nnational conference on Multimedia, pages 689–692. ACM,\\n2015.\\n[37] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-\\nlongie, and P. Perona. Caltech-UCSD Birds 200. Technical\\nReport CNS-TR-2010-001, California Institute of Technol-\\nogy, 2010.\\n[38] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram.\\nMulti-\\ntask learning for classiﬁcation with dirichlet process priors.\\nJournal of Machine Learning Research, 8(Jan):35–63, 2007.\\n[39] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V.\\nLe, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, et al. On\\nrectiﬁed linear units for speech processing. In 2013 IEEE\\nInternational Conference on Acoustics, Speech and Signal\\nProcessing, pages 3517–3521. IEEE, 2013.\\n3375\\n', 'source_name': 'Expert Gate: Lifelong Learning with a Network of Experts', 'source_url': 'https://arxiv.org/abs/1611.06194'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "cBTM.pdf #23\n",
      "{'content': 'Scaling Expert Language Models with Unsupervised Domain Discovery\\nSuchin Gururangan * 1 † Margaret Li * 1 2 Mike Lewis 2 Weijia Shi 1 2 Tim Althoff 1\\nNoah A. Smith 1 3 Luke Zettlemoyer 1 2\\nAbstract\\nLarge language models are typically trained\\ndensely: all parameters are updated with respect\\nto all inputs. This requires synchronization of\\nbillions of parameters across thousands of GPUs.\\nWe introduce a simple but effective method to\\nasynchronously train large, sparse language mod-\\nels on arbitrary text corpora. Our method clusters\\na corpus into sets of related documents, trains\\na separate expert language model on each clus-\\nter, and combines them in a sparse ensemble for\\ninference. This approach generalizes embarrass-\\ningly parallel training by automatically discover-\\ning the domains for each expert, and eliminates\\nnearly all the communication overhead of existing\\nsparse language models. Our technique outper-\\nforms dense baselines on multiple corpora and\\nfew-shot tasks, and our analysis shows that spe-\\ncializing experts to meaningful clusters is key to\\nthese gains. Performance also improves with the\\nnumber of experts and size of training data, sug-\\ngesting this is a highly efﬁcient and accessible\\napproach to training large language models.\\n1. Introduction\\nLanguage models (LMs) are trained on up to trillions of\\ntokens of text (Hoffmann et al., 2022; Touvron et al., 2023).\\nThis improves performance on many tasks, but also incurs\\nan extreme cost: thousands of GPUs need to be active si-\\nmultaneously to update all parameters at each step (Zhang\\net al., 2022; Chowdhery et al., 2022). Branch-Train-Merge\\n(BTM; Li et al. 2022) alleviates this cost by dividing the\\ntotal compute among a collection of smaller expert language\\nmodels (ELMs), each independently trained on a distinct\\nsubset (or domain) of the training corpus and ensembled\\n*Equal\\ncontribution\\n†Work\\ndone\\nwhile\\nat\\nMeta\\nAI.\\n1University\\nof\\nWashington\\n2Meta\\nAI\\n3Allen\\nInstitute\\nfor Artiﬁcial Intelligence.\\nCorrespondence to:\\nSuchin\\nGururangan\\n<sg01@cs.washington.edu>,\\nMargaret\\nLi\\n<margsli@cs.washington.edu>.\\n1B\\n2B\\n5B\\n10B\\n20B\\n40B\\n80B\\n168B\\nToken Count\\n12\\n13\\n14\\n15\\n16\\n17\\nPerplexity\\nDense\\nc-BTM (k=4)\\nc-BTM (k=16)\\nC4\\nFigure 1. We present\\nC-BTM, a new technique to asyn-\\nchronously scale expert LMs (§2). C-BTM splits a corpus into\\nk clusters, trains an expert LM on each cluster, and creates a sparse\\nensemble during inference. Above, LMs trained with C-BTM\\n(with 4 or 16 clusters) achieve lower validation perplexity than\\ncompute-matched dense LMs. These LMs begin with OPT-1.3B\\n(Zhang et al., 2022), and are further trained on C4 (Raffel et al.,\\n2019). The optimal cluster count for C-BTM, and its performance\\ngains, increase with the size of training data (shown in log-scale).\\nduring inference. However, BTM relies on document meta-\\ndata to identify domains, and such supervision is not always\\navailable (e.g., in large Internet crawls; Raffel et al., 2019;\\nRae et al., 2021; Gao et al., 2021). Moreover, the optimal\\nnumber of metadata-based domains for a ﬁxed budget is un-\\nknown, since metadata cannot be easily merged or divided.\\nIn this work, we introduce Cluster-Branch-Train-Merge (C-\\nBTM; Figure 1), a metadata-free algorithm to scale LMs\\nwithout massive multi-node synchronization. We use un-\\nsupervised clustering to discover domains in a corpus, and\\ntrain an ELM on each cluster independently (§2.1). At infer-\\nence time, we sparsely activate a subset of the trained ELMs\\n(§2.2). We ensemble ELMs by weighting their outputs with\\nthe distances between an embedding of the current context\\nand each expert’s cluster center. This enables simple and ef-\\nﬁcient sparse computation (Fedus et al., 2022) by retrieving\\nonly the top-k experts when predicting each new token.\\narXiv:2303.14177v1  [cs.CL]  24 Mar 2023\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nFigure 2. C-BTM training process (§2.1). C-BTM begins with unsupervised domain discovery using k-means clustering. We then\\ninitialize expert language models (ELMs) with a seed language model (e.g., OPT; Zhang et al. 2022) and train an ELM on each cluster.\\nThe resulting experts are added to a larger collection for sparse inference.\\nC-BTM generalizes BTM by allowing for ﬁne-grained con-\\ntrol over the number and size of data clusters, since they are\\nautomatically learned without being constrained by avail-\\nable metadata. We use this new capability to investigate the\\nscaling properties of C-BTM as a function of the number of\\nexperts trained, controlling for a variety of factors (§3). Ex-\\ntensive experiments show that training more clusters always\\nresults in better validation perplexity than single cluster (i.e.,\\ndense) models, and the optimal cluster count increases with\\nthe overall compute (§4.1). These results are consistent for\\nboth 1.3B and 6.7B parameter experts.\\nWith more clusters, we can aggressively parallelize expert\\ntraining: for example, we train 128 ELMs (168B parameters\\nin total) on 168B tokens of text in aggregate with only 8\\nGPUs at a time. This enables us to avoid many practical\\ndifﬁculties associated with training large LMs across many\\nnodes simultaneously (§4.2). Moreover, the number of\\nparameters at inference time can be kept constant even as the\\nnumber of experts grows (§4.3): using just the top-2 or top-4\\nexperts is comparable to using all experts, while using just\\nthe top-1 expert still outperforms the dense model. Training\\nwith more clusters is also more effective than training larger\\ndense models: in §4.4, we demonstrate that training many\\n1.3B expert LMs, and sparsifying them to a 5.2B parameter\\nLM, achieves the same perplexity as a 6.7B dense model,\\nbut with only 29% as many training FLOPs. These gains\\nare also reﬂected in few-shot text classiﬁcation experiments\\n(§5), which show that C-BTM models outperform dense\\nbaselines even with heavily sparsiﬁed inference.\\nC-BTM provides a radically simpliﬁed sparse modeling\\napproach that eliminates nearly all communication over-\\nhead from existing sparse LM schemes. Existing sparse\\nLMs typically route different tokens to specialist parameters\\n(Lepikhin et al., 2021; Fedus et al., 2021; Clark et al., 2022).\\nHowever, they have yet to be widely adopted, perhaps due\\nin part to the communication costs of routing each token\\nin each sparse layer (Artetxe et al., 2021), challenges in\\nlearning to specialize experts to tokens (Zhou et al., 2022),\\nand the necessity of additional mechanisms to balance ex-\\npert utilization (Lewis et al., 2021). C-BTM improves over\\nsparse LMs by routing sequences (instead of tokens) using\\nofﬂine balanced clustering (instead of online load balancing)\\nwith no shared parameters between experts. We compare\\ndirectly to a mixture-of-experts model with top-2 routing\\n(Lepikhin et al., 2021) in §6.\\nOur ﬁnal analysis (§7) shows that balanced clustering is\\nkey to C-BTM performance; it works as well as expert\\nassignment with gold metadata, and strongly outperforms\\nrandom and unbalanced clustering baselines. Overall, our\\nﬁndings suggest that C-BTM is an efﬁcient and accessible\\nmethod to scale large language models into massive datasets.\\nWe release our code and models publicly.1\\n2. C-BTM\\nWe introduce C-BTM, a method for embarrassingly parallel\\ntraining that specializes expert language models to domains\\ndiscovered through clustering instead of metadata. C-BTM\\nenables scaling to arbitrary numbers of domains and com-\\npute budgets on any corpus. In this section, we outline\\nC-BTM training (Figure 2) and inference (Figure 3).\\n2.1. Training\\nStep 0: Cluster\\nTo segment our corpus, we employ\\nk-means clustering, enforcing balanced clusters. ELMs\\ntrained without this constraint perform worse (§7.2).2\\nConsider the iterative, hard expectation-maximization view\\nof k-means clustering. In the expectation step, each docu-\\nment embedding is assigned to a cluster center based on its\\nEuclidean distance to each center. In the maximization step,\\neach cluster center is updated to be the mean embedding of\\nthe current set of documents assigned to it. To balance the\\n1https://github.com/kernelmachine/cbtm\\n2Other techniques to improve clusters, e.g. k-means++ (Arthur\\n& Vassilvitskii, 2007), can be used to improve performance.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nclusters, we formulate the expectation step as a balanced lin-\\near assignment problem (Malinen & Fränti, 2014). Given D\\ndocument embeddings with representations {w1, . . . , wD}\\nand K cluster centers with representations {h1, . . . , hK},\\nwe assign each document d to a cluster with the assignment\\nindex ad ∈{0, . . . , K}:\\nmax\\na1,...,aD\\nD\\nX\\nd=1\\n−dist(had, wd) s.t. ∀k,\\nD\\nX\\nd=1\\n1ad=k = D\\nK\\n(1)\\nwhere dist is the Euclidean distance. Many algorithms\\nexist to solve this problem; we follow Lewis et al. (2021)\\nand use the auction algorithm (Bertsekas, 1992). We only\\nuse balancing when estimating the cluster centers; we use\\ngreedy inference when predicting clusters, as balancing at\\ninference time is cumbersome for massive corpora.\\nIn our experiments, we use a simple tf-idf embedding func-\\ntion, which is highly efﬁcient at scale and leads to inter-\\npretable clusters.3 We only use a single shard of each cor-\\npus to train our clustering model. Any new document, once\\nembedded, can be efﬁciently mapped to its nearest cluster(s)\\nwithout additional training. Any embedding function can be\\nused, though the choice of embedder may apply different\\nassumptions of what constitutes a textual domain and come\\nwith efﬁciency trade-offs.4 Comparing to other embedding\\nor clustering methods is an interesting area for future work,\\nand could likely improve performance.\\nStep 1: Branch (from seed LM)\\nTo begin training ex-\\nperts on each of the k clusters from Step 0, we ﬁrst branch\\nfrom (i.e., make k copies of) a seed LM. Seed LMs are\\ncritical for the overall functionality of ELMs, and ELMs\\nperform best when the seed LM has been trained with a\\ndiverse corpus (Li et al., 2022). In our experiments, we use\\nan OPT LM (Zhang et al., 2022) as our seed.5\\nStep 2: Train\\nWe assign each ELM to a single cluster,\\nand train on each cluster with the log likelihood objective.\\nStep 3: Merge\\nAfter training on the assigned domain, we\\nadd the new ELM into a larger collection for inference.\\nIn this work, we focus on a single iteration of C-BTM for\\nsimplicity. Future work may explore branching from already\\ntrained experts in multiple iterations.\\n3In initial experiments, tf-idf outperformed other scalable text\\nembeddings, like hash embeddings (Svenstrup et al., 2017).\\n4tf-idf assumes that domains are lexically-driven, which may\\nnot correspond with other notions of domain.\\n5Li et al. 2022 ﬁnd that dedicating more compute to branching\\n(rather than seed training) leads to better in-domain performance,\\nand the choice of seed LM has a strong effect on the modularity\\nof the resulting ELMs. Future work may explore the effect of\\ndifferent seed LMs on C-BTM performance.\\nFigure 3. C-BTM inference process (§2.2). At inference time,\\nwe embed each incoming context and estimate a probability dis-\\ntribution over clusters, by calculating the distance between the\\nembedded context and each cluster center. We use this probability\\ndistribution, optionally sparsiﬁed to use only the top-k experts, to\\nweight an output ensemble of the ELMs.\\n2.2. Inference\\nAt inference time, we use a sparse ensemble of the outputs\\nof ELMs for incoming test contexts (Figure 3). Formally,\\nconsider that the language model provides, at each timestep,\\np(Xt | x<t). We introduce a domain variable D, alongside\\neach sequence. Then the next-step conditional distribution\\non the history x<t is:\\np(Xt | x<t)=\\nk\\nX\\nj=1\\np(Xt | x<t, D = j) · p(D = j | x<t)\\n|\\n{z\\n}\\nensemble weights\\n(2)\\nWith the pretrained embedder and clustering model from\\nStep 0 (§2.1), we embed the context hx<t and use the k\\ncluster centers {hc0 . . . hck}. We set ensemble weights as:\\np(D = j | x<t) ∝topk[exp(−dist(hx<t, hcj)2/T)] (3)\\nWhere dist is the Euclidean distance, T is a temperature pa-\\nrameter which sharpens or smoothes the probability distribu-\\ntion over cluster centers, and the top-k function ﬁlters for the\\ntop-k probabilities and renormalizes the distribution to sum\\nto 1. This formulation is reminiscent of nearest-neighbor re-\\ntrieval mechanisms for language models (Khandelwal et al.,\\n2019; Shi et al., 2022).\\nThese ensemble weights are updated for every incoming\\ntoken, although in separate experiments we observe that\\nwe ﬁnd that cluster assignments (and in effect, ensemble\\nweights) can be ﬁxed for the second half of a document with\\nno drop in performance; this can further speedup inference.\\nWe ﬁnd that, in practice, the performance of our models\\nis robust to even top-2 or top-4 experts, meaning that the\\ninference costs of the language model are equivalent to\\na much smaller LM. We perform an empirical study of\\ninference variations in §4.3.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\n2.3. Comparing to Dense Training\\nDense LMs are typically trained using hundreds or thou-\\nsands of concurrent GPUs, all of which synchronize gradi-\\nents each update. For example, OPT-175B (Zhang et al.,\\n2022) was trained on 992 80GB A100 GPUs, and PaLM-\\n540B (Chowdhery et al., 2022) was trained on 6144 TPU\\nv4 chips. C-BTM improves training efﬁciency by reduc-\\ning communication overhead, as only GPUs training the\\nsame ELM must communicate. Furthermore, the chance\\nof a GPU failure can grow considerably with the number\\nof GPUs. C-BTM improves the resiliency of distributed\\ntraining, since a single GPU failure only delays training for\\na single ELM, whereas in dense training, a single GPU fail-\\nure afﬂicts training on all other GPUs. C-BTM also makes\\ntraining large LMs more feasible on shared GPU clusters,\\nsince it effectively decomposes training into smaller jobs\\nwhich can run asynchronously. This makes job scheduling\\nmore efﬁcient by reducing the number of GPUs that need to\\nbe allocated simultaneously.\\n2.4. Comparing to BTM (Li et al., 2022)\\nOur method addresses several limitations of the training and\\ninference techniques proposed by Li et al. (2022).\\nFirst, BTM is limited to training data with metadata which\\ncan be used to determine its domains. Typical LM corpora,\\nincluding C4 (Raffel et al., 2019) and the Pile (Gao et al.,\\n2021), are sourced from the Internet without retaining doc-\\nument provenance at collection time, and are infeasible to\\nlabel manually. Also, the optimal number of experts for a\\nﬁxed corpus size, model architecture, and budget remains\\nunknown, and is difﬁcult to explore with metadata-based\\ndomains, since they cannot be easily merged or divided.\\nC-BTM broadens the applicability of BTM to arbitrary\\ndatasets.\\nMoreover, BTM inference follows the cached prior method\\nintroduced by Gururangan et al. (2022), where the ensem-\\nble weights are estimated using Bayes’ rule on additional\\nheld out data, and the prior P(D = j) is estimated with an\\nexponential moving average over sequences of posterior es-\\ntimates that require forward passes on experts. This estimate\\nis then ﬁxed during test data evaluation.\\nWith C-BTM, we route based only on the current context.\\nThus, no additional data or forward passes through the ex-\\nperts are needed to estimate ensemble weights, nor do we\\nneed to assume that adjacent documents in the test set come\\nfrom the same distribution. This also implies that the param-\\neter averaging technique of Li et al. (2022) is not well suited\\nto our setting, as it requires ﬁxing the weights assigned to\\neach expert for a set of evaluation documents. Future work\\nmay explore merging expert parameters for each context\\nduring inference.\\n2.5. Comparing to Mixture-of-Experts (MoE)\\nLike MoE models (e.g., Fedus et al., 2022), C-BTM allows\\nfor efﬁcient scaling of large LMs while keeping inference\\ncosts manageable. However, C-BTM routes sequences (in-\\nstead of tokens) using ofﬂine balanced clustering (instead\\nof online load balancing) with no shared parameters be-\\ntween experts. This eliminates effectively all complexities\\nassociated with balancing expert utilization (Lewis et al.,\\n2021), avoids expensive all-to-all operations between ex-\\nperts (Artetxe et al., 2021), and naturally leads to inter-\\npretable expert specialization to domains of the training cor-\\npus. In §6, we compare directly to MoE baselines trained\\nwith sparse upcycling (Komatsuzaki et al., 2022), which\\ninitializes the MoE with a dense checkpoint, mirroring how\\nC-BTM initializes ELMs.\\n3. Experimental Setup\\nWe design a set of experiments to study C-BTM on two large\\ncorpora (Figure 4) selected to be distinct from the corpus\\nused to train our seed OPT model, and report perplexity on\\nheld out data from each corpus.\\n3.1. Data\\nC4 (Raffel et al., 2019)\\nC4 is a publicly available\\ndistribution of a Common Crawl snapshot on Hug-\\ngingface datasets.6\\nWe use the no blocklist version\\n(en.noblocklist) to train on a dataset that is out of\\ndistribution to our seed (OPT) pretraining corpus. C4 con-\\nsists of 393M documents totaling 220B BPE tokens. We\\ntrain on up to 168B tokens.\\nS2ORC (Lo et al., 2019)\\nThe Semantic Scholar Research\\nOpen Corpus (S2ORC) is a publicly available corpus of\\nfull-text academic papers from the Semantic Scholar.7 The\\ncorpus spans 20 ﬁelds of study (e.g., Biology, Computer\\nScience, Art), and contains 16M documents, totaling 87B\\nBPE tokens. We train on up to 168B tokens over multiple\\nepochs.8\\nEvaluation data\\nFor all experiments, we report language\\nmodeling perplexity on 200 randomly-sampled held out doc-\\numents. Because S2ORC does not come with pre-deﬁned\\nvalidation data, we create a validation corpus by sampling\\nan equal number of documents from each ﬁeld of study.\\n6https://huggingface.co/datasets/c4\\n7https://allenai.org/data/s2orc\\n8While it is not common to train large LMs for multiple epochs,\\nwe do not observe overﬁtting in any of our experiments, consistent\\nwith other studies that train LMs on academic literature for multiple\\nepochs (Taylor et al., 2022).\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nstudy, participant, disease\\ntheorem, proof, let\\nquark, mass, Higgs\\nprotein, DNA, genes\\ntraining, data, learning\\nsocial, political, people\\ncell, expression, mice\\nlaser, beam, optical\\nlanguage, words, speech\\nflow, velocity, field\\nSemantic Scholar Open Research Corpus (S2ORC)\\nfood, recipes, minutes\\ngames, team, season\\nreligion, life, people\\npolitics, president, said\\nproducts, online, quality\\nfilm, movie, story\\nmusic, album, band\\nmarket, company, year\\npatient, cancer, body\\nColossal Cleaned Crawled Corpus (C4)\\nfile, window, click\\nFigure 4. We train and evaluate on two large text corpora (§3.1). C4 (left; Raffel et al. 2019) and S2ORC (right; Lo et al. 2019) are\\ndiverse and contain many different clusters of text, indicated by these UMAP visualizations of 400K random documents in each corpus,\\ncolored with 32 automatically discovered and annotated clusters. See §7.3 for the description of our clustering and annotation procedure,\\nand Figure A in the appendix for annotations of all clusters in these plots.\\n3.2. Experimental Setup\\nClustering the data\\nWe segment each corpus using bal-\\nanced k-means clustering for k ∈{2, 4, 8, 16, 32, 64, 128}\\n(§2.1). To train the clustering models, we ﬁrst embed all data\\nwith a tf-idf vectorizer using scikit-learn,9 with minimal as-\\nsumptions: we only remove stop-words from a ﬁxed lexicon\\nand replace numbers with a dummy token. We then reduce\\nthe dimensionality of the resulting embeddings; we perform\\ntruncated SVD with 100 dimensions, then normalize the vec-\\ntor by removing its mean and scaling to unit variance, which\\nwe observed in initial experiments improved the clustering\\nquality. Finally, these representations are clustered using\\na custom Pytorch implementation.10 We present learned\\nclusters and visualizations in Figure 4 and Figure A (in the\\nappendix). We use a single shard of each training corpus\\n(384K documents for C4, 155K documents for S2ORC) to\\ntrain the clustering model and its embedder. No evaluation\\ndata is used in this process.\\nSeed LM\\nAs LMs trained on diverse corpora make for bet-\\nter seeds (Li et al., 2022), we use pretrained OPT language\\nmodels (Zhang et al., 2022) as our seed for all experiments.\\nModel hyperparameters\\nWe use the OPT architecture\\nimplemented in Metaseq (Zhang et al., 2022). We use OPT-\\n9https://scikit-learn.org/\\n10https://github.com/kernelmachine/\\nbalanced-kmeans\\n1.3B for the initial set of experiments, and replicate our\\nexperiments with OPT-6.7B. Following Zhang et al. 2022,\\nwe use the GPT-2 vocabulary of 50,257 BPE types (Rad-\\nford et al., 2019), and train with 2,048-token sequences,\\nacross document boundaries. We prepend a beginning-of-\\ndocument token to each document. We set dropout to 0.1\\nfor all parameters except those of the embedding layer.\\nTraining hyperparameters\\nFor all models, we ﬁx the\\nlearning rate to that used during OPT pretraining (2e-4 for\\n1.3B parameter models; 1.2e-4 for 6.7B parameter models;\\nZhang et al. 2022) using a linear decay learning rate sched-\\nule to zero (with no warmup), which we found to work well\\nfor most settings after a grid search of fastest learning rates\\nthat avoided divergence. We use a batch size of 8 for each\\nGPU, and train with fp16 and fully-sharded data-parallel\\n(Artetxe et al., 2021). We train on NVIDIA V100 32GB\\nGPUs. All models are trained with Metaseq (Zhang et al.,\\n2022). For a given number of clusters k and total GPU bud-\\nget n, each ELM is allocated n/k GPUs, keeping the total\\neffective number of FLOPs ﬁxed across models exposed to\\nthe same number of tokens. See §A.2 for more details.\\nScaling\\nWe train for a total of 10K steps in each run; to\\nexpose the model to more tokens, we increase the total GPU\\nbudget proportionally, up to 64 GPUs. We simulate larger\\nbudgets, up to 1024 GPUs, by increasing gradient accumu-\\nlation steps with 64 GPUs. This method of scaling increases\\nthe model’s effective batch size for the same number of\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nsteps, and maintains near constant run-times across our\\nmany experiments. This experimental setup also means that\\nas the number of clusters increases, the overall set of ELMs\\nis exposed to more data with less simultaneous computation\\namong GPUs.\\nOther ways of training on more data (e.g., by keeping to-\\ntal batch size ﬁxed and increasing step count) may yield\\ndifferent results. The best batch size and learning rate com-\\nbinations for training language models are likely speciﬁc to\\na variety factors, including the model size, dataset, and total\\ncompute available (Shallue et al., 2018; McCandlish et al.,\\n2018; Yang et al., 2021). In preliminary experiments, we\\nfound that expert models beneﬁt from faster learning rates\\nand larger batch sizes. Given a sufﬁciently large batch size,\\nexperts are robust to a variety of learning rates. Our larger\\nbudget experiments might beneﬁt from higher learning rates,\\nbut we leave further tuning for future work.\\nInference\\nOne of the key hyperparameters for inference\\nis the temperature T (Equation 3), which governs the sharp-\\nness of the probability distribution over experts for a given\\ncontext. We ﬁnd that setting T=0.1 works well for most\\nsettings (see §A.6 for more details). We also compute the\\nnearest cluster centers for every incoming context, regard-\\nless of how stable the cluster assignments already are for\\na document. However, we ﬁnd that these assignments can\\nbe ﬁxed for the second half of a document with no drop in\\nperplexity; this can further speedup inference. The other\\nimportant hyperparameter is the top-k value, which sparsi-\\nﬁes the probability distribution over experts. For our core\\nexperiments in §4.1, we set top-k to the total number of\\nexperts we have trained for each model. We explore the\\neffect of enforcing sparsity with lower top-k values in §4.3.\\nBaselines\\nIn our primary experiments (§4), we compare\\nwith a strong dense baseline (i.e., our 1-cluster model) fol-\\nlowing OPT pretraining. We also progressively increase\\nthe number of clusters we train with for a ﬁxed number\\nof tokens. In subsequent experiments (§6), we compare to\\nMoE language models initialized from a dense checkpoint.\\n3.3. Making Fair Model Comparisons\\nWe follow the recommendations of Dehghani et al. (2021)\\nand report results with multiple cost metrics, and detail\\nour choices here. When comparing model training budgets,\\nwe are primarily concerned with the true monetary cost of\\nmodel training, which is typically billed in direct propor-\\ntion to GPU-time. Model inference comparisons have two\\nmain considerations: monetary cost incurred by the model\\ndeployer, again measured in GPU-time, and latency for end-\\nusers, or wall-clock time (i.e., how slow a model inference\\nis for an end-user).\\n1B 20B 40B\\n80B\\n168B\\nToken Count\\n13\\n14\\n15\\n16\\n17\\nPerplexity\\nC4\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\n16 clusters\\n1B 20B 40B\\n80B\\n168B\\nToken Count\\n10.5\\n11.0\\n11.5\\n12.0\\n12.5\\n13.0\\n13.5\\n14.0\\nPerplexity\\nS2ORC\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\n16 clusters\\nFigure 5. Increasing cluster count in C-BTM improves lan-\\nguage modeling performance for a ﬁxed compute budget\\n(§4.1). Performance of ELMs trained with C-BTM as a function\\nof the total number of tokens trained on, which, in our experiments,\\nequalizes FLOP count. Training with more than one cluster always\\noutperforms the compute-matched, single cluster dense model, and\\nwe observe improving performance (and in §4.2, faster updates) as\\nwe increase the number of clusters.\\nWe explicitly do not compare or match the number of model\\nparameters during training, which has minimal bearing on\\nthe cost of model training separately from its inﬂuence on\\nGPU-time. The number of training parameters is a particu-\\nlarly misleading cost measure that is unsuitable for sparse\\nmodels, since they can maintain the FLOPs and inference\\nspeed of dense models despite training many more parame-\\nters (Dehghani et al., 2021).\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nTraining GPU-time\\nAssuming ﬁxed hardware, GPU-\\ntime during model training is determined mostly by FLOPs\\nand inter-machine communications. However, prior work\\ntypically only FLOP-matches, ignoring the additional inter-\\nGPU communications incurred by some models (e.g., MoE)\\nthat increase training costs. Ideally, our comparisons could\\ndirectly ﬁx GPU-time. This is challenging in practice, as\\neven identical computations on the same GPU node at dif-\\nferent times can vary wildly in speed due factors like tem-\\nperature, other activity on the node, or the quality of GPU\\ninterconnect. To maintain consistency and fairness despite\\nthese confounds, our results compare FLOP-matched mod-\\nels with the same training data budget over the same number\\nof updates (§4.1), but also report the speed of training for\\neach FLOP-matched model (§4.2). This allows us to disen-\\ntangle and accurately reﬂect multiple cost metrics of train-\\ning. Since, in our experiments, models exposed to the same\\nnumber of tokens incur the same number of FLOPs, we use\\ntraining data size as a more interpretable measurement of\\nthe overall training budget (see §A.2 for more details).\\nInference GPU-time\\nInference GPU-time is also primar-\\nily the result of FLOPs and communication costs. Since\\ncommunication during inference is minimal, we compare\\nFLOPs via inference parameters (§4.3). We do not account\\nfor the FLOPs of the C-BTM router, which varies based on\\nthe clustering approach, and is relatively negligible.\\nInference latency\\nFLOPs is not an ideal metric for infer-\\nence latency of our models, because C-BTM allows for par-\\nallel inference across ELMs. This means that if ELMs share\\nthe same architecture (e.g., OPT-1.3B), inference latency\\nis always equivalent to that of a single ELM, regardless of\\nthe number of experts active. However, inference latency\\nmay be quite different between model architectures (e.g.,\\nOPT-1.3B and OPT-6.7B); we discuss this further in §4.4.\\nAs with inference GPU-time, we do not consider the latency\\nof the C-BTM router.\\n4. Language Modeling Results\\nWe begin with a set of experiments in which we train LMs\\nwith C-BTM on datasets from §3.1. We are interested in\\nmeasuring how performance changes as we increase overall\\ncompute. We ﬁrst compare models against training costs:\\ntotal training tokens (§4.1) and training time (§4.2). Then,\\nin §4.3, we compare model performance along an axis of\\ninference costs: the total parameter count at inference time.\\nFinally, in §4.4 we compare model performance by ﬁxing\\nboth training and inference costs. Across all computational\\nbudgets, C-BTM provides substantial beneﬁts over dense\\ntraining, and performance improvements increase as the\\ntotal compute grows.\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\nNumber of Clusters\\n13\\n14\\n15\\n16\\n17\\nPerplexity\\nC4\\nTokens\\n1.3B\\n2.6B\\n5.2B\\n10B\\n21B\\n42B\\n84B\\n168B\\n1\\n2\\n4\\n8\\n16\\n32\\nNumber of Clusters\\n11\\n12\\n13\\n14\\n15\\nPerplexity\\nS2ORC\\nTokens\\n1.3B\\n2.6B\\n5.2B\\n10B\\n21B\\n42B\\n84B\\n168B\\nFigure 6. There exists an optimal cluster count for each com-\\npute budget (§4.1). The optimal cluster count increases as one\\nincreases the compute budget, but using too many clusters without\\nsufﬁciently increasing compute can degrade performance. For\\nboth C4 and S2ORC, 16 clusters gets the best performance at the\\nhighest budget (168B tokens), although higher cluster counts still\\noutperform the 1-cluster (dense) model (x = 1 in this graph).\\n4.1. Controlling for Total Training Tokens\\nFirst, we compare model performance controlling for over-\\nall training data size (or equivalently, training FLOPs; §3.3).\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nFigure 5 shows evaluation perplexity on C4 and S2ORC\\nwith up to 16 clusters. Training on more than one cluster\\nalways outperforms training with a single cluster (i.e., a\\ndense model). As the amount of training data grows, the\\ngap between our models and the dense one widens, indicat-\\ning that experts make better use of larger training datasets,\\npossibly due to their increased specialization. These results\\nsuggest that as we increase the amount of data available,\\nC-BTM beneﬁts from more clusters.\\nHowever, Figure 6 shows that there exists an optimal cluster\\ncount for each token budget that we consider. Each number\\nof clusters has a budget range in which they are optimal,\\nand the optimum smoothly progresses from smaller to larger\\ncluster counts as we increase the training data size. If we\\nincrease the cluster count past the optimum, each expert\\nhas an insufﬁcient share of the data, resulting in worse\\nperformance.\\nNevertheless, we observe that using more clusters than opti-\\nmal for the highest token budget settings still outperforms\\nthe dense model. Since it is cheaper to train with more\\nclusters for a ﬁxed training data size due to parallelism, it\\nmay be preferable in some settings to train with a large num-\\nber of clusters despite their less-than-optimal performance.\\nBased on the trends we observe at this scale, we expect that\\nhigher cluster counts would become optimal as we scale the\\ntraining data size even further.\\nThe consistency of our results on C4 and S2ORC suggests\\nthat these general trends may be widely applicable to many\\ndatasets. However, the optimal number of clusters for a\\ngiven computational budget is likely dataset speciﬁc. Future\\nwork may explore relationships between dataset features\\nand the optimal cluster count.\\nThese trends are consistent as we increase the size of our\\nexperts to 6.7B parameters (Figure 7), although the gaps\\nbetween our baselines reduce, likely due to the substantial\\nincrease in pretraining FLOPs for OPT-6.7B.11\\n4.2. Comparing Training Time\\nNow, we turn to comparing our models based on training\\ntimes. We measure the speed of training each model with\\nthe maximum seconds-per-update for each training run.12\\nFor C-BTM models with more than one cluster, we use the\\nmaximum seconds-per-update across all experts. To make\\nour comparisons fair, we only compare the training times of\\nmodels that have the same effective batch size (§3.3). Our\\nresults are displayed in Figure 8. As we increase the number\\n11OPT-6.7B was pretrained for 1.83 ZFLOPs, while the OPT-\\n1.3B was trained for 0.34 ZFLOPS.\\n12Other measures of seconds-per-update (e.g., average, median)\\ntend to be noisy, due to factors such as dataloading and bursty\\nGPU activity.\\n1B\\n5B\\n10B\\n20B\\nToken Count\\n11.5\\n12.0\\n12.5\\n13.0\\n13.5\\nPerplexity\\nC4\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\n1\\n2\\n4\\n8\\n16\\n32\\nNumber of Clusters\\n11.5\\n12.0\\n12.5\\n13.0\\n13.5\\nPerplexity\\nC4\\nTokens\\n1.3B\\n2.6B\\n5.2B\\n10B\\n21B\\nFigure 7. Our results are consistent even as we increase expert\\nsize to 6.7B parameters (§4.1). 8 clusters is optimal at 21B to-\\nkens, as it is for the 1.3B parameter ELMs. However, the gaps\\nbetween these models are smaller, due to the substantial increase\\nin pretraining FLOPs for the OPT-6.7B checkpoint.\\nof clusters and training data size, the update speed for C-\\nBTM increases, since models with higher cluster counts\\nuse fewer GPUs per expert under a ﬁxed budget, and there\\nis no communication between experts. This suggests that\\nC-BTM models with more clusters can be exposed to more\\ndata for the same amount time as dense models.\\nAs discussed in §2.3, C-BTM also provides important prac-\\ntical speedups when training large LMs at scale. C-BTM\\ndivides large compute budgets among many models, such\\nthat we can train on 168B tokens with only 8 GPUs per\\nexpert in the 128-cluster setting. On shared multi-node clus-\\nScaling Expert Language Models with Unsupervised Domain Discovery\\n8\\n16\\n32\\n64\\nTotal GPUs\\n7\\n8\\n9\\n10\\nMax seconds/update\\nMoE\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\nFigure 8. Models trained with more clusters have faster up-\\ndates as we increase the total compute (§4.2). We display the\\nmaximum seconds-per-update for C-BTM and MoE models with\\nvarying GPU counts (across all experts). Under ﬁxed compute,\\ntraining with more clusters uses fewer GPUs per expert, and C-\\nBTM avoids communication between experts, resulting in faster\\nupdates. On the other hand, MoE models are much slower to train,\\ndue to extensive communication between experts (§6.3), as well as\\nadditional FLOPs from top-2 routing (Artetxe et al., 2021).\\nters, allocating many smaller jobs incurs shorter cumulative\\nwait times than a single, large synchronous job, since they\\ncan make more efﬁcient use of shared resources, and run on\\nshort-lived, idle nodes (Wortsman et al., 2022). Furthermore,\\nlarge LM training is prone to node failures, gradient spikes,\\nand other unexpected behaviors (Zhang et al., 2022). With\\ndense models, when one node fails, all nodes must restart\\ndue to synchronization. With C-BTM, experts are trained\\nindependently; if a node fails, only the corresponding expert\\nneeds to be restarted, and all other experts are unaffected.\\n4.3. Controlling for Inference Costs via Parameter\\nCount\\nComparing models with just training budgets ignores the\\nfact that C-BTM inference GPU-time costs grow as we\\nincrease the number of clusters, since we train more param-\\neters. To mitigate these costs, we can use the top-k function\\n(Equation 3) to dynamically use a subset of experts for each\\nincoming context during evaluation (§2.2). Next, we study\\nthe effect of inference parameter count on model perfor-\\nmance. We focus on the largest training budget (i.e., 168B\\ntokens) for these experiments.\\nResults (Figure 9) show that despite training many more\\nparameters, training C-BTM with many clusters and then us-\\ning only the top-1 expert still outperforms the dense model.\\nFurther, using the top-2 or top-4 experts yields comparable\\n1.3B\\n(top-1)\\n2.6B\\n(top-2)\\n5.2B\\n(top-4)\\n10B\\n(top-8)\\n20B\\n(top-16)\\n# of Inference Parameters\\n12.0\\n12.5\\n13.0\\n13.5\\n14.0\\nPerplexity\\nC4\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\n16 clusters\\n1.3B\\n(top-1)\\n2.6B\\n(top-2)\\n5.2B\\n(top-4)\\n10B\\n(top-8)\\n20B\\n(top-16)\\n# of Inference Parameters\\n10.4\\n10.6\\n10.8\\n11.0\\n11.2\\n11.4\\n11.6\\nPerplexity\\nS2ORC\\n1 cluster\\n2 clusters\\n4 clusters\\n8 clusters\\n16 clusters\\nFigure 9. Sparse top-k inference performance at 168B token\\nbudget (§4.3). ELMs perform well even with heavily sparsiﬁed\\ninference. Top-1 inference substantially outperforms the densely\\ntrained baseline at no additional inference cost, and top-2 and top-4\\ninference performs comparably to (and is sometimes slightly better\\nthan) activating all experts. In our setup, inference parameters are\\nproportional to inference FLOP count (§3.3).\\nperformance to activating all experts. Sometimes we ob-\\nserve that sparsifying can even slightly improve performance\\nover using all experts (for example, see the 16 cluster model\\nfor C4 in Figure 9). We speculate that having all experts\\nactive may introduce interference effects from experts that\\nare specialized to clusters unrelated to test-time contexts.\\nOur results in Figure 10 suggest that sparsifying even larger\\nexpert models (i.e., those with more clusters than the optimal\\nScaling Expert Language Models with Unsupervised Domain Discovery\\n1.3B\\n2.6B\\n5.2B\\n10B\\n20B\\n42B\\n84B\\n168B\\n# of Inference Parameters\\n12.00\\n12.25\\n12.50\\n12.75\\n13.00\\n13.25\\n13.50\\n13.75\\n14.00\\nPerplexity\\n1 cluster\\n2 clusters\\n4 clusters\\ntop-1\\ntop-2\\ntop-4\\ntop-16\\ntop-32\\ntop-64\\ntop-128\\nC4\\n16 clusters\\n32 clusters\\n64 clusters\\n128 clusters\\nFigure 10. Train large, then sparsify (§4.3). Despite using more\\nthan the optimal cluster count for the 168B token budget, sparsify-\\ning the 32, 64, and 128-cluster models with top-1, top-2, or top-4\\nexperts is usually better than training 1-, 2-, or 4-cluster models.\\nfor a given token budget) is still highly effective. At the\\nmost extreme setting, using the top-1 expert for the 128\\ncluster model (using 0.7% of total parameters at inference\\ntime for each context) still outperforms the dense model, and\\nthe top-4 expert model (3.1% of total parameters) performs\\ncomparably to using all experts.\\nThese results suggest that C-BTM results in a highly sparse\\nLM, and that inference costs can be kept constant even as\\nthe number of experts grows, though additional experts can\\nbe added to further boost performance.\\n4.4. Comparing to a Larger Dense Model\\nIn our ﬁnal comparison of this section, we consider both\\ntraining and inference costs together. We compare a 6.7B\\n1-cluster (dense) model and C-BTM model with 1.3B pa-\\nrameter experts, which uses 16 clusters (optimal in our ex-\\nperiments from §4.1) and top-4 inference, resulting in 5.2B\\ninference parameters. This C-BTM model has lower infer-\\nence cost than the larger 6.7B parameter dense model (§3.3).\\nThe former uses fewer inference parameters, incurring a\\nsmaller inference GPU-time cost, and has lower latency,\\ncomparable to that of a single 1.3B-parameter ELM.\\nWe compare the FLOPs used to train each model. Follow-\\ning Artetxe et al. (2021), we build continuous efﬁciency\\ncurves by interpolating between our empirical observations.\\nSpeciﬁcally, we calculate the speedup between our cluster\\nexpert models and dense model by interpolating between the\\ndiscrete observations of perplexity values for a given empir-\\nical number of FLOPs.13 Our goal is to identify the FLOP\\n13See §A.3 for details on this interpolation.\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nTotal ZFLOPs\\n12\\n14\\n16\\n18\\n20\\nPerplexity\\n3.5x speedup\\n  0 tokens \\n (zero-shot)\\n8B tokens\\n168B tokens\\n45B tokens\\n 20B tokens\\n 4B tokens\\n  0 tokens \\n (zero-shot)\\nC4\\n16 clusters (5.2B inference params)\\n1 cluster (6.7B inference params)\\nFigure 11. Training with C-BTM is substantially more efﬁ-\\ncient than training a larger dense LM (§4.4).\\nWe train a\\n16-cluster C-BTM model and use top-4 inference, resulting in\\na 5.2B parameter LM, and compare its performance with a 6.7B\\nparameter dense LM. The C-BTM model at 168B tokens achieves\\nthe same perplexity on C4 as a 6.7B dense model with 3.5x fewer\\nZFLOPs. The total ZFLOPs includes the cost of pretraining the\\nseed OPT checkpoints.\\ncount necessary to achieve a particular perplexity value. If\\nELMs trained with C-BTM achieve the same perplexity\\nas the dense model with half the FLOPs, we conclude that\\nC-BTM achieves a 2× speedup.\\nOur results are presented in Figure 11. A smaller C-BTM\\nmodel, exposed to 168B tokens of text, can achieve the\\nsame perplexity as the larger 6.7B dense model with 3.5×\\nspeedup. These speedup estimates are dependent on the\\namount of pretraining performed on each model. Future\\nwork may perform these experiments with larger models\\nand many more ELMs.\\n4.5. Summary\\nOur results demonstrate that controlling for a variety of\\ndifferent types of computational budget, C-BTM outper-\\nforms dense training in language modeling. Furthermore,\\nwe demonstrate that C-BTM results in an effective sparse\\nlanguage model, where the top-1, top-2 and top-4 experts\\nfrom models with at least 8 clusters signiﬁcantly outper-\\nform 1-cluster, 2-cluster and 4-cluster models. These results\\nsuggest the possibility of outperforming dense models by in-\\ncreasing margins, while keeping both training and inference\\ncosts ﬁxed, as compute and the number of experts grow.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nFew-shot Text Classiﬁcation Accuracy (%)\\nAGNews\\nDBPedia\\nSST-2\\nAmazon\\nPhrasebank\\nTwitter\\n↓Model (inference parameters)\\nTopic\\nTopic\\nSentiment\\nSentiment\\nSentiment\\nHatespeech\\nAverage\\nRandom chance\\n25.0\\n7.10\\n50.0\\n50.0\\n33.3\\n50.0\\n35.9\\nOPT (1.3B)\\n42.9\\n57.2\\n72.8\\n81.3\\n72.5\\n65.1\\n65.3\\nOPT (6.7B)\\n51.9\\n58.9\\n77.0\\n83.8\\n76.4\\n39.6\\n64.6\\nC4\\n1-cluster (1.3B)\\n47.4\\n61.1\\n80.2\\n80.7\\n66.6\\n60.9\\n66.2\\n1-cluster (6.7B)\\n68.1\\n62.4\\n80.7\\n84.9\\n80.6\\n37.4\\n69.0\\n16-cluster; top-1 (1.3B)\\n47.1\\n62.9\\n74.3\\n79.1\\n72.9\\n56.4\\n65.4\\n16-cluster; top-4 (5.2B)\\n49.3\\n62.3\\n80.0\\n81.3\\n78.7\\n61.3\\n68.8\\n16-cluster; top-16 (20.8B)\\n50.6\\n62.0\\n84.0\\n83.2\\n78.6\\n61.7\\n69.9\\nTable 1. C-BTM models outperform dense counterparts on downstream text classiﬁcation tasks (§5.2). We display accuracy of\\nmodels from §4 on six text classiﬁcation tasks, using eight demonstrations for each example and no additional ﬁne-tuning. We report\\naccuracy averaged over ﬁve random seeds. The 1- and 16-cluster models are trained on 168B tokens of C4 (i.e., our highest budget). The\\n16-cluster model, with top-4 or top-16 inference, always outperforms the 1-cluster model, and top-1 inference usually outperforms the\\n1-cluster model at no additional inference cost. We include average performance of models across tasks for readability.\\n5. Downstream Task Results\\nDo the trends from §4 extend to downstream settings? To be-\\ngin to answer this question, we perform few-shot evaluation\\non six downstream text classiﬁcation tasks. We indeed ﬁnd\\nthat models trained with C-BTM outperform their dense\\ncounterparts in these settings.\\n5.1. Experimental Setup\\nTasks\\nWe experiment with six text classiﬁcation tasks,\\nspanning topic, sentiment, and hatespeech classiﬁcation.\\nDetails of the datasets are in Appendix A.7.\\nFew-shot inference\\nWe perform 8-shot evaluations. For\\neach task, we randomly sample 8 examples with their la-\\nbels from the train set, and prepend them as demonstrations\\nfor each test example. For C-BTM models, we estimate\\nensemble weights for each example by passing both the\\nexample and the demonstrations through our pretrained clus-\\nterer (§2.2). We calculate the probability of each label for\\nthe task under the model, and report accuracy by counting\\nthe proportion of test examples where the gold label has\\nthe highest probability. We report average accuracy over 5\\nrandom seeds. We leave careful analysis of C-BTM with\\nvarying numbers of demonstrations and few-shot inference\\ntechniques to future work.\\nBaselines\\nWe compare the performance of 1- and 16-\\ncluster C-BTM models trained on 168B tokens of C4 (i.e.,\\nour highest budget from §4). For the 16-cluster model, we\\nalso perform top-1 and top-4 inference (§4.3). We addi-\\ntionally compare against a random baseline, the original\\nOPT-1.3B and 6.7B models (without any additional train-\\ning), and the 6.7B parameter 1-cluster model trained on 20B\\ntokens of C4.\\n5.2. Results\\nOur results in Table 1 show that the 16-cluster C-BTM\\nmodel always outperforms the 1-cluster, 1.3B parameter\\nbaseline, sometimes dramatically. This aligns with our lan-\\nguage modeling results (§4.1). The 1-cluster model achieves\\nlower accuracy than OPT-1.3B on some tasks despite addi-\\ntional training, suggesting that our models may suffer from\\ncatastrophic forgetting, since the C4 corpus is out-of-domain\\nto OPT.\\nNevertheless, the 16-cluster model outperforms OPT-1.3B\\non all tasks other than Twitter. Also, top-1 and top-4 infer-\\nence matches or exceeds using all experts in some settings,\\nconsistent with our language modeling results in §4.3. We\\nexamine the clusters associated with the most likely experts\\nfor each task, and ﬁnd that their top-terms are relevant to the\\ntask’s domain (Table 10 in the appendix). This supports our\\nhypothesis that C-BTM is able to leverage any part of the\\ncorpus which is in-domain to the test task, even if the train-\\ning corpus as a whole might be sufﬁciently out-of-domain\\nas to have a negative effect on performance.\\nWe then mirror the analysis in §4.4, by comparing our 16-\\ncluster models to 6.7B parameter dense models. First, we ob-\\nserve that our 1-cluster 6.7B model outperforms OPT-6.7B\\non all tasks except Twitter, possibly because this model has\\nhad less exposure to C4, and suffers from less catastrophic\\nforgetting. Our 16-cluster model performs comparably to\\nboth 6.7B models, and on multiple tasks, our 16-cluster\\nmodel outperforms both 6.7B models, which have been\\ntrained with at least 3.5× more compute (§4.4). With top-4\\ninference, C-BTM models activate even fewer parameters\\nthan the 6.7B parameter models, yet perform comparably.\\nThese results corroborate our ﬁndings in §4.4 that compared\\nto larger dense models, models trained with C-BTM have\\nmore training efﬁciency and lower inference latency, and\\nresult in comparable or better performance.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nIn separate experiments, we observe that routing examples\\nto experts based on their performance on few shot examples,\\nrather their clusters, results in even better downstream task\\nperformance with C-BTM models. This is likely because\\nfew-shot performance depends on factors such as example\\norder, label distributions, and the quality of the demonstra-\\ntions, which not necessarily tied to the domain of the task\\n(Min et al., 2022; Lu et al., 2022). We analyze this ﬁnd-\\ning further in §A.7, and leave more careful development of\\nrouting protocols for downstream tasks to future work.\\n5.3. Summary\\nWe demonstrate that, consistent with the language modeling\\nresults in §4.1, C-BTM improves downstream performance\\non a variety of few-shot text classiﬁcation tasks. C-BTM\\nmodels consistently outperform dense 1-cluster baselines,\\nand usually outperform the original OPT models, despite\\nbeing trained on an out-of-domain corpus. We also ﬁnd\\nthat top-k activation reduces inference costs with negligible\\neffects on downstream task performance. C-BTM models\\nperform comparably to larger, 6.7B OPT and 1-cluster dense\\nbaseline models, despite being trained with 3.5x less com-\\npute, and even when activating fewer inference parameters.\\n6. Comparing to Mixture-of-Experts\\nFinally, we compare C-BTM against an alternative sparse\\nLM, a mixture-of-experts (MoE) which learns a routing\\nbetween tokens and feedforward experts in the transformer\\n(Lepikhin et al., 2021; Fedus et al., 2021). As discussed in\\n§2.5, C-BTM is substantially simpler than MoE.\\n6.1. Sparse Upcycling\\nTo mirror C-BTM seed initialization, we initialize our MoE\\nwith a dense checkpoint. We use the sparse upcycling tech-\\nnique from Komatsuzaki et al. (2022). Upcycling a dense\\nmodel into an MoE with k experts entails initializing shared\\nparameters (e.g., attention and embedding layers) and k ex-\\npert parameters (e.g., every other feedforward layer) from\\na dense checkpoint, and initializing new parameters for the\\nrouter. Then the model is simply trained as an MoE. Here,\\nwe use top-2, token-level routing (Lepikhin et al., 2021).\\n6.2. Experimental Setup\\nHyperparameters\\nWe train an MoE with sparse upcy-\\ncling on C4, starting from OPT-1.3B and using the same\\ngeneral experimental setup detailed in §3.2. We follow the\\nsettings from Komatsuzaki et al. (2022) as closely as pos-\\nsible. We conducted experiments with 8, 16, 32, 64, and\\n128 experts for each compute budget. 8 and 16 experts are\\nsimilar to, but slightly worse than, 32 experts; 64 experts\\nand 128 experts consistently have exploding losses, and the\\n1B\\n10B\\n20B\\nNumber of tokens\\n14.0\\n14.5\\n15.0\\n15.5\\n16.0\\n16.5\\n17.0\\nPerplexity\\nC4\\ncBTM (1 Cluster)\\nMoE (32 experts)\\ncBTM (16 clusters)\\nFigure 12. MoE underperforms C-BTM (§6.3). We compare a\\n32-expert MoE with top-2 routing (Lepikhin et al., 2021) trained\\nwith sparse-upcycling (Komatsuzaki et al., 2022). While the MoE\\noutperforms C-BTM models with 16 experts at small budgets, it\\nfails at larger budgets, even under-performing the dense model. We\\nspeculate this could be due to distribution shifts after pretraining,\\nwhich might increase the instability of upcycling.\\nfew which successfully train are also similar to but slightly\\nworse than 32 experts. In general, we ﬁnd that both large\\nexpert count (and higher compute budgets) result in sparse\\nupcycling training instability.\\nWe use 32 experts in our MoE, a capacity factor of 2, and\\ncontinue training without resetting the optimizer from that\\nused during OPT pretraining. We set all hyperparameters to\\nbe the same as our C-BTM models (§3.2), except that we\\nuse a peak learning rate of 2e-5, which we found to be the\\nhighest learning rate that that did not result in divergence\\nafter a sweep. We release our code for sparse upcycling,\\nimplemented in Fairseq (Ott et al., 2019), publicly.14\\nBaselines\\nWe compare the 32-expert MoE LM to 1-cluster\\n(i.e., dense) and 16-cluster C-BTM models.\\n6.3. Results\\nThe MoE expends more FLOPs than the other models due to\\nthe additional feedforward layer at every other transformer\\nblock for top-2 routing, as well as the routing projection\\n(Artetxe et al., 2021). For clarity and consistency, we update-\\nmatch and separately report GPU-time cost of updates, as\\nin §4.1.\\nWe display results in Figure 12. MoE substantially under-\\nperforms C-BTM with 16 clusters as the compute budget\\n14https://github.com/kernelmachine/\\nmoe-fairseq\\nScaling Expert Language Models with Unsupervised Domain Discovery\\ngrows. Surprisingly, we observe that with enough compute,\\nMoE underperforms even the dense LM, and that when\\ncompute budgets are further increased, losses consistently\\nexplode. This suggests that sparse upcycling is highly un-\\nstable, possibly due to distribution shifts from pretraining.\\nIn Figure 8, we compare the maximum seconds-per-update\\nof the MoE model with that of the C-BTM models. MoE\\nbecomes substantially slower as more GPUs are used during\\ntraining. This is largely due to expensive all-to-all commu-\\nnication that occurs between experts during MoE training,\\nwhich is necessary to route tokens to experts (Artetxe et al.,\\n2021). On the other hand, our method does not have any\\nshared parameters between experts. Also, MoE expends\\nmore FLOPs during training than the C-BTM models. Fi-\\nnally, MoE still requires synchronous compute to train ex-\\nperts due to shared parameters, so they are also afﬂicted by\\nthe practical difﬁculties of training dense language models\\nat scale §4.2.\\n6.4. Summary\\nOur results suggest that language models trained with C-\\nBTM substantially outperform MoEs trained to the same\\nbudget. The performance gains of our technique likely are a\\nresult of the simplicity of our deterministic routing (based\\non empirically derived clusters), instabilities associated with\\nsparse upcycling, and other factors.\\n7. Analysis\\nIn §4, §5, and §6, we demonstrate that C-BTM outperforms\\ncompute-matched densely trained and MoE baselines. We\\nnow study our clustering approach in more detail and de-\\nscribe its effect on overall performance of C-BTM.\\n7.1. Is clustering important?\\nTo assess the importance of the clustering algorithm, we\\nperform C-BTM as above, except that we assign each doc-\\nument to a random cluster, rather than a learned one. This\\nis equivalent to the random ensemble baseline from Li et al.\\n(2022). Results in Figure 13 demonstrate that using random\\nclusters dramatically underperforms both our method and\\nthe dense baseline. Therefore, cluster specialization is vital\\nfor C-BTM. This conﬁrms results from Li et al. (2022),\\nwho found that domain specialization of ELMs is critical\\nfor performance the ensemble, as well as those from Jang\\net al. (2023), who show that instruction-specialized ELMs\\ntransfer to other tasks with similar instructions.\\n7.2. Is it important to balance the clusters?\\nApplying a balancing constraint in k-means avoids the de-\\ngenerate outcome of a long tail in cluster sizes (Chang et al.\\n1B\\n10B\\n20B\\n40B\\n80B\\nNumber of tokens\\n14.0\\n14.5\\n15.0\\n15.5\\n16.0\\n16.5\\nPerplexity\\nC4\\n1 cluster\\n8 random clusters\\n32 random clusters\\nFigure 13. Random clusters underperform (§7.1). Training ex-\\nperts on random clusters underperforms even the dense, single\\ncluster model, showing the importance of cluster specialization.\\nRandom clusters become more harmful as the cluster count grows.\\n20B\\n40B\\n80B\\nNumber of tokens\\n13.0\\n13.5\\n14.0\\n14.5\\nPerplexity\\nC4\\n1 cluster\\n32 unbalanced clusters\\n32 balanced clusters\\nFigure 14. Cluster balancing improves performance (§7.2).\\nWhen training on C4 with 32 clusters, balancing consistently im-\\nproves over the unbalanced version, suggesting that cluster size\\nimportant aspect to C-BTM.\\n2014). Indeed, with 10K documents of held out validation\\ndata in C4, we observe that balanced clustering signiﬁcantly\\nincreases the median cluster size, and narrows its range, rel-\\native to an unbalanced baseline (§A.4). To assess the effect\\nof balancing cluster size on the performance of C-BTM,\\nwe perform C-BTM with a k-means clustering model but\\nremove the balancing constraint. For the 8-cluster model,\\nwe observe that balancing has little effect. However, for the\\n32-cluster model (Figure 14), unbalanced clustering consis-\\ntently leads to worse performance. These results suggest\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nthat balancing becomes more important as one scales the\\nnumber of clusters. This is consistent with separate experi-\\nments that show that the long tail in cluster sizes becomes a\\nmore consistent problem with higher cluster counts.\\n7.3. Are clusters well deﬁned? Do experts specialize?\\nSince we use tf-idf as our document embedder in C-BTM,\\nwe can perform an inverse transform from the cluster centers\\ninto the vocabulary space to identify terms that most likely\\nwould have been embedded as the cluster center. We display\\nthe top ﬁve terms per cluster in §A.1. We observe that as the\\nnumber of clusters increases, the top terms across clusters\\nbecome more speciﬁc and varied.\\nNext, we study whether ELMs trained on these clusters spe-\\ncialize. Using the 32-cluster model trained on 84B tokens\\nof C4, we compute perplexity of all experts across 200 held\\nout documents in each cluster. For each cluster, we then\\nmeasure the ratio of the perplexity of each expert to the per-\\nplexity of the expert trained on that cluster. We display those\\nratios in Figure 15. We see that all experts perform best on\\ntheir own cluster. Some experts do not transfer well at all\\nto other clusters, while others do reasonably well. Cross\\nreferencing with the cluster term tables in §A.1, we see that\\ncluster experts 3 and 5 tend to generalize well and the top\\nterms in these clusters are more generic (with words such\\nas \"just, like, love\"). The experts specialized to content such\\nas \"site, page, website\" (cluster 0) and \"app, phone, video\"\\n(cluster 29), tend to do poorly on all other clusters.15 These\\nresults suggest that experts specialize to their cluster. We\\ninfer that the success of sparse C-BTM inference is a result\\nof expert specialization, and that C-BTM performance gains\\nmay be partially due to the sample efﬁciency of specialized\\ntraining.\\n7.4. How do clusters and metadata domains compare?\\nThe key motivation for C-BTM is to remove the reliance on\\nmetadata to delineate the domains to which ELMs specialize.\\nHow well do clusters reﬂect the dataset segmentation pro-\\nduced by metadata? We use S2ORC to study this question.\\nFirst, we align the learned clusters from a 32-cluster model\\nwith the ﬁelds-of-study metadata available from S2ORC\\n(Lo et al., 2019). Then we visualize the overlap between\\nthe metadata and clusters (Figure 16). We observe only a\\npartial alignment between metadata and clusters in S2ORC.\\nDocuments with some metadata labels (e.g., Enviromental\\nScience, Political Science) are mostly assigned to their own\\nclusters, while documents with other labels (e.g., Computer\\nScience, Physics) are distributed across multiple clusters.\\n15This result imply that cluster experts can be removed to ﬁlter\\nout unwanted generations after training, without signiﬁcantly im-\\npacting performance on other content. We leave such exploration\\nto future work.\\n0\\n2\\n4\\n6\\n8\\n10 12 14 16 18 20 22 24 26 28 30\\nCluster\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n28\\n30\\nExpert\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nperplexity increase\\nFigure 15. Experts specialize to their cluster (§7.3). Here, we\\nuse the 32-cluster model trained on 84B tokens of C4. Each cell\\nis a ratio between one expert’s test perplexity on a cluster to that\\nof the expert trained on that cluster. The diagonal indicates that\\nexperts perform best on the cluster they were trained on. Many\\nexperts transfer well across clusters, but some do not.\\nThe partial alignment between metadata and clusters sug-\\ngests that C-BTM models may not have the same perfor-\\nmance as those trained with metadata labels to delineate\\ndomains. To investigate this hypothesis further, we perform\\nexperiments using a subset of the Pile (Gao et al., 2021) to\\ncompare the performance of experts trained with metadata\\nand experts trained with clusters. See §A.5 for more details\\non this corpus. We observe that experts trained with learned\\nclusters perform slightly better than those with metdata la-\\nbels on a held out validation data (Table 8 in the appendix).\\nBoth techniques perform better than training with just a\\nsingle cluster on the Pile, conﬁrming our results from §4.1.\\nThese results imply that metadata may not correspond with\\nthe most optimal segmentation of the corpus. However,\\nusing metadata has the advantage of interpretability and\\nsimplicity, and metadata can identify domains that are not\\njust lexically driven (e.g., Lucy & Bamman, 2021; Guru-\\nrangan et al., 2022). Future work may explore combining\\nmetadata- and cluster-specialized ELMs.\\n7.5. Summary\\nOur analysis demonstrates that the improvements from C-\\nBTM are not the result of ensembling alone. Various com-\\nponents of our training method, particularly the nature of\\nthe learned clusters, play a critical role in C-BTM perfor-\\nmance. Improving the representation of domains in a corpus,\\nperhaps using other pretrained representations (Aharoni &\\nGoldberg, 2020) or more sophisticated clustering algorithms\\n(Ester et al., 1996; Chronopoulou et al., 2022), are likely to\\nimprove C-BTM performance.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nEnv. Science\\nPoli. Science\\nBusiness\\nGeology\\nMathematics\\nSociology\\nPhilosophy\\nHistory\\nGeography\\nArt\\nChemistry\\nBiology\\nmaterials\\nEconomics\\nPsychology\\nMedicine\\nEngineering\\nPhysics\\nComp. Science\\nMetadata\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n28\\n30\\nCluster\\n0\\n20\\n40\\n60\\n80\\n100\\n% overlap\\nFigure 16. Clusters and metadata do not perfectly align (§7.4).\\nEach cell in the heatmap is the % overlap between a cluster and\\na metadata label identifying the ﬁeld-of-study of a document in\\nS2ORC; high overlap indicates that most documents with the cor-\\nresponding label get assigned to the corresponding cluster. While\\ndocuments with certain labels (e.g., Environmental Science, Politi-\\ncal Science, Business) get primarily assigned to a single cluster,\\ndocuments with other labels (e.g., Engineering, Physics, Computer\\nScience) are distributed across multiple clusters.\\n8. Related Work\\nSparse Models\\nC-BTM is closely related to sparse mod-\\nels which activate only a subset of parameters (Evci et al.,\\n2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer,\\n2019). C-BTM is inspired by MoE, but is much simpler\\nand more efﬁcient to train. Most MoE methods rely on\\ntraining token-based routing mechanisms (Lepikhin et al.,\\n2021; Fedus et al., 2021; Lewis et al., 2021; Roller et al.,\\n2021), but others rely on task (Kudugunta et al., 2021) or\\ndomain (Gururangan et al., 2022) routing.\\nExpert Language Models\\nAs we note throughout the\\nstudy, this work is most directly related to BTM (Li et al.,\\n2022). BTM is in turn partially inspired by prior work on\\nvariations of MoE models (Jacobs et al., 1991), but espe-\\ncially DEMix layers (Gururangan et al., 2022), which re-\\nplace transformer feedforward layers with metadata-deﬁned\\ndomain experts. Jang et al. (2023) train expert language\\nmodels on instruction-based tasks, while Pfeiffer et al.\\n(2022) train expert language models on different languages.\\nCluster\\nRouting\\nChronopoulou\\net\\nal.\\n(2022)\\nand\\nChronopoulou et al. (2023) use hierarchical clustering to\\nidentify domains to specialize adapter experts to, and use the\\nadapters in an ensemble or parameter average at inference\\ntime. Duan et al. (2021) build ensembles of task-speciﬁc\\nmodels by clustering the training data of supervised tasks.\\nGross et al. (2017) employ a cluster-based router similar to\\nours in an image classiﬁcation setting using ResNets. How-\\never, they use a hard routing (or only activate a single expert)\\nin both training and inference, while we use hard routing\\nduring training but ensemble experts during inference. Our\\ninference technique is inspired by nearest neighbor retrieval\\nmechanisms in language models (Khandelwal et al., 2019;\\nShi et al., 2022).\\nCommunication-efﬁcient\\ntraining\\nOur\\nstudy\\ncon-\\ntributes to a line of research into communication-efﬁcient\\nalgorithms for training large models. Some previous work\\nproposes ways to train large dense models collaboratively\\nover distributed networks of servers (Borzunov et al., 2022;\\nYuan et al., 2022). Other works focus on new forms of\\ndata (Gan et al., 2021), model (Ryabinin et al., 2023),\\nand pipeline (Wang et al., 2022) parallelism to allow for\\nmodel training on heterogeneous devices that can recover\\nfrom node failures.\\nWortsman et al. (2022) propose a\\ncommunication-efﬁcient method of ﬁne-tuning by training\\na collection of models with different hyperparameters on\\nindividual GPU nodes, and then averaging their parameters\\nafter training.\\nOur work uses expert specialization for\\ncommunication efﬁcient training, and C-BTM can be\\ncombined with any of these other techniques to improve\\ntraining efﬁciency.\\n9. Conclusion\\nWe introduce C-BTM, a new technique to efﬁciently train\\nsparse LMs. C-BTM splits a corpus into k clusters, trains\\nan expert LM on each cluster, and creates a sparse ensemble\\nduring inference. We observe that the optimal number of\\nclusters for C-BTM increases with the amount of data, and\\nusing more clusters also allows us to aggressively parallelize\\ntraining to efﬁciently scale into massive datasets. Future\\nwork could investigate C-BTM in multitask or multilingual\\nsettings, the usefulness of multiple iterations of C-BTM on\\na corpus (perhaps with hierarchical clustering), or the possi-\\nbility of combining metadata- and cluster-based routing to\\nscale into many heterogeneous datasets in parallel.\\nAcknowledgements\\nThis paper beneﬁted from thoughtful feedback from a num-\\nber of people: Armen Aghajanyan, Tim Dettmers, Sneha\\nKudugunta, Stephen Roller, Swabha Swayamdipta, and\\nMitchell Wortsman.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nReferences\\nAharoni, R. and Goldberg, Y.\\nUnsupervised domain\\nclusters in pretrained language models.\\nIn Proceed-\\nings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, pp. 7747–7763, Online,\\nJuly 2020. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2020.acl-main.692.\\nURL https:\\n//aclanthology.org/2020.acl-main.692.\\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,\\nShleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R.,\\nAnantharaman, G., Li, X., Chen, S., Akin, H., Baines, M.,\\nMartin, L., Zhou, X., Koura, P. S., O’Horo, B., Wang, J.,\\nZettlemoyer, L., Diab, M., Kozareva, Z., and Stoyanov,\\nV. Efﬁcient large scale language modeling with mixtures\\nof experts, 2021. URL https://arxiv.org/abs/\\n2112.10684.\\nArthur, D. and Vassilvitskii, S. K-means++: The advantages\\nof careful seeding. In Proceedings of the Eighteenth\\nAnnual ACM-SIAM Symposium on Discrete Algorithms,\\nSODA ’07, pp. 1027–1035, USA, 2007. Society for Indus-\\ntrial and Applied Mathematics. ISBN 9780898716245.\\nBarbieri, F., Camacho-Collados, J., Espinosa Anke,\\nL., and Neves, L.\\nTweetEval:\\nUniﬁed bench-\\nmark and comparative evaluation for tweet classiﬁca-\\ntion.\\nIn Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2020, pp. 1644–1650, On-\\nline, November 2020. Association for Computational\\nLinguistics.\\ndoi:\\n10.18653/v1/2020.ﬁndings-emnlp.\\n148. URL https://aclanthology.org/2020.\\nfindings-emnlp.148.\\nBertsekas, D. P. Auction algorithms for network ﬂow prob-\\nlems: A tutorial introduction. Computational Optimiza-\\ntion and Applications, 1:7–66, 1992.\\nBorzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M.,\\nBelkada, Y., Chumachenko, A., Samygin, P., and Raf-\\nfel, C. Petals: Collaborative inference and ﬁne-tuning\\nof large models, 2022. URL https://arxiv.org/\\nabs/2209.01188.\\nChang, X., Nie, F., Ma, Z., and Yang, Y. Balanced k-means\\nand min-cut clustering, 2014. URL https://arxiv.\\norg/abs/1411.6235.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\\nG., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,\\nS., Michalewski, H., Garcia, X., Misra, V., Robinson,\\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,\\nH., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,\\nAgrawal, S., Omernick, M., Dai, A. M., Pillai, T. S.,\\nPellat, M., Lewkowycz, A., Moreira, E., Child, R., Polo-\\nzov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,\\nM., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.,\\nEck, D., Dean, J., Petrov, S., and Fiedel, N.\\nPalm:\\nScaling language modeling with pathways, 2022. URL\\nhttps://arxiv.org/abs/2204.02311.\\nChronopoulou, A., Peters, M., and Dodge, J. Efﬁcient hierar-\\nchical domain adaptation for pretrained language models.\\nIn Proceedings of the 2022 Conference of the North Amer-\\nican Chapter of the Association for Computational Lin-\\nguistics: Human Language Technologies, pp. 1336–1351,\\nSeattle, United States, July 2022. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2022.naacl-main.\\n96.\\nURL https://aclanthology.org/2022.\\nnaacl-main.96.\\nChronopoulou, A., Peters, M. E., Fraser, A. M., and\\nDodge, J. Adaptersoup: Weight averaging to improve\\ngeneralization of pretrained language models. ArXiv,\\nabs/2302.07027, 2023.\\nClark, A., Casas, D. d. l., Guy, A., Mensch, A., Paganini,\\nM., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,\\nBorgeaud, S., Driessche, G. v. d., Rutherford, E., Henni-\\ngan, T., Johnson, M., Millican, K., Cassirer, A., Jones,\\nC., Buchatskaya, E., Budden, D., Sifre, L., Osindero,\\nS., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K.,\\nand Simonyan, K. Uniﬁed scaling laws for routed lan-\\nguage models, 2022. URL https://arxiv.org/\\nabs/2202.01169.\\nDehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay,\\nY. The efﬁciency misnomer, 2021. URL https://\\narxiv.org/abs/2110.12894.\\nDettmers, T. and Zettlemoyer, L. Sparse networks from\\nscratch:\\nFaster training without losing performance.\\nCoRR, abs/1907.04840, 2019. URL http://arxiv.\\norg/abs/1907.04840.\\nDuan, Z., Zhang, H., Wang, C., Wang, Z., Chen, B., and\\nZhou, M. Enslm: Ensemble language model for data\\ndiversity by semantic clustering. In Annual Meeting of\\nthe Association for Computational Linguistics, 2021.\\nEster, M., Kriegel, H.-P., Sander, J., and Xu, X. A density-\\nbased algorithm for discovering clusters in large spatial\\ndatabases with noise. In Proceedings of the Second Inter-\\nnational Conference on Knowledge Discovery and Data\\nMining, KDD’96, pp. 226–231. AAAI Press, 1996.\\nEvci, U., Gale, T., Menick, J., Castro, P. S., and\\nElsen, E.\\nRigging the lottery:\\nMaking all tickets\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nwinners.\\nIn III, H. D. and Singh, A. (eds.), Pro-\\nceedings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Machine\\nLearning Research, pp. 2943–2952. PMLR, 13–18 Jul\\n2020. URL https://proceedings.mlr.press/\\nv119/evci20a.html.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\\ners: Scaling to trillion parameter models with simple and\\nefﬁcient sparsity. J. Mach. Learn. Res, 23:1–40, 2021.\\nFedus, W., Dean, J., and Zoph, B. A review of sparse\\nexpert models in deep learning, 2022. URL https:\\n//arxiv.org/abs/2209.01667.\\nGan, S., Lian, X., Wang, R., Chang, J., Liu, C., Shi, H.,\\nZhang, S., Li, X., Sun, T., Jiang, J., Yuan, B., Yang, S.,\\nLiu, J., and Zhang, C. Bagua: Scaling up distributed\\nlearning with system relaxations, 2021. URL https:\\n//arxiv.org/abs/2107.01499.\\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\\nPresser, S., and Leahy, C. The pile: An 800gb dataset of\\ndiverse text for language modeling, 2021. URL https:\\n//arxiv.org/abs/2101.00027.\\nGross, S., Ranzato, M., and Szlam, A. Hard mixtures of\\nexperts for large scale weakly supervised vision. In Pro-\\nceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 6865–6873, 2017.\\nGururangan, S., Lewis, M., Holtzman, A., Smith, N. A.,\\nand Zettlemoyer, L. DEMix layers: Disentangling do-\\nmains for modular language modeling. In Proceedings\\nof the 2022 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pp. 5557–5576, Seat-\\ntle, United States, July 2022. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2022.naacl-main.\\n407. URL https://aclanthology.org/2022.\\nnaacl-main.407.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., de Las Casas, D., Hendricks,\\nL. A., Welbl, J., Clark, A., et al.\\nTraining compute-\\noptimal large language models. 2022.\\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E.\\nAdaptive mixtures of local experts. Neural computation,\\n3(1):79–87, 1991.\\nJang, J., Kim, S., Ye, S., Kim, D., Logeswaran, L., Lee, M.,\\nLee, K., and Seo, M. Exploring the beneﬁts of training\\nexpert language models over instruction tuning, 2023.\\nURL https://arxiv.org/abs/2302.03202.\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,\\nand Lewis, M. Generalization through memorization:\\nNearest neighbor language models, 2019. URL https:\\n//arxiv.org/abs/1911.00172.\\nKomatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C. R.,\\nMustafa, B., Ainslie, J., Tay, Y., Dehghani, M., and\\nHoulsby, N.\\nSparse upcycling: Training mixture-of-\\nexperts from dense checkpoints, 2022. URL https:\\n//arxiv.org/abs/2212.05055.\\nKudugunta, S., Huang, Y., Bapna, A., Krikun, M., Lepikhin,\\nD., Luong, M.-T., and Firat, O. Beyond distillation: Task-\\nlevel mixture-of-experts for efﬁcient inference, 2021.\\nURL https://arxiv.org/abs/2110.03742.\\nLehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,\\nD., Mendes, P., Hellmann, S., Morsey, M., Van Kleef, P.,\\nAuer, S., and Bizer, C. Dbpedia - a large-scale, multilin-\\ngual knowledge base extracted from wikipedia. Semantic\\nWeb Journal, 6, 01 2014. doi: 10.3233/SW-140134.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\\nKrikun, M., Shazeer, N., and Chen, Z. {GS}hard: Scal-\\ning giant models with conditional computation and auto-\\nmatic sharding. In International Conference on Learning\\nRepresentations, 2021. URL https://openreview.\\nnet/forum?id=qrwe7XHTmYb.\\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettle-\\nmoyer, L. Base layers: Simplifying training of large,\\nsparse models, 2021. URL https://arxiv.org/\\nabs/2103.16716.\\nLi, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T.,\\nSmith, N. A., and Zettlemoyer, L. Branch-train-merge:\\nEmbarrassingly parallel training of expert language mod-\\nels, 2022. URL https://arxiv.org/abs/2208.\\n03306.\\nLo, K., Wang, L. L., Neumann, M., Kinney, R., and Weld,\\nD. S. S2orc: The semantic scholar open research cor-\\npus, 2019. URL https://arxiv.org/abs/1911.\\n02782.\\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stene-\\ntorp, P.\\nFantastically ordered prompts and where to\\nﬁnd them: Overcoming few-shot prompt order sensi-\\ntivity.\\nIn Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pp. 8086–8098, Dublin, Ireland,\\nMay 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.acl-long.556.\\nURL https:\\n//aclanthology.org/2022.acl-long.556.\\nLucy, L. and Bamman, D. Characterizing English vari-\\nation across social media communities with BERT.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nTransactions of the Association for Computational\\nLinguistics, 9:538–556, 2021.\\ndoi:\\n10.1162/tacl_\\na_00383.\\nURL https://aclanthology.org/\\n2021.tacl-1.33.\\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\\nA. Y., and Potts, C. Learning word vectors for sentiment\\nanalysis. In Proceedings of the 49th Annual Meeting\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies, pp. 142–150, Portland, Ore-\\ngon, USA, June 2011. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nP11-1015.\\nMalinen, M. I. and Fränti, P. Balanced k-means for cluster-\\ning. In International Workshop on Structural and Syntac-\\ntic Pattern Recognition, 2014.\\nMalo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala,\\nP. Good debt or bad debt: Detecting semantic orienta-\\ntions in economic texts. Journal of the Association for\\nInformation Science and Technology, 65, 2014.\\nMcCandlish, S., Kaplan, J., Amodei, D., and Team, O. D.\\nAn empirical model of large-batch training, 2018. URL\\nhttps://arxiv.org/abs/1812.06162.\\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,\\nHajishirzi, H., and Zettlemoyer, L.\\nRethinking the\\nrole of demonstrations: What makes in-context learn-\\ning work? In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing, pp.\\n11048–11064, Abu Dhabi, United Arab Emirates, De-\\ncember 2022. Association for Computational Linguis-\\ntics. URL https://aclanthology.org/2022.\\nemnlp-main.759.\\nMostafa, H. and Wang, X. Parameter efﬁcient training of\\ndeep convolutional neural networks by dynamic sparse\\nreparameterization. In Chaudhuri, K. and Salakhutdinov,\\nR. (eds.), Proceedings of the 36th International Confer-\\nence on Machine Learning, volume 97 of Proceedings of\\nMachine Learning Research, pp. 4646–4655. PMLR, 09–\\n15 Jun 2019. URL https://proceedings.mlr.\\npress/v97/mostafa19a.html.\\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\\nN., Grangier, D., and Auli, M. fairseq: A fast, extensible\\ntoolkit for sequence modeling, 2019. URL https://\\narxiv.org/abs/1904.01038.\\nPfeiffer, J., Goyal, N., Lin, X., Li, X., Cross, J., Riedel, S.,\\nand Artetxe, M. Lifting the curse of multilinguality by pre-\\ntraining modular transformers. In Proceedings of the 2022\\nConference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language\\nTechnologies, pp. 3479–3495, Seattle, United States, July\\n2022. Association for Computational Linguistics. doi:\\n10.18653/v1/2022.naacl-main.255.\\nURL https://\\naclanthology.org/2022.naacl-main.255.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-\\nmann, J., Song, F., Aslanides, J., Henderson, S., Ring,\\nR., Young, S., Rutherford, E., Hennigan, T., Menick,\\nJ., Cassirer, A., Powell, R., Driessche, G. v. d., Hen-\\ndricks, L. A., Rauh, M., Huang, P.-S., Glaese, A., Welbl,\\nJ., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Hig-\\ngins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E.,\\nJayakumar, S., Buchatskaya, E., Budden, D., Sutherland,\\nE., Simonyan, K., Paganini, M., Sifre, L., Martens, L.,\\nLi, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya,\\nE., Donato, D., Lazaridou, A., Mensch, A., Lespiau,\\nJ.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sotti-\\naux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama,\\nD., d’Autume, C. d. M., Li, Y., Terzi, T., Mikulik, V.,\\nBabuschkin, I., Clark, A., Casas, D. d. L., Guy, A.,\\nJones, C., Bradbury, J., Johnson, M., Hechtman, B., Wei-\\ndinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osin-\\ndero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K.,\\nStanway, J., Bennett, L., Hassabis, D., Kavukcuoglu,\\nK., and Irving, G. Scaling language models: Methods,\\nanalysis & insights from training gopher, 2021. URL\\nhttps://arxiv.org/abs/2112.11446.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Explor-\\ning the limits of transfer learning with a uniﬁed text-to-\\ntext transformer, 2019. URL https://arxiv.org/\\nabs/1910.10683.\\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. E.\\nHash layers for large sparse models. In Beygelzimer,\\nA., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.),\\nAdvances in Neural Information Processing Systems,\\n2021. URL https://openreview.net/forum?\\nid=lMgDDWb1ULW.\\nRyabinin, M., Dettmers, T., Diskin, M., and Borzunov, A.\\nSwarm parallelism: Training large models can be sur-\\nprisingly communication-efﬁcient, 2023. URL https:\\n//arxiv.org/abs/2301.11913.\\nShallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, J.,\\nFrostig, R., and Dahl, G. E. Measuring the effects of\\ndata parallelism on neural network training. 2018. doi:\\n10.48550/ARXIV.1811.03600. URL https://arxiv.\\norg/abs/1811.03600.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nShi, W., Michael, J., Gururangan, S., and Zettlemoyer, L.\\nknn-prompt: Nearest neighbor zero-shot inference, 2022.\\nURL https://arxiv.org/abs/2205.13792.\\nSvenstrup, D., Hansen, J. M., and Winther, O. Hash em-\\nbeddings for efﬁcient word representations, 2017. URL\\nhttps://arxiv.org/abs/1709.03933.\\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\\nGalactica: A large language model for science, 2022.\\nURL https://arxiv.org/abs/2211.09085.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,\\nE., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and\\nLample, G. Llama: Open and efﬁcient foundation lan-\\nguage models, 2023. URL https://arxiv.org/\\nabs/2302.13971.\\nWang, J., Yuan, B., Rimanic, L., He, Y., Dao, T., Chen,\\nB., Re, C., and Zhang, C. Fine-tuning language models\\nover slow networks using activation compression with\\nguarantees, 2022. URL https://arxiv.org/abs/\\n2206.01299.\\nWortsman, M., Gururangan, S., Li, S., Farhadi, A., Schmidt,\\nL., Rabbat, M., and Morcos, A. S. lo-ﬁ: distributed\\nﬁne-tuning without communication, 2022. URL https:\\n//arxiv.org/abs/2210.11948.\\nYang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X.,\\nFarhi, D., Ryder, N., Pachocki, J., Chen, W., and\\nGao, J.\\nTuning large neural networks via zero-shot\\nhyperparameter transfer. In Ranzato, M., Beygelzimer,\\nA., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.),\\nAdvances in Neural Information Processing Systems,\\nvolume 34,\\npp. 17084–17097. Curran Associates,\\nInc.,\\n2021.\\nURL\\nhttps://proceedings.\\nneurips.cc/paper/2021/file/\\n8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.\\npdf.\\nYuan, B., He, Y., Davis, J. Q., Zhang, T., Dao, T., Chen, B.,\\nLiang, P., Re, C., and Zhang, C. Decentralized training of\\nfoundation models in heterogeneous environments, 2022.\\nURL https://arxiv.org/abs/2206.01288.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-\\nhaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,\\nKoura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,\\nL. Opt: Open pre-trained transformer language mod-\\nels, 2022. URL https://arxiv.org/abs/2205.\\n01068.\\nZhang, X., Zhao, J., and LeCun, Y. Character-level convo-\\nlutional networks for text classiﬁcation, 2016.\\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,\\nDai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-\\nexperts with expert choice routing, 2022. URL https:\\n//arxiv.org/abs/2202.09368.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nA. Appendix\\nroom, home, kitchen\\npark, hotel, beach\\nart, design, image\\nmusic, album, band\\nsex, women, dating\\ncar, vehicle, road\\nmarket, company, year\\nuniversity, research, development\\npolitics, president, said\\nschool, student, education\\nday, event, year\\nbusiness, marketing, management\\nbooks, read, writing\\nreligion, life, people\\ngames, team, season\\nfood, recipe, minutes\\npatients, treatment, body\\nhealth, care, children\\nlove, great, like\\njust, like, know\\nblack, white, color\\nfilm, movie, story\\nproducts, online, quality\\nFile, window, click\\ndata, information, software\\nemail, contact, com\\nColossal Cleaned Crawled Corpus (C4)\\nlanguage, words, speech\\nstudents, learning, teachers\\nsocial, political, people\\nbook, research, social\\nuser, data, information\\nnetwork, node, algorithms\\nalgorithm, graph, vertices\\nimage, object, segmentation\\ntraining, learning, data\\ntheorem, let, proof\\nwater, soil, data\\nprotein, gene, DNA\\nquark, mass, Higgs\\nstars, galaxies, universe\\nequation, field, theory\\nflow, velocity, field\\nstress, strain, surface\\nquantum, spin, state\\nsurface, nm, film\\nlaser, beam, optical\\nparticipant, task, visual\\npower, noise, channel\\nchildren, women, health\\npatient, treatment, disease\\nstudy, risk, age\\ncell, expression, mice\\nSemantic Scholar Research Corpus (S2ORC)\\nFigure 17. Fully annotated UMAP visualization of 32 clusters in C4 and S2ORC. We annotate the clusters using an inverse transformation\\nfrom our cluster centers back into the tf-idf vocabulary space, identifying the most likely words to generate the cluster center. We display\\nthe top 3 terms associated with each cluster center here.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nA.1. Clusters\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\nﬁg\\nenergy\\nal\\net\\nﬁeld\\n1\\ndata\\npatients\\nsocial\\ntime\\nal\\nTable 2. Top terms associated with each cluster in S2ORC (2 clusters)\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\neq\\nenergy\\nquantum\\nequation\\nﬁeld\\n1\\nalgorithm\\nimage\\nmodel\\ndata\\nnoise\\n2\\ncells\\ncell\\nﬁg\\nal\\nprotein\\n3\\nstudents\\nsocial\\npeople\\nresearch\\neducation\\n4\\npatients\\npatient\\nstudy\\nparticipants\\nchildren\\n5\\nﬁg\\ntemperature\\nenergy\\nsurface\\nbeam\\n6\\nuser\\ndata\\nnode\\nnetwork\\nnodes\\n7\\net\\nal\\nstars\\ngalaxies\\nvelocity\\nTable 3. Top terms associated with each cluster in S2ORC (8 clusters)\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\nstudents\\nlearning\\nteachers\\nteaching\\neducation\\n1\\nlanguage\\nword\\nwords\\nspeech\\nenglish\\n2\\nnetwork\\nnode\\nnodes\\nnetworks\\nalgorithm\\n3\\nstudy\\nrisk\\nage\\ndata\\ngroup\\n4\\npower\\nnoise\\nchannel\\nsignal\\nfrequency\\n5\\nquantum\\nspin\\nstate\\nmagnetic\\nstates\\n6\\nparticipants\\ntask\\nvisual\\nstimulus\\net\\n7\\ntheorem\\nlet\\nproof\\nlemma\\nset\\n8\\ntraining\\nlearning\\ndata\\nnetwork\\nmodel\\n9\\nlaser\\nbeam\\noptical\\nﬁg\\npulse\\n10\\nprotein\\ngenes\\ngene\\ndna\\nproteins\\n11\\nwater\\nsoil\\ndata\\net\\nal\\n12\\nalgorithm\\ngraph\\nvertices\\nproblem\\nvertex\\n13\\nﬂow\\nvelocity\\nﬁeld\\nmagnetic\\nwave\\n14\\nuser\\ndata\\nusers\\ninformation\\nservice\\n15\\nstars\\ngalaxies\\net\\nal\\nstar\\n16\\nquark\\nmass\\ngev\\nhiggs\\nenergy\\n17\\net\\nal\\nbrain\\nneurons\\nﬁg\\n18\\nstress\\nstrain\\nsurface\\nshear\\nﬁg\\n19\\nimage\\nimages\\nalgorithm\\nobject\\nsegmentation\\n20\\nenergy\\nelectron\\nﬁg\\neq\\nstate\\n21\\nequation\\nﬁeld\\neq\\nequations\\ntheory\\n22\\npatients\\npatient\\ntreatment\\nstudy\\ndisease\\n23\\nmodel\\ntime\\nrobot\\nstate\\ncontrol\\n24\\nchildren\\nchild\\nwomen\\nhealth\\nsocial\\n25\\nsurface\\nnm\\nﬁlm\\nlayer\\ngraphene\\n26\\npatient\\npatients\\npain\\nsurgery\\ntreatment\\n27\\nsocial\\npolitical\\npeople\\npolicy\\npublic\\n28\\nsocial\\nparticipants\\nhealth\\nself\\npeople\\n29\\nbook\\nresearch\\nsocial\\ninformation\\ndata\\n30\\ntemperature\\nwater\\nheat\\nphase\\nthermal\\n31\\ncells\\ncell\\nexpression\\nmice\\nﬁg\\nTable 4. Top terms associated with each cluster in S2ORC (32 Clusters).\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\nlike\\njust\\ntime\\nnew\\ngreat\\n1\\nnew\\ninformation\\nuse\\nbusiness\\nservices\\nTable 5. Top terms associated with each cluster in C4 (2 clusters)\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\nstudents\\nhealth\\nresearch\\nschool\\nuniversity\\n1\\nmusic\\nnew\\nlove\\nﬁlm\\nart\\n2\\nsaid\\nyear\\nstate\\nteam\\nnew\\n3\\nbusiness\\ncompany\\nservices\\nservice\\nmarket\\n4\\nlike\\njust\\ntime\\ndon\\nreally\\n5\\nuse\\ninformation\\ndata\\npage\\nclick\\n6\\ndesign\\nblack\\nwhite\\ncolor\\nhigh\\n7\\nhome\\nwater\\nfood\\npark\\narea\\nTable 6. Top terms associated with each cluster in C4 (8 clusters)\\nCluster\\nTerm 0\\nTerm 1\\nTerm 2\\nTerm 3\\nTerm 4\\n0\\nsite\\npage\\nwebsite\\nweb\\nsearch\\n1\\nart\\ndesign\\nimage\\nimages\\ngallery\\n2\\nhealth\\ncare\\nchildren\\nchild\\nmedical\\n3\\njust\\nlike\\ndon\\nknow\\nve\\n4\\ndata\\ninformation\\nsoftware\\nuse\\nmanagement\\n5\\nlove\\ngreat\\nlike\\njust\\nreally\\n6\\ngame\\nteam\\ngames\\nseason\\nplay\\n7\\ninformation\\nemail\\ncontact\\ncom\\naddress\\n8\\npower\\nhigh\\nuse\\nlight\\nsteel\\n9\\nstudents\\nschool\\nstudent\\neducation\\nlearning\\n10\\nmarket\\ncompany\\nyear\\nﬁnancial\\ntax\\n11\\npatients\\ntreatment\\nbody\\ncancer\\npain\\n12\\nroom\\nhome\\nkitchen\\nbedroom\\nliving\\n13\\nmusic\\nalbum\\nband\\nsong\\nsongs\\n14\\ncar\\nvehicle\\ncars\\nnew\\nroad\\n15\\npark\\nhotel\\nbeach\\narea\\ntravel\\n16\\nday\\nevent\\nyear\\ntime\\nwedding\\n17\\nservice\\nservices\\nquality\\ncustomer\\nbest\\n18\\nbook\\nbooks\\nread\\nwriting\\nstory\\n19\\nﬁlm\\nmovie\\nstory\\nnew\\nlike\\n20\\nblack\\nwhite\\ncolor\\ndress\\nlook\\n21\\nbusiness\\ncompany\\nmarketing\\nmanagement\\ncustomers\\n22\\nuniversity\\nresearch\\ndevelopment\\neducation\\nscience\\n23\\ncity\\ncommunity\\ncounty\\nsaid\\npolice\\n24\\nsex\\nwomen\\nporn\\ndating\\ngirls\\n25\\nproducts\\nproduct\\nonline\\nquality\\norder\\n26\\nreligion\\nlife\\npeople\\njesus\\ntime\\n27\\nwater\\nskin\\nuse\\noil\\nlike\\n28\\nsaid\\npolitics\\npresident\\ngovernment\\nstate\\n29\\napp\\nphone\\nvideo\\nmobile\\ncasino\\n30\\nﬁle\\nwindows\\nuse\\nclick\\nsoftware\\n31\\nfood\\nadd\\nrecipe\\nminutes\\nwine\\nTable 7. Top terms associated with each cluster in C4 (32 Clusters).\\nA.2. Comparing FLOP counts via training data size\\nTo make fair comparisons across models with different numbers of ELMs, for a given number of clusters k and total GPU\\nbudget n, each ELM is allocated n/k GPUs. This keeps the total effective number of FLOPs ﬁxed across models exposed to\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nthe same number of tokens. We can show this analytically; following Artetxe et al. 2021, we calculate the number of FLOPs\\nto train a single ELM in our experiments:\\nFELM(T,k) = 96lh2T\\nk\\n\\x12\\n1 + s\\n6h +\\nV\\n16lh\\n\\x13\\nwhere T is the total training tokens (i.e., sequence length × batch size per GPU × number of GPUs), k is the number of\\nclusters, l is the number of layers, h is the hidden dimension, s is the sequence length, and V is the vocabulary.\\nTherefore, the total cost in FLOPs to train k ELMs with a particular model architecture (e.g., OPT-1.3B) on T tokens in\\naggregate is equivalent to that of a single dense LM of the same architecture trained on T tokens:\\nk · FELM(T,k) = FELM(T, 1)\\nThis means that even though C-BTM trains k times more parameters than an equivalent dense model, it does so at the\\nsame overall cost in FLOPs. So, our comparisons of models with various numbers of ELMs are fair, as long as they have\\nbeen exposed to the same number of training tokens and have the same underlying architecture for each ELM. Therefore,\\nthroughout the paper, we use training data size as a more interpretable metric of the overall compute budget.\\nA.3. Interpolating between empirical observations when comparing training costs and performance\\nWe interpolate between our empirical observations using the following function, proposed in Artetxe et al. 2021:\\nc(t) = exp(log clo(t) + r(log chi(t) −log clo(t)))\\nwhere r =\\nt−tlo\\nthi−tlo , thi and tlo are the closest empirical performances to t and clo(t) and chi(t) are their corresponding\\ntraining cost in FLOPs. We use this interpolation to compute the speedup factor cdense(t)/ccbtm(t).\\nA.4. Effect of cluster balancing on cluster sizes\\nUsing a held out set of 10K documents from C4, we ablate our balancing procedure from §2.1, and display a boxplot\\nshowing cluster sizes in Figure 18.\\nBalanced\\nUnbalanced\\nClustering type\\n0\\n200\\n400\\n600\\n800\\n1000\\nCluster size\\nFigure 18. Cluster balancing narrows the range, and increases the median size, of clusters (§7.2). Here, we ablate our balancing\\nprocedure from §2.1 on a 10K held-out documents in C4.\\nA.5. The Pile experiments\\nFor the experiment in §7.4, we additionally train on The Pile, which is a publicly available corpus of diverse language\\nmodeling datasets. We use the ﬁltered version from the OPT pretraining data (Zhang et al., 2022), and subselect 8 datasets\\nof the 13 used for OPT pretraining, which enabled easier experimentation §7.4. These 8 datasets include: Common\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nCrawl, HackerNews comments, Reddit comments16, the Gutenberg corpus, STORIES corpus, OpenWebText2, Deepmind-\\nMathematics, and English Wikipedia. In aggregate, these datasets consist of 420M documents, totaling 116B BPE tokens.\\nFor evaluation, we sample an equal number of documents from each constituent dataset. We also train a k=8 clustering\\nmodel on this dataset with one shard, or 1.5M documents.\\n5.2B Tokens\\n8 Metadata\\n8 Clusters\\n1 Cluster\\n8.4\\n8.3\\n8.5\\nTable 8. Experts trained with clusters perform slightly better than experts trained with metadata (§7.4). Here, we train each model\\nfor 5.2B tokens on 8 corpora of the Pile, and evaluate perplexity on a held out random sample of the constituent datasets. See §A.5 for\\nmore details on the dataset.\\nA.6. Sparsity\\nDense\\n2-cluster\\n8-cluster\\n32-cluster\\ntop-1\\ntop-2\\ntop-1\\ntop-2\\ntop-4\\ntop-8\\ntop-1\\ntop-2\\ntop-4\\ntop-8\\ntop-16\\ntop-32\\nTEMPERATURE\\n0.01\\n13.82\\n13.64\\n13.60\\n13.64\\n13.50\\n13.49\\n13.49\\n13.55\\n13.45\\n13.45\\n13.45\\n13.45\\n13.45\\n0.05\\n-\\n-\\n13.51\\n-\\n13.32\\n13.25\\n13.25\\n-\\n13.25\\n13.19\\n13.17\\n13.17\\n13.17\\n0.1\\n-\\n-\\n13.50\\n-\\n13.27\\n13.22\\n13.24\\n-\\n13.16\\n13.05\\n13.00\\n13.01\\n13.01\\n0.2\\n-\\n-\\n13.54\\n-\\n13.29\\n13.34\\n13.50\\n-\\n13.14\\n13.06\\n13.07\\n13.07\\n13.28\\n0.3\\n-\\n-\\n13.59\\n-\\n13.33\\n13.45\\n13.72\\n-\\n13.17\\n13.15\\n13.27\\n13.54\\n13.78\\n0.5\\n-\\n-\\n13.64\\n-\\n13.38\\n13.59\\n13.97\\n-\\n13.22\\n13.30\\n13.57\\n14.02\\n14.46\\n1\\n-\\n-\\n13.69\\n-\\n13.43\\n13.73\\n14.19\\n-\\n13.30\\n13.49\\n13.89\\n14.46\\n15.04\\nTable 9. Results with the dense (1-cluster), 2-cluster, 8-cluster, and 32-cluster C4 models trained on 84B tokens when varying the\\ntemperature (T) and topk hyperparameters. Optimal performance for almost every model and topk value is achieved at T = 0.1.\\nA.7. Downstream tasks\\nTasks\\nWe experiment with six text classiﬁcation tasks which span topic classiﬁcation (AGNews; Zhang et al. 2016 and\\nDBPedia; Lehmann et al. 2014); sentiment analysis (SST-2; Maas et al. 2011, Amazon; Zhang et al. 2016, and Phrasebank\\nMalo et al. 2014); and hatespeech detection (Twitter; Barbieri et al. 2020).\\nPerformance Routing\\nWe introduce additional routing procedures for few-shot text classiﬁcation, as the clustering\\nmethod described in §2.2 ignores the signiﬁcance of labels and does not take into account the context’s word order, which\\nmay be crucial when the context contains demonstrations for a downstream task. Further, the speciﬁc ordering of in-context\\ndemonstrations is known to affect model performance (Lu et al., 2022). There is not sufﬁcient evidence to suggest that the\\nrelative rank of different models on a task stays constant through these performance variances; thus it may be important\\nto route to experts differently depending on demonstration example order. To take into account the order of tokens in the\\ncontext, we introduce 3 variations on routing, based on expert performance on the end task, which take inspiration from the\\nmixture probability estimation methods of Gururangan et al. (2022); Li et al. (2022).\\nTo perform Fixed Performance Routing with demonstrations and validation set examples, we select 8 examples randomly\\nfrom the validation set, such that there is no overlap with the 8 demonstration examples used in the context at test time. We\\nconcatenate the 8 demonstrations with one validation set context and evaluate the accuracy of each ELM on the sequence,\\nrepeating for each of the 8 examples from the validation set. The ﬁnal routing probability distribution over experts for this\\ntask is determined by a softmax over the average accuracy of the model over the 8 examples. We use this ﬁxed probability\\ndistribution for all test examples.\\nTo perform Fixed Performance Routing with only demonstrations, we adapt the procedure above such that no validation\\nexamples are necessary. Instead, we take a random permutation of the 8 demonstration examples. We remove the label of\\nthe last, such that we effectively use the ﬁrst 7 as demonstrations, and rely on the last example to estimate performance.\\n16Reddit comments, like the rest of these datasets, are not collected by us but were part of the Pile (Gao et al., 2021), a publicly available\\nthird-party dataset.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nTask\\n1st\\n2nd\\n3rd\\nAGNews\\ngame, team, season\\nsaid, government, president\\nbusiness, company, market\\nDBPedia\\nstudents, school, university\\nsaid, government, president\\ncity, park, hotel\\nSST-2\\nbook, ﬁlm, life\\nlove, family, great\\nmusic, art, band\\nAmazon\\nbook, ﬁlm, life\\njust, like, know\\ngame, team, season\\nPhrasebank\\nbusiness, company, market\\nservice, customer, products\\nsaid, government, president\\nTwitter\\ndata, software, download\\nclick, website, page\\njust, like, know\\nTable 10. Top-terms of clusters associated with top-3 experts for each classiﬁcation task (§5.2) By inspection, the highest probability\\nexperts are usually quite relevant to task’s domain.\\nWe repeat this 8 times, generating a new random permutation each time, and once again take the softmax over the average\\naccuracy of the model for each permutation, ﬁxing this distribution for all test examples.\\nFinally, we have Updating Performance Routing, in which no estimations are done before test-time. At test time, we begin\\nwith a uniform probability over all experts, which we update with an exponential moving average with each test example:\\nafter each example (prepended with the 8 demonstrations), we update the expert probabilities with the softmax over the\\naccuracy of each expert on that example. Once again, this distribution is ﬁxed for all test examples.\\nResults\\nFull results, in Table 11, show that Fixed Performance Routing with demonstrations and validation set examples\\nachieves the best performance overall, with optimal performance occurring at top-4 expert activation, which we also found in\\nthe language modeling results of §4.1. Both Fixed Performance Routing methods perform best with top-4 expert activation,\\nand only suffer small performance degradations when reduced to top-1 expert activation. This aligns well with the patterns\\nobserved in §4.1, which further supports the incorporation of end task performance in routing when adapting to new tasks,\\neven when we base this evaluation only on the demonstration examples – that is, without any additional data. We leave for\\nfuture work further tuning of the optimal settings for Performance Routing.\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nFew-shot Text Classiﬁcation Accuracy (%)\\nAGNews\\nDBPedia\\nSST-2\\nAmazon\\nPhrasebank\\nTwitter\\n↓Model (inference parameters)\\nTopic\\nTopic\\nSentiment\\nSentiment\\nSentiment\\nHatespeech\\nAverage\\nRandom chance\\n25.0\\n7.10\\n50.0\\n50.0\\n33.3\\n50.0\\n35.9\\nOPT (1.3B)\\n42.9\\n57.2\\n72.8\\n81.3\\n72.5\\n65.1\\n65.3\\nOPT (6.7B)\\n51.9\\n58.9\\n77.0\\n83.8\\n76.4\\n39.6\\n64.6\\n1-cluster (1.3B)\\n47.4\\n61.1\\n80.2\\n80.7\\n66.6\\n60.9\\n66.2\\n1-cluster (6.7B)\\n68.1\\n62.4\\n80.7\\n84.9\\n80.6\\n37.4\\n69.0\\nCluster Routing\\n16-cluster; top-1 (1.3B)\\n47.1\\n62.9\\n74.3\\n79.1\\n72.9\\n56.4\\n65.4\\n16-cluster; top-4 (5.2B)\\n49.3\\n62.3\\n80.0\\n81.3\\n78.7\\n61.3\\n68.8\\n16-cluster; top-16 (20.8B)\\n50.6\\n62.0\\n84.0\\n83.2\\n78.6\\n61.7\\n69.9\\nUpdating Performance Routing\\n16-cluster; top-1 (1.3B)\\n54.5\\n63.4\\n83.4\\n83.6\\n74.7\\n64.8\\n63.3\\n16-cluster; top-4 (5.2B)\\n51.6\\n61.5\\n88.6\\n83.7\\n80.7\\n65.3\\n68.8\\n16-cluster; top-16 (20.8B)\\n51.1\\n60.4\\n86.0\\n83.5\\n79.8\\n62.9\\n69.1\\nFixed Performance Routing (8 demonstrations)\\n16-cluster; top-1 (1.3B)\\n45.3\\n61.9\\n81.2\\n83.6\\n76.4\\n60.1\\n68.1\\n16-cluster; top-4 (5.2B)\\n51.2\\n60.9\\n81.4\\n83.0\\n80.5\\n60.9\\n69.6\\n16-cluster; top-16 (20.8B)\\n50.6\\n60.2\\n84.1\\n83.5\\n79.1\\n60.5\\n69.6\\nFixed Performance Routing (8 demonstrations + 8 validation examples)\\n16-cluster; top-1 (1.3B)\\n54.5\\n63.4\\n83.4\\n83.6\\n74.7\\n64.8\\n70.7\\n16-cluster; top-4 (5.2B)\\n51.6\\n61.5\\n88.6\\n83.7\\n80.7\\n65.3\\n71.9\\n16-cluster; top-16 (20.8B)\\n51.1\\n60.4\\n86.0\\n83.5\\n79.8\\n62.9\\n70.6\\nTable 11. C-BTM models with performance routing achieve even better performance on downstream tasks (§A.7). We display\\nperformance of models on six text classiﬁcation tasks, using eight demonstrations for each example and no additional ﬁne-tuning. We\\ncompare our cluster routing method (described in §5) to variants of performance routing (described in §A.7). Fixed performance routing\\nwith 8 demonstrations and 8 validation examples usually gets the best performance on downstream tasks, consistently outperforming even\\nthe 6.7B parameter baselines. Fixed performance routing with top-4 inference always improves performance over using all experts, and\\ntop-1 inference does substantially better than the dense baselines at no additional inference costs. We include average performance across\\ntasks for readability.\\n', 'source_name': 'Scaling Expert Language Models with Unsupervised Domain Discovery', 'source_url': 'https://arxiv.org/abs/2303.14177'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Benefits_of_ELMs_NOTES.pdf #24\n",
      "{'content': 'Exploring the Benefits of Training Expert Language Models over Instruction Tuning \\nMain Idea: this paper looks to explore the author’s discovery that training an expert LM fine-\\ntuned on a single task can outperform a multi-task (MT) LM trained on hundreds of tasks (more \\nspecifically regarding multi-task performance). This goes against other findings that claim that \\nscaling the number of tasks in MT-LMs is key to making them have stronger performance. \\nReferring to LMs fine-tuned on a single task means a system of multiple Expert Language Models \\n(ELMs), each fine-tuned on a single task, not a single LM trained on a single task. \\nOBS: Instruction-tuning -> fine-tuning LMs with instructions (prompts). \\n \\nELM Framework \\n- \\nTraining experts – two types of experts are trained: \\no Prompt Experts \\n▪ Trained via PEFT through an adapter (an adapter layer is trained on top of \\nthe pre-trained LLM, with the pre-trained LLM’s weights kept frozen). \\n▪ Trained to perform well on a single prompt specific to the task. \\no Dataset Experts \\n▪ Trained via regular fine-tuning of the pre-trained LLM’s weights on a single \\ntask (all weights are updated). \\n▪ Idea is to train an expert that will perform well to different prompts, so it \\ncan merge with other experts. \\n- \\nRouting mechanism – Retrieval-of-Experts \\no Consists of constructing an Expert Library and training a dense retriever. \\n▪ Each row in the Expert Library corresponds to an expert and consists of \\nkeys of S random training instances of that expert and a corresponding \\nexpert id. \\n• S used was 100. \\n▪ The dense retriever is a Sentence Transformer, and it also assumes that Q \\nexamples of the target task are available. It takes the embeddings of the \\ninput task and chooses the most relevant expert(s) based on each expert’s \\nsimilarity to this input task (based on the training instances stored for each \\nexpert and the target task instances). \\n• Q used was 32. \\n- \\nMerging of experts \\no The merging of Dataset Experts is also explored, retrieving more than one expert \\nfor an unseen task. \\n▪ Merging does not make sense with Prompt Experts, since they were \\ntrained to perform well on a single prompt, therefore they would not be \\nperformant at this setting (at merging). \\no The merged LM ends up being created at the parameter level. It is a weighted-\\naverage (parameter-average) of the selected experts. \\n▪ Since the parameters are merged, the inference cost will be the same as \\nthe inference cost of the single MT-LM trained on hundreds of tasks. \\n \\nExperimental Setup \\n- \\n296 Prompt Experts, 36 Dataset Experts (on around 8 prompts each) trained. \\n- \\n50,000 samples used for training each classification task. 10,000 for each generative task. \\no On top of the pre-trained T5 model. \\n- \\n5 epochs used for training with a constant learning rate of 1e-4. \\n- \\nRouge-L score used for evaluating generative tasks. \\nResults – Prompt Experts \\n1. A single Prompt Expert significantly outperforms its dense MT-LM baseline (trained on \\nhundreds of tasks). \\na. The single Prompt Expert that achieved this was trained on CosmosQA. \\nb. Perhaps this means that the dataset being diverse is more important than the \\nnumber of tasks trained? \\n2. The Retrieval-of-Experts (ROE) method with an oracle gate significantly outperforms all \\nother models, including T0-11B (the base LLM used for the adapters was T5-3B) and GPT-\\n3. \\na. This shows that improving the retrieval method is a promising area of future \\nresearch. \\n3. A simple ROE approach outperforms T0-3B (the MT-LM baseline) on classification tasks, \\nbut not on generative tasks. \\na. A better ROE method reverses this. \\nb. Using more diverse data (in quantity) seems to help seems to help generative tasks \\n(perhaps due to the higher complexity in text generation compared to \\nclassification?). \\nResults – Dataset Experts \\n1. There was negative task transfer when merging the adapter experts (Prompt Experts). \\na. Merging Prompt Experts results in worse performance – does not work. \\n2. Merging the fully fine-tuned experts (Dataset Experts) resulted in positive task transfer. \\na. Merging resulted in improved performance (merged capabilities > individual \\ncapabilities). \\n- \\nThe 3 datasets that show the best performance on unseen tasks (when training on a single \\ntask) are all commonsense reasoning datasets (for both merging and not merging). \\no Points to models trained on commonsense reasoning having higher generalization \\nabilities to unseen tasks – commonsense reasoning data is higher quality data. \\n- \\nRetrieval of the correct expert(s) seems important as the best expert on unseen \\ngenerative tasks performed poorly on unseen classification tasks. \\n \\nBenefits of Expert LMs over MT-LMs \\n- \\nELMs are less susceptible to negative task transfer on seen tasks (the tasks used for \\ntraining). \\n- \\nELMs have continual learning abilities on new tasks without needing access to all the data \\nat the same time. \\n- \\nELMs allow for merging experts on compositional instructions (merging of task prompts). \\nLimitations of ELMs over MT-LMs \\n- \\nThe method explored assumes a batch of the target task is available for RoE, which is not \\nalways a realistic assumption. \\n- \\nMT-LMs bigger than 11B parameters, which might not suffer from negative task transfer \\ndue to increased capacity, were not analyzed. \\n- \\nFor some tasks, merging experts on compositional instructions may not be so simple. \\n \\nMy takeaways: \\n- \\nA system of ELMs outperforming a single LM in a multi-task setting seems to show that \\nthe benefits of specialization outweigh the benefits of shared knowledge between tasks. \\no An ELM system also allows for choosing an expert trained on a task that resembles \\nthe target data – ensemble of closely-related experts sounds, in theory, better \\nthan a single LM fine-tuned on multiple tasks (that could be both relevant and \\nirrelevant to the target task). \\n- \\nMore exploration is needed in the Retrieval-of-Experts (routing mechanism used) to \\nalleviate the constraint of having training and target instances stored, as well as to \\nappropriate it to scenarios where we do not have examples of the target task available \\nsince this task would be unknown. \\n \\n', 'source_name': 'Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Benefits_of_ELMs_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MoE_Mamba.pdf #25\n",
      "{'content': 'MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nMaciej Pi´\\noro 1 2 Kamil Ciebiera 1 3 Krystian Kr´\\nol 1 3 Jan Ludziejewski 1 3 Sebastian Jaszczur 1 3\\nAbstract\\nState Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the\\ndominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-\\nbased LLMs, including recent state-of-the-art open-source models. We propose that to unlock the potential of\\nSSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model\\nthat achieves remarkable, Transformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and\\nTransformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in 2.2x less training\\nsteps while preserving the inference performance gains of Mamba against the Transformer.\\nFigure 1. Log perplexity throughout the training of different methods. From top to bottom: Transformer; Mamba interleaved with\\nfeed-forward layers (Mamba-MLP); Transformer-Moe; Vanilla Mamba; MoE-Mamba.\\n1. Introduction\\nState Space Models (SSMs), e.g. (Gu et al., 2021; 2022; Gu & Dao, 2023), have recently been gaining attention as a possible\\nalternative to Transformers due to linear-time inference, parallelizable training, and strong performance on long-context\\ntasks. In particular, Mamba introduced in (Gu & Dao, 2023) achieves excellent results through the use of selective SSMs\\nand hardware-aware design, being a promising alternative to the attention-based Transformer architecture.\\nIn this paper, we advocate that to unlock the potential of SSMs for scaling up, they should be combined with Mixture of\\nExperts (MoE). MoEs (Fedus et al., 2022; Sanseviero et al., 2023) are efficient techniques that are now routinely used for\\nscaling up Transformers, e.g., in the recent Mixtral model (Mistral, 2023).\\nContributions: Maciej integrated Mamba into the codebase, ran preliminary experiments, and oversaw the course of the project. Kamil\\nran the bulk of the experiments. Krystian explored alternative Mamba block designs with Jan’s help. Sebastian supervised the project,\\nsetting the research direction and leading experiments and analyses. 1IDEAS NCBR 2Polish Academy of Sciences 3University of Warsaw.\\nCorrespondence to: Sebastian Jaszczur <s.jaszczur@uw.edu.pl>.\\n1\\narXiv:2401.04081v1  [cs.LG]  8 Jan 2024\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nWe introduce MoE-Mamba, a model that combines Mamba with a Mixture of Experts layer. MoE-Mamba enables efficiency\\ngains of both SSMs and MoE. We also show that MoE-Mamba acts predictably when the number of experts varies (Section\\n5).\\nOur experiments, see Figure 1, confirm that MoE-Mamba requires 2.2x less training steps to achieve the same performance\\nas Mamba and shows potential gains over Transformer and Transformer-MoE. The preliminary results indicate a very\\npromising research direction that may allow scaling SSMs to tens of billions of parameters.\\n2. Related Work\\n2.1. State Space Models\\nState Space Models (SSMs) form a family of architectures used for sequence modeling. Stemming from the field of control\\ntheory, these models can be seen as a combination of RNNs and CNNs (Gu & Dao, 2023). Although they potentially offer\\nconsiderable benefits, a number of issues have been identified with SSMs (Gu et al., 2022), preventing SSMs from becoming\\nthe leading architecture in the task of language modeling. However, recent breakthroughs (Gu et al., 2022; Fu et al., 2023;\\nSmith et al., 2023), have allowed deep SSMs to be scaled to billions of parameters while retaining computational efficiency\\nand strong performance.\\n2.2. Mamba\\nBuilding on SSMs, Mamba (Gu & Dao, 2023) offers linear-time inference (with respect to the context length) and an\\nefficient training process via hardware-aware design. By employing a work-efficient parallel scan, Mamba mitigates the\\nimpact of the sequential nature of recurrence, whereas fusing GPU operations removes the requirement to materialize the\\nexpanded state. Intermediate states necessary for backpropagation are not saved but instead recomputed during the backward\\npass, thus reducing memory requirements. The advantages of Mamba over the attention mechanism are especially prominent\\nduring inference, as not only the computational complexity is lowered, but also the memory usage is not dependent on the\\ncontext length.\\nMamba addresses the fundamental trade-off between efficiency and effectiveness in sequence models, emphasizing the\\nsignificance of state compression. Efficient models necessitate a small state, while effective models require a state containing\\nall crucial information from the context. Departing from other SSMs’ requirements of time and input invariance, a selection\\nmechanism is introduced, controlling how information propagates along the sequence dimension. This design choice\\nis inspired by intuition derived from synthetic tasks such as selective copy and induction heads, allowing the model to\\ndifferentiate and retain essential information while filtering out the irrelevant.\\nMamba’s performance is showcased through its ability to efficiently utilize longer contexts (up to 1M tokens), with\\nimproved pretraining perplexity as the context length increases. The Mamba model, consisting of a stack of Mamba blocks,\\nachieves very strong performance across diverse domains (NLP, genomics, audio), matching or exceeding the performance\\nof established Transformer models. Thus, Mamba emerges as a promising candidate for a general sequence modeling\\nbackbone.\\n2.3. Mixture of Experts\\nMixture of Experts (MoE) is a class of techniques that allow drastically increasing the number of parameters of a model\\nwithout much impact on the FLOPs required for the model’s inference and training. Introduced in (Jacobs et al., 1991),\\nMoE was applied in the context of NLP by (Shazeer et al., 2017).\\nMoE models benefit from sparse activation - for each token processed, only a subset of the model’s parameters is used.\\nDue to their computational demands, feed-forward layers in Transformers have become the standard target of various MoE\\ntechniques (Lepikhin et al., 2020; Fedus et al., 2022).\\nA number of approaches have been proposed to address the core problem of MoE, i.e., the process of assigning tokens\\nto experts (routing). Two basic routing algorithms include Token Choice (Shazeer et al., 2017) (each token is routed to a\\nconstant number of experts K) and Expert Choice (Zhou et al., 2022) (the number of tokens routed to each expert is constant\\nacross experts). Switch (Fedus et al., 2022) is a Token Choice architecture that routes each token to a single expert (K = 1)\\nand has successfully been used to scale Transformers up to 1.6T parameters. In our experiments, we follow this MoE design.\\n2\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nMore recently, MoE models have found their way onto the open-source scene (Xue et al., 2023; Fedus et al., 2022). In\\nparticular, Mistral has open-sourced Mixtral 8×7B (Mistral, 2023) that fares comparably to LLaMa 2 70B (Touvron et al.,\\n2023) while requiring only around 1/6th of its inference computational budget.\\n3. Model Architecture\\nAlthough the main underlying mechanism of Mamba differs significantly from the attention mechanism used in Transformers,\\nMamba retains the high-level, block-based structure of Transformer models. In this paradigm, identical blocks comprising\\none or more layers are stacked one after another, with each layer’s output being added to the residual stream (Figure 2). The\\nfinal value of the residual stream can subsequently be used to predict the next token in the language modeling task.\\nIn our design, we leverage the compatibility of the two architectures. In MoE-Mamba, every other Mamba layer is replaced\\nwith a MoE feed-forward layer based on Switch (Fedus et al., 2022), as shown in Figure 2. We note some similarities of this\\ndesign to one of the approaches explored by (Gu & Dao, 2023), in which interleaving Mamba layers with feed-forward\\nlayers resulted in a small decrease in performance compared to vanilla Mamba. This setup is denoted as Mamba-MLP in\\nFigure 1.\\nMoE-Mamba separates unconditional processing of every token by the Mamba layer - which can efficiently integrate the\\nwhole context of the sequence into an internal representation - and conditional processing by a MoE layer that can apply\\nthe most relevant expert for each token. The idea of interleaving conditional and unconditional processing is used in some\\nMoE-based models, typically by alternating vanilla and MoE feed-forward layers (Lepikhin et al., 2020; Fedus et al., 2022).\\nFigure 2. Diagrams of the architectures. From the left: vanilla Transformer, MoE Transformer, Mamba, MoE-Mamba\\n3.1. Alternative Designs\\nIn addition to the experiments related to interleaving Mamba with MoE, we also conducted other experiments, modifying\\nthe original block design by (Gu & Dao, 2023) to feature conditional computation. We expect this research direction to be\\nimportant in future attempts to improve the Mamba architecture. We address those experiments in the Appendix, Section B.\\n4. Main Results\\n4.1. Training Setup\\nWe compare 5 different settings: vanilla Transformer, Mamba, Mamba-MLP, MoE and MoE-Mamba. In most Transformers,\\nthe feed-forward layer contains 8dm2 parameters, whereas (Gu & Dao, 2023) makes Mamba layers smaller (ca. 6dm2)\\nso that two Mamba layers match the combined parameter count of a feed-forward layer and an attention mechanism. To\\nkeep the number of active parameters per token roughly the same in Mamba and in our model, we scale down the size of\\neach expert feed-forward layer to 6dm2. Excluding embedding and unembedding layers, all models access around 26M\\nparameters per token. We train the models on approximately 6.5B tokens and 100k steps.\\nWe train the model using the English C4 dataset (Raffel et al., 2020) on the task of next token prediction. The text is\\n3\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\ntokenized using GPT2 tokenizer (Radford et al., 2019). LR was tuned for vanilla Mamba (see Appendix, Section D) and\\nre-used for all other training runs. For a full rundown of hyperparameters, see Table 3.\\nFigure 3. Training loss for a differing number of experts.\\n4.2. Results\\nModel\\n# Parameters\\n# Active Parameters\\nper Token\\nLoss After\\n100k Steps\\n% Steps to\\nTransformer Loss\\n% Steps to\\nVanilla Mamba Loss\\nTransformer\\n25M\\n25M\\n3.66\\n100%\\n>100%\\nMamba-MLP\\n26M\\n26M\\n3.56\\n38%\\n>100%\\nTranformer-MoE\\n545M\\n25M\\n3.54\\n42%\\n>100%\\nVanilla Mamba\\n27M\\n27M\\n3.51\\n30%\\n100%\\nMoE-Mamba\\n416M\\n26M\\n3.41\\n21%\\n46%\\nTable 1. Results of comparison between different architectures after 100k steps. Note that the parameter counts exclude embedding and\\nunembedding layers.\\nTable 1 presents the results of training. MoE-Mamba shows a remarkable improvement over the vanilla Mamba model.\\nNotably, MoE-Mamba was able to achieve the same performance as vanilla Mamba in just 46% of training steps. Because\\nthe learning rate was tuned for vanilla Mamba (see Appendix, Section D), we expect even better performance if the training\\nprocedure is optimized for MoE-Mamba. Like (Gu & Dao, 2023), we observe that Mamba-MLP achieves slightly worse\\nperformance than vanilla Mamba.\\n5. Ablations\\nNumber of Experts\\n# Parameters\\n# Active Parameters\\nper Token\\nLoss After\\n100k Steps\\n% Steps to\\nVanilla Mamba Loss\\nN/A - Vanilla Mamba\\n27M\\n27M\\n3.51\\n100 %\\n1 (Mamba-MLP)\\n26M\\n26M\\n3.56\\n>100%\\n4 experts\\n64M\\n26M\\n3.55\\n>100%\\n8 experts\\n114M\\n26M\\n3.51\\n91%\\n16 experts\\n215M\\n26M\\n3.45\\n56%\\n32 experts\\n416M\\n26M\\n3.41\\n46%\\nTable 2. Loss after 100k steps for different numbers of experts. Note that the parameter counts exclude embedding and unembedding\\nlayers.\\n4\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nTo assess whether Mamba scales well as the number of experts increases, we compare different numbers of experts in our\\nmodel. For reference, we also include Mamba and Mamba-MLP (the latter is equivalent to MoE-Mamba with a single\\nexpert). Figure 3 shows the training runs for different numbers of experts. Table 2 shows results after 100k steps. The\\nresults show that our approach scales well with the number of experts. If the number of experts is 8 or more, our model\\nachieves better final performance than vanilla Mamba. Since Mamba-MLP is worse than vanilla Mamba, we should expect\\nMoE-Mamba with a small number of experts to exhibit poorer performance than Mamba. We obtain the best result with 32\\nexperts.\\n6. Future Work and Limitations\\nScaling. In this preliminary investigation, we only perform experiments on models smaller than 1B parameters. Since MoE\\nhas enabled Transformers to be scaled to unprecedented sizes (Fedus et al., 2022), we will be excited to see the impact of\\nscaling on the approaches proposed in our work.\\nIntegrating MoE into the Mamba Layer. Our experiments show that interleaving Mamba layer with a performant sparse\\nMoE feed-forward layer results in a promising model. However, in the dense setting, Mamba performs slightly better without\\nthe feed-forward layer. This suggests that integrating sparse computation within the Mamba layer itself could yield even\\nbetter results while conserving a simple, homogeneous architecture. We include some related preliminary investigations in\\nthe Appendix, Section B.\\nExploration of Different Types of MoE in MoE-Mamba. While we base our design on the commonly used Switch,\\nnumerous other architectures have been proposed since. Not only may those designs perform better overall, but it is possible\\nthat with Mamba a different type of MoE will be optimal. Among possible changes in this regard are Expert-Choice routers\\n(Zhou et al., 2022), fully differentiable architectures (Puigcerver et al., 2023; Antoniak et al., 2023), varying number of\\nexperts and their granularity, and other modifications.\\n7. Conclusions\\nIn this work, we presented the first integration of Mixture of Experts with Mamba architecture, MoE-Mamba. We showed\\npossible ways of combining those techniques and performance improvements achieved with their combination.\\nWe look forward to the upcoming developments in both Mixture of Experts and deep State Space Models. We hope this\\nwork will spark further research on combining conditional computation (and Mixture of Experts in particular) with State\\nSpace Models (and Mamba in particular). We believe that this path will enable more efficient scaling to even larger language\\nmodels.\\nAcknowledgements\\nWe would like to express sincere gratitude to the rest of our team members and past team members - Jakub Krajewski,\\nSzymon Antoniak, Michał Krutul, and Tomasz Odrzyg´\\no´\\nzd´\\nz - for engineering contributions made to our shared repository\\nand shared research intuitions, as without them it would be impossible to proceed with our project with this velocity. We also\\nthank our advisors and managers, Marek Cygan, Piotr Miło´\\ns, and Piotr Sankowski, for creating a supportive environment\\nand direction.\\nThis work was funded by IDEAS NCBR, which also provided significant computational resources. The research was\\nsupported by PL-Grid infrastructure (grant PLG/2023/016148). We acknowledge snakes and experts as essential to our\\nwork. We also benefited from the Entropy cluster (hosted at the Faculty of Mathematics, Informatics and Mechanics of the\\nUniversity of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center grant 2022/45/N/ST6/02222, and ERC\\nStarting Grant TOTAL.\\n5\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nReferences\\nAntoniak, S., Jaszczur, S., Krutul, M., Pi´\\noro, M., Krajewski, J., Ludziejewski, J., Odrzyg´\\no´\\nzd´\\nz, T., and Cygan, M. Mixture of\\ntokens: Efficient llms through cross-example aggregation, 2023.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann,\\nS., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient\\nsparsity, 2022.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and R´\\ne, C. Hungry hungry hippos: Towards language modeling\\nwith state space models, 2023.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.\\nGu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R´\\ne, C. Combining recurrent, convolutional, and continuous-\\ntime models with linear state-space layers, 2021.\\nGu, A., Goel, K., and R´\\ne, C. Efficiently modeling long sequences with structured state spaces, 2022.\\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computation, 3(1):\\n79–87, 1991. doi: 10.1162/neco.1991.3.1.79.\\nLangley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on\\nMachine Learning (ICML 2000), pp. 1207–1216, Stanford, CA, 2000. Morgan Kaufmann.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant\\nmodels with conditional computation and automatic sharding, 2020.\\nMistral. Mixtral of experts, Dec 2023. URL https://mistral.ai/news/mixtral-of-experts/.\\nPuigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N. From sparse to soft mixtures of experts, 2023.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask\\nlearners. OpenAI blog, 1(8):9, 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551,\\n2020.\\nSanseviero, O., Tunstall, L., Schmid, P., Mangrulkar, S., Belkada, Y., and Cuenca, P. Mixture of experts explained, 2023.\\nURL https://huggingface.co/blog/moe.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks:\\nThe sparsely-gated mixture-of-experts layer, 2017.\\nSmith, J. T. H., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling, 2023.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S.,\\nBikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C.,\\nGoswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann,\\nI., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov,\\nT., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith,\\nE. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y.,\\nFan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation\\nand fine-tuned chat models, 2023.\\nXue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., and You, Y. Openmoe: Open mixture-of-experts language models.\\nhttps://github.com/XueFuzhao/OpenMoE, 2023.\\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-experts with\\nexpert choice routing, 2022.\\n6\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nA. Hyperparameters\\nTable 3. Hyperparameters\\nModel\\nTotal Blocks\\n8 (16 in Mamba)\\ndmodel\\n512\\nFeed-Forward\\ndff\\n2048 (with Attention) or 1536 (with Mamba)\\nMixture of Experts\\ndexpert\\n2048 (with Attention) or 1536 (with Mamba)\\nExperts\\n32\\nAttention\\nnheads\\n8\\nTraining\\nTraining Steps\\n100k\\nContext Length\\n256\\nBatch Size\\n256\\nLR\\n1e-3\\nLR Warmup\\n1% steps\\nGradient Clipping\\n0.5\\nB. Alternative Designs\\nIn this section we explore three possible designs different than the one presented in Section 3. While we don’t present\\nconcrete results from those experiments, we think that in such a fast-moving field there is a value in sharing even rough\\nideas.\\nOne of the conducted experiments involved replacing the Output Projection with MoE (Figure 4). The resulting model, which\\nhad fewer blocks to match the number of active parameters, achieved similar results to the original Mamba architecture.\\nSimilarly, substituting the Conv Projection layer with a MoE layer (Figure 4) yielded similar results to vanilla Mamba,\\nwhich do not justify the added complexity of conditional computation. We attribute this to the reduction in the number of\\nblocks due to the increase in the effective number of parameters used in each Mamba block by adding the MoE layer.\\nAnother idea, inspired by (Chowdhery et al., 2023), was the parallel execution of a Mamba layer and MoE (Figure 4).\\nHowever, this architecture yielded worse results even compared to vanilla Mamba when matching the number of active\\nparameters per token.\\n7\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nFigure 4. Diagram of Parallel Mamba+MoE architecture (left) and Mamba Block (right)\\nC. Active Parameters vs FLOPs\\nIn this work we report the number of active parameters (excluding embedding and unembedding layers) and not the number\\nof floating-point operations (FLOPs), following (Zhou et al., 2022). Both numbers will be roughly similar, but the number\\nof FLOPs is both harder to calculate and less relevant for hardware-aware architecture like Mamba with its optimizations.\\nD. Learning Rate Tuning\\nIn this preliminary investigation, we decide to tune the learning rate specifically for vanilla Mamba and re-use it for other\\nmodels. This approach may only underestimate the gains of MoE-Mamba over vanilla Mamba, therefore it does not impact\\nthe main conclusions.\\nLR\\nLoss After 100k Steps\\n1e-4\\n3.68\\n2e-4\\n3.60\\n5e-4\\n3.53\\n1e-3\\n3.51\\n2e-3\\n3.55\\n5e-3\\nunstable\\nTable 4. LR tuning results after 100k steps.\\n8\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nFigure 5. LR tuning runs for Mamba. 5e-3 is not included in the plot, as it was unstable.\\nE. Reproducibility\\nThe codebase used to run the experiments is available at https://github.com/llm-random/llm-random.\\n9\\n', 'source_name': 'MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts', 'source_url': 'https://arxiv.org/abs/2401.04081'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BlackMamba.pdf #26\n",
      "{'content': 'BlackMamba: Mixture of Experts for State-Space\\nModels\\nQuentin Anthony∗\\nYury Tokpanov∗\\nPaolo Glorioso∗\\nBeren Millidge∗\\n{quentin, yury, paolo, beren}@zyphra.com\\nZyphra\\nPalo Alto, CA\\nAbstract—State-space models (SSMs) have recently demon-\\nstrated competitive performance to transformers at large-scale\\nlanguage modeling benchmarks while achieving linear time and\\nmemory complexity as a function of sequence length. Mamba,\\na recently released SSM model, shows impressive performance\\nin both language modeling and long sequence processing tasks.\\nSimultaneously, mixture-of-expert (MoE) models have shown\\nremarkable performance while significantly reducing the com-\\npute and latency costs of inference at the expense of a larger\\nmemory footprint. In this paper, we present BlackMamba, a novel\\narchitecture that combines the Mamba SSM with MoE to obtain\\nthe benefits of both. We demonstrate that BlackMamba performs\\ncompetitively against both Mamba and transformer baselines,\\nand outperforms in inference and training FLOPs. We fully train\\nand open-source 340M/1.5B and 630M/2.8B BlackMamba models\\non 300B tokens of a custom dataset. We show that BlackMamba\\ninherits and combines both of the benefits of SSM and MoE\\narchitectures, combining linear-complexity generation from SSM\\nwith cheap and fast inference from MoE. We release all weights,\\ncheckpoints, and inference code open-source. 1\\nI. INTRODUCTION\\nThe advent of Large Language Models (LLMs) built from\\ndecoder-only transformer models [1], [2] have revolutionized\\nNatural Language Processing (NLP) [3], [4], [5], along with\\ndiverse deep learning application domains such as image\\nprocessing [6], time-series [7], and reinforcement learning [8].\\nDespite the strong performance and scalability of the trans-\\nformer architecture, however, there remain significant short-\\ncomings. While maximally expressive, the attention mech-\\nanism is computationally demanding both during training\\nand inference, naively requiring both quadratic FLOPs and\\nmemory in the sequence length. This limits the context length\\nof transformer models, makes autoregressive generation in-\\ncreasingly expensive with scale, and generally inhibits truly\\nunlimited sequence processing and learning from continual\\ndatastreams.\\nIn order to ameliorate these problems, significant effort\\nhas recently been directed towards architectural alternatives\\nto the canonical dense attention transformer model. Some of\\nthe most promising candidate architectures are State Space\\nModels (SSMs) [9], [10] and Mixture of Experts (MoE) [11],\\n[12], [13]. The key practical benefit of SSMs over transformers\\n*All authors contributed equally to this work\\n1Inference code at: https://github.com/Zyphra/BlackMamba\\nis their linear computational complexity with respect to input\\nsequence length (as opposed to the quadratic complexity of\\ntransformers). This theoretically enables SSMs to process\\nvastly longer sequences than transformers for a given FLOP\\nbudget, and to render autoregressive generation constant in\\ncompute without a KV cache. Notable recent examples of\\nSSMs include Mamba [9], RWKV [10], and RetNet [14],\\nall of which demonstrate efficient long-sequence training and\\ninference, efficient implementations in CUDA, and competi-\\ntive language modeling task performance to transformers with\\nsimilar scaling properties. At the same time mixture of expert\\n(MoE) architectures [15], [16], [11], [12] have become an\\nemerging advance over dense transformers which allow for\\nsignificantly reduced training and inference FLOPs required\\nto achieve comparable quality to a comparable dense model.\\nMoE models allow for only a sparse subset of the total\\nparameters to be activated on a single forward pass, relying\\non a routing function to gate which ’experts’ are utilized\\nor not depending on the context. This sparsity decouples\\nthe inference cost and parameter count of a model, enabling\\nsignificantly stronger performance for a given inference budget\\nat the cost of many more parameters and a correspondingly\\ngreater memory footprint.\\nThese architectural improvements over transformers are\\ncompelling on their own, but we believe that their combination\\nis a natural next step that could enable significantly improved\\nlanguage modelling speed and performance against the\\ncanonical transformer. Specifically, we expect a Mamba-MoE\\narchitecture would have the following improvements over a\\ndense transformer:\\n• Mamba: Linear computational complexity with respect\\nto input sequence length for both training and inference.\\nAutoregressive generation in constant time and memory.\\n• MoE: Inference latency and training FLOPs of the\\nequivalent smaller dense base model, while preserving\\nmodel quality close to an equi-parameter dense model.\\nIn this paper, we begin to demonstrate that these improve-\\nments are achievable and that, when put together, these two\\napproaches synergize to produce a model with compelling\\narXiv:2402.01771v1  [cs.CL]  1 Feb 2024\\n(a) Transformer\\n(b) Mamba\\n(c) Transformer-MoE\\n(d) Mamba-MoE\\nFig. 1. Architecture of dense transformer, dense Mamba, transformer-MoE, and Mamba-MoE\\nevaluation performance (Figs. 8-14), compute (Fig. 4), and\\nlatency advantages (Figs. 5 and 3) over existing transformer\\nmodels and which can be trained at a fraction of the FLOP cost\\nfor similar performance (Fig. 4). We study the MoE routing\\nstatistics exhibited by our model across training time and\\nacross model depth. Additionally, we introduce a novel initial-\\nization for our routing Sinkhorn algorithm which significantly\\nreduces the number of iterations required until convergence,\\nthus improving routing speed.\\nII. CONTRIBUTIONS\\nThe main achievements of this work are:\\n• We design, implement, and evaluate BlackMamba: a\\ncombination of alternating attention-free Mamba blocks\\nand routed MLPs.\\n• We train and open-source two BlackMamba Models:\\n340M/1.5B BlackMamba and 630M/2.8B BlackMamba2.\\n• We demonstrate that BlackMamba requires significantly\\nfewer training FLOPs to achieve comparable downstream\\ntask performance to a dense transformer model.\\n• We explore the compounding inference benefits of\\nthe combination of attention-free architectures such as\\nMamba along with routed sparsity architectures such as\\nMoE.\\nThe rest of this paper is organized as follows. We first\\nprovide an overview of related works on SSM, MoE, and SSM\\nwith MoE in Section IV. We then provide background into the\\nunderlying concepts behind SSMs and MoE that are necessary\\nto understand our contributions in Section III. Our architecture\\nis described in Section V, and its training/inference dynamics\\nare explored in Section VI. Finally, we describe the implica-\\ntions and limitations of our approach in Section VII along with\\nour conclusions from this work in Section VIII.\\n2In this paper, we denote an MoE model with X forward-pass parameters\\nand Y total parameters as X/Y .\\nThe final checkpoints are open-sourced on HuggingFace\\nwith Apache 2.0 licensing, and intermediate training check-\\npoints are available upon request. Inference code is provided\\nat https://github.com/Zyphra/BlackMamba.\\nIII. BACKGROUND\\nA. Transformers\\nThe transformer architecture [2] has demonstrated excep-\\ntionally strong and consistent performance at language mod-\\nelling, as well as almost all other sequence processing tasks,\\nremaining state-of-the-art and essentially unchanged since its\\nintroduction. The core operation of the transformer is self-\\nattention, which performs a quadratic all-to-all comparison\\nof the dot-product similarities between the embeddings of\\ndifferent tokens in a sequence before normalizing it and\\nperforming a linear map to an output vector. Mathematically,\\nself-attention can be written as,\\nz = WV xσ( 1\\n√\\nd\\nxWQW T\\nKx ◦M)\\n(1)\\nWhere σ denotes the softmax function, M denotes a binary\\nmask which enforces specific constraints, such as causal mask-\\ning, on the computation, the superscript T denotes transposi-\\ntion, and ◦denotes element-wise multiplication. The quadratic\\ncost in sequence length is caused by the xWQW T\\nKx term\\nwhich computes a L × L matrix of similarity scores between\\nthe embeddings of different tokens where L is the sequence\\nlength.\\nThe transformer model consists of a stack of self-attention\\nblocks interleaved with multi-layer-perceptron (MLP) blocks\\nwhich consist of a two-layer MLP with a given activation\\nfunction. A layer of a transformer model can thus be written\\nas,\\nxl+1 = xl + MLP(LN(xl + attention(LN(xl))))\\n(2)\\nWhere LN represents the layernorm operation which is used\\nto normalize the inputs to the attention and MLP blocks.\\n2\\nB. Mamba\\nState-space models (SSMs) are a class of sequence models\\nthat possess linear complexity with respect to the sequence\\nlength. SSMs are more closely related to RNN and CNN\\narchitectures than the attention mechanism, and draw in-\\nspiration from a continuous dynamical system (depicted in\\nEquation 3) mapping a 1-dimensional function or sequence\\nx(t) ∈R 7→y(t) ∈R through an implicit latent state\\nh(t) ∈RN:\\nh′(t) = Ah(t) + Bx(t),\\ny(t) = Ch(t)\\n(3)\\nWhere the ‘time’ t now represents the sequence position of a\\ntoken. A linear dynamical system like this can be efficiently\\ncomputed in parallel via a convolution or associative scan,\\nwhile the recurrent form presented above can be utilized for\\nrapid generation at inference time. The fundamental innovation\\nof the Mamba architecture is to make the A, B, and C\\nmatrices of the SSM linearly input-dependent. That is, the\\nnew dynamics can be written as,\\nh′(t) = A(x(t))h(t) + B(x(t))x(t),\\ny(t) = C(x(t))h(t)\\n(4)\\nIntuitively, this enables the updates to the SSM’s recurrent\\nstate to selectively depend upon the tokens being processed,\\nwith the SSM being able to decide to store or remove specific\\ninformation from its recurrent state dynamically. This renders\\nthe A,B,C matrices loosely analogous to the Q,K,V matrices\\nin attention and significantly increases the expressivity of the\\nSSM block and could potentially enable context to persist\\nmuch longer in the hidden state than otherwise, since it must\\nexponentially decay in a linear dynamical system with fixed\\nweights. Empirically, [17] found that this closed much of the\\ngap with transformers.\\nIn practical terms, the recurrent nature of SSMs has long\\nprevented their adoption on the reigning highly-parallel AI\\nhardware like GPUs. However, recent implementations of\\nrecurrent and state-space models such as Mamba [9] and\\nRWKV [10] have mapped these operations efficiently to GPU\\nhardware via parallel scan kernels, thus enabling training of\\nsuch novel architectures with efficiencies approaching that of\\nwell-optimized transformer models.\\nFor more details on Mamba, please see Appendix C which\\ndescribes in details the internal computations of a Mamba\\nblock as well as [9] and its associated codebase.\\nC. Mixture of Experts\\nMixture of Expert (MoE) models allow for the inference\\ncost and number of parameters of a model to be decoupled\\nby not activating all parameters on the forward pass and\\ninstead routing tokens to specific MLP experts. Each expert\\ntheoretically specializes in a certain kind of input, and the\\nrouter (a small neural network) learns which expert to route\\neach token to. Theoretically, this enables the model to maintain\\nalmost all the expressivity of the parameter-equivalent dense\\nmodel at significantly fewer FLOPs.\\nIn standard implementations [11], which we follow in this\\npaper, the router is a linear layer mapping from tokens to ex-\\npert indices, and each expert is simply a standard transformer\\nMLP. The expert that the token is routed to is chosen as the\\ntop-k of the expert probabilities, where k is a hyperparameter\\nof the architecture. Given an input token to the MoE layer x,\\nthis is mapped through the router to a probability distribution\\npi(x), where i labels the experts. Upon selecting the top-k\\nprobabilities, the output of the MoE layer y can be expressed,\\nschematically, as,\\ny =\\nX\\ni∈top-k\\nciEi(x)\\n(5)\\nwhere E1, E2, . . . denote the MLP experts,\\nEi(x) = Woutf(Win(LN(x))\\n(6)\\nwhere f is the activation function of the MLP, and ci are\\ncoefficients that are often identified with pi, the probability\\noutput by the router of choosing a specific expert. The optimal\\nmethod for training the router is still uncertain since the\\n“correct” expert assignment problem is non-differentiable, and\\nMoE models often struggle with training stability and load-\\nbalancing between different experts for hardware efficiency.\\nNevertheless, MoE models have demonstrated the ability to\\nachieve superior performance for a given compute budget over\\ndense transformer models. Lastly, due to complexity of report-\\ning MoE models, where different papers have reported either\\nthe forward pass size of the MoE, the total parameters, or\\nboth, we here present a consistent convention of denoting MoE\\nmodels as: (forward parameters)/(total parameters). For more\\ndetails on the MoE architecture and its typical implementation,\\nsee [16].\\nIV. RELATED WORK\\nA. State-space Models\\nThe quadratic complexity of transformers in the sequence\\nlength has long been recognized as a primary bottleneck to\\nextremely long context reasoning and understanding. While\\nrecent work has pioneered the concept of context-length\\nextension [18], [19] allowing transformers to be trained at\\na manageable scale and then inferenced successfully at a\\nsignificantly longer context, the inference cost in terms of both\\nFLOPs and the memory required for the KV cache remains\\nsubstantial.\\nEarly state-space models were inspired by linear dynamical\\nsystems which can be efficiently computed as a convolution\\n[17], [20] for sequence processing and as a recurrence for ef-\\nficient autoregressive generation. However, such models were\\nnoticeably less expressive and performant than transformers.\\nA number of recent works [14], [21] has aimed to increase\\nthe expressivity of the state-space model by using input-\\ndependent gating, similar to the QKV matrices of attention,\\nwhile maintaining the fundamentally linear nature of the state-\\nspace recursion. This thus enables efficient implementation via\\nconvolution or selective-scan to be maintained while substan-\\ntially closing the gap to transformer performance in practice.\\n3\\nMamba [9] is a recently released state-space model in line with\\nthese previous works which demonstrates strong performance\\ncomparable to transformers up to the 2.8B scale, as well as\\npromising scaling laws. Mamba uses input-dependent gating\\nof the inputs to the SSM recursion while maintaining efficient\\ncomputation via customized selective scan kernels.\\nB. Mixture of Experts\\nMoE models have been demonstrated to achieve signifi-\\ncantly higher performance in both training and inference per\\nFLOP than the equivalent dense models [11], [12]. Moreover,\\nscaling laws for MoE models have been put forward [22]\\nwhich show that MoE performance improves smoothly with\\ncompute, data, and the number of experts being routed to.\\nThis latter is especially important since it provides a route\\nto continually increasing the capability of the model while\\nholding the inference cost fixed.\\nWhile MoE models hold significant promise, the architec-\\nture still retains many drawbacks. Increasing the number of\\nexperts increases the parameter count and hence memory cost\\nsubstantially, while many works report MoE models being\\nless stable and more challenging to train. Moreover, effective\\nmethods for training the router are still open, since the decision\\nto route to an expert or not is discrete and cannot be easily\\nbackpropagated through. The large memory cost of MoEs\\nrelative to their dense counterparts is especially important\\nfor users running on relatively low-end GPUs or when the\\nmemory size extends beyond that provided by a single GPU\\nnecessitating model-parallelism for inference.\\nRecently, [13] released a powerful open source mixture of\\nexperts model which performs competitively with Llama 2\\n70B [5] and close to GPT-3.5 in evaluations while requiring\\nonly the forward pass FLOP cost of the original Mistral 7B\\nmodel [23], thus demonstrating and solidifying the promise\\nof MoE models at scale. The Mixtral architecture also differs\\nin a few ways from earlier MoE work, especially in its use\\nof relatively few experts, a design which we also utilize and\\nhave independently found promising for balancing the FLOP\\nand memory cost of MoE models successfully.\\nC. State-space models with Mixture of Experts\\nWhile both state-space models and Mixture of Experts have\\nbeen proposed as promising architectures able to improve the\\ncomputational cost of inferencing language models, no works\\nhave ever tested their combination at scale.\\nConcurrently with this work, [24] demonstrate the per-\\nformance of extremely small mamba-MoE models in the\\nhundred-million scale of total parameters and the forward\\npass FLOPs of a 25M model, trained on <10B tokens. In\\ncontrast, we demonstrate empirically the scaling potential and\\nperformance of such models at meaningful scales in terms of\\nboth parameters and data, by training multi-billion parameter\\nmodels on 300B tokens. Our work thus demonstrates the\\nstrong scaling potential of the combination of state-space\\nmodels and MoE models while resulting in competitive and\\nusable language models which are extremely efficient for\\ninference.\\nV. DESIGN\\nA. Architecture\\nA standard transformer model [2] consists of interleaved\\nattention and MLP blocks added in sequence along a residual\\nstream. The equation for a single transformer layer is written\\nin Equation 2.\\nMost MoE architectures simply replace the MLP blocks\\nwith a routed expert layer. Our BlackMamba architecture\\nsimply replaces both the MLP layer in a transformer with an\\nexpert layer, and the attention layer with a mamba SSM layer\\n(see Figure 1). A single block of our architecture can thus be\\nwritten as,\\nxl+1 =xl + MoE(LN(xl + mamba(LN(xl))))\\n(7)\\nWe trained BlackMamba 340M/1.5B and 630M/2.8B mod-\\nels for 300B tokens on our custom dataset. We used the\\nSwiGLU activation function [25] for the expert MLPs. We\\ntrained with 8 experts, a number that we found balanced\\nwell the trade-off between the inference cost and memory\\nfootprint of the model. We tested whether sequential or parallel\\n[26] blocks performed better and found a slight advantage\\nfor sequential. Following [5], we trained without biases. For\\nthe expert router, we used top-1 routing with a Sinkhorn\\nrouting function to load-balance between experts. We utilized\\na novel custom version of the Sinkhorn algorithm which\\nconverges substantially faster than vanilla Sinkhorn (Appendix\\nF). We trained using the Megatron-LM [27] distributed training\\nframework. The model was trained in bf16 precision. All\\nfurther model architectures and training hyperparameters are\\ndescribed in Appendix A and B, respectively.\\nB. Dataset\\nFig. 2. Ratio of data categories in the pretraining dataset of BlackMamba\\nTo train BlackMamba, we constructed a custom dataset\\ncomprised of a mixture of existing open-source datasets. The\\nsubsets included: The Pile [28], SlimPajama [29], Starcoder\\n[30], PeS2o [31], and ProofPile [32]. The weights for each\\ndataset is provided in Table I. Tokens were sampled without\\nreplacement from each of the subsets according to the proba-\\nbility of sampling from a subset upweighted by these weights.\\n4\\nFig. 3. Comparison of BlackMamba average evaluation performance across activated forward parameters.\\nDataset\\nTokens\\nWeight\\nPile [28]\\n300B\\n2\\nSlimPajama [29]\\n600B\\n1.2\\nStarcoder [30]\\n250B\\n0.75\\nPeS2o [31]\\n50B\\n5\\nProofpile [32]\\n40B\\n2\\nPG19 [33]\\n2.2B\\n5\\nTABLE I\\nDATASET SUBSETS AND THEIR RESPECTIVE WEIGHTS IN OUR TRAINING\\nMIXTURE\\nFig. 4. Comparison of BlackMamba average evaluation performance across\\ntraining FLOPs.\\nThe total dataset comprised 1.8 trillion tokens and thus we\\ntrained for significantly less than a single epoch. Preliminary\\nexperiments3 show that long-form text and academic work\\nappears to improve natural language modeling when included\\nin the pretraining phase, so we weigh it heavily in the training\\nrecipe. Further, we find that including significant portions of\\ncode and math during the pretraining phase meaningfully im-\\nproves the model’s reasoning ability. We note that this dataset\\nis comparatively heavy on unfiltered web data and contains\\nmany duplicates due to the upweighting of smaller subsets,\\nwhich may limit the quality of the model and leaves significant\\nroom for improvement, as well as potentially causing undue\\nmemorization of specific common fragments.\\nVI. RESULTS\\nTo ensure a fair comparison vs Mamba, we trained our\\nown 340M Mamba model with the same dataset and train-\\ning hyperparameters reported for BlackMamba. This Mamba\\n340M model used a hidden size of 1152 and 34 mamba lay-\\ners. Notably, BlackMamba performs significantly better than\\nequivalent pretrained models (both transformer and Mamba)\\nfor the same forward pass model size at inference time, as\\nwell as training FLOPs. In Figure 5, we plot the time taken\\nto autoregressively generate a sequence of a given length\\nstarting from an initial one-token prompt as a function of\\nsequence length. We observe that the established latency\\nbenefits of both Mamba and MoE models are combined in\\nBlackMamaba to result in inference times significantly faster\\nthan canonical transformer models, MoE transformer models,\\nand pure Mamba models. Moreover, the inference advantage of\\n3We believe that such experiments are not yet rigorous enough for\\npublication, and will be included in future work.\\n5\\nFig. 5. Generation latency of BlackMamba compared to dense transformers,\\ndense mamba, and transformer-MoE\\nBlackMamba increases with greater sequence lengths, making\\nBlackMamba extremely competitive at long sequence genera-\\ntion. Moreover, although not reflected in this Figure, it must\\nbe recognized that while the transformer inference latency\\nalso increases linearly, this is due to KV caching which has\\nadditional linearly increasing memory requirements and would\\neventually OOM on large enough sequences. By contrast,\\nMamba models (and BlackMamba) can generate sequences\\nof arbitrary length with a constant memory footprint.\\nFigures 6 and 7 illustrate the token counts assigned to\\neach expert in each layer of the BlackMamba 340M/1.5B\\nand the BlackMamba 630M/2.8B models respectively. Most\\nlayers display a high degree of expert balance, as expected\\nby our improved Sinkhorn algorithm. Yet, intriguingly, both\\nmodels show a clear transition towards expert imbalance in the\\nfinal layers (at layer 20 for the 340M/1.5B model and layer\\n25 for the 630M/2.8B model). This may reflect increasing\\nspecialization in later layers or else reflect numerical insta-\\nbilities that develop deeper in the network. While the true\\ncause of this imbalance remains unknown, we also note that a\\nsimilar pattern of imbalance but convergence to a stable expert\\nassignment has also been observed in previous MoE models\\n[34].\\nIn Table I, we report evaluation scores of BlackMamba\\nagainst a suite of open-source pretrained language model\\nbaselines. We re-evaluated all models on the same version of\\nlm-eval (v0.3.0) that we evaluated our own model on*.\\nIn Appendix E, we provide evaluation scores for our model\\nduring training from checkpoints taken every 10k steps. We\\ngenerally found relatively smooth but noisy improvements in\\nthe evaluation scores during training. To prevent overfitting\\nto the evaluations, we only looked at the evaluation scores\\nafter the models had finished training and did not use them\\nfor model selection.\\nAdditionally, in Appendix F, we describe a novel initial-\\nization for the classical Sinkhorn algorithm used for MoE\\nrouting which significantly improves convergence speed of\\nthe approach, often requiring only a single iteration for con-\\nvergence. This provides notable speed improvements for the\\nrouted expert layers and results in a similar latency to a\\nrouter with a regularized balancing loss, providing superior\\nbalancing performance while requiring much less complexity\\nof implementation.\\nFinally, in Appendix C, we provide a detailed mathematical\\ndescription of the internal computations of a Mamba Block\\nand in Appendix D, we provide detailed and explicit formulas\\nfor computing the parameters and training FLOPs for Mamba\\nand MoE models which we hope aid the community in further\\ndeveloping and exploring novel SSM and MoE architectures.\\n*We use the non-normalized HellaSwag evaluation results in this paper,\\nwhich differs from those in [9]\\nForward Pass Parameters\\nTotal Parameters\\nTraining FLOPs\\nHellaSwag\\nPIQA\\nWinoGrande\\nLambada\\nARC-e\\nARC-c\\nOpenBookQA\\nDownstream Average\\nCerebras-GPT\\n111M\\n111M\\n2.6e18\\n0.268*\\n0.594\\n0.488\\n0.194\\n0.38\\n0.166\\n0.118\\n0.315\\nOPT\\n125M\\n125M\\n4.1e20\\n0.313*\\n0.63\\n0.503\\n0.379\\n0.435\\n0.189\\n0.166\\n0.371\\nPythia\\n160M\\n160M\\n4.1e20\\n0.293*\\n0.627\\n0.519\\n0.389\\n0.452\\n0.181\\n0.16\\n0.375\\nCerebras-GPT\\n256M\\n256M\\n1.3e19\\n0.286*\\n0.613\\n0.511\\n0.293\\n0.41\\n0.17\\n0.158\\n0.347\\nBlackMamba\\n342M\\n1.5B\\n6.4e20\\n0.365*\\n0.690\\n0.526\\n0.493\\n0.561\\n0.241\\n0.196\\n0.439\\nOPT\\n350M\\n350M\\n1.1e21\\n0.366*\\n0.644\\n0.523\\n0.452\\n0.44\\n0.207\\n0.176\\n0.395\\nMamba\\n343M\\n343M\\n8.0e20\\n0.335*\\n0.665\\n0.516\\n0.453\\n0.540\\n0.212\\n0.198\\n0.417\\nPythia\\n410M\\n410M\\n1.1e21\\n0.333*\\n0.668\\n0.53\\n0.505\\n0.504\\n0.213\\n0.178\\n0.419\\nBlackMamba\\n631M\\n2.8B\\n1.2e21\\n0.397*\\n0.712\\n0.521\\n0.542\\n0.603\\n0.245\\n0.242\\n0.466\\nPythia\\n1B\\n1B\\n2.2e21\\n0.376*\\n0.705\\n0.545\\n0.566\\n0.559\\n0.243\\n0.196\\n0.456\\nOPT\\n1.3B\\n1.3B\\n3.2e21\\n0.4537*\\n0.717\\n0.595\\n0.579\\n0.57\\n0.234\\n0.234\\n0.478\\nCerebras-GPT\\n1.3B\\n1.3B\\n2.8e20\\n0.384*\\n0.664\\n0.521\\n0.462\\n0.508\\n0.224\\n0.166\\n0.410\\nPythia\\n1.4B\\n1.4B\\n3.2e21\\n0.398*\\n0.711\\n0.565\\n0.604\\n0.576\\n0.256\\n0.204\\n0.474\\nOPT\\n2.8B\\n2.8B\\n6.1e21\\n0.606*\\n0.738\\n0.61\\n0.637\\n0.609\\n0.268\\n0.25\\n0.510\\nCerebras-GPT\\n2.8B\\n2.8B\\n1.1e21\\n0.488*\\n0.701\\n0.559\\n0.567\\n0.571\\n0.246\\n0.206\\n0.462\\nPythia\\n2.8B\\n2.8B\\n6.1e21\\n0.451*\\n0.737\\n0.612\\n0.654\\n0.629\\n0.288\\n0.22\\n0.513\\nTABLE II\\nEVALUATION PERFORMANCE OF BLACKMAMBA COMPARED TO SIMILAR MODELS\\n6\\nFig. 6. Token distribution across experts in 340M/1.5B BlackMamba\\nVII. DISCUSSION\\nThis work is a preliminary exploration and validation of\\nthe core concept of combining together recent advances in\\nSSMs with MoEs to produce a highly competitive and efficient\\narchitecture both in terms of inference and generation time\\nand training FLOPs. While initial results are promising, much\\nwork needs to be done to improve both the SSM and MoE\\ncomponents as well as investigation of the optimal way to\\napproach their combination. We ultimately believe that by\\nexploring promising emerging architectures architectures and\\nnovel ways of merging and combining them, significant ad-\\nvances in performance, efficiency, and speed can be obtained\\nover standard transformer recipes.\\nWe believe that our work can be extended in many fruitful\\ndirections. The evaluations presented in this paper are limited\\nin scope. While we provide general coverage of standard\\npure language modelling evaluations in the zero-shot setting,\\nthe performance of the model in the many-shot in-context-\\nFig. 7. Token distribution across experts in 630M/2.8B BlackMamba\\nlearning setting remains unexplored. Additionally, there are\\nmany facets of behaviour of our models which we have\\nnot explicitly investigated. We have not tested for factual\\naccuracy, profanity, toxicity, or any other socially undesirable\\ntext generation. Similarly, our training dataset blend has not\\nbeen explicitly scraped for socially undesirable tokens, nor\\nits potential overlap with any evaluation tasks4. Although\\nour dataset remains imperfect, we have released all major\\ndetails as to its construction and composition with the goal\\n4In particular, we are aware of the possibility of evaluation dataset\\ncontamination present in the widely used RedPajama dataset [35], and will\\nattempt to explicitly deduplicate this dataset if used in future work.\\n7\\nof aiding community understanding of the effects of dataset\\non pretraining performance and model behaviours.\\nIn terms of scaling laws, while our models are highly\\ncompetitive for a given inference cost and FLOP training\\nbudget, it is impossible to make conclusive scaling extrap-\\nolations both in terms of data and parameter counts with only\\ntwo models trained on 300 billion tokens. Additionally, many\\nof our training hyperparameters may be suboptimal as we\\nperformed only basic hyperparameter tuning of the learning\\nrate. Additionally, while we performed some ablations on the\\ncore architecture, it is possible that a superior method of\\ncombining state-space models and mixture of experts would\\nprovide significant benefits. Additionally, the efficacy and per-\\nformance of well-established finetuning and RLHF pipelines\\nfor instruction following and general alignment, as well as\\nstandard techniques for parameter-efficient-finetuning of SSM\\nand MoE models remains almost completely unexplored, as\\ndoes how such models perform under quantization.\\nOur work also raises interesting questions as to the mod-\\nularity of different neural network components that can be\\nplaced together into a final model architecture. We show\\nthat it is relatively straightforward to combine SSM blocks\\nwith MoE blocks from transformers at scale with competitive\\nperformance. However, whether Mamba and other SSMs show\\nthe same degree of improvement in performance with MoE as\\ntransformers remains uncertain, as well as whether combining\\nthese architectural pieces has the same effect on the internal\\nrepresentations and behaviours of the model. Additionally, it is\\nunclear the extent to which routing serves the same function in\\nBlackMamba as in more classical transformer MoE models.\\nVIII. CONCLUSION\\nIn this paper, we have proposed, implemented and trained\\nBlackMamba, a model that combines both recent advances in\\nstate-space models and mixture-of-experts into a single unified\\narchitecture. We demonstrate that our BlackMamba architec-\\nture performs highly competitively to strong pretrained LLM\\nbaselines in terms of inference cost and training flops, and\\nmoreover that it inherits the reduced training and generation\\nFLOPs of both SSMs and MoEs simultaneously. Moreover,\\nwe show that BlackMamba is capable of rapid generation\\nwith both linear time and memory cost. We release Black-\\nMamba 340M/1.5 and 630M/2.8 billion parameter models and\\nintermediate checkpoints, as well as inference code, under a\\npermissive Apache 2.0 license with the goal of enabling and\\nfostering further study, experimentation, and understanding\\nof the potential of this novel architecture by the broader\\ncommunity.\\nACKNOWLEDGEMENT\\nThe Zyphra team would like to thank Adam Ibrahim for\\nhelpful discussions and comments on training stability and\\nhyperparameters, and Albert Gu for general discussions on\\nstate space models.\\n8\\nREFERENCES\\n[1] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\\n2014.\\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.\\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288, 2023.\\n[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\\n“An image is worth 16x16 words: Transformers for image recognition\\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\\n[7] K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos,\\nR. Bhagwatkar, M. Biloˇ\\ns, H. Ghonia, N. V. Hassen, A. Schneider et al.,\\n“Lag-llama: Towards foundation models for time series forecasting,”\\narXiv preprint arXiv:2310.08278, 2023.\\n[8] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov,\\nG. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg\\net al., “A generalist agent,” arXiv preprint arXiv:2205.06175, 2022.\\n[9] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\\n[10] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\\n[11] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to\\ntrillion parameter models with simple and efficient sparsity,” The Journal\\nof Machine Learning Research, vol. 23, no. 1, pp. 5232–5270, 2022.\\n[12] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A.\\nAwan, J. Rasley, and Y. He, “Deepspeed-moe: Advancing mixture-of-\\nexperts inference and training to power next-generation ai scale,” in\\nInternational Conference on Machine Learning.\\nPMLR, 2022, pp.\\n18 332–18 346.\\n[13] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam-\\nford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al.,\\n“Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024.\\n[14] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei,\\n“Retentive network: A successor to transformer for large language\\nmodels (2023),” URL http://arxiv. org/abs/2307.08621 v1.\\n[15] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun,\\nN. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding,” arXiv preprint arXiv:2006.16668,\\n2020.\\n[16] W. Fedus, J. Dean, and B. Zoph, “A review of sparse expert models in\\ndeep learning,” arXiv preprint arXiv:2209.01667, 2022.\\n[17] A. Gu, K. Goel, and C. R´\\ne, “Efficiently modeling long sequences with\\nstructured state spaces,” arXiv preprint arXiv:2111.00396, 2021.\\n[18] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, “Yarn: Efficient\\ncontext window extension of large language models,” arXiv preprint\\narXiv:2309.00071, 2023.\\n[19] S. Chen, S. Wong, L. Chen, and Y. Tian, “Extending context window\\nof large language models via positional interpolation,” arXiv preprint\\narXiv:2306.15595, 2023.\\n[20] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\\nY. Bengio, S. Ermon, and C. R´\\ne, “Hyena hierarchy: Towards larger con-\\nvolutional language models,” arXiv preprint arXiv:2302.10866, 2023.\\n[21] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou,\\nA. Rudra, and C. R´\\ne, “Zoology: Measuring and improving recall in\\nefficient language models,” arXiv preprint arXiv:2312.04927, 2023.\\n[22] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoff-\\nmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud et al., “Unified\\nscaling laws for routed language models,” in International Conference\\non Machine Learning.\\nPMLR, 2022, pp. 4057–4086.\\n[23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\\n“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\\n[24] M. Pi´\\noro, K. Ciebiera, K. Kr´\\nol, J. Ludziejewski, and S. Jaszczur, “Moe-\\nmamba: Efficient selective state space models with mixture of experts,”\\narXiv preprint arXiv:2401.04081, 2024.\\n[25] N.\\nShazeer,\\n“Glu\\nvariants\\nimprove\\ntransformer,”\\narXiv\\npreprint\\narXiv:2002.05202, 2020.\\n[26] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter autore-\\ngressive language model,” 2021.\\n[27] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-\\nzaro, “Megatron-lm: Training multi-billion parameter language models\\nusing model parallelism,” arXiv preprint arXiv:1909.08053, 2019.\\n[28] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima et al., “The pile: An\\n800gb dataset of diverse text for language modeling,” arXiv preprint\\narXiv:2101.00027, 2020.\\n[29] D. Soboleva, F. Al-Khateeb, R. Myers, J. Steeves, J. Hestness, and\\nN. Dey, “Slimpajama: A 627b token cleaned and deduplicated version of\\nredpajama,” 7 2023. [Online]. Available: https://www.cerebras.net/blog/\\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\\n[30] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161, 2023.\\n[31] L. Soldaini and K. Lo, “peS2o (Pretraining Efficiently on S2ORC)\\nDataset,” Allen Institute for AI, Tech. Rep., 2023, oDC-By, https:\\n//github.com/allenai/pes2o.\\n[32] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer,\\nA. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, “Llemma: An open\\nlanguage model for mathematics,” arXiv preprint arXiv:2310.10631,\\n2023.\\n[33] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, “Com-\\npressive transformers for long-range sequence modelling,” 2019.\\n[34] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, “Faster-\\nmoe: modeling and optimizing training of large-scale dynamic pre-\\ntrained models,” in Proceedings of the 27th ACM SIGPLAN Symposium\\non Principles and Practice of Parallel Programming, 2022, pp. 120–134.\\n[35] Y. Elazar, A. Bhagia, I. Magnusson, A. Ravichander, D. Schwenk,\\nA. Suhr, P. Walsh, D. Groeneveld, L. Soldaini, S. Singh, H. Hajishirzi,\\nN. A. Smith, and J. Dodge, “What’s in my big data?” 2023.\\n[36] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi,\\nC. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell,\\nN. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf,\\nA. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and\\nA. Zou, “A framework for few-shot language model evaluation,” 12\\n2023. [Online]. Available: https://zenodo.org/records/10256836\\n[37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\\nCan a machine really finish your sentence?” 2019.\\n[38] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning\\nabout physical commonsense in natural language,” 2019.\\n[39] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “Winogrande:\\nAn adversarial winograd schema challenge at scale,” 2019.\\n[40] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, and R. Fern´\\nandez, “The lambada\\ndataset: Word prediction requiring a broad discourse context,” 2016.\\n[41] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” 2018.\\n[42] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor\\nconduct electricity? a new dataset for open book question answering,”\\n2018.\\n[43] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\\n“Pythia: A suite for analyzing large language models across training and\\nscaling,” in International Conference on Machine Learning.\\nPMLR,\\n2023, pp. 2397–2430.\\n[44] R. Sinkhorn and P. Knopp, “Concerning nonnegative matrices and\\ndoubly stochastic matrices,” Pacific Journal of Mathematics, vol. 21,\\nno. 2, pp. 343–348, 1967.\\n9\\nAPPENDIX\\nA. Model Hyperparameters\\nHyperparameter\\n1.5B\\n2.8B\\nNumber of Layers\\n30\\n36\\nHidden Size\\n1152\\n1472\\nNumber of Experts\\n8\\n8\\nSequence Length\\n2048\\n2048\\nState Size\\n16\\n16\\nConvolution Dimension\\n4\\n4\\nFFN Hidden Size\\n3072\\n3872\\nExpansion Factor\\n2\\n2\\nTABLE III\\nARCHITECTURE HYPERPARAMETERS FOR THE 340M/1.5B AND\\n630M/2.8B MODELS\\nB. Training Hyperparameters\\nHyperparameter\\n340M/1.5B\\n630M/2.8B\\nLearning Rate\\n0.0002\\n0.00015\\nBatch Size\\n2064384 tokens\\n2162688 tokens\\nDropout\\n0.0\\n0.0\\nLearning Rate Schedule\\ncosine\\ncosine\\nMin Learning Rate\\n0.00002\\n0.00002\\nWeight Decay\\n0.0\\n0.0\\nTABLE IV\\nTRAINING HYPERPARAMETERS FOR THE 340M/1.5B AND 630M/2.8B\\nMODELS\\nC. Mamba Block Internals\\nIn this appendix, we provide a precise and detailed walk-\\nthrough of the core computations that comprise a Mamba\\nblock. Mamba derives from a line of work on state-space\\nmodels, which are expressive recurrent models which have\\nrecently been shown capable of competing with transformers\\non large scale sequence modelling. The recurrence of these\\nmodels enables them to be used efficiently for generation\\nwithout a KV cache and causes them to scale in FLOPs and\\nmemory linearly in the sequence length. The core insight is to\\nutilize recurrence [17] or selective scan [9] to efficiently map\\nthe central recurrence to parallel GPU hardware. The base\\nof all such models is the following state-space equations (in\\ncontinuous time):\\ndh\\ndt = A h + B x\\n(8)\\ny = C h\\n(9)\\nwhich define a classical linear time-invariant dynamical sys-\\ntem. Here h denotes the state of a system at one instant. A\\ndenotes a matrix which governs the ’natural dynamics’ of h\\nover time. x denotes a ’control’ input to the system – i.e. one\\nprovided by the controller or experimenter and B denotes a\\ndynamics matrix which controls how x interacts with system.\\nFinally, the states are transformed into ’observations’, denoted\\ny, through the observation matrix denoted C.\\nThe Mamba block utilizes this dynamical system across\\ntokens as its core computation implemented as a hardware\\nefficient selective scan. The innovation of Mamba specifically\\nis to make the A,B,and C matrices a linear function of the\\ninput x, analogous to the Q,K,V matrices of a self-attention\\nblock. Beyond this, Mamba wraps the SSM component in\\na linear projection to and from the residual stream and a\\nconvolution of the input, as well as an additional gating\\nprojection path which gates the output of the SSM based on\\na projection of the input to the block.\\nWe denote the input to the mamba block x, the recurrent\\nhidden state h, the sequence length as l. We set the hidden\\nrecurrent state dimension to some factor of the input dimen-\\nsion.\\nThe mamba block contains matrices A which defines the\\ndynamics for the recurrent state, B which is the projection\\nfor the inputs, C which is the projection to the outputs y,\\nthe matrix D which is a learnable bias on the output, a\\ndiscretization timestep dt, and a gating vector z. The Mamba\\nblock also performs a linear projection of the input x and z\\nprior to the SSM with weight matrices Wx and Wz and an\\noutput projection matrix Wy.\\nThe computation inside a Mamba block runs as follows.\\nFirst, the x and z projections are computed. This projection\\noccurs for every token in the sequence independently.\\nx = Wx x\\n(10)\\nz = Wz z\\n(11)\\nSecondly, after the projection, the Mamba block performs a\\n1d convolution (∗) across the input sequence embeddings. This\\nconvolution cannot be merged with the projection Wx because\\nthis projection acts at the embedding level, and the convolution\\nis acting at the sequence of tokens level.\\nxt = Wfilter t ∗xt\\n(12)\\nThe input-dependent ‘weights’ B, C, and dt can then be\\ncomputed, which are analogous to the Query, Key, and Value\\nweights in attention.\\nB = WB x\\n(13)\\nC = WC x\\n(14)\\ndt = WD x\\n(15)\\nThe matrix A is trained with a special initialization given\\nin the matrix below. Note that updates are trained via the\\nparameterization ln(A), presumably to make A positive and\\nto improve stability, and then computed as A = exp( ln(A) ).\\nA =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n1\\n2\\n3\\n· · ·\\n1\\n2\\n3\\n· · ·\\n.\\n.\\n.\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(16)\\n10\\nThe weights are then discretized prior to use in the SSM\\nkernel. Note that the discretization for B does not follow\\nEquation 4 in [9].\\ndt = softplus(dt + dtbias)\\n(17)\\ndA = exp(−A dt)\\n(18)\\ndB = B dt\\n(19)\\nA single step of the ssm is then performed to obtain the new\\nrecurrent state. Note that h+ →h when dt →0, as expected\\nh+ = dA h + dB x\\n(20)\\nFrom the new recurrent state, the output C h+ can be\\ncomputed. This output is also gated by the learnt gating vector\\nz and passed through a final output projection before being\\naddded back into the residual stream.\\ny = C h+ + D x\\n(21)\\ny = silu(z) y\\n(22)\\ny = Wy y\\n(23)\\n(24)\\nThe output of the SSM block is then the hidden state h+\\nand the output y.\\nA Mamba block can operate in two modes. The first mode\\nis the recurrent method, which directly follows the steps\\ndescribed here. This approach is linear in both memory and\\ncomputational cost for a single step since it only utilizes the\\nrecurrent state to predict the next token. The second way is\\nto run the SSM across the whole sequence at once using the\\n’selective scan’ operation and kernel introduced by [9]. For\\nfurther reference on the implementation of the selective scan\\nrefer to [9].\\nD. Computing Parameters and FLOPs for Mamba-MoE\\nLet us denote the embedding dimension D, the Mamba\\ninner state as I, the recurrent state dimension H, the dt rank\\ndt and the convolution dimension C. We denote the batch size\\nB and the sequence length L.\\nThe number of parameters in a Mamba block can then be\\ncomputed as,\\n3ID\\n|{z}\\nWx,Wz,Wy\\n+2I(\\nH\\n|{z}\\nWA,WB\\n+ dt\\n|{z}\\nWdt\\n+ C\\n2\\n|{z}\\nconv\\n) +\\nI\\n|{z}\\nD\\n+\\n2D\\n|{z}\\nlayernorm\\n(25)\\nThe number of parameters in a MoE block can be computed\\nas\\n8D2E\\n| {z }\\nexperts\\n+ DE\\n|{z}\\nrouter\\n(26)\\nWhere E is the number of experts in the layer. For a network\\nof L layers, there are thus\\nL\\n2 Mamba blocks and\\nL\\n2 MoE\\nblocks.\\nTo begin approximating the number of FLOPs involved in\\na single Mamba block, we make the following observation.\\nGiven two matrices A\\n∈\\nRK×M and B\\n∈\\nRM×J,\\nthen the total FLOPs involved in the matrix product AB is\\napproximately 2KMJ, where the factor of 2 arises from the\\nfact that matrix multiplication requires both a multiply and an\\nadd operation. In the following calculations, we assume that\\nthe matrix multiplications dominate the total FLOP count of\\nthe model and hence ignore the nonlinearities, layernorms, and\\nother computations.\\nFirst, let us consider the projection operation involving the\\nweights Wx,Wz, and Wy. All are of shape I × D and hence\\nthe total FLOPs for these are 6IDLB.\\nThere is also the convolution which can be treated as a\\nsingle I × C matrix multiply requiring 2ICLB FLOPs.\\nNow, we turn to the SSM block itself. We first compute the\\ninput-dependent B and C matrices requiring a matrix multiply\\nof shape I × H each thus resulting in 4IH FLOPs. The A\\nmatrix is not multiplied by the input but goes through an\\nelementwise transform costing IH FLOPs. The dt projection\\nfirst goes through an elementwise operation of order I FLOPs.\\nNext, the discretization. The A matrix is multiplied by\\nthe dt vector resulting, costing IH FLOPs. The B matrix is\\nmultiplied by the input costing 2IH FLOPs. The SSM linear\\nstate space step itself is just a matrix multiply and add so\\ncosts 2IH FLOPs, and then the output projection using the\\nC matrix also costs 2IH FLOPs. Putting this all together, we\\nobtain the following expression,\\nBLI(\\n11H\\n|{z}\\nWx,Wz,Wy,SSM\\n+\\n4dt\\n|{z}\\ndt proj, discretization\\n+\\n1\\n|{z}\\ndt nonlinearity\\n) + IH\\n|{z}\\nA\\n(27)\\nThe MoE blocks consist of E standard mlp blocks and a\\nrouter. The FLOPs for each mlp block is simply 16D2 since\\nthere are two weight matrices of shape 4D×D, and a multiply\\nand add per matrix multiply. The router cost is simply 2DE.\\nPutting this together, we obtain DE(16D + 2) FLOPs for an\\nMoE block.\\nE. Evaluations During Training\\nWe evaluate BlackMamba on a suite of eight diverse eval-\\nuation tasks in the zero-shot setting. We use the EleutherAI\\nevaluation harness (version 0.3.0) [36]. Specifically, we eval-\\nuate our models on the HellaSwag [37], PIQA [38], Wino-\\nGrande [39], Lambada [40], ARC [41] (both the easy and\\nchallenge versions), and OpenBookQA [42]. The evaluations\\nwere run on model checkpoints taken every 10, 000 steps.\\nWe observe that most evaluation metrics appear to increase\\nsmoothly but noisily throughout training, before appearing to\\nplateau towards their final values. This is broadly in line with\\nprevious findings in the Pythia model suite [43], which find\\nrelatively smooth improvements across training in many of\\ntheir evaluation metrics. This provides some evidence that\\nthe development of capabilities in language models occurs\\nsmoothly and can be tracked during training and perhaps\\npredicted ahead of time. Two evaluation metrics, however,\\nWinoGrande and BoolQ, violate this trend for reasons that we\\ndo not currently understand. We note that [43] also observe\\n11\\nno consistent trend on Winogrande. Between the BlackMamba\\n340M/1.5B and 630M/2.8B models, we observe a clear\\nbenefit of scale at the same iteration and token count on\\nmost evaluations. In addition, we observe significant noise in\\nsome of the evaluation metrics which may suggest that small\\ndifferences in evaluations between different LLMs may not be\\nsignificant.\\nFig. 8. OpenBookQA evaluation accuracy over time\\nFig. 9. ARC-Easy evaluation accuracy over time\\nFig. 10. ARC-Challenge evaluation accuracy over time\\nFig. 11. WinoGrande evaluation accuracy over time\\nFig. 12. HellaSwag evaluation accuracy over time\\nFig. 13. PIQA evaluation accuracy over time\\nFig. 14. Lambada evaluation accuracy over time\\nF. Sinkhorn MoE Routing Modifications\\nRecall from the main text eq. (5) that the output token y of\\nan MoE layer is given by\\ny =\\nX\\ni∈top-k\\nciEi(x)\\n(28)\\nwhere E1, E2, . . . , EN denote the MLP experts according to\\nthe top-k probabilities pi.\\nMost commonly, the probabilities pi(x) are obtained act-\\ning by a trainable linear layer on the input x ∈Rd and\\nsubsequently applying a non-linearity: pi(x) = σ(Wi · x),\\nwith Wi ∈Rd. An important issue when training MoE\\nmodels is that expert utilization should be balanced across\\ntokens in a batch, which is required for compute efficiency.\\nStandard approaches to ensure balanced usage include adding\\na balancing regularization term to the loss as well imposing\\nhard constraints bounding the number of tokens a given expert\\ncan receive [15]. We instead use the Sinkhorn activation\\nfunction for the router which, in the context of top-1 expert\\n12\\nselection, has proven to solve the balancing issue without\\nthe need for additional regularization or constraints on expert\\nusage [22].\\nThe key property of the Sinkhorn activation function is\\nthat, in addition to requiring normalization with respect to the\\nexpert index i in pi(x), one additionally imposes normalization\\nalong the samples dimension (which comprises batch size and\\nsequence length). More explicitly, we require that σ satisfies:\\nN\\nX\\ni=1\\nσ(Wi · xα) = 1,\\nS\\nX\\nα=1\\nσ(Wi · xα) = S/N\\n(29)\\nwhere α denotes the sample index, and S is the number\\nof samples (batch size × sequence length). Now, note that\\nthe softmax, which only satisfies the first condition, can be\\nvariationally defined by maximizing:\\nsoftmax(L) ≡argmaxπ{π · L + S(π)}\\n(30)\\nwhere Liα\\n=\\nWi · xα are the logits, and S(π)\\n=\\n−P\\niα πiα log πiα is the Shannon entropy. The Sinkhorn acti-\\nvation can be defined through the same variational formulation\\nexcept that it further satisfies the second constraint in (29).\\nDenoting the solution to this maximization by\\nπiα = eLiαd(0)\\ni d(1)\\nα\\n(31)\\nwhere d(0) ∈RN and d(1) ∈RS, maximization of the right-\\nhand side of (30) subject to (29) is obtained by solving\\nd(0)\\ni\\n=\\n1\\nP\\nα eLiαd(1)\\nα\\n,\\nd(1)\\nα\\n= S\\nN\\n1\\nP\\ni eLiαd(0)\\ni\\n(32)\\nUnfortunately, these equations cannot be solved explicitly and\\nthus, unlike the softmax case, there is no analytic form for the\\nSinkhorn activation. These equations are solved approximately\\nthrough an optimization loop, called the Sinkhorn algorithm\\n[44].5 Our improvement is in the choice of the initial condition\\nfor this optimization loop, which consists of taking d(0)\\ni\\n= 1\\nand d(1)\\nα\\n=\\nS\\nN\\nP\\ni eLiα. This corresponds to initializing πiα\\nto be the softmax normalized along the sample index α,\\nthus immediately guaranteeing balanced usage of experts. We\\nverified empirically that choosing this initial condition leads\\nto much faster convergence of the Sinkhorn loop. Addition-\\nally, a temperature rescaling Liα →2Liα further improves\\nconvergence. Overall this led to shrinking the number of\\niterations from 10-20 to just 1 across various models sizes,\\nthus shortening the iteration time in our training experiments.\\n5We need to additionally choose ci. One natural choice is ci = pi, but\\nwith the Sinkhorn activation we verified that it is more efficient to choose\\nci = f(Wi · x) with f a simple activation function such as the sigmoid. We\\nthink this is due to the Sinkhorn flattening out more quickly than e.g. sigmoid\\nor softmax due to normalization along both dimensions.\\n13\\n', 'source_name': 'BlackMamba: Mixture of Experts for State-Space Models', 'source_url': 'https://arxiv.org/abs/2402.01771'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MoE_meets_instruction_tuning.pdf #27\n",
      "{'content': 'Mixture-of-Experts Meets Instruction Tuning:\\nA Winning Combination for Large Language Models\\nSheng Shen♮∗\\nLe Hou†\\nYanqi Zhou†\\nNan Du†\\nShayne Longpre⊤∗\\nJason Wei†,\\nHyung Won Chung†\\nBarret Zoph†\\nWilliam Fedus†\\nXinyun Chen†\\nTu Vu‡∗,\\nYuexin Wu†\\nWuyang Chen§∗\\nAlbert Webson†\\nYunxuan Li†\\nVincent Zhao†\\nHongkun Yu†\\nKurt Keutzer♮\\nTrevor Darrell♮\\nDenny Zhou†\\n†Google\\n♮University of California, Berkeley\\n⊤Massachusetts Institute of Technology\\n‡University of Massachusetts Amherst\\n§The University of Texas at Austin\\nAbstract\\nSparse Mixture-of-Experts (MoE) is a neural architecture design that can be uti-\\nlized to add learnable parameters to Large Language Models (LLMs) without\\nincreasing inference cost. Instruction tuning is a technique for training LLMs to\\nfollow instructions. We advocate combining these two approaches, as we find that\\nMoE models benefit more from instruction tuning than dense models. In particular,\\nwe conduct empirical studies across three experimental setups: (i) Direct finetun-\\ning on individual downstream tasks devoid of instruction tuning; (ii) Instruction\\ntuning followed by in-context few-shot or zero-shot generalization on downstream\\ntasks; and (iii) Instruction tuning supplemented by further finetuning on individual\\ndownstream tasks. In the first scenario, MoE models overall underperform dense\\nmodels of identical computational capacity. This narrative, however, dramatically\\nchanges with the introduction of instruction tuning (second and third scenario),\\nused independently or in conjunction with task-specific finetuning. Our most\\npowerful model, FLAN-MOE32B, surpasses the performance of FLAN-PALM62B\\non four benchmark tasks, while using only a third of the FLOPs. The advance-\\nments embodied by FLAN-MOE inspire a reevaluation of the design principles of\\nlarge-scale, high-performance language models in the framework of task-agnostic\\nlearning.\\n1\\nIntroduction\\nThe recent years have witnessed remarkable advancements in the field of natural language processing\\n(NLP), driven by the development of increasingly large and sophisticated deep learning models.\\nAmong these models, transformer-based language models [49] have emerged as the de facto standard\\nfor a wide range of NLP tasks, owing to their unparalleled capabilities in capturing complex linguistic\\npatterns and generalizing across diverse contexts. One particularly successful paradigm for training\\nsuch models is instruction-tuning [44, 52, 4, 28, 34, 38], which enhances their performance on\\nspecific tasks by adapting their pre-trained representations to follow natural language instructions.\\n* Work done at Google\\nPreprint. Under review.\\narXiv:2305.14705v2  [cs.CL]  5 Jul 2023\\nWhile the benefits of Large Language Models (LLMs) are indisputable, their rapidly growing size\\nand computational requirements pose significant challenges in terms of training efficiency, memory\\nfootprint, and deployment costs. Consequently, there is a pressing need for developing scalable\\ntechniques that can harness the power of these models without incurring prohibitive computational\\noverheads.\\nOn the other hands, models with sparsely activated Mixture of Experts (MoEs) significantly reduce\\nthe computational cost of LLMs. MoE models build upon the observation that language models can\\nbe decomposed into smaller, specialized sub-models, or \"experts\", that focus on distinct aspects of\\nthe input data, thereby enabling more efficient computation and resource allocation. However, we\\nshow that conventional, task-specific finetuning MoE models lead to suboptimal performance, often\\neven worse than finetuning dense models with the same computational cost. One of the possible\\nreasons is the discrepancy between general pretraining and task-specific finetuning.\\nIn this paper, we illuminate the pivotal role of instruction-tuning within the context of Mixture-of-\\nExperts (MoE) models, specifically in terms of their successful scalability on downstream tasks.\\nWe demonstrate this through a two-fold analysis: Firstly, we expand on the known benefits of\\ninstruction-tuning for task-specific downstream finetuning [28], illustrating its significantly larger\\nimpact when applied to MoE models compared to their dense equivalents. Secondly, we emphasize the\\nnecessity of an instruction-tuning stage for MoE models [45, 10, 12, 23] to surpass the performance\\nof dense models on downstream and held-out tasks. Our unique amalgamation, FLAN-MOE, is an\\ninstruction-tuned model built on the Flan mixture[4], which successfully harnesses the strengths of\\nboth instruction-tuning and the sparse MoE technique. FLAN-MOE effectively and efficiently scales\\nup language models, without necessitating a rise in computational resources or memory requirements.\\nWe subject our model, FLAN-MOE, to a battery of tests across an array of tasks encompassing natural\\nlanguage understanding, reasoning, and question answering. Our evaluation framework consists of\\nthree distinct setups: (i) Direct finetuning of the model on individual downstream tasks; (ii) Instruction\\ntuning succeeded by in-context, few-shot, or zero-shot generalization on downstream tasks; and\\n(iii) Instruction tuning enhanced with subsequent finetuning on individual downstream tasks. The\\nresults spotlight FLAN-MOE’s marked superiority over its dense counterparts in the second and third\\nsettings. Notably, these advancements materialize without the need for augmented computational\\nresources or memory requisites. Our top-tier model, in fact, manages to eclipse the performance of a\\nFLAN-PALM equivalent, requiring only a third of the computational cost per token on four separate\\nbenchmarks.\\nTo summarize, our contributions are as follows:\\n• We establish the critical role of instruction-tuning in the efficacy of MoE models:\\n– We demonstrate that in the absence of instruction tuning, MoE models fall short in\\nperformance when compared to dense models on downstream tasks.\\n– We highlight that when supplemented with instruction tuning, MoE models exceed the\\nperformance of dense models on downstream tasks, as well as on held-out zero-shot\\nand few-shot tasks.\\n• We present a comprehensive series of experiments, offering a comparative analysis of the\\nperformance of diverse MoE models subjected to instruction-tuning.\\n2\\nMethod\\n2.1\\nModel Architecture\\nWe leverage sparsely activated Mixture-of-Experts (MoE) [23, 12, 55] in FLAN-MOE models. Similar\\nto the Switch Transformer [12], we replace the feed-forward component of every other Transformer\\nlayer with an MoE layer. Each MoE layer consists of a collection of independent feed-forward\\nnetworks as the ‘experts’. A gating function then uses a softmax activation function to model a\\nprobability distribution over these experts. This distribution indicates how well each expert is able to\\nprocess the incoming input. Even though each MoE layer has many more parameters, the experts are\\nsparsely activated. This means that for a given input token, only a limited subset of experts is used,\\ngiving the model more capacity while limiting computation. In our architecture, the subset size is\\neither one or two depending on the routing strategy. Each MoE layer’s learnable gating network is\\n2\\n0\\n9\\n89\\n282\\n682\\n1,836\\n# Tasks for Instruction-Finetuning\\n50\\n60\\n70\\n80\\n90\\nAvg Eval Metrics (%)\\n-7.1\\n+6.6\\n+9.3\\n+9.7\\n+10.2\\n+0.2\\n+13.2\\n+14.4\\n+15.0\\n+15.6\\nHeld-Out Eval\\n0\\n16\\n32\\n64\\n128\\n# Experts for Flan-MoE\\n50\\n60\\n70\\n80\\n90\\nAvg Eval Metrics (%)\\n+10.2\\n+13.0\\n+13.3\\n+13.6\\n+13.9\\nHeld-Out Eval\\n9\\n89\\n282\\n682\\n1,836\\n#Taks for Instruction-Finetuning\\n50\\n60\\n70\\n80\\n90\\nAvg Eval Metrics (%)\\n-7.1\\n+-6.6\\n+-9.3\\n+-9.7\\n+-10.2\\n+-0.2\\n+-13.2\\n+-14.4\\n+-15.0\\n+-15.6\\nT5\\nFT\\nFlan-T5\\nFT\\nMoE\\nFT\\nFlan-MoE\\nFT\\nFigure 1: The effect of instruction tuning on MOE models versus dense counterparts for base-size\\nmodels (same flops across all models in this figure). We perform single-task finetuning for each\\nmodel on held-out benchmarks. Compared to dense models, MoE models benefit more from\\ninstruction-tuning, and are more sensitive to the number of instruction-tuning tasks. Overall,\\nthe performance of MoE models scales better with respect to the number of tasks, than the number of\\nexperts.\\ntrained to use its input to activate the best two experts for each token of an input sequence. During\\ninference, the learned gating network dynamically picks the two best experts for each token. For an\\nMoE layer with E experts, this essentially provides a collection of O(E2) different combinations\\nof feed-forward networks instead of one in the classic Transformer architecture, enabling greater\\ncomputational flexibility. The final learned representation of a token will be the weighted combination\\nof the outputs from the selected experts.\\n2.2\\nInstruction Fine-tuning Recipe\\nWe fine-tune FLAN-MOE using the prefix language model objective on the FLAN collective dataset [4,\\n28]. Each FLAN-MOE will inherit the auxiliary loss setting during pre-training. All the model\\nparameters will be updated. We adapt the sequence length of each FLAN-MOE to 2, 048 for input\\nand 512 for output based on the relative position embedding. The dropout rate is 0.05 and the expert\\ndropout rate is 0.2. The learning rate is 1e−4. The optimizer setting follows [4].\\n3\\nExperiment\\nWe study FLAN-MOE in the context of instruction-tuning. We first perform a controlled comparison\\nof FLAN-MOE to an equivalent “standard” dense encoder-decoder Transformer (T5), across a range\\nof model sizes in Section 3.2. We subsequently demonstrate in Section 3.3 that scaling up our model,\\nreferred to as FLAN-MOE, can attain remarkable performance levels. Our most extensive model,\\nFLAN-ST32B, surpasses the performance of FLAN-PALM62B while utilizing less than 30% of FLOPs\\nper token. We further ablate the various design decisions in the next Section.\\n3.1\\nSettings\\nTraning Data.\\nBy default, all models are trained on the 1,836 finetuning tasks by combining four\\nmixtures from prior work: Muffin, T0-SF, NIV2, and CoT, as in [4]. Specifically, Muffin comprises\\n80 tasks from [52] and 26 dialog/program synthesis tasks; T0-SF comprises 193 tasks from [44];\\nNIV2 comprises 1554 tasks from [51]; CoT comprises 9 reasoning tasks.\\nEvaluations.\\nWe conduct both zero-shot and few-shot evaluations on held-out tasks as in [4] which\\nwere not included as part of the finetuning data. We use MMLU [16] that includes exam questions\\nfrom 57 tasks such as mathematics, history, law, and medicine; BBH includes 23 challenging\\n3\\nModel\\nFLOPs\\nTotal\\nMMLU\\nBBH\\nReasoning\\nQA\\nNorm. Avg.\\nper token\\n# Params\\nDirect\\nCoT\\nDirect\\nCoT\\nCoT\\nDirect\\nT5SMALL\\n0.06G\\n80M\\n26.7\\n7.2\\n26.7\\n5.6\\n10.3\\n33.8\\n26.3\\nFLAN-T5SMALL\\n0.06G\\n80M\\n28.7\\n12.1\\n29.1\\n19.2\\n15.0\\n40.9\\n28.7 (+2.4)\\nT5BASE\\n0.3G\\n250M\\n25.7\\n14.1\\n27.7\\n14.6\\n14.7\\n35.3\\n26.2\\nFLAN-T5BASE\\n0.3G\\n250M\\n35.6\\n33.3\\n30.3\\n26.8\\n16.4\\n48.8\\n33.9 (+7.7)\\nT5LARGE\\n1.0G\\n780M\\n25.1\\n15.3\\n27.7\\n16.2\\n11.9\\n36.4\\n25.7\\nFLAN-T5LARGE\\n1.0G\\n780M\\n44.7\\n38.9\\n34.7\\n28.5\\n22.2\\n64.6\\n42.0 (+16.3)\\nT5XL\\n3.6G\\n3B\\n25.3\\n14.1\\n27.4\\n19.3\\n14.2\\n38.2\\n25.9\\nFLAN-T5XL\\n3.6G\\n3B\\n50.3\\n46.1\\n40.2\\n35.9\\n33.9\\n74.1\\n48.0 (+22.1)\\nT5XXL\\n13.9G\\n11B\\n26.1\\n19.1\\n29.5\\n19.3\\n21.4\\n47.4\\n27.7\\nFLAN-T5XXL\\n13.9G\\n11B\\n52.6\\n47.9\\n45.6\\n41.6\\n46.3\\n80.4\\n51.7 (+24.0)\\nPaLM\\n12.6G\\n8B\\n24.3\\n24.1\\n30.8\\n30.1\\n24.9\\n47.6\\n27.1\\nFLAN-PaLM\\n12.6G\\n8B\\n49.3\\n41.3\\n36.4\\n31.1\\n36.9\\n75.1\\n47.5 (+20.4)\\nPaLM\\n91.6G\\n62B\\n55.1\\n49.0\\n37.4\\n43.0\\n50.6\\n70.4\\n51.0\\nFLAN-PaLM\\n91.6G\\n62B\\n59.6\\n56.9\\n47.5\\n44.9\\n59.7\\n85.3\\n57.6 (+6.6)\\nPaLM\\n847G\\n540B\\n71.3\\n62.9\\n49.1\\n63.7\\n72.6\\n86.0\\n66.2\\nFLAN-PaLM\\n847G\\n540B\\n73.5\\n70.9\\n57.9\\n66.3\\n76.5\\n89.9\\n70.3 (+4.1)\\nSwitchBASE\\n0.3G\\n3.5B\\n28.3\\n13.6\\n0.1\\n1.4\\n5.2\\n35.8\\n20.2\\nFLAN-SwitchBASE\\n0.3G\\n3.5B\\n38.0\\n34.2\\n33.2\\n29.4\\n18.6\\n58.0\\n36.8 (+16.6)\\nSwitchLARGE\\n1.0G\\n26B\\n24.0\\n23.1\\n0.2\\n7.2\\n12.4\\n33.7\\n17.7\\nFLAN-SwitchLARGE\\n1.0G\\n26B\\n46.1\\n40.3\\n36.3\\n28.0\\n25.3\\n66.5\\n43.5 (+25.8)\\nSwitchXXL\\n13.9G\\n395B\\n24.6\\n15.1\\n0.0\\n6.7\\n9.2\\n32.5\\n17.8\\nFLAN-SwitchXXL\\n13.9G\\n395B\\n55.6\\n50.1\\n47.9\\n43.5\\n46.6\\n78.8\\n54.2 (+36.4)\\nGSSMALL\\n0.06G\\n0.3B\\n23.9\\n0.0\\n0.2\\n0.8\\n0.8\\n24.1\\n16.7\\nFLAN-GSSMALL\\n0.06G\\n0.3B\\n32.6\\n26.9\\n29.6\\n20.9\\n16.1\\n48.9\\n31.8 (+15.1)\\nGSBASE\\n0.3G\\n1.3B\\n25.0\\n15.9\\n0.0\\n4.8\\n3.8\\n26.8\\n17.6\\nFLAN-GSBASE\\n0.3G\\n1.3B\\n39.9\\n33.6\\n33.7\\n25.1\\n22.0\\n57.9\\n38.3 (+20.7)\\nGSLARGE\\n1.0G\\n9.2B\\n26.4\\n12.8\\n0.2\\n14.3\\n13.0\\n31.9\\n19.2\\nFLAN-GSLARGE\\n1.0G\\n9.2B\\n47.8\\n40.8\\n35.0\\n29.2\\n27.6\\n69.5\\n44.5 (+25.3)\\nGSXL\\n03.6G\\n17.4B\\n25.7\\n10.0\\n0.0\\n0.0\\n10.4\\n35.0\\n18.7\\nFLAN-GSXL\\n3.6G\\n17.4B\\n51.1\\n42.3\\n40.1\\n31.4\\n34.3\\n73.9\\n48.7 (+30.0)\\nECSMALL\\n0.06G\\n0.3B\\n25.3\\n1.2\\n0.1\\n2.3\\n0.8\\n36.0\\n18.1\\nFLAN-ECSMALL\\n0.06G\\n0.3B\\n34.1\\n25.1\\n29.2\\n22.1\\n16.6\\n58.1\\n33.1 (+15.0)\\nECBASE\\n0.3G\\n1.3B\\n25.0\\n25.9\\n0.0\\n1.4\\n14.3\\n35.7\\n18.5\\nFLAN-ECBASE\\n0.3G\\n1.3B\\n42.7\\n33.0\\n34.0\\n26.7\\n22.2\\n61.5\\n40.3 (+21.8)\\nECLARGE\\n1.0G\\n9.2B\\n23.4\\n12.6\\n0.0\\n8.6\\n6.7\\n40.1\\n17.3\\nFLAN-ECLARGE\\n1.0G\\n9.2B\\n48.3\\n44.5\\n37.9\\n32.0\\n32.2\\n73.1\\n46.4 (+29.1)\\nECXL\\n3.6G\\n17.4B\\n26.7\\n11.0\\n0.0\\n1.9\\n12.4\\n34.2\\n19.4\\nFLAN-ECXL\\n3.6G\\n17.4B\\n52.1\\n41.4\\n40.3\\n33.2\\n38.1\\n74.3\\n49.4 (+30.0)\\nSTBASE\\n0.3G\\n1.3B\\n25.2\\n17.7\\n0.0\\n14.0\\n12.6\\n25.7\\n18.1\\nFLAN-STBASE\\n0.3G\\n1.3B\\n42.4\\n35.5\\n34.9\\n26.4\\n22.5\\n61.5\\n40.4 (+21.8)\\nST32B\\n32.1G\\n259B\\n25.5\\n15.1\\n0.0\\n5.5\\n9.8\\n32.1\\n18.4\\nFLAN-ST32B\\n32.1G\\n259B\\n65.4\\n63.0\\n54.4\\n47.4\\n66.3\\n63.9\\n63.6 (+45.2)\\nTable 1: MoE models improve instruct fine-tuning performance on top of dense counterparts. The\\nbenchmark suites are MMLU (57 tasks), BBH (23 tasks), Reasoning (4 Tasks), and QA (4 Tasks).\\nThe evaluation metric across all benchmarks is few-shot prompted accuracy, specifically the exact\\nmatch. To calculate this metric, we take an unweighted average across all tasks. For a comprehensive\\nevaluation, we report the normalized average of MMLU-direct, BBH-direct, Reasoning-CoT, and\\nQA-Direct. The MMLU and BBH evaluation benchmarks are held-out (not included in the finetuning\\ndata.) while the Reasoning and QA evaluation benchmarks are held-in. (Noted that FLAN-ST32B\\noutperforms FLAN-PALM62B while being <30% of the FLOPS.)\\n4\\nArch:\\nType:\\n10¤1\\n100\\n101\\nGFlops Per Token Prediction\\n20\\n30\\n40\\n50\\nAvg. Held-Out Score\\nMMLU-Direct 0Shot\\n10¤1\\n100\\n101\\nGFlops Per Token Prediction\\n20\\n25\\n30\\n35\\n40\\n45\\nAvg. Held-Out Score\\nBBH-Direct 0Shot\\nFigure 2: Average zero performance of FLAN-MOE models versus FLAN-T5 dense models for\\nsimilar effective FLOPs per token over the 57 MMLU tasks and 23 BBH tasks.\\ntasks from BIG-Bench [47]; The reasoning benchmark comprises four tasks: GSM8K [8] and\\nSVAMP [40]/ASDIV [32] incorporate the grade school math word problems and the elementary-level\\nmath word problems, and StrategyQA [13] measures open-domain questions where the required\\nreasoning steps are implicit in the question; The QA benchmark include four QA tasks: the elementary\\nAI2 science category in UnifiedQA [20], BoolQ [6], ARC-easy and ARC-challenge [7] that covers\\nQA tasks in abstract, yes/no, multiple-choice formats. For MMLU and BBH, we evaluate both\\nthe ability of directly predicting the answer via direct prompting, where the model directly gives\\nthe answer [4], as well as via chain-of-thought (CoT) prompting, where the model must provide\\na reasoning chain before giving the final answer [53]. For reasoning tasks, we only measure CoT\\nprompting accuracy. For all benchmarks except for QA we use the given few-shot exemplars, with\\nthe number of exemplars following prior work: five-shot for MMLU, three-shot for BBH, eight-shot\\nfor reasoning tasks, and zero-shot for QA. For a given model we also report a single “normalized\\naverage” metric, following the “normalized preferred metric” in BIG-Bench [47]. Our normalized\\naverage metric is the macro-average over four normalized scores: MMLU-Direct, BBH-Direct,\\nReasoning-CoT, and QA-Direct. Results for all tasks in each benchmark are reported in Appendix.\\n3.2\\nControlled study across scales\\nWe instruction finetune a range of FLAN-MOE models at batch size 32 and sequence length 2048 for\\n200k steps. This matches the number of training examples used for FLAN-T5 [4]. We re-finetuning\\nour own FLAN-T5 variants for fair comparisons.\\nDense Model Size.\\nFigure 2 shows the performance of each model (dense and sparse) against\\nforward-pass FLOPs. The cost-performance Pareto frontier for FLAN-MOE dominates the dense\\nmodels by a wide margin, indicating that FLAN-MOE offers strong improvements across all scales\\nfrom small, up to xxl. The effect is particularly large on zero-shot and few-shot MMLU-Direct,\\nwith absolute performance improvements of 7.1% on average. For challenging tasks in BBH-Direct,\\nFLAN-MOE offers a strong boost at small scales, while at larger scales the gains are more modest but\\nstill significant.\\nExpert Number.\\nThe performance of FLAN-MOE models has been observed to scale with the\\nnumber of experts included in the architecture, but it tends to saturate beyond a certain threshold.\\nInitially, as the number of experts increases in Figure 4, the model benefits from a richer repertoire of\\nspecialized sub-networks, each capable of handling distinct tasks or aspects of the problem space.\\nThis diverse ensemble enables the MoE model to demonstrate enhanced adaptability and efficiency\\nin processing complex tasks, leading to improved performance overall. However, as the number of\\nWe use 64 experts for SMALL, BASE, 32B, XL and 128 experts for all the other model sizes following [12,\\n55, 56]\\n5\\n50\\n100\\n150\\n200\\nNumber of Steps (k)\\n10\\n20\\n30\\n40\\n50\\nAvg. Held-Out Score\\nMMLU-Direct Few-Shot\\n50\\n100\\n150\\n200\\nNumber of Steps (k)\\n15\\n20\\n25\\n30\\n35\\nAvg. Held-Out Score\\nBBH-Direct Few-Shot\\nFigure 3: Learning efficiency comparison. Average zero-shot, and few-shot performance of FLAN-\\nMOE models versus FLAN-T5 dense models as more tokens are processed during training on FLAN\\nTasks.\\nexperts continues to grow, the performance gains begin to diminish, eventually reaching a point of\\nsaturation for BASE-sized model.\\nRouting Strategy\\nRouting strategy is an essential component of Mixture-of-Experts (MoE) models,\\nplaying a pivotal role in determining the effectiveness and efficiency of these models. The primary\\nfunction of the routing strategy is to intelligently distribute input data among multiple specialized\\nexperts, each optimized for handling specific subsets of the input space. This distribution process is\\ncrucial for maximizing the utilization of the model’s capacity while minimizing the risk of overfitting.\\nAn effective routing strategy not only ensures that the appropriate experts are selected for a given\\ninput, but also that resources are allocated optimally, leading to enhanced computational efficiency\\nand faster training times. Consequently, there have been two trending strategies, token-choice [23]\\nwhich lets the token select the top-K experts, and expert-choice [55] which lets the experts select the\\ntop-K tokens.\\nWe presented a detailed study about how different routing decisions affect the instruct fine-tuning\\nperformance in Figure 3 and Table 1, which includes the checkpoints from Switch Transformer\\ntop-1 token-choice gating (FLAN-Switch), GShard top-2 token-choice gating (FLAN-GS) and expert-\\nchoice top-2 gating (FLAN-EC) models pre-trained on the same GLaM [10] dataset. It is evident\\nthat activating more experts, as demonstrated by the comparison between the FLAN-Switch and\\nFLAN-GS strategies, results in enhanced performance across all four benchmarks. Among these\\nbenchmarks, the MMLU-Direct model shows the most significant improvement, with an increase\\nfrom 38.0% to 39.9% for BASE/LARGE-sized models. Although the gains at the extra-large scale\\nare more modest, they remain noteworthy and meaningful. It’s noteworthy that instruction-tuning\\nsignificantly amplifies the performance of both held-out MMLU, BBH, and held-in QA and reasoning\\nbenchmarks for MoE models in comparison to dense models of equivalent capacity. The advantages\\nare amplified even further for larger MoE models. For instance, instruction-tuning enhances the\\nperformance of ST32B by a substantial 45.2%, while the improvement observed for FLAN-PALM62B\\nis comparatively modest at around 6.6%.\\nFurthermore, the FLAN-EC strategy consistently outshines the FLAN-GS approach for the given\\nmodel across various scales and tasks. It is noteworthy that the performance gap between the token-\\nchoice and expert-choice models can be bridged when we incorporate advanced auxiliary loss and\\npre-training strategy as exhibited in ST-MOE [56]. This integration led to the development of our\\nFLAN-ST models. Considering that the largest ST-MOE set the benchmark in a variety of NLP tasks\\nwhen appropriately fine-tuned, we have also decided to scale up FLAN-ST, employing instruction\\nfine-tuning.\\n3.3\\nScaling up FLAN-MOE\\nWe increase the architecture size to assess the performance of FLAN-MOE in the large-scale regime.\\nAs discussed above, we instruction fine-tune the largest ST-MoE32B [56] model with 12 expert layers\\nin encoder, and decoder, respectively; these are non-uniformly distributed, with 64 experts per layer,\\n6\\n40\\n60\\n80\\n100\\n120\\nExpert Number\\n30.5\\n31.0\\n31.5\\n32.0\\n32.5\\n33.0\\nAvg. Held-Out Score\\n(a) Scaling (MMLU)\\n40\\n60\\n80\\n100\\n120\\nExpert Number\\n32\\n34\\n36\\n38\\n40\\nAvg. Held-Out Score\\n(b) Scaling (BBH)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nGFlops Per Token Prediction\\n30\\n32\\n34\\n36\\n38\\nAvg. Held-Out Score\\nFlan-MoE-Switch\\nFlan-MoE-GS\\nFlan-MoE-EC\\n(c) Routing (MMLU)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nGFlops Per Token Prediction\\n35\\n40\\n45\\nAvg. Held-Out Score\\nFlan-MoE-Switch\\nFlan-MoE-GS\\nFlan-MoE-EC\\n(d) Routing (BBH)\\nFigure 4: Average few-shot performance of FLAN-MOE models over the 57 MMLU tasks and 23\\nBBH tasks. (Different color represents different dense model sizes.)\\n50\\n100\\n150\\n200\\nNumber of Steps (k)\\n20\\n25\\n30\\n35\\n40\\nAvg. Held-Out Score\\nMMLU-Direct Few-Shot (Flan-ECbase)\\nBaseline\\nFreeze-Gate\\nFreeze-Expert\\nFreeze-MoE\\nZ-loss\\nBalance-loss\\n50\\n100\\n150\\n200\\nNumber of Steps (k)\\n15\\n20\\n25\\n30\\n35\\n40\\nAvg. Held-Out Score\\nMMLU-Direct Few-Shot (Flan-STbase)\\nBaseline\\nFreeze-Gate\\nFreeze-Expert\\nFreeze-MoE\\nZ-loss\\nBalance-loss\\nFigure 5: Average few-shot performance of FLAN-MOE with different finetuning strategy.\\nand K = 2 activated per token. It was trained at a batch size of 32 and sequence length of 2048 for\\n200k steps. We average checkpoints towards the end of training. The model FLAN-ST32B, comprising\\na total of 32 billion parameters, only utilizes 32.1 GFLOPs per token, which amounts to merely\\none-third of the computational power required by a FLAN-PALM62B model. Additionally, all the\\nrouters combined account for less than 4 million parameters. Table 1 illustrates the performance of\\nthis model alongside current state-of-the-art instruct fine-tuned models.\\nFLAN-ST32B achieves a 65.4% few-shot MMLU benchmark accuracy and a 54.4% few-shot BBH\\nbenchmark accuracy, with a relatively modest architectural size and training count. Notably, FLAN-\\nST32B surpasses the performance of FLAN-PALM62B, which consumes nearly triple the compute\\nresources, by a substantial margin across all four benchmarks. However, it is important to ac-\\nknowledge the considerable performance gap that persists between the largest FLAN-PALM540B and\\nFLAN-ST32B models.\\n4\\nDiscussion\\n4.1\\nFinetuing Strategy\\nSparse models have performed remarkably well in the regime of large datasets, but have sometimes\\nperformed poorly when finetuning data is limited [56, 12]. Instruction finetuning can also be viewed\\nas a continual finetuning stage, so we present a detailed study about how different factors impact the\\ninstruct finetuning performance of FLAN-MOE and offer a practical recipe. All the discussion here is\\nbased on instruction finetuning FLAN-ECBASE/FLAN-STBASE for 100k steps.\\nAuxiliary Loss.\\nThe incorporation of auxiliary loss [23, 56] helps mitigate the risk of overfitting by\\npromoting the diversification of the experts’ knowledge and improving the model’s generalization\\ncapabilities for sparsely gated mixture-of-expert models. Furthermore, auxiliary losses can be\\nemployed to address specific issues, such as load balancing among experts or preventing expert\\ncollapse, which can further enhance the model’s overall performance. We experiment with both\\nbalancing loss that is used in [23] and router Z-loss that is used in [56] in Table 2. The implementation\\nof balancing loss contributed to enhanced performance on MMLU, BBH, and GSM8K for FLAN-\\n7\\nFinetuning\\nMMLU\\nBBH\\nGSM8K\\nAvg.\\nStrategy\\nDirect\\nDirect\\nCoT\\nBaselineFLAN-ECBASE\\n40.0\\n33.2\\n6.6\\n37.7\\nFreeze-GateFLAN-ECBASE\\n40.2\\n33.9\\n6.6\\n38.0\\nFreeze-ExpertFLAN-ECBASE\\n38.3\\n32.5\\n5.4\\n36.2\\nFreeze-MoEFLAN-ECBASE\\n38.4\\n32.2\\n5.3\\n36.2\\nZ-lossFLAN-ECBASE\\n38.9\\n32.8\\n5.7\\n36.8\\nBalance-lossFLAN-ECBASE\\n40.8\\n33.4\\n7.1\\n38.3\\nFinetuning\\nMMLU\\nBBH\\nGSM8K\\nAvg.\\nStrategy\\nDirect\\nDirect\\nCoT\\nBaselineFLAN-STBASE\\n40.1\\n33.3\\n6.4\\n37.8\\nFreeze-GateFLAN-STBASE\\n40.6\\n33.5\\n6.4\\n38.2\\nFreeze-ExpertFLAN-STBASE\\n39.6\\n32.9\\n4.5\\n37.3\\nFreeze-MoEFLAN-STBASE\\n39.2\\n32.9\\n3.6\\n36.9\\nZ-lossFLAN-STBASE\\n40.6\\n33.4\\n6.5\\n38.1\\nBalance-lossFLAN-STBASE\\n38.8\\n31.3\\n3.6\\n36.2\\nTable 2: Ablations on different finetuning strategies of FLAN-ECBASE and FLAN-STBASE.\\nECBASE, whereas Z-loss resulted in a deterioration of performance. Conversely, for FLAN-STBASE,\\nwe observed a contrasting trend. We conjecture that the discordance between the auxiliary loss during\\npre-training and instruction-tuning could potentially disrupt the optimization process, thereby leading\\nto a suboptimally optimized FLAN-MOE model.\\nExpert/Gating Freeze.\\nIn an effort to enhance the generalization capabilities of sparse models and\\ncombat overfitting, researchers have discovered that finetuning a subset of model parameters results\\nin improved generalization performance for ST-MoE models, as noted in the study by ST-MoE [56].\\nInterestingly, it was observed that updating non-MoE parameters yields similar outcomes to updating\\nall parameters, while updating only expert parameters performs slightly better.\\nWe conducted experiments by freezing the gating function, expert modules, and MoE parameters\\nof the given model, as presented in Table 2. The results indicate that freezing either the expert or\\nMoE components negatively impacts performance. Conversely, freezing the gate slightly improves\\nperformance, albeit not significantly. We postulate that this observation is related to the under-fitting\\nof the FLAN-MOE, as in Figure 5, which depicts the finetuning data efficiency ablation study.\\nHyperparameter Sensitivity.\\nFollowing ST-MoE [56], we further experiment with expert dropout\\n(0.0, 0.1, 0.5), varying the learning rate (1e−4, 5e−4, 1e−3) and batch size (16, 32, 64) to examine\\nthe hyperparameter sensitivity of FLAN-MOE. We found that the performance varies in different\\ntasks but not significantly with all the hyperparameters, but lower learning rate and small batch size\\nlead to a more stable instruction finetuning process of the model at extra-large scales.\\nFinetuning v.s. Instruction Finetuning.\\nTo compare the gap between finetuning MoE directly and\\nFLAN-MOE, we experiment with single-task finetuned MoE, single-task finetuned FLAN-MOE, and\\ndense counterparts in Figure 6. We perform hyper-parameter search for each finetuning setting.\\nFor the examined Held-Out tasks, we observed that the improvement of FLAN-MOE over finetuning\\nMoE is noticeably larger compared to the performance gap between FLAN-T5 and T5. This difference\\nbecomes even more pronounced when there is a scarcity of labeled data or when the model size is\\nincreased. These observations confirm the benefits of FLAN-MOE in mitigating overfitting issues\\nassociated with directly finetuning MoE.\\nDespite their advantages such as increased adaptability and efficiency in managing complex tasks,\\nMoE architectures are prone to overfitting during the finetuning process, as discussed in citation. This\\ncan be seen in Figures 6 and 1, where single-task fine-tuned MoE models sometimes underperform\\ntheir dense T5 counterparts.\\nInterestingly, compared to dense models, MoE models derive greater benefits from instruction-tuning\\nand are more sensitive to the number of instruction-tuning tasks. In general, MoE model performance\\nscales better with respect to the number of tasks rather than the number of experts. We hypothesize\\nthis is primarily due to the specialized nature of individual experts, which can lead to heightened\\nsensitivity to noise and limited generalization capabilities when exposed to unseen data.\\n4.2\\nAdditional Analysis\\nExpert Specialization.\\nAs the size of a FLAN-MOE model increases in Figure 7, a notable rise in\\nexpert specialization tends to occur. Larger models entail a higher number of parameters and more\\ncomplex structures, which inherently provide a broader scope for each expert to specialize in specific\\nfacets of the problem space. This increased specialization can be understood as a form of division of\\nlabor, where each expert sub-network becomes adept at handling a certain type of task or data pattern.\\n8\\nCondaQA\\nCxC\\nPubmedQA\\nSearchQA\\n50\\n60\\n70\\n80\\n90\\nEval Metrics (%)\\n+8.3\\n+12.7\\n+7.8\\n+12.0\\n+14.9\\n+17.8\\n+13.2\\n+16.4\\nHeld-Out Eval\\n(a) FLAN-ECBASE v.s. FLAN-T5BASE\\nCondaQA\\nCxC\\nPubmedQA\\nSearchQA\\n50\\n60\\n70\\n80\\n90\\nEval Metrics (%)\\n+17.2\\n+13.4\\n+2.2\\n+6.6\\n+22.3\\n+19.4\\n+19.6\\n+13.7\\nHeld-Out Eval\\n(b) FLAN-ECLARGE v.s. FLAN-T5LARGE\\n9\\n89\\n282\\n682\\n1,836\\n#Taks for Instruction-Finetuning\\n50\\n60\\n70\\n80\\n90\\nAvg Eval Metrics (%)\\n-7.1\\n+-6.6\\n+-9.3\\n+-9.7\\n+-10.2\\n+-0.2\\n+-13.2\\n+-14.4\\n+-15.0\\n+-15.6\\nT5\\nFT\\nFlan-T5\\nFT\\nMoE\\nFT\\nFlan-MoE\\nFT\\nFigure 6: FLAN-MOE Outperforms MoE on Single-Task Finetuning. We compare single-task\\nfinetuned MoE, single-task finetuned FLAN-MOE, and dense counterparts. The performance gap\\nbetween FLAN-MOE and MoE is noticeably larger than that between FLAN-T5 and T5.\\nConsequently, the overall model can demonstrate a higher degree of adaptability and precision in\\ntackling diverse and complex tasks. We also observe that after instruction-tuning, the MoE models\\nexhibit better expert usage, which may help prevent the expert collapse for generalization after\\ninstruction-tuning as in [57].\\n50\\n100\\n150\\n200\\nNumber of Steps (k)\\n63\\n64\\n65\\n66\\nExpert Usage (%)\\nFigure 7: Expert usage of FLAN-EC at differ-\\nent scales during instruction finetuning, where\\nlarger models entail smaller expert usage.\\nFailure Cases.\\nThe fine-grained specialization of\\nFLAN-MOE models, particularly when fine-tuned on\\nEnglish-only instructions, can inadvertently lead to a\\nnarrowing of the model’s capacity to effectively pro-\\ncess and generate content in multiple languages. We\\nfound all the FLAN-MOE perform poorly on multi-\\nlingual benchmarks including TyDiQA and MGSM.\\nEven the largest FLAN-ST32B only achieves 15.5%\\non MGSM and 25.1% on TyDiQA, which is only\\ncomparable to the vanilla PaLM62B with 18.2% on\\nMSGM, and PaLM8B with 25.0% on TyDiQA. It also\\nunderperform FLAN-PALMvariants. We hypothe-\\nses that this issue may stes from the model’s over-\\noptimization towards the specificities of the English\\nlanguage during finetuning, which can impede its\\nability to navigate the complexities of other languages. Consequently, while MoE models offer\\nsignificant benefits in terms of task-specific adaptability and efficiency, their potential shortcomings\\nin multilinguality highlight the importance of incorporating diverse linguistic data during the training\\nprocess to ensure broad and effective language coverage.\\n5\\nRelated Work\\nInstruction Tuning.\\nInstruction tuning has evolved as a strategy to enhance the functionality\\nand interactivity of large language models (LLMs) for dialogues and complex tasks. Prior studies,\\nincluding [41, 27, 1], have delved into large-scale multi-task fine-tuning to enhance the downstream\\nsingle target fine-tuning, albeit without instruction prompts. Initiatives such as UnifiedQA [20, 31, 19]\\nhave amalgamated a multitude of NLP tasks into a singular generative question answering format,\\nutilizing prompt instructions for multi-task fine-tuning and evaluation.\\nEfforts like Natural Instructions [33], Flan 2021 [52], and P3 (the Public Pool of Prompts, [44]) have\\ncollated vast NLP task collections, templatizing them with instructions for fine-tuning models to en-\\nhance their adaptability to unseen instructions. Some studies, such as Super-Natural Instructions [51]\\n9\\nand OPT-IML [18], took this a step further by combining numerous datasets and tasks into a single\\nresource. In the meantime, others like xP3 [35] introduced multilingual instruction tuning and Flan\\n2022 [4] employed Chain-of-Thought training prompts.\\nRecently, there has been a move towards expanding task diversity more assertively using synthetic\\ndata generation, particularly for creative and open-ended dialogue [50, 17, 54]. Some researchers\\nhave also tried to provide human feedback on language model responses [39, 14, 37, 3, 2], or bridge\\nthe modality gap with multi-modal instruction fine-tuning [26, 9, 25].\\nSparse Mixture of Experts models.\\nThe foundation of our work is built on the concept of deep\\nsparse Mixture-of-Experts (MoEs), a topic that has been independently explored in both Computer\\nVision [42, 29, 36, 46] and Natural Language Processing [29, 36, 45, 23, 12, 10, 56, 5, 55, 21, 22, 57].\\nThe idea revolves around conditional computation, which aims to enhance the number of model\\nparameters without a corresponding rise in computational expense. This is achieved by selectively\\nactivating only the relevant portions of the model, based on input-dependent factors. MoE models\\nleverage a learned gating mechanism that triggers only a select subset of k experts out of a total of E\\nfor a given input. This approach allows an input to either select all experts [11] or merely a sparse\\nmixture of them, as observed in recent massive language models [12, 10]. While a number of studies\\nhave sought to enhance the gating mechanism itself [15, 24, 43, 55], MoE models have also been\\nexplored in the context of multitask learning [15, 22]. Typically, a shared pool of experts is used,\\nalthough there has been investigation into per-task routers [30]. This essentially permits an input to\\nchoose the most relevant expert(s) for a given task, thereby optimizing the processing and results.\\nNevertheless, the instability of MoE models during fine-tuning or multitask learning has consistently\\nbeen a challenge. Our study aims to investigate whether instruction fine-tuning with scaled tasks\\nmight contribute to mitigating the generalization issues inherent to MoE models.\\n6\\nConclusion\\nIn this work, we have introduced FLAN-MOE, an innovative method to amplify the scalability of\\ninstruction-tuned language models by employing the sparse Mixture-of-Experts (MoE) technique. Our\\nstrategy amalgamates the merits of instruction-finetuning, which bolsters task-specific performance,\\nand MoE, which provides computational efficiency coupled with diminished memory requirements.\\nWe have substantiated the effectiveness of FLAN-MOE through comprehensive experiments across a\\nwide spectrum of Natural Language Processing (NLP) tasks, such as natural language understanding,\\nquestion answering, and reasoning. Our results consistently underscore the superior performance\\nof FLAN-MOE over current state-of-the-art methods, marking substantial advancements in both\\naccuracy and efficiency. Notably, these advancements are attained without necessitating an increase\\nin computational resources or memory usage during training and inference, often even reducing the\\nresource requirements in the process.\\nReferences\\n[1] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\\nZhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer\\nlearning. arXiv preprint arXiv:2111.10952, 2021.\\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\\n[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from\\nai feedback. arXiv preprint arXiv:2212.08073, 2022.\\n[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\\npreprint arXiv:2210.11416, 2022.\\n[5] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed\\nlanguage models. In ICML, pages 4057–4086. PMLR, 2022.\\n10\\n[6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\\nToutanova.\\nBoolq: Exploring the surprising difficulty of natural yes/no questions.\\narXiv preprint\\narXiv:1905.10044, 2019.\\n[7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\\narXiv:1803.05457, 2018.\\n[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\\nproblems. arXiv preprint arXiv:2110.14168, 2021.\\n[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\\ninstruction tuning. arXiv preprint arXiv:2305.06500, 2023.\\n[10] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-\\nof-experts. In ICML, pages 5547–5569. PMLR, 2022.\\n[11] David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\n[12] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models\\nwith simple and efficient sparsity. CoRR, abs/2101.03961, 2021.\\n[13] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\\nuse a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–361, 2021.\\n[14] Amelia Glaese, Nat McAleese, Maja Tr˛\\nebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\\nLaura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via\\ntargeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\\n[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul\\nMazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture of experts with\\napplications to multi-task learning. In Advances in Neural Information Processing Systems 34: Annual\\nConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\\n2021.\\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\\nSteinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n[17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\\n[18] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt\\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction\\nmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.\\n[19] Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question answering,\\ntext classification, and regression via span extraction. arXiv preprint arXiv:1904.09286, 2019.\\n[20] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Han-\\nnaneh Hajishirzi.\\nUnifiedqa: Crossing format boundaries with a single qa system.\\narXiv preprint\\narXiv:2005.00700, 2020.\\n[21] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from\\ndense checkpoints. arXiv preprint arXiv:2212.05055, 2022.\\n[22] Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong,\\nand Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Findings of\\nthe Association for Computational Linguistics: EMNLP 2021, pages 3577–3599, 2021.\\n[23] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\\nand automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n11\\n[24] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers: Simplify-\\ning training of large, sparse models. In ICML. PMLR, 2021.\\n[25] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\\narXiv:2304.08485, 2023.\\n[27] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\\nnatural language understanding. arXiv preprint arXiv:1901.11504, 2019.\\n[28] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,\\nBarret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction\\ntuning. In ICML, 2023.\\n[29] Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Cross-token modeling with conditional\\ncomputation. arXiv preprint arXiv:2109.02008, 2021.\\n[30] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. Modeling task relationships\\nin multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD\\nInternational Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23,\\n2018. ACM, 2018.\\n[31] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\\n[32] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\\nenglish math word problem solvers. In ACL, 2020.\\n[33] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via\\nnatural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through\\nmultitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\\n[35] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through\\nmultitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\\n[36] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal\\ncontrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770,\\n2022.\\n[37] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021.\\n[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744,\\n2022.\\n[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\\n[40] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word\\nproblems? arXiv preprint arXiv:2103.07191, 2021.\\n[41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J.\\nMach. Learn. Res., 21:140:1–140:67, 2020.\\n[42] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in\\nNeural Information Processing Systems, 2021.\\n12\\n[43] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021.\\n[44] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot\\ntask generalization. In ICLR, 2022.\\n[45] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and\\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR.\\nOpenReview.net, 2017.\\n[46] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling vision-\\nlanguage models with sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023.\\n[47] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\\n[48] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy\\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in\\nNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\\n2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\\n[50] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\\npreprint arXiv:2212.10560, 2022.\\n[51] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-\\njana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-\\nnaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In EMNLP, 2022.\\n[52] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022.\\n[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain\\nof thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\\n[54] Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task\\ngeneralization in nlp. arXiv preprint arXiv:2104.08835, 2021.\\n[55] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Y Zhao, Andrew M Dai, Zhifeng\\nChen, Quoc V Le, and James Laudon. Mixture-of-experts with expert choice routing. In Advances in\\nNeural Information Processing Systems, 2022.\\n[56] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William\\nFedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906,\\n2022.\\n[57] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng\\nGao. Taming sparsely activated transformer with stochastic experts. arXiv preprint arXiv:2110.04260,\\n2021.\\n13\\nAppendix for\\n“Mixture-of-Experts Meets Instruction Tuning: A Winning\\nCombination for Large Language Models”\\nA\\nFull Experiment Results\\nA.1\\nMMLU\\nIn the case of five-shot MMLU, we employ the \"dev\" set as the small sample exemplars.\\nThe performance of individual tasks in MMLU on the \"validation\" set is detailed in this sec-\\ntion (refer to https://www.tensorflow.org/datasets/community_catalog/huggingface/\\nhendrycks_test for more information). Please note, all MMLU findings presented in this paper\\ncorrespond to the \"validation\" set. We employ the prompts in [4].\\nTable 3: MMLU[:10] individual task performance.\\nMMLU\\nAbstract\\nAlgebra\\nAnatomy\\nAstronomy\\nBusiness\\nEthics\\nClinical\\nKnowledge\\nCollege\\nBiology\\nCollege\\nChemistry\\nCollege\\nComp. Sci.\\nCollege\\nMath\\nCollege\\nMedicine\\nModel\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT\\n-\\ndavinci\\n27.3\\n27.3\\n50.0\\n42.9\\n25.0\\n31.2\\n45.5\\n36.4\\n31.0\\n34.5\\n43.8\\n25.0\\n12.5\\n25.0\\n18.2\\n36.4\\n27.3\\n9.1\\n36.4\\n31.8\\n-\\ntext-davinci-002\\n9.1\\n27.3\\n57.1\\n28.6\\n62.5\\n56.2\\n63.6\\n72.7\\n51.7\\n55.2\\n68.8\\n43.8\\n12.5\\n37.5\\n63.6\\n36.4\\n54.5\\n36.4\\n63.6\\n54.5\\n-\\ntext-davinci-003\\n18.2\\n36.4\\n50.0\\n57.1\\n62.5\\n62.5\\n63.6\\n63.6\\n62.1\\n65.5\\n62.5\\n81.2\\n25.0\\n25.0\\n54.5\\n45.5\\n81.8\\n72.7\\n72.7\\n68.2\\n-\\ncode-davinci-002\\n18.2\\n27.3\\n71.4\\n35.7\\n68.8\\n56.2\\n54.5\\n63.6\\n69.0\\n65.5\\n62.5\\n50.0\\n25.0\\n37.5\\n45.5\\n27.3\\n72.7\\n45.5\\n77.3\\n86.4\\n80M\\nT5-Small\\n18.2\\n0.0\\n42.9\\n0.0\\n31.2\\n0.0\\n27.3\\n0.0\\n27.6\\n3.4\\n18.8\\n0.0\\n37.5\\n0.0\\n72.7\\n0.0\\n27.3\\n0.0\\n18.2\\n0.0\\nFlan-T5-Small\\n27.3\\n9.1\\n42.9\\n7.1\\n18.8\\n6.2\\n18.2\\n27.3\\n34.5\\n20.7\\n31.2\\n18.8\\n12.5\\n0.0\\n18.2\\n0.0\\n36.4\\n9.1\\n50.0\\n18.2\\n250M T5-Base\\n18.2\\n18.2\\n28.6\\n0.0\\n37.5\\n12.5\\n45.5\\n0.0\\n34.5\\n6.9\\n18.8\\n6.2\\n62.5\\n25.0\\n45.5\\n9.1\\n18.2\\n18.2\\n18.2\\n18.2\\nFlan-T5-Base\\n18.2\\n18.2\\n42.9\\n35.7\\n37.5\\n37.5\\n36.4\\n36.4\\n34.5\\n27.6\\n37.5\\n18.8\\n12.5\\n25.0\\n27.3\\n36.4\\n18.2\\n0.0\\n40.9\\n22.7\\n780M T5-Large\\n18.2\\n0.0\\n21.4\\n0.0\\n25.0\\n18.8\\n45.5\\n9.1\\n6.9\\n10.3\\n18.8\\n0.0\\n37.5\\n37.5\\n45.5\\n18.2\\n18.2\\n9.1\\n18.2\\n9.1\\nFlan-T5-Large\\n18.2\\n27.3\\n35.7\\n28.6\\n37.5\\n31.2\\n36.4\\n45.5\\n44.8\\n37.9\\n43.8\\n43.8\\n25.0\\n12.5\\n27.3\\n36.4\\n45.5\\n27.3\\n45.5\\n45.5\\n3B\\nT5-XL\\n18.2\\n0.0\\n14.3\\n0.0\\n31.2\\n0.0\\n9.1\\n0.0\\n10.3\\n17.2\\n31.2\\n12.5\\n25.0\\n12.5\\n45.5\\n0.0\\n9.1\\n9.1\\n18.2\\n0.0\\nFlan-T5-XL\\n27.3\\n36.4\\n35.7\\n35.7\\n50.0\\n62.5\\n45.5\\n45.5\\n55.2\\n55.2\\n56.2\\n50.0\\n25.0\\n37.5\\n45.5\\n27.3\\n18.2\\n27.3\\n50.0\\n50.0\\n11B\\nT5-XXL\\n27.3\\n0.0\\n21.4\\n0.0\\n31.2\\n0.0\\n9.1\\n0.0\\n10.3\\n31.0\\n43.8\\n0.0\\n50.0\\n12.5\\n36.4\\n0.0\\n9.1\\n0.0\\n54.5\\n0.0\\nFlan-T5-XXL\\n36.4\\n45.5\\n28.6\\n28.6\\n62.5\\n50.0\\n63.6\\n54.5\\n58.6\\n44.8\\n68.8\\n56.2\\n25.0\\n50.0\\n36.4\\n18.2\\n27.3\\n36.4\\n68.2\\n45.5\\n8B\\nPaLM\\n36.4\\n9.1\\n28.6\\n7.1\\n18.8\\n37.5\\n18.2\\n36.4\\n24.1\\n24.1\\n25.0\\n43.8\\n12.5\\n12.5\\n9.1\\n9.1\\n27.3\\n0.0\\n13.6\\n9.1\\nFlan-PaLM\\n36.4\\n18.2\\n42.9\\n35.7\\n43.8\\n50.0\\n36.4\\n45.5\\n48.3\\n41.4\\n56.2\\n50.0\\n25.0\\n25.0\\n54.5\\n63.6\\n18.2\\n27.3\\n50.0\\n18.2\\n62B\\nPaLM\\n27.3\\n9.1\\n50.0\\n21.4\\n50.0\\n43.8\\n63.6\\n81.8\\n51.7\\n62.1\\n68.8\\n31.2\\n37.5\\n25.0\\n54.5\\n18.2\\n36.4\\n9.1\\n59.1\\n45.5\\nFlan-PaLM\\n18.2\\n18.2\\n57.1\\n42.9\\n68.8\\n68.8\\n63.6\\n54.5\\n51.7\\n55.2\\n68.8\\n75.0\\n12.5\\n37.5\\n54.5\\n27.3\\n36.4\\n45.5\\n81.8\\n63.6\\n540B\\nPaLM\\n27.3\\n18.2\\n78.6\\n42.9\\n68.8\\n81.2\\n63.6\\n72.7\\n72.4\\n75.9\\n87.5\\n62.5\\n50.0\\n25.0\\n54.5\\n36.4\\n36.4\\n27.3\\n77.3\\n77.3\\nFlan-PaLM\\n0.0\\n9.1\\n50.0\\n71.4\\n81.2\\n75.0\\n63.6\\n54.5\\n79.3\\n62.1\\n87.5\\n62.5\\n62.5\\n62.5\\n81.8\\n63.6\\n36.4\\n63.6\\n86.4\\n86.4\\n250M SwitchBASE\\n9.1\\n18.2\\n14.3\\n21.4\\n43.8\\n31.2\\n36.4\\n0.0\\n10.3\\n10.3\\n37.5\\n37.5\\n37.5\\n50.0\\n36.4\\n0.0\\n36.4\\n18.2\\n40.9\\n0.0\\nFLAN-SwitchBASE\\n18.2\\n27.3\\n28.6\\n50.0\\n43.8\\n37.5\\n36.4\\n36.4\\n31.0\\n24.1\\n31.2\\n6.2\\n37.5\\n12.5\\n36.4\\n36.4\\n27.3\\n18.2\\n36.4\\n22.7\\n780M SwitchLARGE\\n27.3\\n9.1\\n35.7\\n21.4\\n12.5\\n31.2\\n18.2\\n0.0\\n24.1\\n27.6\\n31.2\\n31.2\\n12.5\\n50.0\\n9.1\\n0.0\\n18.2\\n27.3\\n22.7\\n45.5\\nFLAN-SwitchLARGE\\n18.2\\n18.2\\n35.7\\n35.7\\n37.5\\n25.0\\n36.4\\n45.5\\n48.3\\n41.4\\n43.8\\n37.5\\n12.5\\n37.5\\n45.5\\n36.4\\n27.3\\n9.1\\n54.5\\n50.0\\n11B\\nSwitchXXL\\n18.2\\n0.0\\n7.1\\n50.0\\n18.8\\n6.2\\n45.5\\n0.0\\n10.3\\n6.9\\n18.8\\n6.2\\n37.5\\n12.5\\n45.5\\n18.2\\n36.4\\n18.2\\n9.1\\n22.7\\nFLAN-SwitchXXL\\n45.5\\n9.1\\n42.9\\n42.9\\n56.2\\n56.2\\n54.5\\n45.5\\n55.2\\n44.8\\n68.8\\n56.2\\n0.0\\n12.5\\n45.5\\n27.3\\n36.4\\n27.3\\n54.5\\n36.4\\n80M\\nFLAN-GSSMALL\\n18.2\\n18.2\\n35.7\\n35.7\\n12.5\\n18.8\\n27.3\\n9.1\\n31.0\\n34.5\\n25.0\\n12.5\\n25.0\\n12.5\\n36.4\\n9.1\\n9.1\\n18.2\\n50.0\\n27.3\\n250M FLAN-GSBASE\\n18.2\\n18.2\\n50.0\\n35.7\\n50.0\\n18.8\\n45.5\\n63.6\\n41.4\\n34.5\\n43.8\\n18.8\\n12.5\\n0.0\\n36.4\\n27.3\\n18.2\\n27.3\\n50.0\\n45.5\\n780M FLAN-GSLARGE\\n18.2\\n18.2\\n35.7\\n35.7\\n56.2\\n50.0\\n45.5\\n27.3\\n51.7\\n37.9\\n43.8\\n43.8\\n25.0\\n12.5\\n54.5\\n36.4\\n45.5\\n36.4\\n59.1\\n50.0\\n80M\\nFLAN-ECSMALL\\n18.2\\n9.1\\n35.7\\n28.6\\n31.2\\n18.8\\n36.4\\n18.2\\n34.5\\n31.0\\n31.2\\n12.5\\n37.5\\n0.0\\n54.5\\n0.0\\n18.2\\n18.2\\n40.9\\n22.7\\n250M FLAN-ECBASE\\n27.3\\n18.2\\n50.0\\n42.9\\n43.8\\n37.5\\n27.3\\n45.5\\n48.3\\n24.1\\n37.5\\n43.8\\n0.0\\n12.5\\n45.5\\n36.4\\n27.3\\n18.2\\n36.4\\n31.8\\n780M FLAN-ECLARGE\\n9.1\\n36.4\\n35.7\\n28.6\\n50.0\\n43.8\\n63.6\\n63.6\\n51.7\\n55.2\\n43.8\\n50.0\\n0.0\\n12.5\\n45.5\\n36.4\\n27.3\\n36.4\\n72.7\\n45.5\\n3B\\nFLAN-ECXL\\n17.7\\n18.3\\n35.2\\n36.1\\n37.0\\n27.8\\n45.0\\n44.0\\n58.1\\n43.6\\n49.5\\n37.7\\n-0.5\\n38.0\\n45.0\\n36.4\\n17.7\\n10.1\\n58.6\\n49.6\\n250M STBASE\\n18.2\\n18.2\\n7.1\\n21.4\\n31.2\\n12.5\\n45.5\\n45.5\\n10.3\\n6.9\\n12.5\\n37.5\\n25.0\\n37.5\\n45.5\\n45.5\\n36.4\\n18.2\\n18.2\\n9.1\\nFLAN-STBASE\\n11.5\\n9.1\\n45.3\\n28.6\\n21.1\\n31.2\\n47.9\\n36.4\\n47.2\\n31.0\\n27.4\\n37.5\\n52.4\\n25.0\\n56.9\\n18.2\\n20.6\\n18.2\\n56.9\\n22.7\\n32B\\nST32B\\n27.3\\n0.0\\n35.7\\n0.0\\n37.5\\n18.8\\n18.2\\n18.2\\n27.6\\n6.9\\n12.5\\n25.0\\n37.5\\n25.0\\n18.2\\n9.1\\n18.2\\n0.0\\n13.6\\n18.2\\nFLAN-ST32B\\n18.2\\n18.2\\n50.0\\n71.4\\n68.8\\n81.2\\n72.7\\n81.8\\n79.3\\n65.5\\n87.5\\n68.8\\n25.0\\n25.0\\n54.5\\n9.1\\n18.2\\n18.2\\n68.2\\n72.7\\n14\\nTable 4: MMLU[10:20] individual task performance.\\nMMLU\\nCollege\\nPhysics\\nComputer\\nSecurity\\nConceptual\\nphysics\\nEconometrics\\nElectrical\\nEngineering\\nElementary\\nMathematics\\nFormal\\nLogic\\nGlobal\\nFacts\\nHigh School\\nBiology\\nHigh School\\nChemistry\\nModel\\nDirect CoT Direct CoT Direct CoT Direct\\nCoT\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT\\n-\\ndavinci\\n45.5\\n36.4\\n72.7\\n54.5\\n38.5\\n46.2\\n25.0\\n33.3\\n25.0\\n50.0\\n24.4\\n29.3\\n14.3\\n14.3\\n20.0\\n20.0\\n28.1\\n34.4\\n31.8\\n13.6\\n-\\ntext-davinci-002\\n54.5\\n81.8\\n81.8\\n81.8\\n53.8\\n61.5\\n58.3\\n50.0\\n50.0\\n37.5\\n56.1\\n73.2\\n7.1\\n28.6\\n50.0\\n70.0\\n71.9\\n71.9\\n18.2\\n36.4\\n-\\ntext-davinci-003\\n36.4\\n45.5\\n81.8\\n63.6\\n42.3\\n57.7\\n58.3\\n58.3\\n50.0\\n56.2\\n48.8\\n75.6\\n42.9\\n42.9\\n40.0\\n50.0\\n71.9\\n75.0\\n36.4\\n36.4\\n-\\ncode-davinci-002\\n45.5\\n72.7\\n90.9\\n81.8\\n53.8\\n57.7\\n66.7\\n41.7\\n50.0\\n50.0\\n56.1\\n75.6\\n50.0\\n42.9\\n40.0\\n50.0\\n71.9\\n65.6\\n40.9\\n40.9\\n80M\\nT5-Small\\n18.2\\n18.2\\n18.2\\n0.0\\n19.2\\n3.8\\n25.0\\n0.0\\n6.2\\n6.2\\n24.4\\n4.9\\n21.4\\n0.0\\n20.0\\n0.0\\n15.6\\n0.0\\n27.3\\n0.0\\nFlan-T5-Small\\n36.4\\n9.1\\n54.5\\n27.3\\n26.9\\n30.8\\n16.7\\n0.0\\n25.0\\n12.5\\n29.3\\n17.1\\n35.7\\n0.0\\n50.0\\n20.0\\n25.0\\n6.2\\n36.4\\n22.7\\n250M T5-Base\\n9.1\\n18.2\\n0.0\\n9.1\\n23.1\\n26.9\\n25.0\\n0.0\\n18.8\\n25.0\\n24.4\\n22.0\\n14.3\\n0.0\\n20.0\\n20.0\\n25.0\\n9.4\\n27.3\\n18.2\\nFlan-T5-Base\\n72.7\\n45.5\\n27.3\\n27.3\\n19.2\\n26.9\\n41.7\\n33.3\\n25.0\\n37.5\\n26.8\\n14.6\\n28.6\\n42.9\\n40.0\\n20.0\\n37.5\\n28.1\\n45.5\\n31.8\\n780M T5-Large\\n18.2\\n18.2\\n18.2\\n18.2\\n26.9\\n23.1\\n25.0\\n0.0\\n37.5\\n12.5\\n29.3\\n19.5\\n7.1\\n0.0\\n0.0\\n20.0\\n9.4\\n6.2\\n40.9\\n9.1\\nFlan-T5-Large\\n54.5\\n36.4\\n54.5\\n54.5\\n26.9\\n23.1\\n16.7\\n16.7\\n37.5\\n37.5\\n36.6\\n17.1\\n42.9\\n35.7\\n40.0\\n20.0\\n40.6\\n25.0\\n27.3\\n27.3\\n3B\\nT5-XL\\n18.2\\n9.1\\n9.1\\n18.2\\n19.2\\n23.1\\n41.7\\n0.0\\n37.5\\n25.0\\n39.0\\n17.1\\n42.9\\n0.0\\n30.0\\n10.0\\n31.2\\n0.0\\n27.3\\n4.5\\nFlan-T5-XL\\n72.7\\n36.4\\n36.4\\n36.4\\n38.5\\n46.2\\n33.3\\n16.7\\n56.2\\n25.0\\n34.1\\n24.4\\n28.6\\n14.3\\n20.0\\n30.0\\n37.5\\n34.4\\n31.8\\n36.4\\n11B\\nT5-XXL\\n18.2\\n18.2\\n27.3\\n45.5\\n23.1\\n34.6\\n16.7\\n0.0\\n31.2\\n25.0\\n26.8\\n19.5\\n42.9\\n0.0\\n20.0\\n10.0\\n15.6\\n0.0\\n31.8\\n0.0\\nFlan-T5-XXL\\n54.5\\n27.3\\n27.3\\n54.5\\n34.6\\n42.3\\n25.0\\n16.7\\n43.8\\n43.8\\n48.8\\n36.6\\n28.6\\n35.7\\n30.0\\n40.0\\n53.1\\n46.9\\n31.8\\n40.9\\n8B\\nPaLM\\n18.2\\n36.4\\n36.4\\n27.3\\n26.9\\n30.8\\n16.7\\n33.3\\n12.5\\n18.8\\n24.4\\n24.4\\n14.3\\n0.0\\n30.0\\n20.0\\n15.6\\n21.9\\n18.2\\n22.7\\nFlan-PaLM\\n45.5\\n27.3\\n72.7\\n45.5\\n38.5\\n38.5\\n33.3\\n25.0\\n37.5\\n37.5\\n34.1\\n34.1\\n21.4\\n28.6\\n30.0\\n20.0\\n50.0\\n25.0\\n18.2\\n18.2\\n62B\\nPaLM\\n54.5\\n45.5\\n63.6\\n54.5\\n42.3\\n42.3\\n16.7\\n33.3\\n62.5\\n56.2\\n24.4\\n51.2\\n21.4\\n21.4\\n30.0\\n40.0\\n59.4\\n31.2\\n36.4\\n31.8\\nFlan-PaLM\\n72.7\\n45.5\\n45.5\\n45.5\\n61.5\\n65.4\\n50.0\\n33.3\\n56.2\\n50.0\\n41.5\\n61.0\\n28.6\\n28.6\\n20.0\\n50.0\\n71.9\\n59.4\\n27.3\\n40.9\\n540B\\nPaLM\\n63.6\\n36.4\\n81.8\\n81.8\\n61.5\\n65.4\\n66.7\\n41.7\\n87.5\\n62.5\\n61.0\\n73.2\\n28.6\\n35.7\\n40.0\\n50.0\\n68.8\\n59.4\\n54.5\\n40.9\\nFlan-PaLM\\n63.6\\n72.7\\n90.9\\n81.8\\n69.2\\n65.4\\n66.7\\n58.3\\n81.2\\n75.0\\n58.5\\n70.7\\n42.9\\n57.1\\n60.0\\n70.0\\n71.9\\n71.9\\n68.2\\n40.9\\n250M SwitchBASE\\n9.1\\n9.1\\n18.2\\n9.1\\n23.1\\n26.9\\n16.7\\n0.0\\n43.8\\n50.0\\n26.8\\n17.1\\n28.6\\n0.0\\n30.0\\n10.0\\n12.5\\n25.0\\n31.8\\n0.0\\nFLAN-SwitchBASE\\n36.4\\n36.4\\n27.3\\n18.2\\n42.3\\n42.3\\n16.7\\n25.0\\n31.2\\n31.2\\n9.8\\n31.7\\n35.7\\n7.1\\n30.0\\n20.0\\n25.0\\n18.8\\n22.7\\n18.2\\n780M SwitchLARGE\\n27.3\\n36.4\\n36.4\\n18.2\\n30.8\\n26.9\\n25.0\\n25.0\\n18.8\\n0.0\\n26.8\\n24.4\\n7.1\\n28.6\\n30.0\\n10.0\\n37.5\\n25.0\\n22.7\\n36.4\\nFLAN-SwitchLARGE\\n63.6\\n45.5\\n45.5\\n36.4\\n42.3\\n26.9\\n41.7\\n25.0\\n37.5\\n31.2\\n43.9\\n19.5\\n35.7\\n42.9\\n20.0\\n30.0\\n40.6\\n43.8\\n27.3\\n13.6\\n11B\\nSwitchXXL\\n9.1\\n9.1\\n18.2\\n9.1\\n26.9\\n19.2\\n25.0\\n0.0\\n31.2\\n31.2\\n22.0\\n14.6\\n21.4\\n14.3\\n10.0\\n0.0\\n21.9\\n0.0\\n36.4\\n9.1\\nFLAN-SwitchXXL\\n36.4\\n45.5\\n36.4\\n36.4\\n57.7\\n50.0\\n25.0\\n33.3\\n37.5\\n43.8\\n39.0\\n39.0\\n21.4\\n35.7\\n60.0\\n20.0\\n71.9\\n46.9\\n22.7\\n36.4\\n80M\\nFLAN-GSSMALL\\n45.5\\n45.5\\n9.1\\n9.1\\n23.1\\n11.5\\n25.0\\n33.3\\n25.0\\n25.0\\n41.5\\n31.7\\n28.6\\n21.4\\n40.0\\n40.0\\n28.1\\n21.9\\n18.2\\n18.2\\n250M FLAN-GSBASE\\n63.6\\n45.5\\n18.2\\n27.3\\n23.1\\n23.1\\n41.7\\n33.3\\n18.8\\n25.0\\n22.0\\n14.6\\n35.7\\n35.7\\n40.0\\n40.0\\n25.0\\n18.8\\n13.6\\n27.3\\n780M FLAN-GSLARGE\\n54.5\\n45.5\\n45.5\\n36.4\\n30.8\\n38.5\\n41.7\\n50.0\\n43.8\\n50.0\\n29.3\\n34.1\\n50.0\\n14.3\\n40.0\\n20.0\\n50.0\\n43.8\\n18.2\\n18.2\\n80M\\nFLAN-ECSMALL\\n72.7\\n27.3\\n63.6\\n27.3\\n26.9\\n15.4\\n25.0\\n16.7\\n25.0\\n6.2\\n17.1\\n31.7\\n21.4\\n7.1\\n30.0\\n40.0\\n34.4\\n12.5\\n31.8\\n40.9\\n250M FLAN-ECBASE\\n63.6\\n27.3\\n27.3\\n27.3\\n38.5\\n38.5\\n33.3\\n25.0\\n37.5\\n18.8\\n24.4\\n26.8\\n35.7\\n28.6\\n40.0\\n20.0\\n21.9\\n25.0\\n13.6\\n18.2\\n780M FLAN-ECLARGE\\n36.4\\n45.5\\n36.4\\n36.4\\n46.2\\n34.6\\n33.3\\n33.3\\n37.5\\n31.2\\n36.6\\n36.6\\n35.7\\n14.3\\n30.0\\n40.0\\n53.1\\n50.0\\n27.3\\n22.7\\n3B\\nFLAN-ECXL\\n54.0\\n47.3\\n35.9\\n37.4\\n41.8\\n26.3\\n41.2\\n24.3\\n37.0\\n30.9\\n50.7\\n20.7\\n13.8\\n43.1\\n49.5\\n31.0\\n52.6\\n45.0\\n17.7\\n14.4\\n250M STBASE\\n9.1\\n45.5\\n18.2\\n18.2\\n26.9\\n15.4\\n25.0\\n0.0\\n31.2\\n25.0\\n14.6\\n26.8\\n35.7\\n14.3\\n10.0\\n10.0\\n21.9\\n6.2\\n40.9\\n27.3\\nFLAN-STBASE\\n47.9\\n18.2\\n11.5\\n18.2\\n29.3\\n38.5\\n44.1\\n25.0\\n46.1\\n37.5\\n26.8\\n34.1\\n52.4\\n28.6\\n62.4\\n40.0\\n30.5\\n21.9\\n16.0\\n40.9\\n32B\\nST32B\\n54.5\\n0.0\\n27.3\\n27.3\\n23.1\\n42.3\\n41.7\\n0.0\\n31.2\\n12.5\\n24.4\\n12.2\\n21.4\\n0.0\\n50.0\\n20.0\\n15.6\\n12.5\\n13.6\\n22.7\\nFLAN-ST32B\\n36.4\\n36.4\\n36.4\\n45.5\\n65.4\\n57.7\\n58.3\\n58.3\\n62.5\\n68.8\\n51.2\\n65.9\\n50.0\\n57.1\\n40.0\\n50.0\\n78.1\\n68.8\\n31.8\\n40.9\\n15\\nTable 5: MMLU[20:30] individual task performance.\\nMMLU\\nHigh School\\nComp. Sci.\\nHigh School\\nEuropean History\\nHigh School\\nGeography\\nHigh School\\nGvmt & Politics\\nHigh School\\nMacroeconomics\\nHigh School\\nMath\\nHigh School\\nMicroeconomics\\nHigh School\\nPhysics\\nHigh School\\nPsychology\\nHigh School\\nStatistics\\nModel\\nDirect CoT Direct\\nCoT\\nDirect CoT Direct\\nCoT\\nDirect\\nCoT\\nDirect CoT Direct\\nCoT\\nDirect CoT Direct CoT Direct CoT\\n-\\ndavinci\\n55.6\\n44.4\\n38.9\\n33.3\\n63.6\\n63.6\\n52.4\\n52.4\\n39.5\\n51.2\\n13.8\\n10.3\\n34.6\\n46.2\\n29.4\\n11.8\\n50.0\\n65.0\\n34.8\\n26.1\\n-\\ntext-davinci-002\\n100.0 66.7\\n83.3\\n83.3\\n81.8\\n77.3\\n76.2\\n76.2\\n62.8\\n74.4\\n34.5\\n24.1\\n76.9\\n73.1\\n47.1\\n23.5\\n88.3\\n90.0\\n52.2\\n43.5\\n-\\ntext-davinci-003\\n66.7\\n55.6\\n83.3\\n77.8\\n95.5\\n77.3\\n81.0\\n81.0\\n67.4\\n62.8\\n44.8\\n51.7\\n80.8\\n76.9\\n29.4\\n23.5\\n95.0\\n91.7\\n52.2\\n52.2\\n-\\ncode-davinci-002\\n88.9\\n55.6\\n83.3\\n77.8\\n90.9\\n86.4\\n85.7\\n85.7\\n67.4\\n67.4\\n48.3\\n51.7\\n88.5\\n80.8\\n23.5\\n29.4\\n95.0\\n90.0\\n65.2\\n65.2\\n80M\\nT5-Small\\n22.2\\n0.0\\n33.3\\n0.0\\n36.4\\n0.0\\n28.6\\n33.3\\n25.6\\n4.7\\n13.8\\n13.8\\n34.6\\n3.8\\n35.3\\n0.0\\n25.0\\n0.0\\n34.8\\n17.4\\nFlan-T5-Small\\n0.0\\n0.0\\n22.2\\n0.0\\n27.3\\n18.2\\n38.1\\n4.8\\n32.6\\n7.0\\n13.8\\n10.3\\n26.9\\n7.7\\n47.1\\n11.8\\n28.3\\n3.3\\n34.8\\n0.0\\n250M T5-Base\\n33.3\\n0.0\\n27.8\\n0.0\\n4.5\\n13.6\\n38.1\\n52.4\\n27.9\\n23.3\\n17.2\\n13.8\\n23.1\\n23.1\\n17.6\\n23.5\\n20.0\\n11.7\\n34.8\\n34.8\\nFlan-T5-Base\\n44.4\\n22.2\\n50.0\\n55.6\\n50.0\\n50.0\\n66.7\\n47.6\\n23.3\\n32.6\\n13.8\\n17.2\\n42.3\\n38.5\\n11.8\\n17.6\\n30.0\\n38.3\\n30.4\\n17.4\\n780M T5-Large\\n22.2\\n22.2\\n33.3\\n0.0\\n18.2\\n27.3\\n38.1\\n42.9\\n30.2\\n25.6\\n27.6\\n31.0\\n26.9\\n26.9\\n17.6\\n17.6\\n33.3\\n5.0\\n34.8\\n39.1\\nFlan-T5-Large\\n55.6\\n55.6\\n50.0\\n44.4\\n63.6\\n45.5\\n61.9\\n57.1\\n37.2\\n34.9\\n24.1\\n13.8\\n57.7\\n46.2\\n23.5\\n17.6\\n63.3\\n58.3\\n34.8\\n26.1\\n3B\\nT5-XL\\n22.2\\n0.0\\n33.3\\n5.6\\n27.3\\n31.8\\n23.8\\n52.4\\n30.2\\n32.6\\n20.7\\n3.4\\n26.9\\n15.4\\n17.6\\n17.6\\n15.0\\n15.0\\n34.8\\n13.0\\nFlan-T5-XL\\n66.7\\n33.3\\n77.8\\n77.8\\n63.6\\n63.6\\n71.4\\n47.6\\n34.9\\n46.5\\n24.1\\n13.8\\n46.2\\n53.8\\n17.6\\n29.4\\n78.3\\n63.3\\n43.5\\n26.1\\n11B\\nT5-XXL\\n11.1\\n0.0\\n38.9\\n0.0\\n22.7\\n40.9\\n38.1\\n57.1\\n30.2\\n37.2\\n27.6\\n3.4\\n26.9\\n42.3\\n17.6\\n17.6\\n38.3\\n21.7\\n34.8\\n4.3\\nFlan-T5-XXL\\n44.4\\n55.6\\n72.2\\n72.2\\n72.7\\n68.2\\n81.0\\n66.7\\n44.2\\n39.5\\n34.5\\n27.6\\n50.0\\n26.9\\n17.6\\n17.6\\n86.7\\n78.3\\n34.8\\n34.8\\n8B\\nPaLM\\n22.2\\n33.3\\n27.8\\n27.8\\n36.4\\n27.3\\n9.5\\n23.8\\n25.6\\n18.6\\n17.2\\n24.1\\n19.2\\n30.8\\n17.6\\n11.8\\n25.0\\n23.3\\n13.0\\n26.1\\nFlan-PaLM\\n44.4\\n44.4\\n72.2\\n55.6\\n68.2\\n45.5\\n57.1\\n57.1\\n44.2\\n44.2\\n17.2\\n20.7\\n57.7\\n46.2\\n17.6\\n35.3\\n68.3\\n45.0\\n39.1\\n26.1\\n62B\\nPaLM\\n66.7\\n66.7\\n61.1\\n55.6\\n63.6\\n72.7\\n47.6\\n57.1\\n41.9\\n51.2\\n27.6\\n34.5\\n57.7\\n65.4\\n29.4\\n17.6\\n83.3\\n75.0\\n47.8\\n52.2\\nFlan-PaLM\\n55.6\\n55.6\\n88.9\\n72.2\\n81.8\\n77.3\\n76.2\\n71.4\\n58.1\\n60.5\\n17.2\\n34.5\\n69.2\\n69.2\\n23.5\\n29.4\\n88.3\\n85.0\\n52.2\\n30.4\\n540B\\nPaLM\\n100.0 88.9\\n88.9\\n77.8\\n90.9\\n90.9\\n95.2\\n81.0\\n81.4\\n74.4\\n41.4\\n31.0\\n96.2\\n76.9\\n23.5\\n35.3\\n93.3\\n80.0\\n52.2\\n52.2\\nFlan-PaLM\\n100.0 77.8\\n83.3\\n72.2\\n95.5\\n90.9\\n95.2\\n85.7\\n79.1\\n72.1\\n31.0\\n44.8 100.0\\n88.5\\n5.9\\n29.4\\n93.3\\n93.3\\n69.6\\n47.8\\n250M SwitchBASE\\n0.0\\n0.0\\n33.3\\n0.0\\n18.2\\n18.2\\n38.1\\n28.6\\n37.2\\n11.6\\n37.9\\n3.4\\n26.9\\n23.1\\n17.6\\n17.6\\n25.0\\n8.3\\n34.8\\n34.8\\nFLAN-SwitchBASE\\n44.4\\n55.6\\n50.0\\n38.9\\n59.1\\n68.2\\n61.9\\n42.9\\n37.2\\n32.6\\n20.7\\n6.9\\n57.7\\n42.3\\n29.4\\n29.4\\n60.0\\n35.0\\n26.1\\n39.1\\n780M SwitchLARGE\\n22.2\\n33.3\\n27.8\\n16.7\\n27.3\\n18.2\\n9.5\\n33.3\\n25.6\\n30.2\\n10.3\\n24.1\\n34.6\\n38.5\\n41.2\\n17.6\\n21.7\\n15.0\\n13.0\\n26.1\\nFLAN-SwitchLARGE\\n33.3\\n55.6\\n61.1\\n27.8\\n72.7\\n54.5\\n66.7\\n61.9\\n46.5\\n46.5\\n27.6\\n13.8\\n65.4\\n46.2\\n5.9\\n23.5\\n68.3\\n55.0\\n52.2\\n39.1\\n11B\\nSwitchXXL\\n44.4\\n0.0\\n27.8\\n27.8\\n18.2\\n27.3\\n52.4\\n4.8\\n20.9\\n16.3\\n41.4\\n0.0\\n23.1\\n0.0\\n17.6\\n5.9\\n15.0\\n13.3\\n43.5\\n26.1\\nFLAN-SwitchXXL\\n55.6\\n44.4\\n72.2\\n72.2\\n72.7\\n81.8\\n85.7\\n76.2\\n62.8\\n48.8\\n34.5\\n20.7\\n53.8\\n53.8\\n23.5\\n29.4\\n85.0\\n78.3\\n39.1\\n34.8\\n80M\\nFLAN-GSSMALL\\n22.2\\n0.0\\n33.3\\n16.7\\n50.0\\n27.3\\n38.1\\n23.8\\n30.2\\n27.9\\n24.1\\n10.3\\n23.1\\n34.6\\n23.5\\n41.2\\n38.3\\n28.3\\n21.7\\n30.4\\n250M FLAN-GSBASE\\n44.4\\n11.1\\n50.0\\n38.9\\n50.0\\n54.5\\n52.4\\n38.1\\n34.9\\n23.3\\n20.7\\n17.2\\n46.2\\n15.4\\n58.8\\n17.6\\n46.7\\n35.0\\n39.1\\n34.8\\n780M FLAN-GSLARGE\\n44.4\\n22.2\\n61.1\\n27.8\\n72.7\\n59.1\\n81.0\\n76.2\\n41.9\\n32.6\\n27.6\\n31.0\\n61.5\\n50.0\\n29.4\\n41.2\\n80.0\\n66.7\\n30.4\\n34.8\\n80M\\nFLAN-ECSMALL\\n44.4\\n11.1\\n33.3\\n22.2\\n45.5\\n36.4\\n42.9\\n38.1\\n30.2\\n18.6\\n27.6\\n13.8\\n19.2\\n15.4\\n23.5\\n23.5\\n46.7\\n30.0\\n39.1\\n21.7\\n250M FLAN-ECBASE\\n44.4\\n22.2\\n61.1\\n22.2\\n63.6\\n59.1\\n57.1\\n42.9\\n44.2\\n37.2\\n31.0\\n31.0\\n50.0\\n42.3\\n29.4\\n17.6\\n63.3\\n56.7\\n26.1\\n30.4\\n780M FLAN-ECLARGE\\n66.7\\n44.4\\n61.1\\n22.2\\n77.3\\n86.4\\n57.1\\n57.1\\n37.2\\n37.2\\n27.6\\n27.6\\n50.0\\n53.8\\n41.2\\n17.6\\n83.3\\n75.0\\n30.4\\n30.4\\n3B\\nFLAN-ECXL\\n55.1\\n57.7\\n71.7\\n29.4\\n81.3\\n53.9\\n80.5\\n62.2\\n55.3\\n47.4\\n20.2\\n14.9\\n64.9\\n47.5\\n17.1\\n23.7\\n91.2\\n56.5\\n38.6\\n40.6\\n250M STBASE\\n33.3\\n0.0\\n33.3\\n11.1\\n18.2\\n0.0\\n47.6\\n28.6\\n18.6\\n30.2\\n44.8\\n24.1\\n19.2\\n0.0\\n29.4\\n17.6\\n15.0\\n23.3\\n26.1\\n34.8\\nFLAN-STBASE\\n58.0\\n33.3\\n63.5\\n55.6\\n61.5\\n36.4\\n54.8\\n57.1\\n32.6\\n27.9\\n30.0\\n31.0\\n60.1\\n46.2\\n31.8\\n35.3\\n64.1\\n51.7\\n24.1\\n39.1\\n32B\\nST32B\\n11.1\\n0.0\\n27.8\\n16.7\\n31.8\\n13.6\\n23.8\\n28.6\\n32.6\\n23.3\\n24.1\\n3.4\\n23.1\\n15.4\\n23.5\\n11.8\\n26.7\\n10.0\\n13.0\\n17.4\\nFLAN-ST32B\\n66.7\\n66.7\\n77.8\\n77.8\\n95.5\\n81.8\\n95.2\\n90.5\\n76.7\\n69.8\\n37.9\\n41.4\\n76.9\\n76.9\\n17.6\\n11.8\\n95.0\\n86.7\\n65.2\\n60.9\\n16\\nTable 6: MMLU[30:40] individual task performance.\\nMMLU\\nHigh School\\nUS History\\nHigh School\\nWorld History\\nHuman\\nAging\\nHuman\\nSexuality\\nInternational\\nLaw\\nJurisprudence\\nLogical\\nFallacies\\nMachine\\nLearning\\nManagement\\nMarketing\\nModel\\nDirect CoT Direct\\nCoT\\nDirect CoT Direct CoT Direct\\nCoT\\nDirect\\nCoT\\nDirect CoT Direct CoT Direct\\nCoT\\nDirect CoT\\n-\\ndavinci\\n54.5\\n36.4\\n38.5\\n46.2\\n30.4\\n60.9\\n16.7\\n50.0\\n84.6\\n38.5\\n18.2\\n9.1\\n55.6\\n50.0\\n27.3\\n18.2\\n45.5\\n63.6\\n56.0\\n64.0\\n-\\ntext-davinci-002\\n86.4\\n72.7\\n69.2\\n73.1\\n78.3\\n87.0\\n66.7\\n58.3\\n92.3\\n84.6\\n63.6\\n45.5\\n77.8\\n66.7\\n45.5\\n36.4\\n72.7\\n72.7\\n80.0\\n80.0\\n-\\ntext-davinci-003\\n81.8\\n81.8\\n80.8\\n76.9\\n78.3\\n73.9\\n66.7\\n58.3\\n84.6\\n84.6\\n63.6\\n54.5\\n83.3\\n83.3\\n45.5\\n54.5\\n81.8\\n72.7\\n84.0\\n76.0\\n-\\ncode-davinci-002\\n100.0 77.3\\n76.9\\n84.6\\n78.3\\n78.3\\n75.0\\n58.3 100.0\\n92.3\\n63.6\\n72.7\\n83.3\\n72.2\\n54.5\\n63.6\\n90.9\\n81.8\\n80.0\\n80.0\\n80M\\nT5-Small\\n40.9\\n0.0\\n30.8\\n0.0\\n34.8\\n13.0\\n41.7\\n25.0\\n30.8\\n0.0\\n27.3\\n27.3\\n33.3\\n0.0\\n27.3\\n0.0\\n18.2\\n9.1\\n24.0\\n4.0\\nFlan-T5-Small\\n50.0\\n31.8\\n15.4\\n7.7\\n4.3\\n13.0\\n33.3\\n16.7\\n23.1\\n7.7\\n27.3\\n9.1\\n22.2\\n16.7\\n18.2\\n0.0\\n18.2\\n9.1\\n44.0\\n20.0\\n250M T5-Base\\n18.2\\n0.0\\n30.8\\n0.0\\n30.4\\n30.4\\n33.3\\n25.0\\n7.7\\n7.7\\n27.3\\n18.2\\n33.3\\n27.8\\n36.4\\n27.3\\n18.2\\n0.0\\n20.0\\n24.0\\nFlan-T5-Base\\n59.1\\n50.0\\n50.0\\n50.0\\n30.4\\n30.4\\n50.0\\n33.3\\n38.5\\n46.2\\n18.2\\n18.2\\n44.4\\n66.7\\n18.2\\n36.4\\n36.4\\n18.2\\n64.0\\n60.0\\n780M T5-Large\\n13.6\\n0.0\\n30.8\\n0.0\\n47.8\\n39.1\\n41.7\\n41.7\\n7.7\\n0.0\\n18.2\\n0.0\\n33.3\\n22.2\\n36.4\\n9.1\\n18.2\\n27.3\\n20.0\\n16.0\\nFlan-T5-Large\\n54.5\\n54.5\\n57.7\\n42.3\\n52.2\\n56.5\\n41.7\\n41.7\\n53.8\\n30.8\\n45.5\\n36.4\\n77.8\\n55.6\\n18.2\\n18.2\\n63.6\\n63.6\\n84.0\\n68.0\\n3B\\nT5-XL\\n18.2\\n0.0\\n30.8\\n7.7\\n21.7\\n30.4\\n41.7\\n33.3\\n7.7\\n30.8\\n27.3\\n9.1\\n27.8\\n27.8\\n27.3\\n0.0\\n18.2\\n27.3\\n28.0\\n20.0\\nFlan-T5-XL\\n72.7\\n72.7\\n57.7\\n69.2\\n56.5\\n47.8\\n75.0\\n50.0\\n84.6\\n61.5\\n54.5\\n45.5\\n72.2\\n66.7\\n45.5\\n18.2\\n54.5\\n72.7\\n84.0\\n84.0\\n11B\\nT5-XXL\\n22.7\\n0.0\\n34.6\\n0.0\\n8.7\\n43.5\\n25.0\\n25.0\\n46.2\\n0.0\\n27.3\\n9.1\\n22.2\\n44.4\\n9.1\\n0.0\\n54.5\\n45.5\\n20.0\\n60.0\\nFlan-T5-XXL\\n63.6\\n63.6\\n73.1\\n73.1\\n73.9\\n60.9\\n75.0\\n50.0\\n76.9\\n53.8\\n54.5\\n36.4\\n66.7\\n77.8\\n27.3\\n27.3\\n72.7\\n45.5\\n72.0\\n76.0\\n8B\\nPaLM\\n36.4\\n31.8\\n15.4\\n23.1\\n47.8\\n34.8\\n16.7\\n16.7\\n53.8\\n46.2\\n27.3\\n9.1\\n16.7\\n22.2\\n18.2\\n18.2\\n18.2\\n36.4\\n32.0\\n24.0\\nFlan-PaLM\\n72.7\\n54.5\\n61.5\\n61.5\\n52.2\\n56.5\\n66.7\\n50.0\\n76.9\\n38.5\\n72.7\\n36.4\\n61.1\\n72.2\\n45.5\\n45.5\\n81.8\\n36.4\\n72.0\\n68.0\\n62B\\nPaLM\\n77.3\\n40.9\\n57.7\\n38.5\\n69.6\\n65.2\\n58.3\\n25.0\\n76.9\\n61.5\\n45.5\\n27.3\\n61.1\\n66.7\\n45.5\\n18.2\\n72.7\\n81.8\\n84.0\\n80.0\\nFlan-PaLM\\n81.8\\n54.5\\n80.8\\n76.9\\n60.9\\n69.6\\n83.3\\n50.0\\n84.6\\n69.2\\n63.6\\n63.6\\n61.1\\n66.7\\n27.3\\n36.4\\n81.8\\n81.8\\n72.0\\n72.0\\n540B\\nPaLM\\n90.9\\n72.7\\n88.5\\n76.9\\n78.3\\n73.9\\n91.7\\n75.0 100.0\\n61.5\\n63.6\\n72.7\\n83.3\\n66.7\\n27.3\\n27.3\\n81.8\\n81.8\\n84.0\\n84.0\\nFlan-PaLM\\n90.9\\n95.5\\n88.5\\n80.8\\n82.6\\n69.6\\n91.7\\n75.0 100.0\\n84.6\\n81.8\\n81.8\\n72.2\\n66.7\\n45.5\\n54.5\\n81.8\\n90.9\\n84.0\\n84.0\\n250M SwitchBASE\\n27.3\\n0.0\\n11.5\\n0.0\\n34.8\\n4.3\\n58.3\\n0.0\\n46.2\\n7.7\\n45.5\\n36.4\\n27.8\\n0.0\\n27.3\\n9.1\\n54.5\\n27.3\\n32.0\\n8.0\\nFLAN-SwitchBASE\\n50.0\\n36.4\\n46.2\\n19.2\\n47.8\\n47.8\\n25.0\\n25.0\\n46.2\\n30.8\\n36.4\\n18.2\\n55.6\\n50.0\\n18.2\\n45.5\\n45.5\\n54.5\\n68.0\\n56.0\\n780M SwitchLARGE\\n31.8\\n31.8\\n11.5\\n23.1\\n21.7\\n30.4\\n0.0\\n33.3\\n38.5\\n30.8\\n27.3\\n18.2\\n22.2\\n27.8\\n27.3\\n18.2\\n18.2\\n27.3\\n32.0\\n16.0\\nFLAN-SwitchLARGE\\n59.1\\n36.4\\n42.3\\n50.0\\n47.8\\n60.9\\n41.7\\n33.3\\n61.5\\n53.8\\n45.5\\n45.5\\n66.7\\n50.0\\n9.1\\n18.2\\n72.7\\n72.7\\n80.0\\n76.0\\n11B\\nSwitchXXL\\n13.6\\n31.8\\n30.8\\n26.9\\n26.1\\n8.7\\n16.7\\n8.3\\n7.7\\n0.0\\n27.3\\n0.0\\n27.8\\n22.2\\n27.3\\n18.2\\n18.2\\n27.3\\n20.0\\n0.0\\nFLAN-SwitchXXL\\n68.2\\n59.1\\n65.4\\n61.5\\n52.2\\n69.6\\n66.7\\n41.7 100.0\\n76.9\\n27.3\\n27.3\\n77.8\\n66.7\\n36.4\\n36.4\\n63.6\\n72.7\\n92.0\\n80.0\\n80M\\nFLAN-GSSMALL\\n50.0\\n27.3\\n38.5\\n19.2\\n30.4\\n30.4\\n16.7\\n25.0\\n30.8\\n30.8\\n27.3\\n18.2\\n38.9\\n33.3\\n45.5\\n9.1\\n36.4\\n18.2\\n64.0\\n40.0\\n250M FLAN-GSBASE\\n54.5\\n36.4\\n57.7\\n34.6\\n34.8\\n34.8\\n66.7\\n66.7\\n46.2\\n46.2\\n36.4\\n18.2\\n61.1\\n61.1\\n9.1\\n27.3\\n36.4\\n45.5\\n64.0\\n52.0\\n780M FLAN-GSLARGE\\n59.1\\n36.4\\n65.4\\n34.6\\n56.5\\n39.1\\n58.3\\n41.7\\n76.9\\n61.5\\n18.2\\n9.1\\n55.6\\n55.6\\n9.1\\n27.3\\n54.5\\n63.6\\n76.0\\n68.0\\n80M\\nFLAN-ECSMALL\\n27.3\\n31.8\\n50.0\\n30.8\\n21.7\\n26.1\\n50.0\\n25.0\\n30.8\\n30.8\\n36.4\\n9.1\\n44.4\\n27.8\\n27.3\\n0.0\\n54.5\\n27.3\\n32.0\\n64.0\\n250M FLAN-ECBASE\\n72.7\\n27.3\\n57.7\\n26.9\\n52.2\\n43.5\\n25.0\\n41.7\\n76.9\\n53.8\\n45.5\\n36.4\\n77.8\\n61.1\\n18.2\\n18.2\\n36.4\\n18.2\\n76.0\\n48.0\\n780M FLAN-ECLARGE\\n68.2\\n45.5\\n65.4\\n38.5\\n56.5\\n60.9\\n41.7\\n50.0\\n61.5\\n23.1\\n36.4\\n18.2\\n66.7\\n55.6\\n36.4\\n18.2\\n72.7\\n72.7\\n80.0\\n68.0\\n3B\\nFLAN-ECXL\\n76.8\\n38.4\\n61.0\\n50.7\\n73.4\\n60.9\\n66.2\\n35.2\\n68.7\\n53.7\\n45.0\\n47.1\\n71.7\\n51.9\\n26.8\\n19.7\\n72.2\\n73.1\\n95.5\\n78.1\\n250M STBASE\\n13.6\\n31.8\\n30.8\\n19.2\\n26.1\\n13.0\\n41.7\\n41.7\\n7.7\\n0.0\\n27.3\\n0.0\\n27.8\\n22.2\\n27.3\\n18.2\\n18.2\\n45.5\\n24.0\\n0.0\\nFLAN-STBASE\\n75.1\\n54.5\\n63.9\\n46.2\\n37.2\\n34.8\\n44.1\\n50.0\\n63.9\\n46.2\\n29.7\\n36.4\\n46.8\\n61.1\\n29.7\\n9.1\\n38.8\\n36.4\\n66.4\\n60.0\\n32B\\nST32B\\n31.8\\n9.1\\n26.9\\n11.5\\n34.8\\n13.0\\n33.3\\n25.0\\n0.0\\n15.4\\n27.3\\n18.2\\n22.2\\n22.2\\n27.3\\n27.3\\n54.5\\n18.2\\n12.0\\n16.0\\nFLAN-ST32B\\n81.8\\n81.8\\n84.6\\n84.6\\n73.9\\n78.3\\n66.7\\n50.0\\n92.3\\n100.0\\n72.7\\n81.8\\n83.3\\n77.8\\n54.5\\n45.5\\n90.9\\n81.8\\n80.0\\n76.0\\n17\\nTable 7: MMLU[40:50] individual task performance.\\nMMLU\\nMedical\\nGenetics\\nMisc.\\nMoral\\nDisputes\\nMoral\\nScenarios\\nNutrition\\nPhilosophy\\nPrehistory\\nProfessional\\nAccounting\\nProfessional\\nLaw\\nProfessional\\nMedicine\\nModel\\nDirect\\nCoT\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT Direct CoT\\n-\\ndavinci\\n72.7\\n90.9\\n50.0\\n65.1\\n57.9\\n39.5\\n24.0\\n34.0\\n54.5\\n45.5\\n44.1\\n61.8\\n45.7\\n42.9\\n29.0\\n35.5\\n31.2\\n26.5\\n32.3\\n38.7\\n-\\ntext-davinci-002\\n90.9\\n90.9\\n79.1\\n81.4\\n63.2\\n65.8\\n46.0\\n40.0\\n75.8\\n69.7\\n67.6\\n67.6\\n60.0\\n65.7\\n64.5\\n41.9\\n45.3\\n38.8\\n64.5\\n71.0\\n-\\ntext-davinci-003\\n100.0 100.0\\n82.6\\n87.2\\n71.1\\n52.6\\n43.0\\n65.0\\n78.8\\n69.7\\n76.5\\n76.5\\n65.7\\n74.3\\n54.8\\n38.7\\n48.8\\n47.1\\n74.2\\n67.7\\n-\\ncode-davinci-002\\n100.0 100.0\\n84.9\\n87.2\\n68.4\\n50.0\\n41.0\\n60.0\\n69.7\\n66.7\\n79.4\\n76.5\\n77.1\\n77.1\\n51.6\\n51.6\\n54.7\\n38.2\\n77.4\\n80.6\\n80M\\nT5-Small\\n9.1\\n0.0\\n27.9\\n22.1\\n15.8\\n0.0\\n22.0\\n21.0\\n21.2\\n15.2\\n26.5\\n17.6\\n25.7\\n0.0\\n38.7\\n6.5\\n21.2\\n0.0\\n29.0\\n0.0\\nFlan-T5-Small\\n18.2\\n9.1\\n34.9\\n19.8\\n21.1\\n5.3\\n23.0\\n19.0\\n33.3\\n12.1\\n26.5\\n11.8\\n42.9\\n20.0\\n32.3\\n22.6\\n32.4\\n14.1\\n12.9\\n16.1\\n250M T5-Base\\n27.3\\n9.1\\n24.4\\n26.7\\n15.8\\n0.0\\n31.0\\n1.0\\n36.4\\n33.3\\n20.6\\n8.8\\n17.1\\n17.1\\n35.5\\n16.1\\n23.5\\n1.2\\n29.0\\n3.2\\nFlan-T5-Base\\n27.3\\n54.5\\n36.0\\n29.1\\n34.2\\n42.1\\n24.0\\n21.0\\n39.4\\n33.3\\n35.3\\n35.3\\n45.7\\n28.6\\n19.4\\n35.5\\n27.6\\n23.5\\n22.6\\n25.8\\n780M T5-Large\\n27.3\\n0.0\\n26.7\\n29.1\\n15.8\\n0.0\\n24.0\\n14.0\\n33.3\\n0.0\\n23.5\\n23.5\\n17.1\\n11.4\\n32.3\\n12.9\\n23.5\\n0.0\\n29.0\\n0.0\\nFlan-T5-Large\\n45.5\\n72.7\\n47.7\\n51.2\\n50.0\\n39.5\\n24.0\\n27.0\\n45.5\\n42.4\\n52.9\\n52.9\\n45.7\\n40.0\\n35.5\\n19.4\\n32.4\\n30.0\\n41.9\\n29.0\\n3B\\nT5-XL\\n18.2\\n0.0\\n27.9\\n24.4\\n15.8\\n7.9\\n24.0\\n27.0\\n33.3\\n9.1\\n17.6\\n29.4\\n20.0\\n8.6\\n22.6\\n6.5\\n23.5\\n1.2\\n32.3\\n0.0\\nFlan-T5-XL\\n72.7\\n72.7\\n60.5\\n61.6\\n42.1\\n34.2\\n33.0\\n18.0\\n60.6\\n54.5\\n55.9\\n52.9\\n45.7\\n51.4\\n25.8\\n41.9\\n37.1\\n27.6\\n48.4\\n45.2\\n11B\\nT5-XXL\\n18.2\\n36.4\\n34.9\\n43.0\\n18.4\\n7.9\\n31.0\\n0.0\\n30.3\\n24.2\\n23.5\\n44.1\\n17.1\\n45.7\\n16.1\\n22.6\\n23.5\\n0.0\\n29.0\\n0.0\\nFlan-T5-XXL\\n90.9\\n72.7\\n62.8\\n68.6\\n44.7\\n39.5\\n37.0\\n32.0\\n63.6\\n42.4\\n61.8\\n64.7\\n54.3\\n57.1\\n41.9\\n38.7\\n35.9\\n32.9\\n58.1\\n51.6\\n8B\\nPaLM\\n54.5\\n27.3\\n30.2\\n32.6\\n34.2\\n39.5\\n22.0\\n23.0\\n21.2\\n15.2\\n26.5\\n26.5\\n28.6\\n28.6\\n32.3\\n25.8\\n25.9\\n22.9\\n9.7\\n19.4\\nFlan-PaLM\\n63.6\\n54.5\\n68.6\\n59.3\\n39.5\\n36.8\\n25.0\\n29.0\\n57.6\\n33.3\\n61.8\\n61.8\\n45.7\\n45.7\\n35.5\\n45.2\\n32.4\\n27.6\\n51.6\\n35.5\\n62B\\nPaLM\\n100.0 100.0\\n68.6\\n70.9\\n63.2\\n57.9\\n31.0\\n41.0\\n72.7\\n60.6\\n61.8\\n61.8\\n51.4\\n57.1\\n45.2\\n29.0\\n40.0\\n26.5\\n64.5\\n58.1\\nFlan-PaLM\\n90.9\\n90.9\\n81.4\\n76.7\\n65.8\\n60.5\\n22.0\\n38.0\\n72.7\\n60.6\\n67.6\\n67.6\\n51.4\\n57.1\\n35.5\\n32.3\\n45.3\\n32.4\\n61.3\\n71.0\\n540B\\nPaLM\\n100.0 100.0\\n75.6\\n86.0\\n73.7\\n57.9\\n53.0\\n55.0\\n69.7\\n57.6\\n85.3\\n76.5\\n74.3\\n68.6\\n51.6\\n51.6\\n53.5\\n41.8\\n83.9\\n64.5\\nFlan-PaLM\\n90.9\\n100.0\\n83.7\\n84.9\\n76.3\\n71.1\\n54.0\\n71.0\\n87.9\\n75.8\\n79.4\\n79.4\\n82.9\\n77.1\\n64.5\\n61.3\\n60.6\\n54.7\\n90.3\\n77.4\\n250M SwitchBASE\\n45.5\\n18.2\\n25.6\\n17.4\\n7.9\\n2.6\\n24.0\\n5.0\\n30.3\\n27.3\\n29.4\\n8.8\\n11.4\\n28.6\\n19.4\\n0.0\\n24.1\\n0.0\\n35.5\\n0.0\\nFLAN-SwitchBASE\\n36.4\\n45.5\\n41.9\\n47.7\\n36.8\\n34.2\\n32.0\\n33.0\\n48.5\\n27.3\\n38.2\\n29.4\\n40.0\\n31.4\\n19.4\\n32.3\\n26.5\\n17.1\\n29.0\\n38.7\\n780M SwitchLARGE\\n0.0\\n9.1\\n27.9\\n24.4\\n26.3\\n21.1\\n22.0\\n20.0\\n21.2\\n21.2\\n29.4\\n11.8\\n48.6\\n22.9\\n32.3\\n32.3\\n27.6\\n4.1\\n16.1\\n19.4\\nFLAN-SwitchLARGE\\n54.5\\n54.5\\n53.5\\n59.3\\n47.4\\n28.9\\n24.0\\n23.0\\n60.6\\n30.3\\n41.2\\n35.3\\n42.9\\n60.0\\n38.7\\n25.8\\n36.5\\n25.3\\n51.6\\n38.7\\n11B\\nSwitchXXL\\n36.4\\n27.3\\n22.1\\n26.7\\n18.4\\n0.0\\n21.0\\n24.0\\n15.2\\n15.2\\n35.3\\n38.2\\n20.0\\n25.7\\n32.3\\n29.0\\n25.3\\n22.9\\n19.4\\n25.8\\nFLAN-SwitchXXL\\n90.9\\n100.0\\n70.9\\n67.4\\n63.2\\n50.0\\n27.0\\n25.0\\n66.7\\n60.6\\n61.8\\n58.8\\n57.1\\n54.3\\n41.9\\n41.9\\n48.8\\n38.2\\n41.9\\n35.5\\n80M\\nFLAN-GSSMALL\\n36.4\\n27.3\\n32.6\\n25.6\\n42.1\\n50.0\\n29.0\\n25.0\\n45.5\\n54.5\\n20.6\\n23.5\\n34.3\\n28.6\\n29.0\\n35.5\\n31.2\\n22.4\\n22.6\\n12.9\\n250M FLAN-GSBASE\\n54.5\\n63.6\\n46.5\\n46.5\\n44.7\\n39.5\\n27.0\\n25.0\\n45.5\\n30.3\\n38.2\\n47.1\\n34.3\\n25.7\\n16.1\\n19.4\\n24.7\\n24.7\\n45.2\\n25.8\\n780M FLAN-GSLARGE\\n81.8\\n72.7\\n66.3\\n61.6\\n31.6\\n42.1\\n35.0\\n28.0\\n48.5\\n51.5\\n55.9\\n52.9\\n51.4\\n34.3\\n19.4\\n29.0\\n34.7\\n20.0\\n54.8\\n29.0\\n80M\\nFLAN-ECSMALL\\n9.1\\n45.5\\n38.4\\n39.5\\n39.5\\n44.7\\n30.0\\n17.0\\n48.5\\n54.5\\n14.7\\n29.4\\n31.4\\n17.1\\n16.1\\n32.3\\n27.1\\n24.1\\n38.7\\n22.6\\n250M FLAN-ECBASE\\n45.5\\n54.5\\n52.3\\n53.5\\n36.8\\n28.9\\n24.0\\n17.0\\n48.5\\n36.4\\n41.2\\n41.2\\n48.6\\n34.3\\n29.0\\n22.6\\n31.2\\n20.0\\n41.9\\n25.8\\n780M FLAN-ECLARGE\\n63.6\\n72.7\\n67.4\\n65.1\\n36.8\\n39.5\\n25.0\\n23.0\\n57.6\\n42.4\\n47.1\\n47.1\\n51.4\\n45.7\\n29.0\\n35.5\\n32.9\\n25.9\\n41.9\\n38.7\\n3B\\nFLAN-ECXL\\n90.4\\n56.4\\n68.1\\n60.7\\n52.1\\n31.4\\n24.5\\n25.7\\n66.2\\n32.3\\n55.4\\n35.5\\n59.5\\n61.4\\n35.0\\n27.8\\n43.6\\n26.2\\n41.4\\n40.6\\n250M STBASE\\n27.3\\n0.0\\n26.7\\n20.9\\n15.8\\n0.0\\n23.0\\n0.0\\n24.2\\n12.1\\n29.4\\n5.9\\n17.1\\n5.7\\n35.5\\n6.5\\n23.5\\n1.2\\n19.4\\n29.0\\nFLAN-STBASE\\n47.9\\n54.5\\n41.9\\n50.0\\n31.3\\n36.8\\n22.4\\n25.0\\n44.8\\n36.4\\n40.6\\n50.0\\n45.3\\n28.6\\n21.8\\n16.1\\n31.2\\n25.3\\n47.6\\n32.3\\n32B\\nST32B\\n18.2\\n0.0\\n27.9\\n36.0\\n36.8\\n2.6\\n29.0\\n0.0\\n24.2\\n36.4\\n14.7\\n11.8\\n14.3\\n25.7\\n25.8\\n9.7\\n24.7\\n7.1\\n22.6\\n3.2\\nFLAN-ST32B\\n90.9\\n90.9\\n84.9\\n82.6\\n65.8\\n52.6\\n31.0\\n32.0\\n81.8\\n75.8\\n70.6\\n58.8\\n71.4\\n60.0\\n54.8\\n45.2\\n53.5\\n48.2\\n74.2\\n67.7\\n18\\nTable 8: MMLU[50:57] individual task performance.\\nMMLU\\nProfessional\\nPsychology\\nPublic\\nRelations\\nSecurity\\nStudies\\nSociology\\nUS Foreign\\nPolicy\\nVirology\\nWorld Religions\\nAverage\\nModel\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct\\nCoT\\nDirect CoT Direct\\nCoT\\nDirect CoT\\n-\\ndavinci\\n37.7\\n43.5\\n50.0\\n50.0\\n44.4\\n40.7\\n63.6\\n59.1\\n45.5\\n63.6\\n33.3\\n27.8\\n63.2\\n68.4\\n39.7\\n40.5\\n-\\ntext-davinci-002\\n65.2\\n58.0\\n50.0\\n50.0\\n77.8\\n48.1\\n90.9\\n86.4\\n81.8\\n81.8\\n44.4\\n33.3\\n84.2\\n78.9\\n63.1\\n60.0\\n-\\ntext-davinci-003\\n68.1\\n63.8\\n50.0\\n50.0\\n70.4\\n63.0\\n86.4\\n95.5\\n81.8\\n90.9\\n50.0\\n50.0\\n84.2\\n84.2\\n64.8\\n64.6\\n-\\ncode-davinci-002\\n76.8\\n66.7\\n50.0\\n58.3\\n74.1\\n51.9\\n86.4\\n90.9\\n90.9\\n72.7\\n50.0\\n44.4\\n84.2\\n78.9\\n68.2\\n64.5\\n80M\\nT5-Small\\n20.3\\n4.3\\n33.3\\n16.7\\n18.5\\n0.0\\n22.7\\n0.0\\n27.3\\n9.1\\n27.8\\n5.6\\n21.1\\n15.8\\n26.7\\n5.6\\nFlan-T5-Small\\n24.6\\n7.2\\n25.0\\n16.7\\n14.8\\n0.0\\n36.4\\n9.1\\n36.4\\n9.1\\n38.9\\n16.7\\n31.6\\n26.3\\n28.7\\n12.1\\n250M T5-Base\\n21.7\\n13.0\\n41.7\\n16.7\\n37.0\\n7.4\\n18.2\\n4.5\\n18.2\\n18.2\\n33.3\\n11.1\\n21.1\\n21.1\\n25.7\\n14.5\\nFlan-T5-Base\\n39.1\\n40.6\\n41.7\\n33.3\\n29.6\\n29.6\\n54.5\\n59.1\\n36.4\\n45.5\\n44.4\\n33.3\\n31.6\\n15.8\\n35.6\\n33.3\\n780M T5-Large\\n18.8\\n23.2\\n25.0\\n16.7\\n14.8\\n0.0\\n18.2\\n22.7\\n18.2\\n18.2\\n33.3\\n27.8\\n31.6\\n26.3\\n25.1\\n15.0\\nFlan-T5-Large\\n56.5\\n56.5\\n58.3\\n50.0\\n22.2\\n29.6\\n68.2\\n59.1\\n54.5\\n27.3\\n61.1\\n38.9\\n47.4\\n52.6\\n44.7\\n38.8\\n3B\\nT5-XL\\n24.6\\n20.3\\n33.3\\n41.7\\n29.6\\n7.4\\n40.9\\n27.3\\n27.3\\n27.3\\n16.7\\n27.8\\n47.4\\n31.6\\n25.7\\n14.5\\nFlan-T5-XL\\n56.5\\n52.2\\n58.3\\n50.0\\n44.4\\n48.1\\n77.3\\n59.1\\n54.5\\n72.7\\n38.9\\n50.0\\n73.7\\n63.2\\n50.3\\n46.1\\n11B\\nT5-XXL\\n17.4\\n30.4\\n8.3\\n16.7\\n25.9\\n0.0\\n27.3\\n27.3\\n18.2\\n36.4\\n16.7\\n16.7\\n15.8\\n68.4\\n25.9\\n18.7\\nFlan-T5-XXL\\n68.1\\n58.0\\n58.3\\n41.7\\n59.3\\n44.4\\n86.4\\n63.6\\n54.5\\n45.5\\n44.4\\n50.0\\n31.6\\n63.2\\n52.6\\n47.9\\n8B\\nPaLM\\n17.4\\n31.9\\n33.3\\n25.0\\n22.2\\n25.9\\n31.8\\n40.9\\n36.4\\n18.2\\n16.7\\n27.8\\n21.1\\n10.5\\n24.3\\n24.1\\nFlan-PaLM\\n46.4\\n43.5\\n50.0\\n41.7\\n40.7\\n40.7\\n72.7\\n31.8\\n63.6\\n54.5\\n44.4\\n27.8\\n68.4\\n73.7\\n49.3\\n41.3\\n62B\\nPaLM\\n58.0\\n58.0\\n58.3\\n58.3\\n40.7\\n40.7\\n81.8\\n68.2\\n81.8\\n72.7\\n61.1\\n44.4\\n73.7\\n78.9\\n55.1\\n49.0\\nFlan-PaLM\\n71.0\\n63.8\\n50.0\\n50.0\\n70.4\\n55.6\\n81.8\\n77.3\\n90.9\\n100.0\\n55.6\\n44.4\\n89.5\\n73.7\\n59.6\\n56.9\\n540B\\nPaLM\\n73.9\\n60.9\\n66.7\\n58.3\\n74.1\\n40.7\\n95.5\\n81.8 100.0 100.0\\n61.1\\n44.4\\n89.5\\n89.5\\n71.3\\n62.9\\nFlan-PaLM\\n76.8\\n79.7\\n58.3\\n66.7\\n74.1\\n55.6\\n95.5\\n90.9 100.0 100.0\\n50.0\\n44.4\\n89.5\\n89.5\\n73.5\\n70.9\\n250M SwitchBASE\\n34.8\\n13.0\\n16.7\\n16.7\\n25.9\\n0.0\\n27.3\\n13.6\\n18.2\\n18.2\\n22.2\\n5.6\\n36.8\\n26.3\\n28.3\\n13.6\\nFLAN-SwitchBASE\\n42.0\\n39.1\\n50.0\\n50.0\\n18.5\\n22.2\\n68.2\\n72.7\\n63.6\\n45.5\\n44.4\\n33.3\\n42.1\\n52.6\\n38.0\\n34.1\\n780M SwitchLARGE\\n23.2\\n17.4\\n33.3\\n16.7\\n33.3\\n22.2\\n22.7\\n31.8\\n18.2\\n18.2\\n33.3\\n11.1\\n15.8\\n26.3\\n24.0\\n23.1\\nFLAN-SwitchLARGE\\n58.0\\n46.4\\n41.7\\n25.0\\n51.9\\n48.1\\n72.7\\n54.5\\n63.6\\n54.5\\n44.4\\n44.4\\n57.9\\n73.7\\n46.0\\n40.3\\n11B\\nSwitchXXL\\n26.1\\n17.4\\n16.7\\n25.0\\n29.6\\n3.7\\n22.7\\n18.2\\n18.2\\n18.2\\n27.8\\n16.7\\n26.3\\n15.8\\n24.6\\n15.1\\nFLAN-SwitchXXL\\n65.2\\n62.3\\n50.0\\n50.0\\n66.7\\n55.6\\n90.9\\n63.6\\n81.8\\n90.9\\n55.6\\n44.4\\n84.2\\n78.9\\n55.6\\n50.1\\n80M\\nFLAN-GSSMALL\\n31.9\\n26.1\\n58.3\\n33.3\\n37.0\\n44.4\\n54.5\\n54.5\\n36.4\\n45.5\\n44.4\\n38.9\\n31.6\\n31.6\\n32.5\\n26.8\\n250M FLAN-GSBASE\\n50.7\\n42.0\\n41.7\\n33.3\\n29.6\\n40.7\\n63.6\\n40.9\\n36.4\\n36.4\\n55.6\\n50.0\\n42.1\\n36.8\\n39.9\\n33.6\\n780M FLAN-GSLARGE\\n62.3\\n53.6\\n50.0\\n50.0\\n25.9\\n33.3\\n72.7\\n50.0\\n45.5\\n45.5\\n38.9\\n27.8\\n52.6\\n68.4\\n47.8\\n40.8\\n80M\\nFLAN-ECSMALL\\n31.9\\n31.9\\n33.3\\n25.0\\n33.3\\n29.6\\n45.5\\n50.0\\n36.4\\n36.4\\n33.3\\n16.7\\n21.1\\n26.3\\n34.1\\n25.1\\n250M FLAN-ECBASE\\n52.2\\n39.1\\n33.3\\n25.0\\n40.7\\n25.9\\n54.5\\n36.4\\n54.5\\n36.4\\n50.0\\n44.4\\n63.2\\n36.8\\n42.7\\n33.0\\n780M FLAN-ECLARGE\\n52.2\\n52.2\\n50.0\\n58.3\\n40.7\\n25.9\\n77.3\\n68.2\\n63.6\\n54.5\\n55.6\\n55.6\\n73.7\\n68.4\\n48.3\\n43.4\\n3B\\nFLAN-ECXL\\n61.8\\n47.6\\n49.5\\n24.9\\n51.4\\n47.9\\n85.9\\n55.5\\n81.3\\n56.2\\n49.5\\n43.4\\n67.9\\n74.9\\n52.1\\n41.4\\n250M STBASE\\n26.1\\n15.9\\n16.7\\n16.7\\n29.6\\n3.7\\n31.8\\n31.8\\n27.3\\n0.0\\n33.3\\n27.8\\n15.8\\n31.6\\n25.2\\n17.7\\nFLAN-STBASE\\n44.4\\n34.8\\n60.7\\n41.7\\n32.0\\n40.7\\n43.3\\n27.3\\n47.9\\n36.4\\n41.3\\n38.9\\n44.5\\n42.1\\n42.4\\n35.5\\n32B\\nST32B\\n34.8\\n11.6\\n8.3\\n33.3\\n25.9\\n18.5\\n27.3\\n4.5\\n18.2\\n27.3\\n16.7\\n16.7\\n26.3\\n26.3\\n25.5\\n15.0\\nFLAN-ST32B\\n72.5\\n63.8\\n50.0\\n58.3\\n70.4\\n55.6\\n90.9\\n86.4 100.0 100.0\\n44.4\\n44.4\\n84.2\\n84.2\\n65.4\\n63.0\\n19\\nA.2\\nBBSH\\nBBH refers to a subset of difficult tasks from BIG-Bench, handpicked by [48] in 2022, where the\\nmodel proposed by [47] in the same year outperformed the average human rater. [48] mentions 23\\ntasks, two of which consist of three subtasks each. For ease of interpretation, we treat these subtasks\\nas standalone tasks and calculate an unweighted average. We utilize the prompts provided in [48]’s\\nstudy.\\nTable 9: BBH[:9] individual task performance.\\nBBH\\nBoolean\\nExpressions\\nCausal\\nJudgement\\nDate\\nUnderstanding\\nDisambiguation\\nQA\\nDyck\\nLanguages\\nFormal\\nFallacies\\nGeometric\\nShapes\\nHyperbaton\\nLogical Deduction\\nFive Objects\\nModel\\nDirect CoT Direct CoT Direct\\nCoT\\nDirect\\nCoT\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct\\nCoT\\n-\\ndavinci\\n54.0\\n69.2\\n57.8\\n48.1\\n37.6\\n52.4\\n40.0\\n40.8\\n28.0\\n0.0\\n47.2\\n52.8\\n10.4\\n10.8\\n49.6\\n47.6\\n24.4\\n34.4\\n-\\ntext-davinci-002\\n90.0\\n87.6\\n57.8\\n56.1\\n55.6\\n81.6\\n66.4\\n70.8\\n42.0\\n32.0\\n52.4\\n58.4\\n35.2\\n56.0\\n67.2\\n72.4\\n31.6\\n51.2\\n-\\ntext-davinci-003\\n90.0\\n90.8\\n63.6\\n63.6\\n58.8\\n82.0\\n68.4\\n66.8\\n14.8\\n40.0\\n58.0\\n55.2\\n36.8\\n60.4\\n60.8\\n53.2\\n44.0\\n58.0\\n-\\ncode-davinci-002\\n88.4\\n92.8\\n63.6\\n54.0\\n63.6\\n87.2\\n67.2\\n76.0\\n46.8\\n56.8\\n52.4\\n50.4\\n32.0\\n54.4\\n60.4\\n66.4\\n32.4\\n54.8\\n80M\\nT5-Small\\n40.0\\n0.0\\n51.3\\n2.7\\n20.0\\n10.8\\n34.8\\n14.0\\n2.4\\n0.0\\n52.8\\n0.0\\n8.4\\n0.0\\n52.0\\n0.0\\n17.2\\n7.6\\nFlan-T5-Small\\n54.0\\n39.6\\n48.1\\n42.8\\n22.4\\n20.4\\n31.2\\n2.0\\n0.0\\n0.0\\n53.2\\n46.8\\n8.8\\n4.0\\n65.2\\n13.2\\n22.0\\n19.2\\n250M T5-Base\\n46.0\\n45.6\\n51.9\\n38.0\\n20.0\\n19.6\\n33.6\\n30.8\\n1.6\\n0.0\\n46.8\\n31.2\\n22.0\\n0.0\\n51.2\\n0.0\\n19.2\\n9.6\\nFlan-T5-Base\\n48.4\\n46.4\\n52.4\\n47.1\\n18.0\\n20.4\\n54.8\\n44.8\\n7.6\\n0.0\\n53.2\\n49.2\\n0.4\\n12.8\\n67.6\\n58.8\\n27.2\\n22.0\\n780M T5-Large\\n46.0\\n49.2\\n51.9\\n26.2\\n20.8\\n20.0\\n34.8\\n10.8\\n0.4\\n0.0\\n46.8\\n6.0\\n29.6\\n0.0\\n50.0\\n0.0\\n19.6\\n14.8\\nFlan-T5-Large\\n64.0\\n58.0\\n56.1\\n20.9\\n24.4\\n26.8\\n67.6\\n61.2\\n0.8\\n0.0\\n22.8\\n39.6\\n0.8\\n8.0\\n72.4\\n56.0\\n47.6\\n22.4\\n3B\\nT5-XL\\n55.2\\n47.2\\n52.4\\n26.7\\n21.6\\n22.4\\n32.4\\n4.8\\n6.0\\n0.0\\n47.2\\n7.2\\n8.4\\n0.0\\n52.0\\n0.0\\n22.0\\n22.8\\nFlan-T5-XL\\n52.4\\n56.0\\n62.0\\n56.1\\n46.8\\n48.8\\n70.0\\n70.4\\n0.0\\n0.0\\n56.4\\n48.0\\n15.2\\n4.4\\n55.6\\n56.8\\n54.0\\n32.4\\n11B\\nT5-XXL\\n49.6\\n65.2\\n52.4\\n1.6\\n35.2\\n54.0\\n35.2\\n0.0\\n2.0\\n0.0\\n52.4\\n0.0\\n15.6\\n0.0\\n55.6\\n0.0\\n18.0\\n37.2\\nFlan-T5-XXL\\n56.8\\n60.8\\n60.4\\n53.5\\n69.6\\n53.6\\n71.2\\n71.2\\n0.8\\n0.4\\n55.6\\n46.4\\n14.0\\n24.8\\n71.6\\n53.2\\n55.6\\n46.4\\n8B\\nFlan-PaLM\\n48.8\\n52.8\\n60.4\\n54.0\\n10.8\\n28.8\\n58.0\\n55.6\\n20.8\\n0.0\\n52.0\\n50.8\\n15.6\\n4.0\\n65.6\\n36.8\\n25.2\\n22.4\\n62B\\nPaLM\\n69.2\\n70.8\\n59.4\\n54.5\\n39.2\\n58.8\\n52.8\\n54.0\\n19.2\\n3.2\\n53.2\\n54.0\\n34.4\\n9.6\\n48.4\\n72.8\\n24.8\\n26.0\\nFlan-PaLM\\n66.8\\n73.6\\n64.2\\n62.6\\n42.8\\n54.4\\n69.2\\n39.2\\n13.2\\n0.0\\n55.6\\n49.2\\n18.0\\n13.2\\n74.4\\n59.2\\n54.0\\n42.8\\n540B\\nPaLM\\n83.2\\n80.0\\n61.0\\n59.4\\n53.6\\n79.2\\n60.8\\n67.6\\n28.4\\n28.0\\n53.6\\n51.2\\n37.6\\n0.0\\n70.8\\n90.4\\n39.6\\n49.2\\nFlan-PaLM\\n86.0\\n83.2\\n65.2\\n63.1\\n58.0\\n74.0\\n76.8\\n69.6\\n29.2\\n23.6\\n62.4\\n52.8\\n40.0\\n43.6\\n67.6\\n88.8\\n54.4\\n52.4\\n250M SwitchBASE\\n0.0\\n0.0\\n2.7\\n10.7\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.6\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n0.8\\nFLAN-SwitchBASE\\n51.2\\n42.8\\n55.1\\n55.6\\n18.8\\n18.4\\n63.6\\n53.6\\n0.0\\n0.0\\n56.8\\n54.8\\n9.6\\n8.8\\n64.8\\n62.0\\n34.8\\n22.0\\n780M SwitchLARGE\\n0.0\\n26.0\\n5.3\\n5.3\\n0.0\\n10.8\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n15.2\\n0.0\\n8.4\\n0.0\\n48.4\\n0.0\\n0.0\\nFLAN-SwitchLARGE\\n54.0\\n22.0\\n56.7\\n50.8\\n25.2\\n24.0\\n67.2\\n59.2\\n0.8\\n0.0\\n54.8\\n43.6\\n11.6\\n3.6\\n56.8\\n30.0\\n47.2\\n28.0\\n11B\\nSwitchXXL\\n0.0\\n3.2\\n0.0\\n37.4\\n0.0\\n2.4\\n0.0\\n8.8\\n0.0\\n0.0\\n0.0\\n21.6\\n0.0\\n0.4\\n0.0\\n30.4\\n0.0\\n0.4\\nFLAN-SwitchXXL\\n56.2\\n57.3\\n65.5\\n61.4\\n60.9\\n55.3\\n70.4\\n66.4\\n0.8\\n0.4\\n57.3\\n47.7\\n12.8\\n8.8\\n58.1\\n58.0\\n61.2\\n54.9\\n80M\\nFLAN-GSSMALL\\n60.0\\n46.0\\n51.9\\n50.8\\n21.2\\n21.6\\n30.4\\n28.4\\n1.2\\n0.0\\n54.8\\n35.2\\n9.6\\n12.4\\n56.0\\n0.0\\n21.6\\n16.4\\n250M FLAN-GSBASE\\n48.0\\n34.0\\n53.5\\n51.9\\n27.6\\n11.2\\n65.2\\n26.0\\n0.0\\n0.0\\n53.2\\n51.6\\n9.6\\n18.4\\n59.6\\n1.2\\n35.6\\n20.4\\n780M FLAN-GSLARGE\\n46.8\\n41.2\\n53.5\\n50.8\\n5.6\\n37.2\\n68.8\\n66.0\\n2.0\\n0.0\\n51.2\\n12.4\\n19.2\\n12.8\\n54.0\\n50.8\\n47.6\\n28.4\\n80M\\nFLAN-ECSMALL\\n59.6\\n39.2\\n49.7\\n53.5\\n21.6\\n17.2\\n34.0\\n36.4\\n1.2\\n0.0\\n54.4\\n45.6\\n9.6\\n0.4\\n58.0\\n0.4\\n20.4\\n23.2\\n250M FLAN-ECBASE\\n57.6\\n43.6\\n50.3\\n50.8\\n34.4\\n24.8\\n67.6\\n34.4\\n0.8\\n0.0\\n53.6\\n17.2\\n9.6\\n7.6\\n72.0\\n44.0\\n33.6\\n24.0\\n780M FLAN-ECLARGE\\n58.8\\n48.0\\n58.8\\n50.8\\n35.6\\n43.2\\n69.2\\n70.0\\n0.0\\n0.0\\n53.2\\n30.8\\n4.8\\n5.6\\n68.4\\n52.8\\n41.6\\n21.6\\n3B\\nFLAN-ECXL\\n54.3\\n49.7\\n59.9\\n56.2\\n48.4\\n37.4\\n69.0\\n32.9\\n-1.3\\n0.4\\n53.0\\n50.0\\n9.9\\n4.0\\n61.2\\n40.1\\n50.4\\n38.9\\n250M STBASE\\n0.0\\n9.2\\n0.0\\n35.8\\n0.0\\n14.4\\n0.0\\n0.8\\n0.0\\n0.0\\n0.0\\n52.8\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n18.8\\nFLAN-STBASE\\n48.0\\n49.3\\n59.6\\n54.1\\n11.6\\n36.1\\n66.1\\n64.2\\n1.0\\n0.0\\n50.0\\n44.2\\n19.5\\n12.1\\n51.4\\n49.9\\n49.6\\n21.4\\n32B\\nST32B\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n32.8\\n0.0\\n0.4\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.2\\n0.0\\n0.4\\n0.0\\n6.4\\nFLAN-ST32B\\n63.6\\n67.6\\n67.9\\n65.8\\n66.4\\n62.0\\n70.8\\n74.8\\n15.2\\n0.0\\n58.8\\n42.0\\n22.8\\n5.2\\n60.0\\n54.4\\n64.0\\n49.6\\n20\\nTable 10: BBH[9:18] individual task performance.\\nBBH\\nLogical Deduction\\nSeven Objects\\nLogical Deduction\\nThree Objects\\nMovie\\nRecommendation\\nMultistep\\nArithmetic\\nNavigate\\nObject\\nCounting\\nPenguins\\nin a Table\\nReasoning about\\nColored Objects\\nRuin\\nNames\\nModel\\nDirect\\nCoT\\nDirect\\nCoT\\nDirect\\nCoT\\nDirect CoT Direct CoT Direct CoT Direct CoT Direct\\nCoT\\nDirect CoT\\n-\\ndavinci\\n20.0\\n27.2\\n38.0\\n52.0\\n58.8\\n71.2\\n0.8\\n1.6\\n58.0\\n66.0\\n33.2\\n49.6\\n28.1\\n35.6\\n13.2\\n41.2\\n18.4\\n33.2\\n-\\ntext-davinci-002\\n26.8\\n38.0\\n45.2\\n87.6\\n72.0\\n78.8\\n1.2\\n53.2\\n68.0\\n88.8\\n44.0\\n77.2\\n47.3\\n81.5\\n47.6\\n78.4\\n65.6\\n62.8\\n-\\ntext-davinci-003\\n40.0\\n52.4\\n62.0\\n88.0\\n79.2\\n83.6\\n1.2\\n49.6\\n53.2\\n94.4\\n33.2\\n82.0\\n52.1\\n83.6\\n67.2\\n86.8\\n82.0\\n58.8\\n-\\ncode-davinci-002\\n26.0\\n38.8\\n52.8\\n87.6\\n84.8\\n90.4\\n1.2\\n47.6\\n50.4\\n96.4\\n45.2\\n93.2\\n66.4\\n79.5\\n67.6\\n91.6\\n75.2\\n68.4\\n80M\\nT5-Small\\n13.2\\n5.2\\n31.6\\n14.0\\n26.0\\n14.8\\n0.0\\n0.0\\n55.2\\n40.0\\n10.0\\n0.0\\n21.9\\n19.2\\n16.0\\n11.2\\n22.4\\n1.6\\nFlan-T5-Small\\n16.8\\n11.2\\n30.8\\n30.0\\n43.2\\n20.4\\n0.0\\n1.6\\n58.0\\n58.0\\n5.6\\n3.2\\n21.9\\n10.3\\n17.2\\n10.8\\n13.2\\n0.8\\n250M T5-Base\\n14.8\\n2.4\\n29.6\\n22.4\\n27.6\\n0.4\\n0.4\\n0.0\\n48.0\\n42.0\\n8.8\\n0.0\\n21.9\\n19.2\\n15.6\\n12.4\\n28.0\\n2.4\\nFlan-T5-Base\\n24.4\\n19.2\\n42.8\\n40.8\\n39.6\\n32.4\\n0.4\\n0.0\\n62.8\\n32.4\\n22.8\\n11.2\\n17.8\\n9.6\\n22.4\\n23.6\\n13.6\\n10.4\\n780M T5-Large\\n13.2\\n8.0\\n32.4\\n26.0\\n24.8\\n23.2\\n0.4\\n0.0\\n42.0\\n42.0\\n9.6\\n6.4\\n21.9\\n23.3\\n10.4\\n14.8\\n27.6\\n0.4\\nFlan-T5-Large\\n46.8\\n22.4\\n53.2\\n36.8\\n41.6\\n28.0\\n0.4\\n0.4\\n44.8\\n34.0\\n32.8\\n16.8\\n22.6\\n22.6\\n43.6\\n38.4\\n28.8\\n25.6\\n3B\\nT5-XL\\n13.6\\n15.2\\n35.2\\n35.6\\n25.2\\n23.6\\n0.8\\n0.8\\n42.0\\n38.0\\n6.4\\n25.2\\n21.2\\n25.3\\n12.8\\n14.8\\n26.0\\n0.8\\nFlan-T5-XL\\n53.6\\n25.2\\n66.0\\n50.8\\n46.4\\n36.4\\n0.4\\n0.4\\n48.4\\n46.4\\n42.4\\n30.8\\n37.7\\n35.6\\n50.8\\n46.0\\n42.0\\n28.4\\n11B\\nT5-XXL\\n18.0\\n18.0\\n36.8\\n42.8\\n46.0\\n45.2\\n0.0\\n0.0\\n41.6\\n37.2\\n31.6\\n33.2\\n21.2\\n24.7\\n16.4\\n22.8\\n20.8\\n0.0\\nFlan-T5-XXL\\n54.8\\n48.8\\n76.0\\n58.8\\n53.2\\n53.2\\n0.4\\n0.4\\n60.4\\n54.0\\n50.8\\n34.0\\n39.0\\n39.0\\n58.8\\n46.8\\n52.4\\n53.2\\n8B\\nPaLM\\n13.2\\n14.8\\n35.6\\n36.4\\n28.4\\n26.4\\n0.8\\n1.2\\n58.0\\n58.0\\n36.8\\n18.8\\n25.3\\n19.9\\n18.0\\n18.8\\n21.2\\n24.4\\nFlan-PaLM\\n25.6\\n12.8\\n47.6\\n40.8\\n72.8\\n43.6\\n0.8\\n0.8\\n58.4\\n55.6\\n30.0\\n24.8\\n26.7\\n30.1\\n28.4\\n34.0\\n36.8\\n32.0\\n62B\\nPaLM\\n19.6\\n20.0\\n36.8\\n52.4\\n60.8\\n70.8\\n0.8\\n1.6\\n56.4\\n55.2\\n41.6\\n50.4\\n24.0\\n37.0\\n17.2\\n48.0\\n50.4\\n54.0\\nFlan-PaLM\\n48.8\\n34.0\\n74.0\\n56.0\\n82.0\\n72.8\\n1.2\\n1.6\\n60.4\\n49.2\\n50.4\\n51.2\\n37.0\\n49.3\\n50.4\\n46.0\\n63.6\\n54.8\\n540B\\nPaLM\\n24.8\\n43.6\\n63.6\\n78.0\\n87.2\\n92.0\\n1.6\\n19.6\\n62.4\\n79.6\\n51.2\\n83.2\\n44.5\\n65.1\\n38.0\\n74.4\\n76.0\\n61.6\\nFlan-PaLM\\n50.8\\n48.4\\n85.6\\n87.2\\n85.6\\n82.4\\n0.8\\n29.6\\n68.4\\n78.0\\n54.0\\n88.8\\n55.5\\n72.6\\n66.4\\n82.4\\n81.2\\n68.0\\n250M SwitchBASE\\n0.0\\n0.4\\n0.0\\n1.2\\n0.0\\n3.6\\n0.4\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n6.4\\n0.0\\n0.0\\nFLAN-SwitchBASE\\n38.4\\n23.2\\n47.2\\n41.6\\n41.6\\n33.2\\n0.0\\n0.0\\n59.2\\n54.0\\n30.8\\n18.4\\n34.9\\n19.9\\n36.8\\n24.8\\n12.4\\n10.4\\n780M SwitchLARGE\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n0.0\\n0.4\\n0.0\\n0.0\\n17.8\\n0.0\\n4.0\\n0.0\\n0.4\\nFLAN-SwitchLARGE\\n44.8\\n22.8\\n57.2\\n42.0\\n61.2\\n47.2\\n0.4\\n0.8\\n45.6\\n43.2\\n41.6\\n33.2\\n38.4\\n29.5\\n42.0\\n32.4\\n11.6\\n10.8\\n11B\\nSwitchXXL\\n0.0\\n0.0\\n0.0\\n4.0\\n0.0\\n1.2\\n0.4\\n0.0\\n0.0\\n0.0\\n0.0\\n1.6\\n0.0\\n6.8\\n0.0\\n2.0\\n0.0\\n2.0\\nFLAN-SwitchXXL\\n61.1\\n46.9\\n80.6\\n70.6\\n58.5\\n54.1\\n1.5\\n0.4\\n58.4\\n58.2\\n47.2\\n40.3\\n47.6\\n44.2\\n62.8\\n55.7\\n66.4\\n50.4\\n80M\\nFLAN-GSSMALL\\n16.8\\n12.4\\n33.6\\n34.4\\n42.8\\n13.2\\n0.0\\n0.4\\n62.4\\n40.0\\n20.0\\n9.2\\n13.0\\n15.8\\n25.6\\n19.2\\n9.2\\n6.4\\n250M FLAN-GSBASE\\n36.0\\n17.2\\n48.4\\n35.6\\n54.0\\n47.2\\n0.0\\n0.0\\n61.2\\n53.6\\n27.2\\n29.6\\n29.5\\n20.5\\n34.0\\n24.4\\n10.8\\n14.0\\n780M FLAN-GSLARGE\\n46.8\\n26.0\\n60.8\\n34.4\\n45.2\\n39.6\\n1.6\\n0.4\\n57.6\\n44.8\\n36.0\\n21.6\\n31.5\\n25.3\\n25.6\\n32.4\\n29.6\\n32.4\\n80M\\nFLAN-ECSMALL\\n14.8\\n12.8\\n33.6\\n29.6\\n40.4\\n36.0\\n0.8\\n0.4\\n64.4\\n57.6\\n19.6\\n4.0\\n13.7\\n17.8\\n21.6\\n18.8\\n8.8\\n8.0\\n250M FLAN-ECBASE\\n35.2\\n24.0\\n50.8\\n34.8\\n24.8\\n34.0\\n0.4\\n0.4\\n62.0\\n50.4\\n32.8\\n24.8\\n31.5\\n26.0\\n33.2\\n26.0\\n18.0\\n15.2\\n780M FLAN-ECLARGE\\n50.0\\n22.8\\n57.2\\n30.0\\n50.8\\n45.2\\n0.0\\n0.8\\n58.8\\n59.6\\n38.4\\n31.2\\n33.6\\n27.4\\n34.4\\n39.6\\n20.0\\n26.4\\n3B\\nFLAN-ECXL\\n53.4\\n48.6\\n60.8\\n56.5\\n48.6\\n38.4\\n66.7\\n35.1\\n0.0\\n0.4\\n53.6\\n49.2\\n11.0\\n4.5\\n61.4\\n40.3\\n53.0\\n37.9\\n250M STBASE\\n0.0\\n13.2\\n0.0\\n28.8\\n0.0\\n4.0\\n0.0\\n1.6\\n0.0\\n42.0\\n0.0\\n6.4\\n0.0\\n15.8\\n0.0\\n6.4\\n0.0\\n0.8\\nFLAN-STBASE\\n43.5\\n22.7\\n53.7\\n42.6\\n42.9\\n33.9\\n0.4\\n0.4\\n48.1\\n47.2\\n33.1\\n31.6\\n35.0\\n27.7\\n40.0\\n40.7\\n18.9\\n21.0\\n32B\\nST32B\\n0.0\\n1.6\\n0.0\\n20.8\\n0.0\\n0.4\\n0.4\\n0.4\\n0.0\\n0.0\\n0.4\\n3.2\\n0.0\\n0.0\\n0.0\\n10.4\\n0.0\\n0.0\\nFLAN-ST32B\\n62.4\\n44.8\\n90.8\\n79.6\\n69.6\\n66.0\\n0.8\\n0.4\\n63.2\\n48.0\\n52.4\\n49.6\\n61.6\\n55.5\\n78.0\\n72.0\\n72.8\\n64.4\\n21\\nTable 11: BBH[18:27] individual task performance.\\nBBH\\nSalient Translation\\nError Detection\\nSnarks\\nSports\\nUnderstanding\\nTemporal\\nSequences\\nTracking Shuffled\\nObjects (5)\\nTracking Shuffled\\nObjects (7)\\nTracking Shuffled\\nObjects (3)\\nWeb of\\nLies\\nWord\\nSorting\\nAverage\\nModel\\nDirect\\nCoT\\nDirect CoT Direct\\nCoT\\nDirect CoT Direct\\nCoT\\nDirect\\nCoT\\nDirect\\nCoT\\nDirect\\nCoT\\nDirect CoT Direct CoT\\n-\\ndavinci\\n22.4\\n5.2\\n52.2\\n47.8\\n54.4\\n94.0\\n22.8\\n22.4\\n32.0\\n18.0\\n13.6\\n14.8\\n33.6\\n32.0\\n48.8\\n59.2\\n11.2\\n6.0\\n33.6\\n38.3\\n-\\ntext-davinci-002\\n61.6\\n62.4\\n65.2\\n60.7\\n71.6\\n92.0\\n33.6\\n67.2\\n23.2\\n60.8\\n17.2\\n59.6\\n34.8\\n62.8\\n51.6\\n92.0\\n36.8\\n44.4\\n48.6\\n67.2\\n-\\ntext-davinci-003\\n68.0\\n60.8\\n67.4\\n74.2\\n72.4\\n96.0\\n37.6\\n58.0\\n18.0\\n80.8\\n16.0\\n81.2\\n30.4\\n68.4\\n53.2\\n100.0\\n45.6\\n41.6\\n50.9\\n70.7\\n-\\ncode-davinci-002\\n62.0\\n60.8\\n61.2\\n59.6\\n72.8\\n97.6\\n77.6\\n96.8\\n20.4\\n89.6\\n14.4\\n85.6\\n37.6\\n78.4\\n51.6\\n95.2\\n50.4\\n40.4\\n52.8\\n73.7\\n80M\\nT5-Small\\n12.0\\n0.0\\n46.1\\n15.2\\n46.4\\n35.6\\n28.4\\n1.6\\n20.8\\n0.0\\n15.2\\n0.0\\n32.8\\n0.0\\n51.2\\n0.0\\n0.4\\n0.0\\n27.0\\n7.2\\nFlan-T5-Small\\n22.4\\n15.2\\n46.6\\n9.6\\n54.8\\n54.0\\n28.4\\n17.2\\n22.4\\n15.2\\n14.0\\n8.8\\n30.8\\n25.6\\n53.6\\n36.8\\n2.0\\n1.2\\n29.1\\n19.2\\n250M T5-Base\\n22.0\\n0.8\\n46.1\\n5.1\\n46.4\\n38.4\\n28.4\\n28.4\\n20.4\\n5.6\\n15.2\\n5.6\\n31.6\\n9.6\\n51.6\\n22.4\\n0.8\\n3.2\\n27.8\\n14.6\\nFlan-T5-Base\\n11.6\\n18.0\\n42.7\\n46.1\\n52.8\\n46.4\\n18.4\\n20.4\\n16.8\\n19.2\\n10.4\\n11.2\\n33.2\\n32.0\\n52.4\\n47.2\\n4.0\\n2.0\\n30.3\\n26.8\\n780M T5-Large\\n22.4\\n0.0\\n46.1\\n14.6\\n46.8\\n48.4\\n28.0\\n28.4\\n22.0\\n16.4\\n15.2\\n9.2\\n32.0\\n22.8\\n49.2\\n22.8\\n3.2\\n0.0\\n27.7\\n16.1\\nFlan-T5-Large\\n41.6\\n25.6\\n57.9\\n52.8\\n52.0\\n45.2\\n8.4\\n23.2\\n12.4\\n11.2\\n8.4\\n10.4\\n33.6\\n31.6\\n51.2\\n48.4\\n0.8\\n2.4\\n34.7\\n28.5\\n3B\\nT5-XL\\n22.8\\n6.8\\n47.2\\n30.3\\n50.8\\n44.8\\n28.4\\n22.8\\n15.2\\n14.8\\n12.4\\n12.0\\n32.4\\n31.2\\n48.8\\n43.2\\n2.4\\n2.4\\n27.4\\n19.2\\nFlan-T5-XL\\n34.4\\n30.4\\n72.5\\n75.8\\n51.2\\n55.6\\n22.8\\n31.2\\n12.4\\n15.6\\n8.4\\n10.0\\n29.2\\n29.6\\n49.6\\n46.8\\n4.8\\n0.0\\n40.2\\n35.9\\n11B\\nT5-XXL\\n15.2\\n0.0\\n53.9\\n25.3\\n47.2\\n60.0\\n19.2\\n17.2\\n18.4\\n1.6\\n10.0\\n0.0\\n33.2\\n30.0\\n48.8\\n4.4\\n3.2\\n2.0\\n29.5\\n19.3\\nFlan-T5-XXL\\n46.4\\n50.0\\n74.7\\n76.4\\n64.4\\n66.0\\n25.6\\n21.2\\n18.0\\n12.0\\n9.6\\n16.8\\n28.8\\n24.8\\n54.0\\n53.2\\n7.2\\n4.4\\n45.6\\n41.6\\n8B\\nPaLM\\n21.6\\n12.0\\n53.9\\n51.1\\n54.0\\n76.8\\n25.6\\n28.8\\n20.4\\n19.6\\n12.8\\n10.8\\n32.0\\n31.6\\n51.2\\n48.8\\n4.4\\n4.4\\n30.8\\n30.1\\nFlan-PaLM\\n23.2\\n0.8\\n69.1\\n59.6\\n64.4\\n69.6\\n15.6\\n24.0\\n17.2\\n11.2\\n16.8\\n13.6\\n33.2\\n32.0\\n52.0\\n49.2\\n6.0\\n1.2\\n36.4\\n31.1\\n62B\\nPaLM\\n28.0\\n21.6\\n52.8\\n48.3\\n78.4\\n95.6\\n21.2\\n26.4\\n19.6\\n18.8\\n13.6\\n13.6\\n30.4\\n36.4\\n48.8\\n80.8\\n7.6\\n8.4\\n37.4\\n42.3\\nFlan-PaLM\\n45.2\\n40.4\\n83.1\\n78.1\\n79.2\\n81.2\\n30.8\\n36.0\\n21.2\\n18.0\\n15.2\\n18.0\\n22.0\\n29.6\\n48.4\\n92.0\\n11.2\\n10.0\\n47.5\\n44.9\\n540B\\nPaLM\\n48.8\\n54.0\\n78.1\\n61.8\\n80.4\\n98.0\\n39.6\\n78.8\\n16.8\\n57.6\\n13.6\\n42.4\\n28.4\\n58.8\\n51.2\\n100.0\\n32.0\\n21.6\\n49.1\\n62.0\\nFlan-PaLM\\n53.2\\n51.6\\n85.4\\n76.4\\n83.2\\n87.2\\n81.6\\n91.6\\n24.4\\n50.8\\n21.6\\n38.0\\n32.4\\n71.6\\n62.4\\n100.0\\n32.0\\n33.2\\n57.9\\n66.3\\n250M SwitchBASE\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n13.6\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.1\\n1.4\\nFLAN-SwitchBASE\\n27.2\\n25.6\\n39.3\\n39.9\\n53.2\\n54.4\\n10.4\\n15.6\\n11.6\\n13.2\\n14.4\\n14.4\\n32.0\\n33.6\\n49.6\\n53.2\\n2.4\\n1.2\\n33.2\\n29.4\\n780M SwitchLARGE\\n0.0\\n0.4\\n0.0\\n45.5\\n0.0\\n0.0\\n0.0\\n6.4\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n4.0\\n0.0\\n0.0\\n0.2\\n7.2\\nFLAN-SwitchLARGE\\n27.6\\n8.8\\n52.8\\n52.8\\n57.2\\n54.4\\n18.4\\n14.8\\n12.4\\n12.8\\n8.4\\n10.8\\n33.6\\n30.4\\n51.2\\n48.0\\n4.0\\n0.4\\n36.4\\n28.0\\n11B\\nSwitchXXL\\n0.0\\n6.8\\n0.0\\n0.0\\n0.0\\n12.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n39.6\\n0.0\\n0.0\\n0.0\\n6.7\\nFLAN-SwitchXXL\\n51.7\\n41.1\\n81.1\\n74.3\\n68.8\\n74.3\\n40.0\\n36.4\\n19.5\\n18.0\\n21.0\\n14.0\\n20.8\\n25.7\\n50.3\\n49.7\\n8.3\\n4.7\\n47.9\\n43.4\\n80M\\nFLAN-GSSMALL\\n20.8\\n0.0\\n46.6\\n37.1\\n54.0\\n52.8\\n22.4\\n22.4\\n23.6\\n18.0\\n12.4\\n8.8\\n34.4\\n32.0\\n51.6\\n32.0\\n2.4\\n0.0\\n29.6\\n20.9\\n250M FLAN-GSBASE\\n23.2\\n0.0\\n47.8\\n35.4\\n56.4\\n52.8\\n22.8\\n19.2\\n12.4\\n15.6\\n8.4\\n10.8\\n32.4\\n34.8\\n50.0\\n52.8\\n3.6\\n0.4\\n33.7\\n25.1\\n780M FLAN-GSLARGE\\n16.8\\n14.8\\n61.8\\n53.9\\n59.2\\n55.2\\n12.4\\n20.8\\n12.4\\n5.6\\n8.4\\n5.6\\n34.0\\n19.2\\n52.4\\n56.0\\n3.2\\n1.6\\n35.0\\n29.2\\n80M\\nFLAN-ECSMALL\\n23.2\\n3.6\\n48.3\\n23.6\\n54.0\\n54.4\\n17.6\\n23.6\\n24.8\\n18.8\\n11.6\\n14.0\\n30.0\\n28.8\\n50.8\\n30.8\\n2.8\\n0.0\\n29.2\\n22.2\\n250M FLAN-ECBASE\\n22.4\\n13.2\\n41.6\\n44.4\\n57.2\\n54.0\\n16.0\\n11.2\\n14.4\\n14.8\\n8.0\\n10.0\\n34.0\\n34.0\\n53.2\\n52.4\\n2.8\\n1.2\\n34.0\\n26.6\\n780M FLAN-ECLARGE\\n42.0\\n15.6\\n55.6\\n56.7\\n59.2\\n58.4\\n19.6\\n20.8\\n12.4\\n12.8\\n8.4\\n9.2\\n33.6\\n32.0\\n54.4\\n49.2\\n3.6\\n2.8\\n37.9\\n32.0\\n3B\\nFLAN-ECXL\\n38.6\\n21.2\\n64.0\\n53.7\\n63.2\\n59.2\\n16.6\\n22.4\\n13.2\\n17.0\\n8.6\\n8.6\\n26.8\\n28.1\\n50.8\\n48.8\\n6.8\\n2.3\\n40.3\\n33.2\\n250M STBASE\\n0.0\\n10.8\\n0.0\\n44.4\\n0.0\\n47.2\\n0.0\\n2.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n21.2\\n0.0\\n0.0\\n0.0\\n14.0\\nFLAN-STBASE\\n13.3\\n11.6\\n61.0\\n58.1\\n56.0\\n52.2\\n18.4\\n20.2\\n12.2\\n12.3\\n7.9\\n12.2\\n33.9\\n34.5\\n52.5\\n48.6\\n3.3\\n2.2\\n34.7\\n26.6\\n32B\\nST32B\\n0.0\\n10.4\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n18.0\\n0.0\\n9.2\\n0.0\\n32.8\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n5.5\\nFLAN-ST32B\\n57.6\\n52.8\\n88.2\\n86.0\\n73.2\\n75.6\\n75.6\\n44.8\\n27.2\\n18.4\\n28.0\\n19.6\\n21.6\\n28.0\\n40.4\\n48.8\\n15.6\\n4.8\\n54.4\\n47.4\\nA.3\\nReasoning\\nThe four reasoning tasks are held-in, which means we perform instruction finetuning on the training\\nset while evaluating on the “validation” set in a few-shot way. The detailed performance is presented\\nhere.\\n22\\nTable 12: Reasoning[:4] individual task performance.\\nReasoning\\nGSM8K\\nASDIV\\nStrategyQA\\nSVAMP\\nAverage\\nModel\\nCoT\\nCoT\\nCoT\\nCoT\\nCoT\\n80M\\nT5-Small\\n1.1\\n1.7\\n37.1\\n1.3\\n10.3\\nFlan-T5-Small\\n2.1\\n2.8\\n53.2\\n2.1\\n15.0\\n250M\\nT5-Base\\n2.0\\n1.8\\n52.8\\n2.0\\n14.7\\nFlan-T5-Base\\n3.9\\n4.9\\n53.3\\n3.5\\n16.4\\n780M\\nT5-Large\\n1.6\\n2.0\\n42.8\\n1.0\\n11.9\\nFlan-T5-Large\\n8.6\\n14.5\\n54.2\\n11.6\\n22.2\\n3B\\nT5-XL\\n2.7\\n5.2\\n45.9\\n2.9\\n14.2\\nFlan-T5-XL\\n16.9\\n28.2\\n64.6\\n25.9\\n33.9\\n11B\\nT5-XXL\\n2.5\\n15.0\\n55.0\\n12.9\\n21.4\\nFlan-T5-XXL\\n26.7\\n47.4\\n69.9\\n41.4\\n46.3\\n8B\\nFlan-PaLM\\n21.4\\n37.5\\n65.5\\n23.1\\n36.9\\n62B\\nFlan-PaLM\\n47.5\\n64.5\\n76.4\\n50.2\\n47.7\\n540B\\nFlan-PaLM\\n73.0\\n77.7\\n83.0\\n72.2\\n76.5\\n250M\\nSwitchBASE\\n0.6\\n1.0\\n17.5\\n1.5\\n5.2\\nFLAN-SwitchBASE\\n6.4\\n8.4\\n53.3\\n6.3\\n18.6\\n780M\\nSwitchLARGE\\n1.9\\n2.4\\n43.2\\n2.0\\n12.4\\nFLAN-SwitchLARGE\\n12.7\\n19.0\\n56.3\\n13.0\\n25.3\\n11B\\nSwitchXXL\\n0.2\\n0.4\\n36.2\\n0.1\\n9.2\\nFLAN-SwitchXXL\\n27.0\\n47.8\\n70.1\\n41.7\\n46.6\\n80M\\nFLAN-GSSMALL\\n3.7\\n5.0\\n53.3\\n3.3\\n16.1\\n250M\\nFLAN-GSBASE\\n11.1\\n13.9\\n53.7\\n9.9\\n22.2\\n780M\\nFLAN-GSLARGE\\n16.7\\n22.2\\n54.6\\n17.0\\n27.6\\n80M\\nFLAN-ECSMALL\\n5.2\\n5.6\\n53.3\\n5.4\\n16.6\\n250M\\nFLAN-ECBASE\\n10.7\\n13.7\\n53.3\\n10.5\\n22.0\\n780M\\nFLAN-ECLARGE\\n15.9\\n25.7\\n65.5\\n21.7\\n32.2\\n3B\\nFLAN-ECXL\\n21.3\\n33.6\\n67.2\\n30.3\\n38.1\\n250M\\nSTBASE\\n2.0\\n1.9\\n45.0\\n1.3\\n12.6\\nFLAN-STBASE\\n11.2\\n11.1\\n59.8\\n8.0\\n22.5\\nST32B\\n2.7\\n18.4\\n1.7\\n16.2\\n9.8\\nFLAN-ST32B\\n51.1\\n65.3\\n80.8\\n68.1\\n66.3\\n23\\nTable 13: QA[:5] individual task performance.\\nQA\\nUnifiedQA\\nElementary Science\\nARC\\neasy\\nARC\\nchalllenge\\nBoolQ\\nAverage\\nModel\\nDirect\\nDirect\\nDirect\\nDirect\\nDirect\\n80M\\nFlan-T5-Small\\n27.6\\n40.4\\n31.9\\n63.7\\n40.9\\n250M\\nFlan-T5-Base\\n34.1\\n46.1\\n38.7\\n76.2\\n48.8\\n780M\\nFlan-T5-Large\\n43.9\\n76.3\\n53.2\\n84.0\\n64.4\\n3B\\nFlan-T5-XL\\n53.7\\n88.4\\n66.2\\n88.0\\n74.1\\n11B\\nFlan-T5-XXL\\n63.4\\n94.2\\n74.6\\n89.3\\n80.4\\n8B\\nFlan-PaLM\\n72.4\\n83.4\\n61.7\\n83.0\\n75.1\\n62B\\nFlan-PaLM\\n85.4\\n92.0\\n77.3\\n86.3\\n85.3\\n540B\\nFlan-PaLM\\n92.7\\n95.2\\n88.7\\n83.0\\n89.9\\n250M\\nFLAN-SwitchBASE\\n48.1\\n61.4\\n43.2\\n79.3\\n58.0\\n780M\\nFLAN-SwitchLARGE\\n50.3\\n70.3\\n61.7\\n83.8\\n66.5\\n11B\\nFLAN-SwitchXXL\\n60.2\\n73.7\\n91.7\\n89.7\\n78.8\\n80M\\nFLAN-GSSMALL\\n39.0\\n48.5\\n36.0\\n72.0\\n48.9\\n250M\\nFLAN-GSBASE\\n43.9\\n59.3\\n45.9\\n82.5\\n57.9\\n780M\\nFLAN-GSLARGE\\n53.7\\n69.4\\n66.7\\n88.2\\n69.5\\n80M\\nFLAN-ECSMALL\\n37.4\\n61.4\\n50.0\\n83.4\\n58.1\\n250M\\nFLAN-ECBASE\\n51.2\\n61.4\\n50.0\\n83.4\\n61.5\\n780M\\nFLAN-ECLARGE\\n59.3\\n71.8\\n71.3\\n90.1\\n73.1\\n3B\\nFLAN-ECXL\\n60.1\\n71.8\\n75.3\\n90.1\\n74.3\\n250M\\nFLAN-STBASE\\n47.2\\n58.3\\n57.7\\n82.6\\n61.5\\n32B\\nST32B\\n31.7\\n25.8\\n30.1\\n40.6\\n32.1\\nFLAN-ST32B\\n69.9\\n99.2\\n90.8\\n92.1\\n88.0\\nA.4\\nQA\\nWe perform evaluation on four held-out QA tasks and the results are summarized in this section.\\n24\\n', 'source_name': 'Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models', 'source_url': 'https://arxiv.org/abs/2305.14705'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MoE_Mamba_NOTES.pdf #28\n",
      "{'content': 'MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts \\nMain Idea: the goal of this work is to compare vanilla MoE (Transformer MoE) with vanilla Mamba \\nand MoE-Mamba to explore if these architectures are compatible with each other. The main \\nhighlight of this paper is that MoE-Mamba outperforms both Transformer-MoE and vanilla-\\nMamba, reaching the same performance of vanilla-Mamba with 2.2x less training steps, while \\npreserving Mamba inference gains over the Transformer. This shows that MoE results in \\nperformance gains when combined with the Mamba architecture, similarly to when applied to \\nTransformers. In theory, this should result in easier scaling for Mamba, with even more inference \\ngains due to the sparsity of MoE. \\n \\nMamba \\nMamba is an SSM (State-Space Model) architecture (SSM meaning that it is based on hidden \\nstates that update and drop/forget irrelevant information) like RNNs, GRUs and LSTMs.  \\n- \\nMamba is an improvement over previous SSM architectures because it is optimized for \\nGPUs and can make use of parallelism.  \\n- \\nMamba is an improvement over Transformers because the characteristic of dropping \\nirrelevant info of SSM architectures allows for a much lesser complexity as the input size \\nincreases. In theory, this should result in increased quality and reduced inference costs \\nfor Mamba compared to Transformers when scaling the context length. \\no Transformers’ complexity increases quadratically with an increase in input size \\n(O(n^2)). Mamba does not impose this constraint. \\n \\nMoE-Mamba Architecture \\n- \\nMoE-Mamba makes use of a similar architecture to Switch Transformer. \\no Token-choice routing (top-k) with k=1 (one expert used per token) \\no Every other Mamba layer is replaced with an FF MoE (each block alternates \\nbetween dense (Mamba) and sparse (Mamba MoE) layers). \\n- \\nThe active parameters of the models experimented with were ~26M per token. \\no The total number of parameters of the biggest MoE-Mamba model used was \\n416M parameters (32 experts). \\n- \\nMoE-Mamba scales well with an increase in the number of experts (expert size was \\nconstant, so increasing the number of experts means increasing the number of total \\nparameters while keeping the number of active parameters constant). The largest \\nnumber of experts experimented with was 32. \\no MoE-Mamba needed at least 8 experts to improve over vanilla Mamba. \\n \\nMy takeaways: \\n- \\nMamba’s main advantage over Transformers seems to be of the handling of large context \\nlengths due to SSM architectures inherently having the ability to drop irrelevant info from \\ntoken to token. This is not true for the attention process in the Transformer architecture, \\nwhich has an exponential increase in complexity with an increase in the context length. \\no The main questions about Mamba’s legitimacy today are: \\n▪ How will Mamba scale in terms of increasing parameter size and data? \\n▪ Will Mamba work given huge context lengths (tens/hundreds of thousands \\nof tokens)? \\no More research on the Mamba architecture is needed on my end. \\n- \\nMore research on Mamba-MoE needs to be done at increased parameter scales. A 416M \\nparameter model with 26M active parameters per token is too small. Thus, the results of \\nthis paper should be seen as a mere indication and be taken with a grain of salt. \\n \\n', 'source_name': 'MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_Mamba_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sparse_Upcycling.pdf #29\n",
      "{'content': 'Published as a conference paper at ICLR 2023\\nSPARSE UPCYCLING: TRAINING\\nMIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS\\nAran Komatsuzaki∗†\\nJoan Puigcerver†\\nJames Lee-Thorp†\\nGeorgia Institute of Technology\\nGoogle Research\\nGoogle Research\\nCarlos Riquelme\\nBasil Mustafa\\nJoshua Ainslie\\nGoogle Research\\nGoogle Research\\nGoogle Research\\nYi Tay\\nMostafa Dehghani\\nNeil Houlsby\\nGoogle Research\\nGoogle Research\\nGoogle Research\\nABSTRACT\\nTraining large, deep neural networks to convergence can be prohibitively expensive.\\nAs a result, often only a small selection of popular, dense models are reused across\\ndifferent contexts and tasks. Increasingly, sparsely activated models, which seek to\\ndecouple model size from computation costs, are becoming an attractive alternative\\nto dense models. Although more efﬁcient in terms of quality and computation cost,\\nsparse models remain data-hungry and costly to train from scratch in the large\\nscale regime. In this work, we propose sparse upcycling – a simple way to reuse\\nsunk training costs by initializing a sparsely activated Mixture-of-Experts model\\nfrom a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and\\nXL language models and Vision Transformer Base and Large models, respectively,\\nsigniﬁcantly outperform their dense counterparts on SuperGLUE and ImageNet,\\nusing only ∼50% of the initial dense pretraining sunk cost. The upcycled models\\nalso outperform sparse models trained from scratch on 100% of the initial dense\\npretraining computation budget.1\\n1\\nINTRODUCTION\\nIncreased scale is one of the main drivers of better performance in deep learning. From BERT (Devlin\\net al., 2019) to GPT-3 (Brown et al., 2020) to PaLM (Chowdhery et al., 2022) in natural language\\nprocessing, or from AlexNet (Krizhevsky et al., 2017) to ViT-G (Zhai et al., 2022) in vision, break-\\nthroughs in performance have been obtained from larger hardware, datasets, and architectures. This\\ntrend holds true in many other domains too, including speech (Baevski et al., 2020), reinforcement\\nlearning (Schrittwieser et al., 2020), multimodal learning (Yu et al., 2022), and scientiﬁc applications\\nof deep learning (Jumper et al., 2021).\\nHowever, most state-of-the-art neural networks are trained from-scratch; that is, starting from\\nrandomly initialized weights. The cost for training such networks is growing rapidly. For example, in\\nlanguage, BERT-Large (345M parameters, proposed in 2018) required an estimated 0.5 ZFLOPS\\nto train, while GPT-3 (175B parameters, from 2020) required 314 ZFLOPS (Brown et al., 2020),\\nand PaLM (540B parameters, from 2022) required 2527 ZFLOPS (Chowdhery et al., 2022). As a\\nresult of these computation costs, research into new large language models is often limited to a small\\nnumber of teams with access to lots of resources. To enable signiﬁcant further progress, we must\\ndevelop cheaper ways of training giant models.\\nIn this paper, we explore model upcycling: upgrading an existing model with a relatively small\\nadditional computational budget. In particular, we focus on upcycling dense models into larger,\\nsparsely activated Mixture-of-Experts (MoEs). We do not use any new unique sources of data (Wei\\n∗Work done while interning at Google Research.\\n†Contacts: aran1234321@gmail.com, {jpuigcerver, jamesleethorp}@google.com.\\n1Code is available at https://github.com/google-research/vmoe (Vision) and https://\\ngithub.com/google-research/t5x/tree/main/t5x/contrib/moe (Language).\\n1\\narXiv:2212.05055v2  [cs.LG]  17 Feb 2023\\nPublished as a conference paper at ICLR 2023\\net al., 2021; Ouyang et al., 2022). We assume the existence of a pretrained dense Transformer\\ncheckpoint (e.g. (Wolf et al., 2020)), that we then use to warm-start the training of a MoE. By\\nleveraging the additional capacity of from the MoE layers, we obtain an MoE model more performant\\nthan the original model, at a smaller cost than was used to train the original model. Across all model\\nsizes that we study for both language and vision, with less than 40% additional budget, upcycling\\nimproves the network’s performance beyond what would be achieved by continued training the\\noriginal Transformer model.\\nSparse upcycling may be particularly valuable in two scenarios: (i) One has access to a pretrained\\nTransformer (there are many publicly available) and wants to improve it with a modest or constrained\\ncomputational budget. (ii) One is planning to train a large model, and do not know whether a dense\\nor MoE model would be more effective (the latter often being more performant, but more technically\\nchallenging to train): one can have both by ﬁrst training the dense model, then upcycling it into an\\nMoE model once the dense model saturates.\\nA central challenge in model upcycling is overcoming the initial performance decrease entailed by\\nchanging a trained network’s structure. We present a model surgery recipe that is effective in both\\nvision and language, and numerous ablations for the key components that make it work well. In\\nexperiments on Vision Transformers (Dosovitskiy et al., 2021) and T5 language models (Raffel et al.,\\n2020), we show that upcycling is highly effective when the computation budget lies between +10%\\nand +60% of the cost to train the original (dense) network. For example, increasing the performance\\nof ViT-B/16 by at least 1% on ImageNet 10-shot requires an additional 58% extra training time\\n(relative to the original checkpoint) if we continue training the dense model; however, it only takes\\n13% extra training time with the upcycled version. Similarly, upcycled T5-Large and T5-Base models\\noutperform their dense counterparts by 1.5-2 absolute points on SuperGLUE using 46% and 55%\\nextra training, respectively.\\n2\\nBACKGROUND\\nIn this section we recap of the main components used in sparse upcycling: Transformer-based\\nlanguage and vision models, and sparsely activated Mixture-of-Experts (MoEs).\\n2.1\\nSPARSELY ACTIVATED MIXTURE-OF-EXPERTS (MOE)\\nDense models apply all parameters to every input. Accordingly, growing the model capacity results\\nin increased computational cost. Sparse models attempt to alleviate this fundamental issue by only\\nactivating a subset of parameters for each input. Sparsely activated Mixture-of-Experts (MoE) models\\nare an accelerator friendly family of sparse models that allow training of models with up to trillions\\nof parameters (Shazeer et al., 2017; Fedus et al., 2022).\\nMoE models typically alternate standard dense Transformer blocks with MoE blocks. In particular,\\nwe usually replace the MLPs in a Transformer block with a number of “experts” (typically themselves\\nMLPs) with different learnable parameters and a router—a small neural network—that decides which\\nexpert is applied to each individual token. A number of routing algorithms have been developed, for\\nexample Top-K (Shazeer et al., 2017), BASE and Sinkhorn-BASE layers (Lewis et al., 2021; Clark\\net al., 2022), Hash layers (Roller et al., 2021), and Expert Choice routing (Zhou et al., 2022).\\nWe generally focus on Expert Choice routing, which works as follows. Let E denote the total\\nnumber of experts in a MoE layer, and n the total number of tokens. The router outputs a matrix\\nR ∈Rn×E with the routing probabilities, where row ri ∈RE corresponds to the i-th token and is a\\ndistribution over E experts (rij ≥0 and P\\nj rij = 1). Then, every expert e independently chooses\\nthe T tokens with highest probabilities for e (i.e., we perform top-T per column) and processes them.\\nWe parameterize T as T = C(n/E), where C is a capacity factor that we control to choose more or\\nfewer tokens per expert. When C = 1, each expert processes exactly n/E tokens; note that some\\ntokens may be processed by several experts, while others by none. This allows for a model parameter\\ncount increase with minimal FLOPs overhead.2 Letting C > 1 usually leads to higher performance\\nat a higher compute cost.\\n2The FLOPs overhead comes from the (relatively modest) router computation of R.\\n2\\nPublished as a conference paper at ICLR 2023\\nLayer\\nNorm\\nAttention\\nLayer\\nNorm\\nRouter\\nfrom scratch\\nMLP 1\\nMLP 2\\nMLP E\\nWeighted\\nSum\\nMoE\\n...\\nOriginal Dense Block\\nLayer\\nNorm\\nAttention\\nLayer\\nNorm\\nMLP\\nMake E\\nMLP copies\\ncopy weights\\ncopy weights\\ncopy weights\\nUpcycled MoE Block\\nFigure 1: The upcycling initialization process. All parameters, and optionally their optimizer state,\\nare copied from the original checkpoint, except those corresponding to the MoE router, which does\\nnot exist in the original architecture. In particular, the experts in the new MoE layer are identical\\ncopies of the original MLP layer that is replaced.\\n2.2\\nARCHITECTURES\\nWe apply the same sparse upcycling recipe to both language and vision tasks, focusing on the\\nT5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder)\\n(Dosovitskiy et al., 2021) architectures, respectively. We generally adopt the same gating function\\nand MoE hyperparameters in the encoders of both models. See Section 3.1 for speciﬁc design choices\\nand Appendix A for differences between the vision and language upcycling setups.\\nVision. Vision Transformers (ViT) are encoder-only Transformer architectures (Liu et al., 2021;\\nRadford et al., 2021; Touvron et al., 2021; He et al., 2022) which tokenize and embed images. We\\nupcycle models based on the B/32, B/16, L/32 and L/16 variants. The resultant MoEs broadly follow\\nVision MoE Transfomers (“V-MoE”) (Riquelme et al., 2021), with two differences; we perform\\nglobal average pooling (Zhai et al., 2022) and use Expert Choice routing.\\nLanguage. We experiment with the T5 (Raffel et al., 2020) encoder-decoder as our archetypal\\nlanguage model. We upcycle the Base, Large, and XL variants of the model. We sparsify both the\\nencoder and decoder. As in our vision setup, the model encoder applies Expert Choice routing. We\\nuse Top-K routing in the decoder with K = 2; see also Section 3.1.\\n3\\nTHE UPCYCLING ALGORITHM\\nThe algorithm is illustrated in Figure 1. To upcycle a model, we need a dense model’s parameters (i.e.\\na checkpoint). The number and shape of Transformer blocks in the new model is identical to that\\nin the original dense model. A subset of the of the MLP layers are expanded into MoE layers. The\\nremaining MLP layers, along with all of the layer-norm and attention layers, and the embedding and\\noutput layers are copied across from the original model to the new model. Each MoE layer contains\\na ﬁxed number of experts. Each expert is initialized as a copy of the original MLP. In addition, we\\nadd a router whose weights are randomly initialized. In Section 4.2.2, we experiment with different\\nvariations on this basic recipe. After the new model is loaded and initialized, we continue training it\\nfor a number of additional steps depending on the available budget and resources. We use the original\\nhyperparameters: same batch size, learning rate schedule, and weight decay leading to the original\\ncheckpoint; see also Appendix A for full training details.\\n3\\nPublished as a conference paper at ICLR 2023\\n3.1\\nDESIGN DECISIONS\\nAn upcycled model’s performance is heavily inﬂuenced by the conﬁguration of the MoE layers.\\nIncreasing the model capacity by increasing the number of upcycled layers, number of experts or\\nexpert capacity will generally lead to a higher quality model, but will also increase the computational\\ncost and/or result in a greater initial quality drop, due to the more drastic reconﬁguration of the layers.\\nRouter type. For upcycled vision models and for the encoder of upcycled language models, we use\\nExpert Choice routing with capacity factor C = 2. To avoid train time (full batch teacher forcing)\\nversus inference time (single token auto-regressive decoding) discrepancies, we use Top-K (K = 2)\\nrouting in the language decoder. In Section 4.2.2, we show that Expert Choice routing outperforms\\nstandard Top-K routing for upcycling, while both beat dense continuations.\\nNumber layers to upcycle. Adding more MoE layers increases the model capacity dramatically, at\\nthe expense of increasing the model’s cost, and also causing the quality of the upcycled model to\\ninitially drop further relative to the original dense model. Based on our ablation in Section 4.2.2 and\\nprevailing conventions in the MoE literature (Lepikhin et al., 2021), unless otherwise speciﬁed, we\\nreplace half of the MLP layers in our upcycled models with MoE layers.\\nNumber of experts to add in upcycled layers. Each new expert provides new learnable parameters\\nthat extend the model capacity. The expert capacity—the number of tokens expert processes–is\\ninversely proportional to the number of experts, thus adding more experts does not signiﬁcantly affect\\nthe FLOPS or the run time of the model. However, with a very large number of experts, the upcycled\\nmodel experiences a larger initial quality drop relative to the baseline dense model. Given sufﬁcient\\nupcycling compute, this initial drop can be overcome. In our studies, we upcycle with +20% to\\n+100% of the initial dense baseline model’s computational cost, and in this regime we ﬁnd that 32\\nexperts provides a good compromise. We explore varying the number of experts in Section 4.2.2.\\nExpert capacity. By tuning the expert capacity, C, we control the number of experts that process\\neach token on average.3 Larger expert capacity generally yields larger quality but also increases\\nthe FLOPS and run time. Although increasing the expert capacity yields quality gains on a per step\\nbasis, we ﬁnd that C = 2 generally offers good quality on a compute time basis. We ablate through\\ndifferent capacity factors in Section 4.2.2.\\nResuming optimizer state (vision only). When upcycling a model, we can resume the optimizer\\nstate from the original dense checkpoint together with the model parameters. In Appendix B.6, we\\nﬁnd that reusing the optimizer state gives a performance boost for vision models. We did not, however,\\nsee any improvement from reusing the dense model optimizer state in our language experiments, so\\nwe only reuse the optimizer state for vision models.\\nNormalize weights after routing (vision only). In an effort to reduce the performance drop when\\napplying the upcycling model surgery, we attempted to normalize the router combine weights of\\neach token to 1. This follows that intuition that each token was previously only processed by a\\nsingle “expert” MLP in the dense model. Appendix B.7 shows that router weight normalization helps\\nupcycled vision models, but hurts the performance of upcycled language models. One hypothesis\\nfor this different behavior is that the vision models use Expert Choice routing everywhere, but the\\nlanguage models use Expert Choice in the encoder and Top-K routing in the decoder.\\n4\\nEXPERIMENTS\\nIn this section, we present the main experimental results of the paper. We also share the takeaways of\\na number of ablations aimed at identifying the key aspects of our algorithm; full results are included\\nin Appendix B. Most of the results are presented as quality vs. cost plots, where we use the upstream\\nor downstream performance to measure quality, and training time in terms of TPU-core-days (as\\nprominent cost metrics (Dehghani et al., 2021)) or training steps (when the cost per step is the same\\nfor all the compared models) to measure computation cost.\\n3For Expert Choice routing, more capacity means that each expert can choose more tokens. For standard\\nTop-K routing, more capacity means that each token is more likely to ﬁt into the buffer of its desired expert.\\n4\\nPublished as a conference paper at ICLR 2023\\n101\\n102\\nExtra Pretraining Time (TPU-core-days)\\n38%\\n43%\\n49%\\n53%\\n58%\\nJFT Validation Precision at 1\\nB/32\\nB/16\\nL/16\\nmethod\\nDense\\nUpcycling\\nvariant\\nB/32\\nB/16\\nL/16\\n101\\n102\\n103\\nExtra Pretraining Time (TPU-core-days)\\n68%\\n70%\\n72%\\n73%\\n75%\\nC4 Validation Token Accuracy\\nBase\\nLarge\\nXL\\nmethod\\nDense\\nUpcycling\\nvariant\\nBase\\nLarge\\nXL\\nFigure 2: Pretraining performance achieved by the dense continuation and upcycling methods, for\\ndifferent Transformer variants. The left plot shows the performance on the vision task and the right\\none on the text task. The x-axis shows the extra pretraining time (TPU-core-days), with respect to the\\ntotal time needed to train the original dense checkpoints, for each size. The horizontal lines indicate\\nthe quality (y-axis) of the original dense checkpoints.\\n4.1\\nEXPERIMENTAL SETUP\\nAll upcycled experiments begin from a pretrained dense model checkpoint. Because all of our starting\\ndense checkpoints are trained with an inverse square root learning rate schedule, training can be\\ncontinued without discontinuities in the learning rate schedule. We upcycle models and continue\\ntraining, showing performance for varying amounts of continued training. As a baseline, we also\\ncontinue the training of the original dense model (“dense continuation”).\\nVision experiments. MoE Vision Transfomers (“V-MoE”) models are trained broadly following\\nthe protocol of Riquelme et al. (2021). Upstream pretraining is done on JFT300M (Sun et al.,\\n2017), with validation metrics computed on a held-out set of 894,574 examples. Few-shot transfer\\nfollows Dosovitskiy et al. (2021), whereby a least-squares regressor predicts one-hot classes given\\nfrozen image representations. We further validate our results on ImageNet using 10-shot – i.e. 10\\ntraining examples per class. We do this for 5 different training sets, and report average accuracy\\nacross them. For full ﬁnetuning, we replace the pretraining head with a randomly initialized head,\\nand ﬁnetune the entire network. See Appendix A.2.2 for further details.\\nLanguage experiments. Our language experiments follow the setup of Raffel et al. (2020): we\\npretrain using the span corruption task on the English C4 dataset (Raffel et al., 2020) and ﬁnetune on\\na proportional mix of all SuperGLUE (Wang et al., 2019) tasks simultaneously. We include speciﬁc\\ntraining details in Appendix A.2, but highlight one important aspect here: For Base model sizes, for\\nwhich we perform the majority of our ablations, we pretrain the dense baseline starting checkpoint\\nourselves. To highlight the versatility of our upcycling algorithm, for Large and XL models, we\\ninstead begin all experiments from ofﬁcial T5 1.1 checkpoints (Narang et al., 2021; Roberts et al.,\\n2022).\\n4.2\\nEXPERIMENTAL RESULTS\\n4.2.1\\nCORE RESULTS\\nFigure 2 shows a detailed comparison of upstream metrics of upcycled models and dense continuation\\nmodels at various model sizes both for vision (left panel) and language (right panel). For any given\\nmodel size and task, we observe that the dense and upcycled models perform close to each other\\nwhen we apply a very limited extra training budget – indeed, close to their discontinuous horizontal\\nline representing the original checkpoint’s performance. Once we apply a non-trivial amount of extra\\ncompute, a clear pattern emerges showing the strong gains delivered by the upcycled architecture.\\n5\\nPublished as a conference paper at ICLR 2023\\n101\\n102\\nExtra Pretraining Time (TPU-core-days)\\n79%\\n81%\\n83%\\n85%\\n87%\\nILSVRC2012 Finetune Accuracy\\nB/32\\nB/16\\nL/16\\nmethod\\nDense\\nUpcycling\\nvariant\\nB/32\\nB/16\\nL/16\\n101\\n102\\nExtra Pretraining Time (TPU-core-days)\\n73%\\n78%\\n82%\\n85%\\n87%\\nSuperGLUE Finetune Score\\nBase\\nLarge\\nmethod\\nDense\\nUpcycling\\nvariant\\nBase\\nLarge\\nXXL\\nFigure 3: Full ﬁnetuning performance achieved by the dense continuation and upcycling methods.\\nThe left plot shows the performance on ImageNet and the right one on SuperGLUE tasks. The x-axis\\nshows the extra pretraining time (TPU-core-days), with respect to the total time needed to train the\\noriginal dense checkpoints, for each size. The horizontal lines indicate the quality (y-axis) of the\\noriginal dense checkpoints.\\n102\\nExtra Pretraining Time (TPU-core-days)\\n37%\\n41%\\n45%\\n49%\\n53%\\nJFT Validation Precision at 1\\nUpcycling\\nMoE\\n101\\nExtra Pretraining Time (TPU-core-days)\\n67%\\n68%\\n69%\\n70%\\nC4 Validation Token Accuracy\\nUpcycling\\nMoE\\nFigure 4: Pretraining performance achieved by the upcycling method and a MoE model trained from\\nscratch, for B/16 (left plot, vision task) and Base (right plot, text task). The x-axis shows the extra\\npretraining time (TPU-core-days), with respect to the total time needed to train the original dense\\ncheckpoints. The MoE model trained from scratch only catches up to the language upcycled model\\nafter about 120% of the original dense checkpoint computation budget (second last orange and green\\ndots from the right).\\nFigure 3 shows the performance after ﬁnetuning the models trained in Figure 2. For vision (left\\npanel), the upstream performance gains generally transfer fairly cleanly downstream. For language\\n(right panel), there is more variance in the performance after ﬁnetuning. Nevertheless, the trend\\nclearly favors the upcycled language models.\\nFigure 4 compares sparse upcycling with sparse models trained from scratch. As training from\\nscratch does not reuse the computation cost already sunk into the dense checkpoint, it takes longer,\\non a extra train time basis, to catch up with the upcycled models. The language MoE model trained\\nfrom scratch requires about 120% of the original dense checkpoint’s computation budget to catch up\\nto the upcycled model. The relatively faster quality gains, on a per step basis, of the MoE models\\ntrained from scratch can be attributed to the relatively larger learning rate and that the experts are\\nable to independently develop and diversify from the beginning. Figure 4 suggests that, given a very\\nlarge computation budget (> 100% of the initial dense model’s computation budget), the MoE-from-\\n6\\nPublished as a conference paper at ICLR 2023\\n101\\nExtra Pretraining Time (TPU-core-days)\\n68%\\n69%\\n69%\\n70%\\nC4 Validation Token Accuracy\\nBase\\nDense\\nUpcycling\\nWarm start\\n105\\nExtra Pretraining PetaFLOPs\\n68%\\n69%\\n69%\\n70%\\nC4 Validation Token Accuracy\\nBase\\nDense\\nUpcycling\\nWarm start\\nFigure 5: Pretraining performance achieved by sparse upcycling and dense upcycling from a T5 Base\\ncheckpoint (text task). The x-axes show the extra pretraining time (TPU-core-days) and extra pre-\\ntraining (Peta)FLOPs, with respect to the the original dense checkpoints. Following recommendations\\nin (Rae et al., 2021), we only increase the number of layers when warm starting (“depth tiling”) to\\nroughly match the runtime of the sparse model.\\nscratch model will eventually catch the upcycled model. For such large computation regimes, it\\nmay be preferable to train MoE models from scratch. For constrained or limited compute budgets\\n(< 100% of the initial computation budget), sparse upcycling is a more efﬁcient use of resources.\\nFinally, Figure 5 compares sparse upcycling with warm starting (“dense upcycling”). We warm\\nstart larger models from the dense Base checkpoint by replicating new layers (”depth tiling”) in the\\nsame tiling patterns as in (Rae et al., 2021). The densely upcycled models quickly see gains over the\\noriginal dense checkpoint, but underperform the sparse model. We did not attempt to increase the\\nmodel hidden dimensions (“width tiling”), which (Rae et al., 2021) found to be less effective.\\n4.2.2\\nABLATIONS\\nIn this section we summarize important architecture and training ablations relative to the baseline\\nmodel described in Section 3. Full results are provided in Appendix B. Unless stated otherwise,\\nvision ablations use a B/16 sparse model with 32 experts, C = 1 and 6 MoE layers placed in the\\nlast few blocks of the model. The dense checkpoint was trained for 14 epochs, and we train for an\\nadditional 7 epochs (up to a total of 21 epochs). For our language ablations, our default conﬁguration\\nis unchanged: we use a Base model with 32 experts, C = 2 and 6 MoE layers interspersed throughout\\nthe model. We train for between 0.5 million and 1 million extra steps.\\nAmount of dense pretraining. The upcycling efﬁciency may, in principle, depend on how converged\\nthe initial dense model is. To explore this, in Figure 6, we upcycle a B/16 vision model starting from\\ndifferent dense checkpoints with varying amounts of pretraining. From a given dense checkpoint, we\\ncompare upcycling and dense continuation for 200k steps. Independent of when we start upcycling,\\nthe performance improvement from doing so is fairly consistent.\\nRouter type. While our default upcycling recipe uses Expert Choice routing (in the encoder), the\\nsame recipe can be applied to other routing mechanisms. For vision, Appendix B.1 shows that\\nalthough Top-K routing, with Batch Prioritized Routing (BPR) (Riquelme et al., 2021), matches\\nExpert Choice routing performance on a per step basis, as it is slightly slower, Top-K routing\\nunderperforms Expert Choice routing on a per train time basis. Note both approaches beat dense.\\nExpert capacity factor. The more tokens processed per expert, the greater the amount of compute\\nper input example and (generally) the higher the model quality. Our sparse models smoothly control\\nthis via the expert capacity factor C. Appendix B.2 explores how the performance-speed trade-off\\nvaries as a function of C. Although increasing the expert capacity yields quality gains on a per step\\nbasis, we ﬁnd that C = 2 generally offers the best quality on a compute time basis, for both language\\nand vision models.\\n7\\nPublished as a conference paper at ICLR 2023\\n0\\n200k 400k 600k 800k\\n1M 1.2M\\n# dense pre-training steps\\n0.42\\n0.44\\n0.46\\n0.48\\n0.50\\nJFT Validation Precision at 1\\n0\\n200k 400k 600k 800k\\n1M 1.2M\\n# dense pre-training steps\\n0.67\\n0.68\\n0.69\\n0.70\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\nILSVRC2012 10shot Accuracy\\nDense\\nUpcycling\\nFigure 6: Upcycling performance as a function of the amount of pretraining steps for the original\\ndense checkpoint. The y-axis shows the performance after 200k steps of further training on top of\\nthe original dense checkpoint, for both the dense continuation and upcycled models. The x-axis\\nshows for how long the original dense checkpoint was trained. The gains from upcycling are fairly\\nconsistent independent of the amount of initial pretraining. Note: for this particular ablation, we use\\na capacity factor of C = 1, to ensure that the FLOPS and run times of the dense model and sparsely\\nupcycled model are roughly comparable on a per step basis.\\nNumber of MoE layers. A key decision when upcycling a model is how many sparse layers to add.\\nModel capacity and parameter-count increases with the number of MoE layers, but at the expense of\\nslowing down model run time. Appendix B.4 provides an ablation over the number of MoE layers in\\nan upcycled B/16 vision model. In this case, around 6 MoE layers (out of a total of 12 layers) offers\\nthe best precision quality, although fewer sparse layers lower the compute costs.\\nInitialization of experts. The standard upcycling recipe copies and replicates the dense MLP to each\\nexpert. As the router directs different tokens to each expert, the experts will start to diverge from\\none another, and from their initial MLP weights. Appendix B.5 compares the standard recipe with\\nrandomly initializing the experts (i.e. train them from scratch). For limited computation budgets,\\nrandomly initializing experts underperforms the standard recipe; for larger compute budgets it\\neventually matches the standard upcycling recipe performance.\\nWe also tried copying the original MLP weights and adding independent (Gaussian) noise to each\\nexpert, in an effort to promote more expert diversity. Adding too much noise when copying MLP\\nweights into experts hurts performance, while adding small amounts of noise has little to no effect;\\nsee also Appendix B.9.\\nNumber of experts. Adding more experts increases the number of model parameters and, up to a\\npoint, the quality of the model. Given that the number of tokens each expert processes is inversely\\nproportional to the number of experts (see Section 2.1), adding more experts does not signiﬁcantly\\naffect the model FLOPS nor its running time. However, for a very large number of experts, the\\nupcycled model may experience a larger initial quality drop relative to the baseline dense model.\\nAppendix B.3 explores this trade-off and shows that, at least for Base sized models, more experts\\nusually yield better performance.\\n5\\nRELATED WORK\\nReuse of trained parameters. Prior work has focused on speeding up training through a warm start\\nby reusing parameters of an existing model. Berner et al. (2019) explore ideas of reusing the previous\\nedition of a trained model during a long training process using an under-development environment.\\nGiven a trained model, Net2Net (Chen et al., 2015) propose a function-preserving initialization to\\nwarm start training a deeper or wider model. Recently, Gopher (Rae et al., 2021) also explored warm\\nstarting larger models from smaller models in a large compute training regime and show that the\\nlarger, warm started model can converge to a quality comparable to that of the equivalent model\\ntrained from scratch. In Section 4.2.1, we show that warm starting signiﬁcantly underperforms sparse\\nupcycling. Yang et al. (2021); Lin et al. (2021) show that they can reduce the number of training\\n8\\nPublished as a conference paper at ICLR 2023\\niterations with models that inititally share parameters across layers (Lan et al., 2019; Dehghani et al.,\\n2018) but gradually unshare (or “delink”) the parameters while training.\\nIn an effort to reduce total training cost, several works explore progressively growing models during\\ntraining (Gong et al., 2019; Dong et al., 2020; Li et al., 2020; Shen et al., 2022). The core idea is\\nto decompose the training process into stages, each of which apply growth operators to increase\\nthe model size from the previous stage by copying weights or stacking new layers on top. In some\\ncases, each training stage will only update parameters of the new layers, which saves the cost of a full\\nbackward computation (Yang et al., 2020). Gu et al. (2020) show that compound scaling (scaling\\ndepth, width and input length together) is favorable and propose a strategy with various growing\\noperators on each dimension.\\nSparse upcycling, which we introduce in this paper, follows a similar motivation. However, unlike the\\nabove works, we focus on compute regimes that are a fraction of the original model’s training. We\\nalso present a recipe for growing a trained dense model to a sparse model, instead of a larger dense\\nmodel. This enables us to enjoy the extra capacity due to increased parameters, while maintaining the\\ninference cost due to the sparsity of computation.\\nPruning. Pruning is typically employed as a post-training architecture search to construct smaller\\nand faster models from larger models (LeCun et al., 1989; Gale et al., 2019; Blalock et al., 2020).\\nHowever, “dynamic pruning” (Evci et al., 2020) has also been used during training to ﬁnd sparser\\narchitectures from dense models. Similar to pruning, sparse upcycling also introduces sparsity to a\\ndense model, however, unlike pruning, we grow the existing dense models into a larger sparse model.\\nSparsely-activated Mixture-of-Experts (MoE). In this work, we sparsify existing dense models\\ninto MoE models. MoE models (Shazeer et al., 2017) offer the promise of increasing model scale\\n(parameter count) with sublinear increases in computation cost (FLOPS). Recently, there has been a\\ngrowing number of MoE works achieving state-of-the-art quality and remarkable efﬁciency gains\\non both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021;\\nArtetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022). All of these models are\\nlarge and trained from scratch with randomly initialized weights and ﬁxed architectures.\\nSeveral MoE works have also attempted to improve upon typical training algorithms by adapting\\nor “evolving” the model architecture during training. Nie et al. (2021) progressively sparsify MoE\\nlayers during training by slowly adjusting the gating function from a “dense setup”, where all tokens\\nare routed to all experts, to a fully “sparse setup”, where tokens are only routed to a subset of\\nexperts. Zhang et al. (2022) observed that, for most inputs, only a small fraction of Transformer\\nMLP activations are nonzero. Based on this, they propose sparsiﬁcation procedure that splits the\\nparameters of MLP blocks into multiple experts and add a routing mechanism. Similarly, Zuo et al.\\n(2022) split up the MLPs in a pre-trained dense model into multiple experts to form a sparse model\\nfor ﬁne-tuning. Closely related to our work, Wu et al. (2022) present a novel algorithm to sparsify\\ndense models in the context of ﬁnetuning on detection and segmentation tasks. Similar to Nie et al.\\n(2021), the initial performance drop when training on the original dataset is avoided by applying a\\ndense mixture of experts in the forward pass. However, at our target large scales, simultaneously\\nactivating all experts for each token is not feasible. Finally, Gururangan et al. (2022) adapt sparse,\\ndomain expert language models to new domains by initializing a new domain expert from the most\\nprobable existing expert under the domain posterior distribution.\\n6\\nCONCLUSIONS\\nTraining large neural networks on huge datasets has proven to be a remarkably successful trend in\\ndeep learning research, especially in recent years. It has also proven to be very computationally\\nexpensive. Pretrained models are now widely available, thus making it possible for many practitioners\\nto further ﬁnetune and adapt ﬁxed model architectures on their data of interest. However, signiﬁcant\\nprogress requires providing more ﬂexibility in adapting and improving the model architecture itself.\\nWe proposed a simple recipe to reuse pretrained dense checkpoints to initialize more powerful sparse\\nmodels. Our algorithm leverages the pretrained model compute and weights, and provides a smooth\\ntransition to sparsely activated Mixture-of-Experts models that offer more capacity and ﬂexibility at\\ninference. We presented experimental results both for vision and language models at various scales;\\nthese evidence large performance gains relative to continuing to the dense model. Our ablations\\n9\\nPublished as a conference paper at ICLR 2023\\nhighlight the importance of careful algorithmic choices, and suggest key aspects to consider when\\ntrying to ﬁnd good performance-cost trade-offs for speciﬁc compute budgets.\\nTransfer learning and prompt tuning is becoming increasingly popular, and for good reason. It\\nallows the reuse and tuning of models by a larger body of researchers and practitioners that may only\\nhave access to limited computational and data resources. Accordingly, we believe that techniques\\naimed at growing existing models, compartmentalizing or freezing submodules, replicating and then\\ndecoupling model components, and ﬁnally smoothly resuming training after model surgery, will prove\\nessential for a dynamic ecosystem of models. We summarize such process as upcycling, and offer a\\nﬁrst instance in the context of sparse models. We look forward to new extensions and improvements\\non this simple idea.\\nREFERENCES\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui\\nChen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\\nJeff Wang, Luke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, and Ves Stoyanov. Efﬁcient\\nlarge scale language modeling with mixtures of experts. CoRR, abs/2112.10684, 2021. URL\\nhttps://arxiv.org/abs/2112.10684.\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\\nfor self-supervised learning of speech representations. Advances in Neural Information Processing\\nSystems, 33:12449–12460, 2020.\\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy\\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale\\ndeep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of\\nneural network pruning? Proceedings of machine learning and systems, 2:129–146, 2020.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Sys-\\ntems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.\\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge\\ntransfer. arXiv preprint arXiv:1511.05641, 2015.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\nAidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche,\\nEliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones,\\nElena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack W. Rae,\\nErich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Uniﬁed scaling laws for routed language\\nmodels. CoRR, abs/2202.01169, 2022. URL https://arxiv.org/abs/2202.01169.\\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\\nMostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The efﬁciency\\nmisnomer. arXiv preprint arXiv:2110.12894, 2021.\\n10\\nPublished as a conference paper at ICLR 2023\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\\n//aclanthology.org/N19-1423.\\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo Shang. Towards adaptive residual network training:\\nA neural-ode perspective. In International conference on machine learning, pp. 2616–2626. PMLR,\\n2020.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen,\\nand Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts. CoRR,\\nabs/2112.06905, 2021. URL https://arxiv.org/abs/2112.06905.\\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\\nMaking all tickets winners. In International Conference on Machine Learning, pp. 2943–2952.\\nPMLR, 2020.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity. Journal of Machine Learning Research, 23(120):1–39,\\n2022. URL http://jmlr.org/papers/v23/21-0998.html.\\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\\npreprint arXiv:1902.09574, 2019.\\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient training of\\nbert by progressively stacking. In International conference on machine learning, pp. 2337–2346.\\nPMLR, 2019.\\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer\\ngrowth for progressive bert training. arXiv preprint arXiv:2010.12562, 2020.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. DEMix\\nlayers: Disentangling domains for modular language modeling. In Proceedings of the 2022\\nConference of the North American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, pp. 5557–5576, Seattle, United States, 2022. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.407. URL https:\\n//aclanthology.org/2022.naacl-main.407.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´\\nar, and Ross Girshick. Masked\\nautoencoders are scalable vision learners.\\nIn Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pp. 16000–16009, 2022.\\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇ\\nZ´\\nıdek, Anna Potapenko, et al. Highly accurate\\nprotein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-\\ntional neural networks. Communications of the ACM, 60(6):84–90, 2017.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\\narXiv:1909.11942, 2019.\\n11\\nPublished as a conference paper at ICLR 2023\\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\\nprocessing systems, 2, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. In 9th International Conference on Learning Representations,\\nICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/\\nforum?id=qrwe7XHTmYb.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers:\\nSimplifying training of large, sparse models. In Marina Meila and Tong Zhang (eds.), Proceedings\\nof the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\\nEvent, volume 139 of Proceedings of Machine Learning Research, pp. 6265–6274. PMLR, 2021.\\nURL http://proceedings.mlr.press/v139/lewis21a.html.\\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, and Jingbo Zhu.\\nShallow-to-deep training for neural machine translation. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP), 2020.\\nJunyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong\\nLi, Wei Lin, et al. M6-10t: A sharing-delinking paradigm for efﬁcient multi-trillion parameter\\npretraining. arXiv preprint arXiv:2110.03888, 2021.\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multi-\\nmodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint\\narXiv:2206.02770, 2022.\\nSharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena, Karishma\\nMalkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus,\\nAdam Roberts, and Colin Raffel. Do transformer modiﬁcations transfer across implementations and\\napplications? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\\nProcessing, pp. 5758–5773. Association for Computational Linguistics, November 2021. doi:\\n10.18653/v1/2021.emnlp-main.465. URL https://aclanthology.org/2021.emnlp-main.\\n465.\\nXiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang,\\nZhi Yang, and Bin Cui. Evomoe: An evolutional mixture-of-experts training framework via\\ndense-to-sparse gate. arXiv preprint arXiv:2203.02155, abs/2112.14397, 2021. URL https:\\n//arxiv.org/abs/2112.14397.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International Conference on Machine Learning, pp.\\n8748–8763. PMLR, 2021.\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. Journal of Machine Learning Research, 21:1–67, 2020.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´\\ne\\nSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\nAdvances in Neural Information Processing Systems, 34, 2021.\\n12\\nPublished as a conference paper at ICLR 2023\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models\\nand data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large\\nsparse models. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\\nJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual\\nConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual, pp. 17555–17566, 2021. URL https://proceedings.neurips.cc/paper/2021/\\nhash/92bf5e6240737e0326ea59846a83e076-Abstract.html.\\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\\ngo, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\\nIn ICML 2018, 2018.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In\\n5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-\\n26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.\\nnet/forum?id=B1ckMDqlg.\\nSheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training\\nfor transformer language models. arXiv preprint arXiv:2203.06211, 2022.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\\neffectiveness of data in deep learning era. In Proceedings of the IEEE International Conference on\\nComputer Vision (ICCV), Oct 2017.\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´\\ne\\nJ´\\negou. Training data-efﬁcient image transformers & distillation through attention. In International\\nConference on Machine Learning, pp. 10347–10357. PMLR, 2021.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems. Advances in neural information processing systems, 32, 2019.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\\narXiv:2109.01652, 2021.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, R´\\nemi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\\nDrame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language\\nprocessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\\ncessing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational\\nLinguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\\nLemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu Yuan. Residual\\nmixture of experts. arXiv preprint arXiv:2204.09636, 2022.\\nCheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively\\nstacking 2.0: A multi-stage layerwise training method for bert training speedup. arXiv preprint\\narXiv:2011.13635, 2020.\\nShuo Yang, Le Hou, Xiaodan Song, Qiang Liu, and Denny Zhou. Speeding up deep model training\\nby sharing weights and then unsharing. arXiv preprint arXiv:2110.03848, 2021.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\\nCoca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917,\\n2022.\\n13\\nPublished as a conference paper at ICLR 2023\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\n12104–12113, 2022.\\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moeﬁcation:\\nTransformer feed-forward layers are mixtures of experts. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pp. 877–890, 2022.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Y. Zhao, Andrew M. Dai,\\nZhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. CoRR,\\nabs/2202.09368, 2022. URL https://arxiv.org/abs/2202.09368.\\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and\\nWilliam Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022.\\nSimiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. Moebert: from\\nbert to mixture-of-experts via importance-guided adaptation. arXiv preprint arXiv:2204.07675,\\n2022.\\n14\\nPublished as a conference paper at ICLR 2023\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nTraining Steps\\n1e6\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nJFT-300M Precision@1 (%)\\nDense\\nUpcycling (C=1)\\nUpcycling (C=2)\\nUpcycling Starting Point\\nFigure 7: Effect of upcycling a VIT-B/16 model after 14 epochs of dense training. We show a number\\nof cooldowns (decreasing learning rate to zero) for each model, in case that is the maximum training\\nbudget available. The difference in slope for the dense and upcycling training curves is signiﬁcant.\\nA\\nTRAINING AND EVALUATION DETAILS\\nIn this section, we describe the precise setup for our language and vision experiments. Figure 7\\nillustrates the type of combined training curves we obtained before and after upcycling.\\nA.1\\nUPSTREAM TRAINING\\nA.1.1\\nLANGUAGE\\nWe ﬁrst pretrain a dense Base model from scratch for 1 million steps with a batch size of 512\\nsequences. We use the Adafactor optimizer with an inverse square root decay and a peak learning\\nrate of 0.01. This results in plateauing performance for the dense Base model. Upcycled models are\\nthen initialized from the 1 million step dense checkpoint, and compared, on a compute time basis,\\nwith further training of the dense model (“dense continuation”); see Figure 2 in the main text. To\\nhighlight the versatility of our upcycling algorithm, for Large and XL models, we instead begin all\\nexperiments from ofﬁcial T5 1.1 checkpoints (Narang et al., 2021).4\\nWe use the same hyperparameters for the upcycled model as for the corresponding dense model\\nthat we initialized from, continuing the inverse square root learning rate schedule where the dense\\ncheckpoint left off. For all sizes, every other layer was upcycled, using 32 experts, starting with\\nthe second layer. Similar to dense pretraining, we do not include any dropout (or expert dropout;\\nsee Section A.2.1 below). Router parameters are initialized randomly with a zero-mean normal\\ndistribution with standard deviation 0.02. We use a maximum routing group size of 4096 tokens. For\\nTop-2 routing (in the decoder) we include an auxiliary MoE loss, with scaling factor 0.01, to ensure\\ntokens are distributed more evenly across all experts in the decoder (Shazeer et al., 2017; Fedus et al.,\\n2022).\\nUpcycling was performed on TPU v4 accelerators using 64 chips for Base and Large and 256 chips\\nfor XL. All sizes used expert partitioning, but only XL used model partitioning, with 4 partitions.\\n4For experiments starting from the ofﬁcial checkpoints we match the ofﬁcial, larger batch size (2048), to\\nensure no discontinuities in continuing the dense baseline pretraining.\\n15\\nPublished as a conference paper at ICLR 2023\\nA.1.2\\nVISION\\nDense ViT models are pretrained on JFT300M Sun et al. (2017). We train with Adafactor (Shazeer &\\nStern, 2018), and decoupled weight decay (magnitude 3 on head and 0.03 on body) following Zhai\\net al. (2022). We use a batch size of 4096. The learning rate schedule consists of a learning warmup\\nof 10 000 steps, followed by reverse square root decay with timescale 100 000 steps and ending with\\na linear cooldown to 0 over 50 000 steps. We use a ﬁxed peak learning rate of 4 · 10−4. 5 Models\\nemploying a patch size of 32 (i.e. B/32, L/32) were trained for a total of 14 epochs, while those\\nemploying a patch size of 16 (i.e. B/16, L/16) were trained for 28 epochs. By default, we begin\\nupcycling half-way through the total number of epochs in each case. This roughly corresponds to\\nthe number of epochs used to train the corresponding variant in ViT (Dosovitskiy et al., 2021). For\\nB/16, for example, we assume that a dense checkpoint trained for 14 epochs is given, and we either\\ncontinue training or apply upcycling for another 14 epochs.\\nA.2\\nMODEL TRANSFER\\nA.2.1\\nLANGUAGE\\nFor ﬁnetuning on SuperGLUE, we generally adopt the conventional setup (Raffel et al., 2020; Narang\\net al., 2021) where we ﬁnetune on all SuperGLUE tasks, in a proportional mix, for 200K steps with a\\nbatch size of 128. Each example has input length 512 and target decoding length of 62. In our ﬁgures,\\nwe report the average SuperGLUE accuracy score across 3 runs for each data point.\\nFor ﬁnetuning Dense models on SuperGLUE, we use Adafactor with the default, constant learning\\nrate of 10−3 and a dropout rate of 0.1 (Raffel et al., 2020; Narang et al., 2021). For ﬁnetuning\\nupcycled models, because there are many more parameters, it can be helpful to increase the dropout\\nrate for the experts (Fedus et al., 2022), while using the default dropout rate of 0.1 for all “dense”\\nparameters. For upcycled Base models, we obtained the strongest results for a constant learning rate\\nof 10−4 with an expert dropout rate of 0.1. For upcycled Large models, we found slightly stronger\\nresults with a learning rate of 10−3 and an expert dropout rate of 0.3. Decreasing (or increasing) the\\nlearning rate was not helpful for the Dense Base or Large models.\\nA.2.2\\nVISION\\nFew-shot linear evaluation. The fewshot evaluation poses classiﬁcation as a linear regression task,\\nwhere inputs are frozen representations computed by a pretrained model, and outputs are one-hot\\nvectors representing the ground truth (Dosovitskiy et al., 2021). There are two key changes compared\\nto prior works which used this method (Dosovitskiy et al., 2021; Riquelme et al., 2021):\\n• Multiple seeds. The evaluation involves randomly selecting N examples per class. To reduce\\ndependency on that choice, we run 5 random seeds, and report the average test accuracy\\nacross them.\\n• Fixed L2 regularization. Prior works considered a range of L2 regularizations. The optimal\\nvalue was picked based on average test accuracy across all datasets considered for few-shot\\nevaluation. We ﬁx the L2 regularisation at 1024.\\nFull ﬁnetuning. We ﬁnetune models on ImageNet2012 using SGD and a batch size of 512. We use a\\ncosine decay learning rate schedule with a linear warmup. We sweep over two training schedules: (i)\\n5k steps, with a warmup of 200 steps, and (ii) 10k steps, with a warmup of 400 steps. Alongside this\\nwe sweep over learning rates [0.1, 0.03, 0.01, 0.003, 0.001, 0.0003]. For each pretrained model, there\\nare therefore 12 ﬁnetuning sweeps; we select based on optimal validation accuracy, and report the\\ncorresponding test accuracy.\\nA.3\\nMODEL PARAMETERS\\nTable 1 gives the number of parameters for models used in the main text.\\n5Note that this is slightly different to ViT (Dosovitskiy et al., 2021), which changing the learning rate slightly\\nbased on the model variant.\\n16\\nPublished as a conference paper at ICLR 2023\\nTable 1: Model sizes. The number of parameters for sparsely upcycled and MoE-from-scratch models\\nare the same (both are of type “Sparse”). The number of parameters is also unchanged between\\ndifferent routing mechanisms.\\nModality\\nModel\\nType\\nFraction of MoE Layers\\n# Experts\\n# Parameters\\nVision\\nB/32\\nDense\\n–\\n–\\n101M\\nVision\\nB/16\\nDense\\n–\\n–\\n100M\\nVision\\nL/32\\nDense\\n–\\n–\\n324M\\nVision\\nL/16\\nDense\\n–\\n–\\n322M\\nVision\\nB/32\\nSparse\\n6 / 12\\n32\\n980M\\nVision\\nB/16\\nSparse\\n6 / 12\\n32\\n978M\\nVision\\nL/32\\nSparse\\n12 / 24\\n32\\n3.44B\\nVision\\nL/16\\nSparse\\n12 / 24\\n32\\n3.44B\\nLanguage\\nBase\\nDense\\n–\\n–\\n248M\\nLanguage\\nLarge\\nDense\\n–\\n–\\n783M\\nLanguage\\nXL\\nDense\\n–\\n–\\n2.85B\\nLanguage\\nBase\\nSparse\\n6 / 12\\n32\\n2.00B\\nLanguage\\nLarge\\nSparse\\n12 / 24\\n32\\n7.22B\\nLanguage\\nXL\\nSparse\\n12 / 24\\n32\\n26.26B\\nA.4\\nPARALLELIZATION STRATEGIES\\nSparsely activated Mixture-of-Experts (MoE) models combine three types of parallelization strategies\\nto train large models across multiple accelerator chips: data, model and expert parallelism. We use\\ndata parallelism to shard the training batch across devices. We use expert parallelism to partition\\nexperts across devices; for example, placing experts 1 and 2 on device 1, experts 3 and 4 on device 2,\\nand so on. Model parallelism is a third axis along which model weights (matrices) can be sharded\\nacross devices; for example, expert 1 is split across devices 1 and 2, expert 2 is split across devices 3\\nand 4, and so on. Model parallelism is beneﬁcial for scaling to larger model sizes. See also (Fedus\\net al., 2022) for a more detailed discussion of these three parallelization strategies.\\nB\\nABLATIONS AND ADDITIONAL EXPERIMENTS\\nIn this section, we present results for a number of model ablations that try to identify good choices\\nfor the main upcycling algorithm decisions. As mentioned in the main text, unless stated otherwise,\\nvision ablations use a B/16 sparse model with 32 experts, C = 1 and 6 MoE layers placed in the\\nlast few block of the model. The dense checkpoint was trained for 14 epochs, and we train for an\\nadditional 7 epochs (up to a total of 21 epochs). Note that, for C = 1, comparing performance on a\\nper step basis is a reasonably close approximation of a comparison on a per train time basis.\\nFor our language ablations, our default conﬁguration is unchanged: we use a Base model with 32\\nexperts, C = 2 and 6 MoE layers interspersed throughout the model. We train for between 0.5\\nmillion and 1 million extra steps.\\nB.1\\nROUTER TYPE\\nWhile our default upcycling recipe uses Expert Choice routing Zhou et al. (2022) (in the encoder),\\nthe same recipe can be applied to other routing mechanisms. Here, we compare with Top-K rout-\\ning Shazeer et al. (2017), which is a very popular alternative. Table 2 shows that, for vision, sparse\\nupcycling with Top-K routing works comparably well to Expert Choice, on a per step basis, provided\\nwe also use Batch Priority Routing (BPR) (Riquelme et al., 2021). BPR sorts tokens according to a\\nmodel conﬁdence proxy so that –when experts are full– high conﬁdence tokens are given priority. We\\nsuspect this may be helpful right at the beginning, when applying the upcycling, to avoid discarding\\nimportant tokens. Expert Choice avoids this problem by design, as experts are always balanced and\\nselect the most ‘relevant’ tokens.\\n17\\nPublished as a conference paper at ICLR 2023\\nTable 2: Sparse Upcycling on L/32 vision models with Expert Choice and Top-K routing (also known\\nas Top-K). K refers to the number of selected experts per token, while C refers to the capacity factor.\\nNotice that with Expert Choice routing, each token chooses C experts on average. The initial dense\\ncheckpoint was trained for 7 epochs. Note that these comparison are on a per-step basis, and that\\nExpert Choice upcycled models are actually slightly faster than Top-K models; see Figure 8.\\nModel\\nCapacity\\nFrom\\nExtra Epochs\\nVal Prec@1\\nImageNet 10shot\\nDense\\n–\\nDense\\n7\\n49.60\\n73.59\\nExpert Choice\\nC = 1\\nDense\\n7\\n51.91\\n74.04\\nTop-K\\nK = 1\\nDense\\n7\\n51.51\\n74.40\\nExpert Choice\\nC = 2\\nDense\\n7\\n52.80\\n74.83\\nTop-K\\nK = 2\\nDense\\n7\\n52.88\\n74.91\\nExpert Choice\\nC = 1\\nScratch\\n7\\n50.42\\n72.95\\nExpert Choice\\nC = 2\\nScratch\\n7\\n51.28\\n74.01\\nExpert Choice\\nC = 1\\nScratch\\n14\\n54.84\\n75.02\\nExpert Choice\\nC = 2\\nScratch\\n14\\n55.46\\n75.75\\n101\\nExtra Pretraining Time (TPU-core-days)\\n68%\\n68%\\n69%\\n70%\\n70%\\nC4 Validation Token Accuracy\\nSwitch\\nTokens Choice (with BPR)\\nExperts Choice\\nFigure 8: Comparison of Expert Choice, Top-2 and Switch (Top-1) routing mechanisms for a Base\\nupcycled language model.\\nFor language, similar ablations (Figure 8) shows that Expert Choice routing outperforms both Top-2\\nrouting (with BPR) and switch (Top-1) routing, on a train time basis.\\nB.2\\nROUTER CAPACITY FACTOR\\nSparsifying the dense model increases the model capacity (number of parameters). However, if the\\ncapacity factor C = 1, then the FLOPS is very similar to the original, dense model (modulo the small\\nrouting costs). We can increase the per-token compute by increasing C. Figure 9 investigates this,\\nand shows our results for vision (left and center panels) and language (right panel).\\nFor vision, we see that extreme values (C = 1 and C = 5) underperform intermediate values ( C = 2\\nand C = 3) that offer better trade-offs. For language, the trend is even stronger: A capacity factor of\\nC = 2 stands out as the best option on a per compute basis.\\nB.3\\nNUMBER OF EXPERTS\\nAdding more experts increases the number of model parameters and, up to a point, the quality of the\\nmodel. Given that the number of tokens each expert processes is inversely proportional to the number\\nof experts (see Section 2.1), adding more experts usually only leads to very modest computational\\n(and wall time) overheads. However, for a very large number of experts, the upcycled model may\\nexperience a larger initial quality drop relative to the baseline dense model.\\n18\\nPublished as a conference paper at ICLR 2023\\n102\\nExtra Pretraining Time (TPU-core-days)\\n43%\\n46%\\n49%\\n52%\\n54%\\nJFT Validation Precision at 1\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n102\\nExtra Pretraining Time (TPU-core-days)\\n71%\\n73%\\n74%\\n76%\\n77%\\nILSVRC2012 10shot Accuracy\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n101\\nExtra Pretraining Time (TPU-core-days)\\n69%\\n69%\\n70%\\nC4 Validation Token Accuracy\\n1.25\\n2.0\\n4.0\\n6.0\\nFigure 9: Pretraining performance achieved by upcycling using different capacity factors, for a\\nB/16 (left and center panels, vision tasks) and a Base T5 model (right plot, text task). The x-axis\\nshows the extra pretraining time (TPU-core-days), with respect to the total time needed to train the\\noriginal dense checkpoint. Although using a bigger capacity factors can result in an absolute better\\nperformance when runtime is disregarded (e.g. see the vision results), for a given ﬁxed compute\\nbudget, it is usually better to use a capacity factor of around 2.0.\\n102\\nExtra Pretraining Time (TPU-core-days)\\n42%\\n45%\\n48%\\n51%\\n54%\\nJFT Validation Precision at 1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n102\\nExtra Pretraining Time (TPU-core-days)\\n71%\\n72%\\n73%\\n75%\\n76%\\nILSVRC2012 10shot Accuracy\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n101\\n102\\nExtra Pretraining Time (TPU-core-days)\\n41%\\n44%\\n47%\\n50%\\n53%\\nJFT Validation Precision at 1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n101\\n102\\nExtra Pretraining Time (TPU-core-days)\\n68%\\n70%\\n72%\\n74%\\n76%\\nILSVRC2012 10shot Accuracy\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\nFigure 10: Pretraining performance achieved by the upcycling method on the vision tasks, using\\ndifferent number of experts per MoE layer (two left plots, with a total number of 6 MoE layers), and a\\ndifferent number of MoE layers (two right plots, with a total number of 32 experts; all MoE layers are\\nplaced at the top Transformer blocks). The x-axis shows the extra pretraining time (TPU-core-days),\\nwith respect to the total time needed to train the original dense checkpoint.\\nFigure 10 (two left panels) shows the results of a vision experiment with 6 MoE layers with a number\\nof experts ranging from 2 to 128. For a ﬁxed amount of compute (value in the x-axis), we see that\\nmore experts is generally better for this B/16 model. Figure 11 shows the ﬁnal metric values both for\\nupstream (JFT precision at 1) and downstream (ImageNet 10-shot) with respect to the number of\\nexperts. We see steady improvements upstream, and –at some point– diminishing returns downstream.\\nB.4\\nNUMBER OF MOE LAYERS\\nAnother key decision is how many layers to sparsify. More layers leads to higher model capacity,\\nwhile –especially for higher C– it introduces signiﬁcant extra wall time overhead. We ablate this for\\nvision models, as shown in Figure 10 (two right panels). For a B/16 model with 12 blocks, we train\\nupcycled versions with an increasing number of MoE layers; MoE layers are consecutive and start\\nfrom the last layer. For example, the model labeled as ‘5’ corresponds to a model where the last 5\\nMLP layers are sparsiﬁed, and so on. Thus, model ‘1’ only has one MoE layer (the last one) and it is\\nthe computationally cheapest in terms of wall time. We do not include a model where all layers are\\nsparsiﬁed (would correspond to ‘12’) as we found that sparsifying the very ﬁrst block tends to be\\nproblematic.\\nWe see in Figure 10 (two right panels) that more MoE layers is not always better even on a per step\\nbasis; see Figure 12 for both upstream and downstream metrics. Looking at a ﬁxed value of the\\nx-axis in Figure 10 (right panels), we conclude that something between Last-5 and Last-6 (40-50%\\nof layers sparsiﬁed) offers the most attractive trade-off in this case.\\n19\\nPublished as a conference paper at ICLR 2023\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\nNumber of Experts\\n50\\n51\\n52\\n53\\n54\\nJFT-300M Precision@1 (%)\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\nNumber of Experts\\n74.0\\n74.5\\n75.0\\n75.5\\n76.0\\nImageNet 10-shot Acc (%)\\nUpcycling\\nOriginal dense model\\nFigure 11: Final upstream and downstream performance for upcycled B/16 vision models with\\ndifferent number of experts per MoE layer. The number of MoE layers is ﬁxed at 6. The upcycled\\nmodel is trained for an additional 7 epochs (from 14 to 21) relative to the original dense model. The\\ndashed horizontal lines show the performance of the dense model when trained for an additional 7\\nepochs.\\n11 10\\n9\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nMoEs placed in Last-N layers\\n50.0\\n50.5\\n51.0\\n51.5\\n52.0\\n52.5\\nJFT-300M Precision@1 (%)\\n11 10\\n9\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\nMoEs placed in Last-N layers\\n74.0\\n74.5\\n75.0\\n75.5\\nImageNet 10-shot Acc (%)\\nDense Model\\nFigure 12: Performance as a function of the number of MoE layers for upcycled B/16 models (C = 1)\\ntrained for 7 additional epochs on JFT, starting from a dense checkpoint originally trained for 14\\nepochs. MoE layers are consecutively placed starting from the last block. We train models ranging\\nfrom 1 MoE layer (Last-1) to 11 MoE layers (Last-11) – i.e. all but the very ﬁrst. The dashed\\nhorizontal lines show the performance of the dense model when trained for an additional 7 epochs.\\nB.5\\nEXPERT INITIALIZATION\\nThe standard upcycling recipe copies and replicates the dense MLP to each expert. As the router\\ndirects different tokens to each expert, the experts will start to diverge from one another, and their\\ninitial MLP weights. Figure 13 explores whether loading the MLPs is indeed a good idea, or whether\\nthe model would be better off learning the experts from scratch (random initialization). We train for 7\\nextra epochs (dense was trained for 14 epochs, and we keep training up to a total of 21). Note that the\\ncomputational cost of both approaches is identical.\\nIt takes a long time for the model with randomly initialized experts to recover and catch up with the\\nalgorithm that upcycles the expert weights from the dense MLP layers, regardless of the number of\\nexperts. We also tried an intermediate approach (not shown), where we only upcycle a subset of\\nexperts and initialize the rest of scratch, but that also underperformed upcycling all of the experts.\\n20\\nPublished as a conference paper at ICLR 2023\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nJFT-300M Precision@1 (%)\\n8 experts\\n32 experts\\n128 experts\\nLoad Experts = False\\nLoad Experts = True\\n0\\n100k 200k 300k 400k 500k 600k\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Acc (%)\\n0\\n100k 200k 300k 400k 500k 600k\\nExtra Training Steps\\n0\\n100k 200k 300k 400k 500k 600k\\nLoad Experts = False\\nLoad Experts = True\\nFigure 13: Performance comparison between upcycling experts (“Load Experts = True”) and randomly\\ninitializing the experts (“Load Experts = False”). We include upstream (top row) and downstream\\n(bottom row) performance metrics, and also ablate the number of experts per MoE layer (over the\\ncolumns).\\nB.6\\nRESUMING THE OPTIMIZER STATE\\nWhen upcycling a model, we can resume the optimizer state from the original dense checkpoint\\ntogether with the model parameters. Figure 14 shows that reusing the optimizer state gives a\\nperformance boost for vision models, independent of the number of experts.6 We did not, however,\\nsee any improvement from reusing the dense model optimizer state in our language experiments, so\\nwe only reuse the optimizer state for vision models.\\nB.7\\nCOMBINE WEIGHT NORMALIZATION AFTER ROUTING\\nA simple trick that we found useful for the upcycling of vision models was to normalize the combine\\nweights after routing. The weights of each token are normalized so that the sum is 1. This follows\\nthat intuition that each token was previously only processed by a single “expert” MLP in the dense\\nmodel. In the event that a token is not routed at all, the combine weights remain 0.\\nWe illustrate this normalization trick with two simple examples.\\nSeveral experts selected. Suppose a token x is selected by three different experts e1, e2 and e3 with\\nrouting weights w1 = 0.3, w2 = 0.2, and w3 = 0.1 respectively (adding up to 0.6).\\nThe normalized weights are:\\n¯\\nw1 = 0.3\\n0.6 = 0.5,\\n¯\\nw2 = 0.2\\n0.6 = 0.3333...\\n¯\\nw3 = 0.1\\n0.6 = 0.1666...\\nThe ﬁnal output x′ is:\\nx′ = ¯\\nw1 · e1(x) + ¯\\nw2 · e2(x) + ¯\\nw3 · e3(x).\\n6For some parameter, such as the router weights, we do not have any original optimizer state that we can\\nreuse.\\n21\\nPublished as a conference paper at ICLR 2023\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nJFT-300M Precision@1 (%)\\n8 experts\\n32 experts\\n128 experts\\nLoad Optimizer = False\\nLoad Optimizer = True\\n0\\n100k 200k 300k 400k 500k 600k\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\nImageNet 10-shot Acc (%)\\n0\\n100k 200k 300k 400k 500k 600k\\nExtra Training Steps\\n0\\n100k 200k 300k 400k 500k 600k\\nLoad Optimizer = False\\nLoad Optimizer = True\\nFigure 14: Performance comparison between reusing (“Load Optimizer = True”) and not reusing\\n(“Load Optimizer = False”) the optimizer state. We include upstream (top row) and downstream\\n(bottom row) performance metrics, and also ablate the number of experts per MoE layer (over the\\ncolumns).\\nTable 3: Training from scratch on V-MoE-B/32 vision models with Expert Choice routing.\\nComparison with and without weight renormalization after routing.\\nCapacity\\nRenormalization\\nVal Prec@1\\nImageNet 10shot\\nC = 1\\nNo\\n48.71\\n69.68\\nC = 1\\nYes\\n48.23\\n70.19\\nC = 2\\nNo\\n50.02\\n71.26\\nC = 2\\nYes\\n49.75\\n71.55\\nOnly one expert selected. In this case, regardless of the selected weight w1, the output routing\\nweight will be ¯\\nw1 = 1.0 after normalizing it:\\nx′ = ¯\\nw1 · e1(x) = 1.0 · e1(x).\\nWhile this approach can be in principle a bit problematic (those tokens only selected by one ex-\\npert have vanishing routing gradients), Table 3 shows that, even if we are training vision models\\nfrom scratch, applying weight normalization does not hurt performance (while it indeed helps for\\nupcycling).\\nHowever, router weight normalization was not helpful for language models. Upstream accuracy after\\n1M steps was comparable: 70.8% (no normalization) vs 70.7% (normalization), but downstream\\naverage scores on SuperGLUE lagged: 79.3% (no normalization) vs 78.8% (normalization). A similar\\nquality degradation were observed in MoE language models trained from scratch. One hypothesis\\nfor this different behavior is that the vision models use Expert Choice routing everywhere, but the\\nlanguage models use Expert Choice in the encoder and Top-K routing in the decoder.\\n22\\nPublished as a conference paper at ICLR 2023\\n0\\n1\\n2\\n3\\n4\\n5\\nCapacity ratio C\\n0.40\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\nILSVRC2012 10shot Accuracy\\nB/16 models\\nExpChoose (Norm)\\nExpChoose\\nTop-K=1\\nTop-K=2\\nDense\\n0\\n1\\n2\\n3\\n4\\n5\\nCapacity ratio C\\n0.70\\n0.72\\n0.74\\n0.76\\nILSVRC2012 10shot Accuracy\\nL/16 models\\nFigure 15: The effect of capacity size ratio on the initial performance of B/16 (left) and L/16\\n(right) models after upcycling (i.e. at the very ﬁrst new step); when routing weights are normalized\\n(Section B.7), and capacity is large, the upcycled model retains the dense model’s function.\\nB.8\\nCLOSER LOOK AT DESIGN CHOICES IMMEDIATELY FOLLOWING UPCYCLING\\nOnce a model is upcycled, there is a drop in performance due to the sudden deviation from the dense\\nmodel’s learned function, even with MoE experts reusing the dense MLP weights. Routing design\\ndecisions can make a signiﬁcant difference in ameliorating this drop.\\nSection B.2 ablates the long-term upcycled model performance as a function of the capacity factor\\nC. Figure 15 shows the immediate effect of modifying C on the upcycled model. Increasing C\\nreduces the likelihood that tokens are dropped; when routing weights are normalized to sum to 1 (see\\nSection B.7), the upcycled model is exactly equivalent to the dense model for those tokens selected by\\nat least one expert. Note the set of tokens not selected by any expert decreases in size as we increase\\nC.\\nNote that although different routing mechanisms signiﬁcantly impact the starting point for upcycling,\\nthe subsequent training can smooth over many differences. For example, at the start Top-K routing is\\nclearly worse than Expert Choice with weight normalization, but Table 2 shows that a model upcycled\\nwith Top-K routing eventually catches up.\\nFigure 16 shows the effect of the group size parameter. Routing, and in particular the top-k operations,\\nare performed in groups. Using smaller group sizes will speed up routing, but will lead to higher\\nvariance in expert assignment across groups. For smaller groups, we may expect more tokens to be\\ndropped (not selected by any expert).\\nHow many MoE layers we upcycle, and where we place those layers, also affects the initial per-\\nformance drop. Figure 17 shows the effect of this in the initial drop for Expert Choice routing,\\nusing normalized combined weights (Section B.7) Upcycling the bottom layers causes a larger initial\\nperformance drop. Upcycling the last layers consecutive layers or interleaving them –as in every\\nother layer– yields the smallest initial performance drop.\\nFinally, we analyze how the number of experts per MoE layer affects the initial upcycling. Figure 18\\nsuggests that routing to more experts leads to a heavier drop. Figure 12 shows that upcycled models\\ncan recover from this eventually though, and sometimes even achieve higher performance.\\nB.9\\nTHINGS THAT DID NOT WORK\\nWe list several unsuccessful attempts, beyond the preceding ablations, to improve the performance of\\nthe upcycling. Adding (truncated) Gaussian noise to router weights, expert weights or both, in an\\neffort to diversify the router or expert weights, did not help. Modiﬁying the learning rate of experts,\\nrouters or both, to account for the fact that the addition of sparsity may demand different learning\\nrate from the rest of the model, generaly hurt performance; increasing the learning rates sometimes\\nrendered the models more unstable. We attempted to add a ﬁxed temperature term to the softmax of\\n23\\nPublished as a conference paper at ICLR 2023\\n1000\\n2000\\n3000\\nGroup size\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\nILSVRC2012 10shot Accuracy\\nB/16 models\\nExpChoose (Norm)\\nExpChoose\\nTop-K=1\\nTop-K=2\\nDense\\n1000\\n2000\\n3000\\nGroup size\\n0.70\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\n0.76\\n0.77\\nILSVRC2012 10shot Accuracy\\nL/16 models\\nFigure 16: Increasing group size does not signiﬁcantly affect performance using Expert Choice\\nrouting, but improves initial performance for Top-K routing.\\n2\\n4\\n6\\n8\\n10\\n12\\nNumber of MoE layers\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\nILSVRC2012 10shot Accuracy\\nB/16 models\\nLast\\nFirst\\nInterleaved\\nDense\\n5\\n10\\n15\\n20\\n25\\nNumber of MoE layers\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nILSVRC2012 10shot Accuracy\\nL/16 models\\nFigure 17: Effect of position and number of MoE layers on the initial performance after upcycling\\n(i.e. at the very ﬁrst new step). Note that L/16 models have more MLP layers..\\n2\\n4\\n8\\n16\\n32\\n64\\nNumber of experts\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\nILSVRC2012 10shot Accuracy\\nB/16 models\\nExpChoose (Norm)\\nExpChoose\\nTop-K=1\\nTop-K=2\\nDense\\n2\\n4\\n8\\n16\\n32\\n64\\nNumber of experts\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\n0.76\\n0.77\\nILSVRC2012 10shot Accuracy\\nL/16 models\\nFigure 18: Effect of the number of experts per MoE layer on the initial drop of ImageNet 10-shot\\nperformance (i.e. at step 1), for B/16 and L/16 upcycled models with 6 MoE layers and C = 1. The\\ninitial performance is lower when using more experts.\\n24\\nPublished as a conference paper at ICLR 2023\\nrouter to encourage more random or less random routing. We also varied the initialization scheme for\\nthe router, included the router z-loss (Zoph et al., 2022). Unfortunately, none of these attempts led to\\nany signiﬁcant performance improvement.\\n25\\nPublished as a conference paper at ICLR 2023\\nC\\nSELECTED RESULTS\\nTable 4: Selection of results on vision tasks for different methods and architecture variants. We report the JFT-300M Validation Precision at 1 (%), the ILSVRC2012\\n10shot Accuracy (%), the ILSVRC2012 Accuracy after Finetuning (%), and the extra TPUv3-core-days and ExaFLOPs used, both in absolute and relative (%) terms\\nwith respect to the corresponding dense checkpoint (when these are 0, they correspond to the initial dense checkpoints).\\nMethod\\nVariant\\nJFT-300M\\nILSVRC2012\\n10shot\\nILSVRC2012\\nFinetune\\nExtra\\nTPUv3-days\\nRelative Extra\\nTPUv3-days\\nExtra\\nExaFLOPs\\nRelative Extra\\nExaFLOPs\\nDense\\nB/32\\n37.78\\n61.92\\n78.91\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nL/32\\n44.69\\n70.97\\n83.08\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nB/16\\n46.67\\n72.19\\n83.77\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nL/16\\n53.54\\n78.63\\n86.50\\n0.00\\n0.00\\n0.00\\n0.00\\nMoE\\nB/32\\n20.24\\n39.82\\n66.95\\n1.91\\n14.60\\n4.02\\n14.64\\nUpcycling\\nB/32\\n38.73\\n63.01\\n79.58\\n2.53\\n19.31\\n5.32\\n19.36\\nMoE\\nL/32\\n23.76\\n44.63\\n69.11\\n5.70\\n14.11\\n13.79\\n14.36\\nUpcycling\\nL/32\\n45.33\\n71.28\\n83.35\\n5.78\\n14.31\\n13.15\\n13.69\\nMoE\\nB/32\\n38.77\\n63.24\\n78.80\\n9.57\\n73.01\\n20.12\\n73.19\\nUpcycling\\nB/16\\n47.81\\n73.19\\n84.13\\n12.36\\n13.54\\n28.31\\n12.68\\nUpcycling\\nB/32\\n46.34\\n69.98\\n82.18\\n21.60\\n164.88\\n45.43\\n165.30\\nUpcycling\\nL/16\\n53.89\\n78.78\\n86.56\\n33.99\\n10.87\\n81.36\\n10.40\\nUpcycling\\nL/32\\n51.19\\n74.43\\n84.66\\n49.95\\n123.58\\n113.63\\n118.29\\nDense\\nB/16\\n48.12\\n73.28\\n84.23\\n53.19\\n58.26\\n130.04\\n58.26\\nMoE\\nB/16\\n50.24\\n74.96\\n84.57\\n73.58\\n80.59\\n168.49\\n75.49\\nUpcycling\\nB/16\\n52.59\\n76.48\\n85.67\\n85.94\\n94.13\\n196.80\\n88.17\\nUpcycling\\nB/16\\n54.42\\n77.39\\n86.03\\n159.52\\n174.73\\n365.28\\n163.66\\nDense\\nL/16\\n54.91\\n79.17\\n86.68\\n182.16\\n58.26\\n455.85\\n58.26\\nUpcycling\\nL/16\\n56.71\\n79.70\\n87.10\\n236.27\\n75.57\\n565.58\\n72.29\\nDense\\nL/16\\n55.65\\n79.46\\n86.48\\n338.49\\n108.26\\n847.05\\n108.26\\nUpcycling\\nL/16\\n57.84\\n80.01\\n87.17\\n438.55\\n140.27\\n1049.79\\n134.18\\nDense\\nL/16\\n56.19\\n79.63\\n86.89\\n494.81\\n158.26\\n1238.24\\n158.26\\n26\\nPublished as a conference paper at ICLR 2023\\nTable 5: Selection of results on text tasks for different methods and architecture variants. We report the C4 Validation Token Accuracy, the individual metrics on each\\nSuperGLUE task after ﬁnetuning, as well as the overall SuperGLUE score, and the extra TPUv4-core-days and ExaFLOPs used, both in absolute and relative (%)\\nterms with respect to the corresponding dense checkpoint (when these are 0, they correspond to the initial dense checkpoints).\\nMethod\\nVariant\\nC4\\nBoolQ\\nCB\\nCOPA\\nMultiRC\\nReCoRD\\nRTE\\nWiC\\nWSC\\nSuperGLUE\\nScore\\nExtra\\nTPUv4-days\\nRelative Extra\\nTPUv4-days\\nExtra\\nExaFLOPs\\nRelative Extra\\nExaFLOPs\\nDense\\nBase\\n67.97\\n82.48\\n94.64 / 93.69\\n70.00\\n38.30 / 76.52\\n77.26 / 78.24\\n82.31\\n69.59\\n83.65\\n77.17\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nLarge\\n71.80\\n87.95\\n96.43 / 95.03\\n87.00\\n54.35 / 84.35\\n84.84 / 85.78\\n92.06\\n75.55\\n87.50\\n85.06\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nXL\\n74.15\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n0.00\\n0.00\\n0.00\\n0.00\\nDense\\nBase\\n68.20\\n81.77\\n92.86 / 91.79\\n70.00\\n37.88 / 76.04\\n77.25 / 78.15\\n82.67\\n67.55\\n81.73\\n76.34\\n13.33\\n40.00\\n55.15\\n40.00\\nDense\\nBase\\n68.33\\n83.00\\n94.64 / 93.61\\n72.00\\n38.93 / 76.82\\n77.46 / 78.49\\n84.84\\n67.71\\n78.85\\n77.05\\n19.99\\n60.00\\n82.73\\n60.00\\nUpcycling\\nBase\\n69.78\\n82.91\\n98.21 / 98.68\\n74.00\\n38.09 / 77.20\\n80.90 / 81.95\\n82.67\\n69.28\\n87.50\\n79.23\\n20.31\\n60.98\\n82.14\\n59.57\\nMoE\\nBase\\n69.52\\n79.66\\n91.07 / 89.08\\n63.00\\n30.12 / 73.44\\n76.36 / 77.48\\n79.78\\n69.75\\n84.62\\n74.45\\n20.33\\n61.01\\n82.14\\n59.57\\nUpcycling\\nBase\\n70.22\\n84.04\\n100.00 / 100.00\\n76.00\\n40.08 / 78.20\\n82.06 / 83.05\\n83.39\\n68.03\\n84.62\\n79.72\\n30.47\\n91.47\\n123.21\\n89.36\\nMoE\\nBase\\n70.12\\n80.34\\n96.43 / 97.38\\n72.00\\n30.33 / 73.87\\n77.43 / 78.66\\n80.51\\n70.06\\n84.62\\n76.82\\n30.49\\n91.52\\n123.21\\n89.36\\nDense\\nBase\\n68.53\\n82.63\\n96.43 / 93.21\\n70.00\\n38.30 / 77.27\\n77.55 / 78.46\\n83.03\\n69.91\\n82.69\\n77.36\\n33.32\\n100.00\\n137.88\\n100.00\\nUpcycling\\nBase\\n70.83\\n84.31\\n98.21 / 98.68\\n79.00\\n39.77 / 77.84\\n83.02 / 84.15\\n83.39\\n69.59\\n81.73\\n79.86\\n50.79\\n152.45\\n205.35\\n148.94\\nDense\\nLarge\\n72.03\\n88.81\\n96.43 / 94.30\\n89.00\\n56.87 / 85.49\\n87.65 / 88.44\\n90.97\\n73.20\\n90.38\\n85.87\\n123.54\\n40.00\\n637.72\\n40.00\\nDense\\nLarge\\n72.12\\n88.81\\n96.43 / 94.30\\n88.00\\n56.56 / 85.54\\n87.78 / 88.54\\n90.25\\n73.35\\n90.38\\n85.67\\n185.42\\n60.03\\n956.58\\n60.00\\nUpcycling\\nLarge\\n73.34\\n88.62\\n96.43 / 95.04\\n90.00\\n55.93 / 85.22\\n87.65 / 88.80\\n90.61\\n72.26\\n92.31\\n86.04\\n197.38\\n63.91\\n1076.51\\n67.52\\nUpcycling\\nLarge\\n73.68\\n88.90\\n100.00 / 100.00\\n90.00\\n55.40 / 85.04\\n88.58 / 89.62\\n92.78\\n72.57\\n90.38\\n86.74\\n287.48\\n93.08\\n1614.73\\n101.28\\nDense\\nLarge\\n72.26\\n88.38\\n96.43 / 94.30\\n87.00\\n57.08 / 85.43\\n87.83 / 88.57\\n91.34\\n72.57\\n87.50\\n85.20\\n309.04\\n100.06\\n1594.30\\n100.00\\nDense\\nXL\\n74.34\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n508.73\\n40.00\\n2369.07\\n40.00\\nDense\\nXL\\n74.46\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n1050.73\\n82.62\\n3401.70\\n57.44\\nUpcycling\\nXL\\n75.03\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n1405.57\\n110.52\\n4985.16\\n84.17\\n27\\n', 'source_name': 'Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints', 'source_url': 'https://arxiv.org/abs/2212.05055'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Soft_Merging_of_Experts.pdf #30\n",
      "{'content': 'Soft Merging of Experts with Adaptive Routing\\nMohammed Muqeeth\\nHaokun Liu\\nColin Raffel\\nDepartment of Computer Science\\nUniversity of North Carolina at Chapel Hill\\n{muqeeth,haokunl,craffel}@cs.unc.edu\\nAbstract\\nSparsely activated neural networks with conditional computation learn to route their\\ninputs through different “expert” subnetworks, providing a form of modularity that\\ndensely activated models lack. Despite their possible benefits, models with learned\\nrouting often underperform their parameter-matched densely activated counterparts\\nas well as models that use non-learned heuristic routing strategies. In this paper, we\\nhypothesize that these shortcomings stem from the gradient estimation techniques\\nused to train sparsely activated models that use non-differentiable discrete routing\\ndecisions. To address this issue, we introduce Soft Merging of Experts with\\nAdaptive Routing (SMEAR), which avoids discrete routing by using a single\\n“merged” expert constructed via a weighted average of all of the experts’ parameters.\\nBy routing activations through a single merged expert, SMEAR does not incur a\\nsignificant increase in computational costs and enables standard gradient-based\\ntraining. We empirically validate that models using SMEAR outperform models\\nthat route based on metadata or learn sparse routing through gradient estimation.\\nFurthermore, we provide qualitative analysis demonstrating that the experts learned\\nvia SMEAR exhibit a significant amount of specialization. All of the code used in\\nour experiments is publicly available.1\\n1\\nIntroduction\\nNeural networks typically use all of their parameters to process a given input. As such, the capabilities\\nof a model are distributed across the parameters of a model in a self-organizing way [1–5]. Explicitly\\nspecializing different parts of a model to different capabilities can provide various benefits, including\\nreduced interference across downstream tasks [6–9] or languages [10–12]. Furthermore, dedicating\\nspecific parameters to specific capabilities enables a form of modularity where a capability can be\\nadded, removed, or modified by adding, removing, or modifying the corresponding parameters [13].\\nAn orthogonal benefit of using only a subset of the model’s parameter for a given input is the ability\\nto decouple the computational cost of a model from the number of parameters it has, enabling training\\nof models at parameter scales that would otherwise be computationally infeasible [14, 15].\\nConditional computation techniques provide a way to build models that adaptively choose a subset of\\ntheir parameters to apply to a given input. A common way to use conditional computation in this\\nsetting is to introduce specialized subnetworks called experts that are controlled by routers that decide\\nwhich experts should be active. When the model is trained on diverse data, this form of conditional\\ncomputation can enable modular learning by allowing experts to specialize to different types of inputs\\nand flexibly share knowledge [16]. However, because routing involves making a discrete decision as\\nto which expert to use, the loss on the model’s prediction cannot back-propagate though the routing\\ndecision to update the router. Consequently, models with conditional computation often require\\ngradient estimation techniques for training [17, 15, 18].\\n1https://github.com/r-three/smear\\nPreprint. Under review.\\narXiv:2306.03745v1  [cs.LG]  6 Jun 2023\\nRouter\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nMerged Expert\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nRouter\\nGradient estimation\\nDiscrete Routing\\nSMEAR\\nFigure 1: The discrete routing decisions commonly used in models that route activations among\\nexperts require the use of gradient estimation (left). We propose SMEAR (right), which uses a given\\nrouter’s distribution to average the parameters of the corresponding experts and then routes the input\\nthrough a single merged expert. SMEAR achieves better performance than discrete routing, can be\\ntrained with standard backpropagation, and does not incur significant additional computational costs.\\nIn practice, past work has shown that models with conditional computation do not always learn\\neffective routing strategies. For example, Mittal et al. [19] investigate models with a continuous router\\nin a controlled setting and find the models do not route examples from the same group to the same\\nexperts and perform poorly compared to models with oracle routing. However, models with task- or\\ndomain-specific subnetworks [20, 21] provide evidence that it is possible to train performant models\\nwith specialized experts. As an extreme example, Roller et al. [22] achieves results comparable to\\nlearned routing with a fixed random routing. Relatedly, Fedus et al. [15] find the gain from scaling up\\nparameters by 30× with a sparsely activated model is smaller than scaling up both parameters and\\nFLOPs by 3× in a dense model. As a possible explanation, Clark et al. [17] study how models with\\nconditional computation improve with scale and find a detrimental term that scales with the product\\nof the log number of experts and active parameters. Consequently, increasing the number of experts\\nyields limited returns and existing methods for training conditional computation models may only be\\nhelpful when the number of active parameters is moderate.\\nIn this work, we hypothesize that issues with conditional computation stem from issues with gradient\\nestimation. Specifically, we focus on experimental settings where we can compare learned routing to\\na performant hand-designed heuristic routing scheme. We find that the gradient estimation techniques\\nwe consider often produce models that underperform heuristic routing, despite the fact that they could\\nin principle learn a better routing strategy. To address this shortcoming, we introduce Soft Merging\\nof Experts with Adaptive Routing (SMEAR), a method for training models with specialized experts\\nand learned routing. SMEAR works by using the router’s distribution over experts to compute a\\nweighted average of the parameters of the individual experts. Activations are then sent through the\\nmerged expert, which results in a similar computational cost to discrete routing with a single expert.\\nHowever, the fact that all components of SMEAR are fully differentiable enables standard gradient-\\nbased training. Empirically, we show that SMEAR significantly attains a favorable performance/cost\\ntradeoff to 1) discrete routing solutions found via gradient estimation, 2) heuristic routing schemes,\\nand 3) state-of-the-art baselines for learning modular models. We also qualitatively validate that\\nthe experts learned by SMEAR specialize to different types of inputs and share parameters across\\nrelated tasks. Put together, our results show that SMEAR provides an effective alternative for modular\\nmodels that use adaptive routing among expert subnetworks.\\n2\\nBackground\\nTo provide the necessary background for our work, we first explain how sparsely activated neural\\nnetworks use conditional computation, then discuss gradient estimators that enable learning discrete\\nrouting strategies. In addition, we discuss different ways to hand-design “heuristic” routing strategies\\nas well as preexisting techniques for learning modular models that we use as baselines.\\n2.1\\nRouting Among Experts\\nIn models that use discrete routing among experts (i.e. subnetworks), experts are organized into\\nblocks that are incorporated as an intermediate layer in a neural network. An expert routing block\\nB comprises a set of N experts {f1, f2, . . . fN} and a router R. Experts in the same block accept\\n2\\ninputs and produce outputs of the same dimensionality. Given a hidden-state representation u, the\\noutput of the i-th expert with parameters θi is fi(u, θi). In our work, the router chooses a single fi to\\nprocess the input of the block (though models in some other work may activate more than one expert\\n[14, 23]).\\n2.2\\nGradient Estimators\\nIn sparsely activated models that involve discrete adaptive routing, it is not possible to train the\\nrouter’s parameters with standard gradient-based learning. Fortunately, gradient estimators can\\nprovide approximate gradients to the router parameters. There are a few common designs shared\\nby models that use gradient estimators to train routers. Their router R often applies a lightweight\\nnetwork to some intermediate hidden states v in the model. The output of the lightweight routing\\nnetwork R(v) parameterizes a discrete probability distribution over the N experts. Different gradient\\nestimators vary in how they make the routing decision from R(v) and how they construct the output\\nfrom the chosen expert.\\nREINFORCE\\nGradients can be estimated through discrete operations using reinforcement learning\\ntechniques [24, 18]. In reinforcement learning, a policy loss is used to train an agent to learn optimal\\nactions in an environment. In this paper, we experiment with the REINFORCE algorithm which\\ncomputes the policy loss as log(π)r where r denotes the received reward for taking an action whose\\nassigned probability is π. When applied to models that use discrete routing among experts, the goal\\nis to train the model to choose the optimal expert to process a given input. Here, the router R acts an\\nagent that samples an expert to use according to the routing probabilities. In order to train such a\\nrouter, the router’s assigned probability to the sampled expert is used as π and the negative of the\\nmodel’s loss is used as the reward r. The router is therefore trained to pick experts that maximize\\nthe reward which, in turn, minimizes the loss. The REINFORCE estimator often suffers from high\\nvariance because of the sampling operation. This motivates the use of baselines, which reduce\\nvariance without changing the optimal solution. In our work, we follow Clark et al. [17] and use a\\nbaseline b that is generated by a small neural network with a single hidden layer that takes as input v\\nand is trained with the Huber loss. The overall loss function is then\\nL = −Ei∼R(v)α log R(v)i(r −b) −βR(v) log R(v) + γLHuber(r, b)\\n(1)\\nwhere α, β, and γ are hyperparameters that correspond to policy gradient weight, policy entropy\\nweight, and value loss weight. In practice, we approximate the expectation in eq. (1) with a single\\nsample. During inference, the output of the block B is just fi(u, θi) where i = arg max R(v).\\nStraight Through Gumbel-Softmax (ST-Gumbel)\\nThe Gumbel-Softmax trick [25, 26] provides\\na continuous differentiable approximation to sampling from a categorical distribution like the one\\nparameterized by a router. Specifically, Gumbel noise is added to the logits of the distribution and\\na temperature scale is applied in the softmax operation. Denoting gi ∼Gumbel(0, 1) and τ as the\\ntemperature, the Gumbel-Softmax trick produces the following modified distribution:\\nˆ\\nR(v)i =\\nexp((log(R(v)i) + gi)/τ)\\nPN\\nj=1 exp((log(R(v)i) + gi)/τ)\\n(2)\\nThe expert fi with the highest assigned probability is chosen by applying an arg max operation\\nover this distribution. In order to approximate gradients through the arg max operation, we use the\\nStraight-Through estimator which replaces fi(u, θi) with (1 −sg[ ˆ\\nR(v)i] + ˆ\\nR(v)i)fi(u, θi) where sg\\nstands for the stop-gradient operator. During forward pass, the multiplier for fi(u, θi) becomes 1 and\\nthe multiplier receives gradients for the term ˆ\\nR(v)i in the backward pass. In practice, the temperature\\nτ is gradually annealed from a high to low value so that the approximated samples are more and more\\nsimilar to discrete samples. During inference, we choose an expert according to arg max R(v).\\nTop-k\\nShazeer et al. [14] propose a gradient estimation scheme where the router sends the input\\nthrough the k experts that are assigned the highest probability. Fedus et al. [15] later found that this\\nrouter could be used effectively when k = 1. Specifically, the estimator selects the subnetwork with\\nthe highest probability and scales its output using its corresponding routing probability. The output\\nof the block is therefore R(v)ifi(u, θi), where i = arg max R(v).\\n3\\n2.3\\nHeuristic Routing\\nAs a point of comparison for techniques that learn adaptive routing, we experiment with three baseline\\nrouting strategies that do not require a trained router.\\nTag Routing\\nIf we have prior knowledge about the data that a model will be applied to, we can\\nhand-design a heuristic routing strategy for choosing which expert to use for a given example based\\non data properties. Tag routing takes advantage of “tags” associated with a given example (such as its\\ndomain or task) and associates each expert in an expert routing block with a particular tag. In this\\nwork, we assume each example has a single tag and route each example to its tag’s expert.\\nHash Routing\\nRoller et al. [22] propose hash routing, which uses a fixed hashing function to\\ndetermine which expert to use for a given example. Specifically, each example is assigned a random\\nexpert choice in each expert routing block which is used consistently over the course of training and\\ninference. This approach disregards any shared characteristics across examples.\\nSingle-Expert\\nAs an additional baseline, we consider models where all inputs are routed to a single\\nexpert in each routing block. To provide a fair comparison to models with N experts per block on the\\nbasis of both computational cost or parameter count, we consider models with a single expert with\\neither the same number (compute-matched, referred to as “1× compute”) or N× (parameter-matched,\\nreferred to as “1× parameters”) as many parameters as a single expert.\\n2.4\\nMethods for Learning Modular Models\\nBeyond the simple baselines discussed above, we consider two recently proposed methods that aim\\nto learn modular models but do not include a learned per-example router.\\nAdamix\\nAdamix [27] uses random routing for each example during training and adds a consistency\\nloss to encourage experts to share information and discourage divergence. During inference, the\\nparameters of all experts are averaged together to form a single expert and no adaptive routing is\\nused.\\nLatent Skills\\nLatent Skills [28] assumes that the task for each example is known and trains a\\ntask-skill matrix that specifies which experts are active for a given task. The binary task-skill matrix\\nis fixed and learned via the Gumbel-Sigmoid trick [26]. During inference, a merged expert is formed\\nfor each task by averaging the parameters of the skill experts weighted according to the task-skill\\nmatrix.\\n3\\nSoft Merging of Experts with Adaptive Routing\\nAs we will later show in section 4, the gradient estimation techniques used to train models with\\ndiscrete routing often fail to produce performant routing strategies. Our goal in this work is therefore\\nto explore whether it is possible to train models with adaptive routing among experts without resorting\\nto gradient estimation. Specifically, we aim to achieve better performance by designing an expert and\\nrouter architecture that facilitates standard end-to-end gradient-based training but does not increase\\ncomputational costs.\\nEnsemble Routing\\nOne simple idea would be to pass the input of a given expert routing block\\nthrough every expert, and then compute an average of the experts’ outputs weighted according\\nthe router’s distribution, i.e. exactly computing Ei∼R(v)fi(u, θi). We refer to this approach as an\\nensemble routing strategy since it corresponds to using the ensemble prediction of the experts. Since\\nthe operations involved in computing the average are all differentiable, using an ensemble routing\\nstrategy would allow for exact computation of gradients and end-to-end-learning. Unfortunately, such\\nan approach would incur a significant increase in computational costs because it requires computing\\nthe output of every expert rather than a single expert.\\nMerging Experts\\nTo explore an alternative fully-differentiable expert routing block, we take\\ninspiration from recent work on merging models [29–34]. These works have shown that averaging\\n4\\nthe parameters of models that share a common architecture can often produce an aggregate model that\\nshares the capabilities of the individual models. Notably, Wortsman et al. [30], Matena and Raffel\\n[29] found that averaging the weights of multiple fine-tuned models produced a single model that\\nperforms comparably to an ensemble of the models. In addition, both Adamix [27] and Latent Skills\\n[28] include steps that involve averaging expert parameters. Motivated by these findings, we propose\\nSoft Merging of Experts with Adaptive Routing (SMEAR), which constructs a single merged expert\\nwhose parameters are computed as the weighted average of the experts within a routing block. Each\\nexpert’s weight is set according to the corresponding routing probability generated by the router. In\\nSMEAR, the input to the routing block is fed into the merged expert and the merged expert’s output\\nis used as the output of the block. By averaging parameters, SMEAR implicitly assumes that all\\nexperts in the routing block share an identical architecture (thereby inducing a natural one-to-one\\nmapping between parameters in each expert). To the best of our knowledge, all past works focused\\non routing among experts use experts with a shared architecture.\\nMore explicitly, we define SMEAR as computing the output of an expert routing block using a merged\\nexpert computed as ¯\\nf(u, P\\ni R(v)iθi). The merged expert shares the same architecture with the\\nindividual experts fi. Notably, the input of the routing block is only ever processed by ¯\\nf; activations\\nare never fed to any of the individual experts. To break symmetry, all experts are randomly initialized\\nwith different parameter values. Importantly, all operations in SMEAR are fully differentiable; as a\\nresult, SMEAR enables standard gradient-based end-to-end learning. In addition, SMEAR retains\\nthe ability to learn an adaptive routing strategy that can route different examples to different experts\\nwithout relying on hand-specified tags (as in Latent Skills and tag-based routing). We will later show\\nqualitatively that this leads to meaningful specialization of different experts in real-world experiments\\n(section 4.3).\\nComputational Costs\\nImportantly, SMEAR only ever computes the output of a single expert, sug-\\ngesting that SMEAR’s computational cost could be comparable to single-expert discrete routing and\\nsignificantly lower than ensemble routing. However, we note that the averaging operation itself incurs\\na nontrivial computational cost. To quantify this cost, we focus on the common expert architecture\\ncomprising a dense layer that projects from d-dimensional activations to an m-dimensional vector\\nfollowed by a nonlinearity and an additional dense layer projecting from m dimensions back to d.\\nFor simplicity, we ignore the (relatively minor) cost of the nonlinearity. We assume the input is a\\nlength-L sequence of activations with size L × d. In this case, computing the output of the merged\\nexperts incurs a computational cost of approximately L × 4 × d × m FLOPs and ensemble routing\\nwith N experts would require N × L × 4 × d × m FLOPs. SMEAR additionally must average\\ntogether the parameters of N experts, which costs an additional N × 2 × d × m FLOPs. Some past\\nwork on models with discrete routing has the router choose a different expert for each entry in the\\ninput sequence of activations (e.g. 15, 35, 22). This would require computing the expert average L\\ntimes, which would make the cost of SMEAR similar to that of ensemble routing. We therefore focus\\non settings where models make a single routing choice for an entire input example (e.g. 20, 21, 36).\\nThis results in a total cost of approximately (L × 4 + N × 2) × d × m for SMEAR. Consequently,\\nas long as L × 4 ≫N × 2, SMEAR and discrete routing have roughly the same computational costs.\\nGiven that L is on the order of hundreds or thousands of tokens for text-based tasks and on the order\\nof thousands for vision tasks, L × 4 will be much larger than N × 2 as long as there is a modest\\nnumber of experts. Furthermore, we would expect SMEAR to be approximately N×L\\nN+L times cheaper\\nthan ensemble routing. More concretely, we find in section 4.1 that the wall-clock time required to\\nprocess an example with SMEAR in real-world experiments is roughly the same as using discrete\\nrouting and significant faster than ensemble routing.\\n4\\nExperiments\\nIn order to thoroughly evaluate the effectiveness of SMEAR, we perform experiments in two real-\\nworld settings that differ in model architecture and modality. We are particularly interested in\\nwhether a given approach for learning routing outperforms the heuristic routing strategies described\\nin section 2.3. As such, we focus on experimental settings where a performant “tag routing” baseline\\ncan be designed, i.e. where we have oracle access to metadata that can be used to appropriately route\\nexamples. Specifically, we experiment with fine-tuning T5.1.1 Base [37] on datasets from GLUE [38]\\n(referred to as T5-GLUE) and fine-tuning a ResNet18 [39] on DomainNet [40] (ResNet-DomainNet).\\n5\\n250\\n300\\n350\\nExamples/second\\n78\\n79\\n80\\n81\\nAverage accuracy\\nT5-GLUE\\n1400\\n1600\\n1800\\n2000\\nExamples/second\\n58\\n59\\n60\\n61\\n62\\nResNet-DomainNet\\nSMEAR\\n1× Parameters\\n1× Compute\\nAdamix\\nTag\\nLatent Skills\\nTop-k\\nST-Gumbel\\nREINFORCE\\nEnsemble\\nFigure 2: Average accuracy and inference speed (in examples processed per second) for models using\\ndifferent routing approaches on our T5-GLUE and ResNet-DomainNet settings. Routing approaches\\nare grouped by color; groups are (in order of the legend) our method (SMEAR), methods that do not\\nuse adaptive routing (1× compute, 1× parameters, Adamix, and Hash), methods that make use of\\nmetadata (Tag and Latent Skills), methods that learn adaptive routing through gradient estimation\\n(Top-k, ST-Gumbel, and REINFORCE), and methods that ensemble expert outputs (Ensemble). We\\nomit Hash routing from the plots because its poor performance (66.9% on T5-GLUE and 52.4%\\non ResNet-DomainNet) hampers readability. Exact numerical results for all methods and standard\\ndeviation across five runs are provided in appendix F.\\nIn these settings, we add experts to an existing pre-trained backbone in the same way that Adapters\\nare used for parameter-efficient fine-tuning [41].\\nT5-GLUE\\nIn this setting, we focus on training a T5 model [37] on the GLUE meta-benchmark [38]\\nfor natural language understanding. GLUE consists of nine datasets (SST-2 [42], CoLA [43]), MNLI\\n[44], RTE [45], QQP [46], MRPC [47], STS-B [48], QNLI [49], and WNLI [50]) that cover a wide\\nrange of natural language processing tasks. Following convention, we exclude WNLI and use the\\nremaining eight datasets. We use the prompted form of these datasets available in PromptSource [51],\\nwhich maps each example into a natural language request-and-response form. During training, we\\nrandomly select a prompt templates for each example; during evaluation, we evaluate each example\\nusing all of its dataset’s templates. We follow the approach of Mahabadi et al. [52] for splitting each\\nGLUE dataset into train, eval, and test splits. Past work has demonstrated improved performance on\\nRTE by co-training with MNLI [53–57], and we congruously found that sharing an expert between\\nRTE and MNLI produced a stronger tag routing strategy. In the interest of making our baselines as\\nstrong as possible, we use this improved tag routing scheme in all experiments. We use the pretrained\\nT5.1.1 Base model as the backbone and adapt the model in a way similar to adding adapters [41] for\\na single task, i.e. we keep all pretrained parameters frozen except for layer normalization parameters\\nand insert expert routing blocks after self-attention, feed-forward and cross-attention modules. The\\nT5 1.1 Base model has 12 Transformer layers in both the encoder and decoder, resulting in a total\\nof 12 × 2 = 24 blocks in the encoder and 12 × 3 = 36 blocks in the decoder, or 60 expert routing\\nblocks in total. In each block, we introduce eight experts (one for each dataset in GLUE). The\\nrouter architecture is simply a linear classifier, i.e. a linear projection layer followed by a softmax\\nnonlinearity. To help avoid saturating the softmax nonlinearity, we apply layer normalization both to\\nthe input of the router as well as the rows of the linear layer. In the encoder, the router takes as input\\nthe preceding hidden states, which are averaged across the sequence and fed into the router. In the\\ndecoder, the routers receive the average of the encoder’s final hidden states instead of the decoder\\nhidden states to prevent information leakage from later target tokens. We also include expert dropout\\nLiu et al. [58] where each expert is dropped with a probability of 0.1 wherever it was found to be\\nbeneficial (a detailed ablation can be found in table 1). In GLUE, dataset sizes vary by three orders of\\nmagnitude, and we therefore found that expert load-balancing losses (as used e.g. in [14, 15, 59] to\\nencourage uniform usage across experts) tended to hurt performance and therefore did not include\\nthem.\\nResNet-DomainNet\\nIn this setting, we focus on adapting an ImageNet pre-trained ResNet18 model\\n[39] to datasets within DomainNet [40]. DomainNet is a collection of object recognition datasets that\\ncover six distinct domains and all share the same label space corresponding to 345 object categories.\\nWe treat the domain of each example as its tag. As in the T5-GLUE setting, we freeze the pretrained\\nmodel and insert eight expert routing blocks after each of the eight residual layer groups in the\\n6\\nmodel. Each block includes six experts corresponding to the number of domains. We use the same\\narchitecture for routers as in T5-GLUE and feed average-pooled hidden states into the router to\\ncompute the routing probability. Experts in this setting use batchnorm on their input instead of\\nlayer norm in the output, following [60]. As in T5-GLUE, we omit load-balancing losses due to\\ndramatically different sizes across domains in DomainNet.\\nFull details of hyperparameters and training timings for each setting are presented in appendix B.\\n4.1\\nResults\\nTo assess the overall effectiveness of routing strategies learned with SMEAR, we compare to learned\\nrouting using the gradient estimators from section 2.2, heuristic routing strategies from section 2.3,\\nand modular baselines from section 2.4. A summary of our results is shown in fig. 2. First, we find\\nthat models using routing strategies learned through gradient estimation often underperform heuristic\\nrouting strategies – while the best-performing estimator (REINFORCE) in T5-GLUE outperforms\\ntag routing, all estimators perform worse than tag routing in ResNet-DomainNet. On the other\\nhand, we observed some cases where gradient estimation-based routing outperforms hash or single-\\nexpert routing, which suggests that the learned routing strategies were nontrivial. Pertinently, in all\\nexperimental settings, SMEAR matches or outperforms every other routing strategy, including both\\nrouting learned by gradient estimators and all heuristic routing strategies. In particular, SMEAR\\nachieves 2.7% improvement over tag routing in T5-GLUE and 0.6% improvement over tag routing in\\nResNet-DomainNet, suggesting effective specialization and sharing of experts. SMEAR additionally\\noutperforms the single-expert parameter-matched baseline (1× parameters) by 1.4% in T5-GLUE\\nand 1.2% in ResNet-DomainNet, further highlighting the importance of modularity. As an upper\\nbound on performance, we also compare SMEAR to expert ensembling (“Ensemble”) which averages\\nthe outputs of all experts and incurs significantly higher computational cost. SMEAR matches the\\nperformance of ensemble routing in T5-GLUE and slightly underperforms it in ResNet-DomainNet,\\ndespite being significantly computationally cheaper. Compared to Adamix, which similarly averages\\nexperts but does not learn a routing strategy, SMEAR achieves 3.2% higher performance in T5-GLUE\\nand 4% higher in ResNet-DomainNet. Moreover, while the performance improvement of SMEAR\\nover Latent Skills is relatively small (0.6% in T5-GLUE and 0.1% in ResNet-DomainNet), a major\\nadvantage of SMEAR over Latent Skills is that it does not assume access to oracle tags (which are\\nnot always available in real-world settings) and instead learns an adaptive routing strategy.\\nTo compare the inference speed of the various methods, we plot the average accuracy attained by\\neach method against its inference speed (in terms of number of examples processed per second) in\\nfig. 2. The single-expert, Adamix, Hash, and Tag routing methods are the fastest since they do not use\\nany routing networks. Despite the slight overhead of averaging the weights in SMEAR, we observe\\nthat its inference speed is almost identical to that of discrete adaptive routing (as learned via gradient\\nestimation techniques). This confirms that the performance gains attained by SMEAR do not incur\\nsignificant additional costs. Ensembling expert outputs is the slowest, with a 1.2× slowdown in\\nT5-GLUE and 1.3× slowdown in ResNet-DomainNet compared to SMEAR.\\n4.2\\nScaling\\nThus far, we have always set the number of experts equal to the number of tasks (in T5-GLUE) or\\ndomains (in DomainNet). However, with learned routing there is no reason to force this constraint,\\nso we therefore tested the scalability of SMEAR by evaluating its performance with twice as many\\nexperts (16 for T5-GLUE and 12 for ResNet-DomainNet). We found a significant improvement\\n(0.8%) when doubling the number of experts on ResNet-DomainNet, but no significant change on\\nT5-GLUE (81.3 ± 1.1 vs. 81.6 ± 1.1). This suggests there is no benefit to increasing capacity in\\nthe T5-GLUE setting. The complete results for doubling the number of experts are presented in\\nappendix F (labeled as “SMEAR 2×”).\\n4.3\\nQualitative Analysis\\nIn this section, we provide qualitative analysis of the routing learned by SMEAR by visualizing the\\naverage router distribution across all examples in a given dataset for every router in each model.\\nFigure 3 shows four select visualizations (two from a SMEAR-based model trained in T5-GLUE and\\ntwo from ResNet-DomainNet). Across the two T5-GLUE router distributions shown in fig. 3, we\\n7\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\nRTE\\nSST-2\\nMRPC\\nSTS-B\\nQQP\\nMNLI\\nQNLI\\nCoLA\\n0.05 0.01 0.07 0.01 0.51 0.3 0.040.02\\n0.01 0.04 0.01 0.0 0.0\\n0.1\\n0.0 0.84\\n0.330.09 0.01 0.02 0.0 0.48 0.07 0.0\\n0.0 0.97 0.0 0.0 0.0 0.02 0.0 0.0\\n0.0 0.0 0.0\\n1.0\\n0.0 0.0 0.0 0.0\\n0.27 0.04 0.16 0.03 0.0 0.15 0.330.03\\n0.0 0.0 0.0 0.0 0.98 0.01 0.0 0.0\\n0.02 0.01 0.0 0.0 0.83 0.1\\n0.0 0.02\\nRouter for Encoder FFN 3\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.14 0.03 0.0 0.0 0.82 0.0 0.0 0.0\\n1.0\\n0.0 0.0 0.0 0.0 0.0 0.0 0.0\\n0.11 0.05 0.0 0.0 0.8 0.04 0.0 0.0\\n0.14 0.0 0.0 0.0 0.09 0.0 0.0 0.77\\n0.0 0.52 0.0 0.0 0.4 0.01 0.0 0.06\\n0.0 0.0 0.13 0.2 0.22 0.0 0.050.39\\n0.0 0.32 0.0 0.0 0.67 0.0 0.0 0.0\\n0.23 0.0 0.0 0.0 0.67 0.0 0.0 0.11\\nRouter for Decoder FFN 5\\n(a) T5-GLUE model\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nClipart\\nInfograph\\nPainting\\nQuickdraw\\nReal\\nSketch\\n0.19\\n0.17\\n0.16\\n0.11\\n0.14\\n0.23\\n0.23\\n0.14\\n0.13\\n0.12\\n0.13\\n0.26\\n0.18\\n0.12\\n0.12\\n0.15\\n0.14\\n0.3\\n0.1\\n0.35\\n0.28\\n0.08\\n0.1\\n0.1\\n0.16\\n0.12\\n0.13\\n0.14\\n0.16\\n0.29\\n0.19\\n0.2\\n0.14\\n0.19\\n0.12\\n0.15\\nRouter for Residual Layer 1\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\n0.14\\n0.1\\n0.18\\n0.11\\n0.19\\n0.29\\n0.13\\n0.09\\n0.43\\n0.11\\n0.07\\n0.16\\n0.31\\n0.11\\n0.17\\n0.15\\n0.13\\n0.13\\n0.07\\n0.29\\n0.08\\n0.32\\n0.13\\n0.11\\n0.34\\n0.11\\n0.15\\n0.13\\n0.11\\n0.16\\n0.14\\n0.12\\n0.15\\n0.14\\n0.12\\n0.33\\nRouter for Residual Layer 5\\n(b) ResNet-DomainNet model\\nFigure 3: Average routing distributions produced by SMEAR for two routers from the T5-GLUE\\nmodel and two from the ResNet-DomainNet model. For a given router, we average all routing\\ndistributions across all examples from a given dataset.\\nobserve significantly different behavior – one mainly follows a tag routing-style strategy whereas\\nthe other routes most datasets to the same expert. However, we note that the tag-style router utilizes\\nshared experts for RTE, MRPC, and MNLI; notably, these tasks are somewhat similar in that they all\\ninvolve determining similarity among pairs of sentences. In the single-expert-style router, STS-B\\n(the only regression task) and SST-2 (which has a distinct output vocabulary) are given dedicated\\nexperts, and MNLI (a large and relatively challenging dataset) is routed through many different\\nexperts. More broadly, we highlight that there is generally a great deal of sparsity in the learned\\nrouting distributions, suggesting a significant amount of expert specialization. In ResNet-DomainNet,\\nwe can see that examples from the Quickdraw domain are routed to two specific experts in both cases.\\nAdditionally, we observe that the router distribution of the Painting and Real domains are highly\\ncorrelated. Other domains such as Clipart and Sketch seem to evenly use experts. Interestingly, there\\nis less expert specialization in the ResNet-DomainNet model, suggesting that there may be more\\nsimilarities between the individual domains in DomainNet compared to the tasks in GLUE. Routing\\ndistributions for all routers can be found in appendix G.\\n5\\nRelated Work\\nModels with Conditional Computation\\nVarious works have investigated ways learning discrete\\nrouting strategies. Deecke et al. [61], Hazimeh et al. [62], Dua et al. [63] start training with most of\\nthe experts activated and gradually introduce sparsity. Kudugunta et al. [21], Ponti et al. [28], Ma\\net al. [16], Gupta et al. [64] group examples from the same task together and introduce task-specific\\nparameters in the router. Other works avoid learned routing by hand-crafting heuristic routing\\nstrategies. Gururangan et al. [20] built sparsely activated language models where different domains\\nuse separate experts and then weights the experts for new domains. Tang et al. [65], Pfeiffer et al.\\n[66, 67] assign experts based on task-related human knowledge. Our focus on settings where\\nperformant routing schemes can be hand-designed takes inspiration from this line of work.\\nBecause sparsely activated models disentangle computation and parameter count, significant effort\\nhas gone into leveraging conditional computation to create massive pre-trained models with a feasible\\ncomputation cost [68, 14, 15, 23, 69, 70]. Many works explore different routing methods in this\\nsetting, with a major focus on balancing the load across experts [35, 71, 72, 22]. Another line of\\nwork aims to introduce ways to convert trained dense models into similar-sized sparse models with a\\nlower computational footprint [73–75].\\nGradient Estimation Techniques\\nMany gradient estimators have been proposed to produce\\napproximate gradients for backpropagating through discrete steps. Clark et al. [17] uses a learned\\nbaseline to reduce the variance of the REINFORCE estimator. The REBAR estimator [76] adds\\na reparameterizable term to REINFORCE as a baseline that results in a more effective unbiased\\nestimator. This additional term incorporates a relaxed sample similar to Gumbel-Softmax [25].\\nRELAX [77] is similar to REBAR but uses a learnable neural network for the reparameterizable\\nterm. Kool et al. [78] uses additional samples as a built-in baseline for REINFORCE. Yin and Zhou\\n[79] and Dong et al. [80] use the idea of coupling between multiple samples to reduce the variance\\n8\\nof the gradient estimator for improved training of models with binary latent variables. Dong et al.\\n[81] improve upon Yin and Zhou [79] and Dong et al. [80] by extending the estimator to categorical\\nvariables. In preliminary experiments, we did not find any major gains from using more sophisticated\\ngradient estimation techniques, but designing gradient estimators with discrete routing in mind could\\nyield more performant routing strategies.\\nIssues with Conditional Computation\\nA great deal of past work has highlighted issues with models\\nthat use conditional computation. Clark et al. [17] study the scaling laws of sparse language models\\nand discovered a computational cutoff above which no additional benefits are observed. Relatedly,\\nDu et al. [23] observe worse results when further scaling up the number of experts. Chi et al. [82]\\nhighlight that using the model’s activations as input to the router can cause the representations to\\n“collapse”. Dai et al. [83] demonstrate that learned routing desicions can fluctuate significantly over\\ntraining. Mittal et al. [19] create a set of simple and compositional data distributions and show that\\nsystems with modular architecture can not find the most performant solution when trained end-to-end.\\nYe et al. [36] experiment with various designs for multi-task learning with task-level routing and find\\nthat the performance never surpasses simple multi-task baselines. We show a possibility to avoid\\nthese issues with a fully differentiable routing strategy that does not increase computational costs.\\nWeight Averaging Methods\\nMany prior works utilize parameter averaging for ensembling. Worts-\\nman et al. [31], Ilharco et al. [84] average the weights of a pre-trained and a fine-tuned model to\\nimprove performance on target tasks as well as robustness to distribution shift. Choshen et al. [32]\\nsimilarly show that merging multiple models fine-tuned on different datasets can provide a better\\ninitialization than using the original pre-trained model for further fine-tuning on new unseen datasets.\\nYang et al. [85], Zhang et al. [86] compute convolution kernels by averaging weights of individual\\nkernels. The kernels being linear, weight averaging and ensembling are mathematically equivalent.\\nHowever, SMEAR performs averaging on non-linear and parameter efficient experts that, when\\ntrained alone, can match the performance of the fully fine-tuned model [41]. Model averaging is also\\na common step in distributed optimization, where it is widely used in federated learning [34] and has\\nrecently been used for distributed fine-tuning [87], multi-domain training [88], and multitask training\\n[33]. There are also works that utilize different styles of merging instead of weight averaging of\\nparameters, such as reweighting parameters in accordance with their approximate Fisher information\\n[29], aligning features by fitting a linear projection [89], and permuting columns to account for\\npermutation symmetries [90].\\n6\\nConclusion\\nIn this work, we sought to address shortcomings of models with discrete routing among experts\\nthat can lead them to underperform heuristic non-learned routing. We hypothesized that these\\nissues stem from the gradient estimation techniques required to propagate gradients through discrete\\nrouting decisions and therefore focused on designing an expert routing architecture that allows exact\\ncalculation of gradients. Our approach, called SMEAR, works by computing a weighted average\\nof expert parameters where the weighting is set according to the output of a learned router. We\\ncompared the performance of models using SMEAR to discrete routing models that were trained via\\nvarious gradient estimation techniques. In experimental settings covering different modalities and\\nmodel architectures, we found that SMEAR outperformed all models with discrete routing as well\\nas performant heursitic routing strategies. Notably, this performance boost comes with no increase\\nin computational costs. SMEAR also matched or outperformed existing state-of-the-art methods\\nfor learning modular models through expert averaging while removing the requirement for oracle\\ntask labels. Through qualitative analysis, we further confirmed that the experts learned in a model\\nusing SMEAR specialize to different types of inputs and that the router learns a nontrivial strategy\\nthat exploits commonalities across different examples. In future work, we are interested in exploring\\ndifferent expert architectures [91, 92] and improved merging methods [29, 90, 89]. Given access to a\\nlarger amount of compute, we would also be excited to try out SMEAR in the large-scale settings\\nwhere discrete routing has been used [15, 69, 23] to see whether it helps fix the poor scaling properties\\nof models with discrete routing [17].\\n9\\nReferences\\n[1] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In\\n13th European Conference on Computer Vision, 2014.\\n[2] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In\\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\\n2021.\\n[3] Róbert Csordás, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Are neural nets modular? in-\\nspecting functional modularity through differentiable weight masks. In International Conference\\non Learning Representations, 2021.\\n[4] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba.\\nUnderstanding the role of individual units in a deep neural network. Proceedings of the National\\nAcademy of Sciences, 117(48):30071–30078, 2020.\\n[5] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li.\\nFinding skill neurons in pre-trained transformer-based language models.\\narXiv preprint\\narXiv:2211.07349, 2022.\\n[6] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\\nenables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\\n[7] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv\\npreprint arXiv:2109.01652, 2021.\\n[8] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition, pages 3712–3722, 2018.\\n[9] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv\\npreprint arXiv:2106.08254, 2021.\\n[10] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? arXiv\\npreprint arXiv:1906.01502, 2019.\\n[11] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\\nLewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.\\nTransactions of the Association for Computational Linguistics, 8:726–742, 2020.\\n[12] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\\nAditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text trans-\\nformer. arXiv preprint arXiv:2010.11934, 2020.\\n[13] Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli´\\nc, and Edoardo Maria Ponti. Modular deep learning.\\narXiv preprint arXiv:2302.11529, 2023.\\n[14] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[15] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efficient sparsity, 2021.\\n[16] Jiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H Chi. Snr: Sub-network routing\\nfor flexible parameter sharing in multi-task learning. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 33, pages 216–223, 2019.\\n[17] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified\\nscaling laws for routed language models. In International Conference on Machine Learning,\\npages 4057–4086. PMLR, 2022.\\n10\\n[18] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\\n[19] Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie. Is a modular architecture enough? arXiv\\npreprint arXiv:2206.02713, 2022.\\n[20] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer.\\nDemix layers: Disentangling domains for modular language modeling.\\narXiv preprint\\narXiv:2108.05036, 2021.\\n[21] Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-\\nThang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient\\ninference. arXiv preprint arXiv:2110.03742, 2021.\\n[22] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models.\\nAdvances in Neural Information Processing Systems, 34:17555–17566, 2021.\\n[23] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\\nlanguage models with mixture-of-experts. In International Conference on Machine Learning,\\npages 5547–5569. PMLR, 2022.\\n[24] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation\\nusing stochastic computation graphs. Advances in Neural Information Processing Systems, 28,\\n2015.\\n[25] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\\narXiv preprint arXiv:1611.01144, 2016.\\n[26] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\\n[27] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and\\nJianfeng Gao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language\\nmodels. arXiv preprint arXiv:2205.12410, 2022.\\n[28] Edoardo M Ponti, Alessandro Sordoni, and Siva Reddy. Combining modular skills in multitask\\nlearning. arXiv preprint arXiv:2202.13914, 2022.\\n[29] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. arXiv\\npreprint arXiv:2111.09832, 2021.\\n[30] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\\ninference time. In International Conference on Machine Learning, pages 23965–23998. PMLR,\\n2022.\\n[31] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca\\nRoelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong,\\net al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 7959–7971, 2022.\\n[32] Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for\\nbetter pretraining. arXiv preprint arXiv:2204.03044, 2022.\\n[33] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem\\nChoshen. Cold fusion: Collaborative descent for distributed multitask finetuning. arXiv preprint\\narXiv:2212.01378, 2022.\\n[34] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\\nCommunication-efficient learning of deep networks from decentralized data. In Artificial\\nIntelligence and Sstatistics, 2017.\\n11\\n[35] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. In International Conference on Machine Learning,\\npages 6265–6274. PMLR, 2021.\\n[36] Qinyuan Ye, Juan Zha, and Xiang Ren. Eliciting transferability in multi-task learning with\\ntask-level mixture-of-experts. arXiv preprint arXiv:2205.12701, 2022.\\n[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\\ntext-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\\n[38] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461, 2018.\\n[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 770–778, 2016.\\n[40] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment\\nmatching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international\\nconference on computer vision, pages 1406–1415, 2019.\\n[41] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning\\nfor nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.\\n[42] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural\\nlanguage processing, pages 1631–1642, 2013.\\n[43] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019.\\n[44] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus\\nfor sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\\n[45] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing\\ntextual entailment challenge. In TAC, 2009.\\n[46] Iyer\\nShankar,\\nDandekar\\nNikhil,\\nand\\nCsernai\\nKornel.\\nFirst\\nquora\\ndataset\\nrelease:\\nquestion\\npairs\\n(2017).\\nhttps://www.quora.com/q/quoradata/\\nFirst-Quora-Dataset-Release-Question-Pairs, 2017.\\n[47] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nIn Third International Workshop on Paraphrasing (IWP2005), 2005.\\n[48] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[49] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\n[50] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In\\nThirteenth international conference on the principles of knowledge representation and reasoning,\\n2012.\\n[51] Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak,\\nAbheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An\\nintegrated development environment and repository for natural language prompts. arXiv preprint\\narXiv:2202.01279, 2022.\\n12\\n[52] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.\\nParameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. arXiv\\npreprint arXiv:2106.04489, 2021.\\n[53] Jason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on stilts: Supplemen-\\ntary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\\n[54] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\n[55] Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\\nPang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning\\nwith pretrained models for natural language understanding: When and why does it work? arXiv\\npreprint arXiv:2005.00628, 2020.\\n[56] Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew\\nMattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability\\nacross nlp tasks. arXiv preprint arXiv:2005.00770, 2020.\\n[57] Leshem Choshen, Elad Venezian, Shachar Don-Yehia, Noam Slonim, and Yoav Katz. Where to\\nstart? analyzing the potential value of intermediate models. arXiv preprint arXiv:2211.00107,\\n2022.\\n[58] Rui Liu, Young Jin Kim, Alexandre Muzio, and Hany Hassan. Gating dropout: Communication-\\nefficient regularization for sparsely activated transformers. In International Conference on\\nMachine Learning, pages 13782–13792. PMLR, 2022.\\n[59] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n[60] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains\\nwith residual adapters. Advances in neural information processing systems, 30, 2017.\\n[61] Lucas Deecke, Timothy Hospedales, and Hakan Bilen. Latent domain learning with dynamic\\nresidual adapters. arXiv preprint arXiv:2006.00996, 2020.\\n[62] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning. Advances in Neural Information Processing\\nSystems, 34:29335–29347, 2021.\\n[63] Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan.\\nTricks for training sparse translation models. arXiv preprint arXiv:2110.08246, 2021.\\n[64] Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez, Damien Jose,\\nAhmed Hassan Awadallah, and Jianfeng Gao. Sparsely activated mixture-of-experts are robust\\nmulti-task learners. ArXiv, abs/2204.07689, 2022.\\n[65] Duyu Tang, Fan Zhang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi. One model,\\nmultiple tasks: Pathways for natural language understanding. ArXiv, abs/2203.03312, 2022.\\n[66] Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, and\\nMikel Artetxe. Lifting the curse of multilinguality by pre-training modular transformers. In\\nNAACL, 2022.\\n[67] Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. Mad-x: An adapter-based\\nframework for multi-task cross-lingual transfer. In EMNLP, 2020.\\n[68] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.\\narXiv preprint arXiv:2209.01667, 2022.\\n13\\n[69] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer,\\nand William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906,\\n2022.\\n[70] Ping Yu, Mikel Artetxe, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, and Xian Li.\\nEfficient language modeling with sparse all-mlp. arXiv preprint arXiv:2203.06850, 2022.\\n[71] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai,\\nZhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing.\\narXiv preprint arXiv:2202.09368, 2022.\\n[72] Wouter Kool, Chris J Maddison, and Andriy Mnih. Unbiased gradient estimation with balanced\\nassignments for mixtures of experts. arXiv preprint arXiv:2109.11817, 2021.\\n[73] James Lee-Thorp and Joshua Ainslie. Sparse mixers: Combining moe and mixing to build a\\nmore efficient bert. arXiv preprint arXiv:2205.12399, 2022.\\n[74] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication:\\nTransformer feed-forward layers are mixtures of experts. In Findings of the Association for\\nComputational Linguistics: ACL 2022, pages 877–890, 2022.\\n[75] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa,\\nJoshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training\\nmixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022.\\n[76] George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein.\\nRebar: Low-variance, unbiased gradient estimates for discrete latent variable models. Advances\\nin Neural Information Processing Systems, 30, 2017.\\n[77] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropa-\\ngation through the void: Optimizing control variates for black-box gradient estimation. arXiv\\npreprint arXiv:1711.00123, 2017.\\n[78] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for\\nfree! 2019.\\n[79] Mingzhang Yin and Mingyuan Zhou. Arm: Augment-reinforce-merge gradient for stochastic\\nbinary networks. arXiv preprint arXiv:1807.11143, 2018.\\n[80] Zhe Dong, Andriy Mnih, and George Tucker. Disarm: An antithetic gradient estimator for\\nbinary latent variables. Advances in neural information processing systems, 33:18637–18647,\\n2020.\\n[81] Zhe Dong, Andriy Mnih, and George Tucker. Coupled gradient estimators for discrete latent\\nvariables. Advances in Neural Information Processing Systems, 34:24498–24508, 2021.\\n[82] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,\\nPayal Bajaj, Xia Song, and Furu Wei. On the representation collapse of sparse mixture of\\nexperts. arXiv preprint arXiv:2204.09179, 2022.\\n[83] Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei.\\nStablemoe: Stable routing strategy for mixture of experts. arXiv preprint arXiv:2204.08396,\\n2022.\\n[84] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi,\\nSimon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by\\ninterpolating weights. arXiv preprint arXiv:2208.05592, 2022.\\n[85] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally\\nparameterized convolutions for efficient inference. Advances in Neural Information Processing\\nSystems, 32, 2019.\\n14\\n[86] Mingda Zhang, Chun-Te Chu, Andrey Zhmoginov, Andrew Howard, Brendan Jou, Yukun\\nZhu, Li Zhang, Rebecca Hwa, and Adriana Kovashka. Basisnet: Two-stage model synthesis\\nfor efficient inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 3081–3090, 2021.\\n[87] Mitchell Wortsman, Suchin Gururangan, Shen Li, Ali Farhadi, Ludwig Schmidt, Michael\\nRabbat, and Ari S Morcos. lo-fi: distributed fine-tuning without communication. arXiv preprint\\narXiv:2210.11948, 2022.\\n[88] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and\\nLuke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language\\nmodels. ArXiv, abs/2208.03306, 2022.\\n[89] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion\\nby merging weights of language models. arXiv preprint arXiv:2212.09849, 2022.\\n[90] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\\nmodulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022.\\n[91] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\\nColin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\\nlearning. arXiv preprint arXiv:2205.05638, 2022.\\n[92] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021.\\n15\\nA\\nCompute resources used\\nHere we provide details on the compute resources used in our experiments. All models were trained\\non 48GB A6000s, except for the Ensemble method, which was trained on 80GB A100s. The training\\ntime for each T5-GLUE experiment was approximately 108 hours while each ResNet-DomainNet\\nexperiment required approximately 11 hours of training.\\nB\\nExperiment Details\\nWe provide details on the experimental setup and hyperparameter choices for the T5-GLUE and\\nResNet-DomainNet experiments described in the main text. We implemented Adamix [27] and ran\\nwith the hyperparameters listed in the below subsections. In Latent Skills [28], we use Adapters\\nconsistent with all other experiments and chose learning rate ratio of 10 for skill matrix, which we\\nfound to be best after sweeping for {1, 10, 100} in both the settings. For exact implementation details\\nof above methods, we refer the reader to each of the respective works.\\nIt is also important to mention that SMEAR’s memory footprint is comparable to that of other\\nmethods. Since each expert’s size is moderate, we compute expert outputs by preparing an expert\\nfor each example and applying them to the examples in parallel. Thus, none of the methods need an\\nextra weight-sized tensor. Training on T5-GLUE requires around 30GB of memory for SMEAR and\\nother methods, with no significant differences observed in ResNet-DomainNet.\\nB.1\\nT5-GLUE\\nIn the T5-GLUE experiments, We concatenated all 8 datasets of GLUE and perform multitask training.\\nT5 models were trained for 600k steps using a learning rate of 3e−4, with 2k warmup steps, and\\nbatch size of 128. The AdamW optimizer was used with its default settings. We ran the ST-Gumbel\\nestimator with a τ value of 10 and an anneal rate of 1e−6 by sweeping τ in the range of {1, 10} and\\nthe anneal rate in the range of {1e−4, 1e−5, 1e−6}. For the REINFORCE estimator in Equation 1,\\nwe used the same values as in [17], α = 1e−2, β = 5e−4, and γ = 1e−2. The adapters here use swish\\nnon-linearity in between.\\nB.2\\nResNet-DomainNet\\nIn the ResNet-DomainNet experiments, all the domains from DomainNet were concatenated to\\nperform multitask training similar to T5-GLUE. ResNet models were trained for 100k steps with\\nbatch size of 128 and a learning rate of 1e−3, with no warm up, using Adam optimizer. We used\\nτ value of 10 and anneal rate of 1e−4 for the ST-Gumbel estimator by sweeping τ in the range of\\n{1, 10} and the anneal rate in the range of {1e−4, 1e−5, 1e−6}. The values of α, β, and γ for the\\nREINFORCE estimators in Equation 1 are same as in T5-GLUE experiments. The adapters also used\\nswish non-linearity in between.\\nC\\nExpert dropout\\nTable 1 illustrates the impact of expert dropout on methods that do not make use of metadata but\\nlearn adaptive routing, namely methods that are trained through gradient estimation and SMEAR.\\nWe conduct this ablation on a single seed to limit the amount of computation. The results show that\\nSMEAR benefits from an improvement of 2.9% on T5-GLUE, and Top-k achieves a 1.2% and 0.2%\\nimprovement on T5-GLUE and ResNet-DomainNet respectively. As a result, we include expert\\ndropout for these two methods when discussed in the main text. However, expert dropout has a\\nnegative impact on the performance of ST-Gumbel and REINFORCE methods, and thus, we exclude\\nit for these two methods in the main text.\\nD\\nLicense\\nT5 is licensed under Apache 2.0. The ResNet model we used is licensed under BSD 3-Clause License.\\nQNLI uses CC BY-SA 4.0 license. MultiNLI uses data sources of multiple different licenses[44].\\n16\\nRouting\\nT5-GLUE\\nResNet-DomainNet\\nTop-k\\n77.2\\n59.8\\nw/ Expert dropout 0.1\\n78.4 (+1.2)\\n60.0 (+0.2)\\nST-Gumbel\\n78.3\\n58.3\\nw/ Expert dropout 0.1\\n77.2 (-1.1)\\n57.9 (-0.4)\\nREINFORCE\\n79.8\\n59.8\\nw/ Expert dropout 0.1\\n78.2 (-1.6)\\n59.8 (+0.0)\\nSMEAR\\n80.2\\n62.0\\nw/ Expert dropout 0.1\\n83.1 (+2.9)\\n62.0 (+0.0)\\nTable 1: Performance comparision of different adaptive routing methods w and w/o dropout on a\\nsingle seed. The results indicate that SMEAR and Top-k method benefit from the expert dropout,\\nwhile ST-Gumbel and REINFORCE are negatively affected.\\nCoLA, SST-2, RTE, MRPC, STS-B, QQP, and DomainNet allow non-commercial research use cases.\\nOur code for T5-GLUE is based on Hyperformer[52], which is shared under Apache 2.0. The code\\nfor ResNet-DomainNet is developed by us.\\nE\\nSocietal Impacts Statement\\nWe are not aware of any negative ethical implications of our work. Our work does not involve human\\nsubjects and is primarily focused on diagnosing issues with an efficient class of neural networks.\\nWhile conditional computation has been used to design extremely large neural networks [14, 15, 23]\\nthat have high computational costs (and, correspondingly, energy usage), our work primarily focuses\\non smaller-scale models.\\nF\\nFull results on T5-GLUE and ResNet-DomainNet\\nWe show the full results of T5-GLUE in table 2 and ResNet-DomainNet in table 3.\\nG\\nRouting distribution in all routing blocks\\nHere we put the routing distribution in all routing blocks in both T5-GLUE and ResNet-DomainNet\\nlearnt by SMEAR in fig. 4, fig. 5, fig. 6, and fig. 7.\\nClipart\\nInfograph\\nPainting\\nQuickdraw\\nReal\\nSketch\\n0.19\\n0.17\\n0.16\\n0.11\\n0.14\\n0.23\\n0.23\\n0.14\\n0.13\\n0.12\\n0.13\\n0.26\\n0.18\\n0.12\\n0.12\\n0.15\\n0.14\\n0.3\\n0.1\\n0.35\\n0.28\\n0.08\\n0.1\\n0.1\\n0.16\\n0.12\\n0.13\\n0.14\\n0.16\\n0.29\\n0.19\\n0.2\\n0.14\\n0.19\\n0.12\\n0.15\\nExpert Routing Block 1\\n0.12\\n0.1\\n0.28\\n0.16\\n0.17\\n0.16\\n0.08\\n0.16\\n0.34\\n0.13\\n0.14\\n0.14\\n0.12\\n0.25\\n0.21\\n0.11\\n0.14\\n0.18\\n0.16\\n0.03\\n0.07\\n0.18\\n0.5\\n0.05\\n0.12\\n0.21\\n0.21\\n0.12\\n0.14\\n0.19\\n0.11\\n0.14\\n0.21\\n0.18\\n0.2\\n0.17\\nExpert Routing Block 2\\n0.23\\n0.15\\n0.17\\n0.16\\n0.16\\n0.13\\n0.12\\n0.12\\n0.16\\n0.11\\n0.39\\n0.11\\n0.13\\n0.13\\n0.27\\n0.15\\n0.17\\n0.14\\n0.08\\n0.5\\n0.04\\n0.2\\n0.06\\n0.11\\n0.13\\n0.15\\n0.29\\n0.14\\n0.16\\n0.12\\n0.15\\n0.12\\n0.16\\n0.12\\n0.16\\n0.29\\nExpert Routing Block 3\\n0.16\\n0.23\\n0.16\\n0.25\\n0.12\\n0.09\\n0.19\\n0.22\\n0.18\\n0.18\\n0.09\\n0.14\\n0.13\\n0.11\\n0.39\\n0.15\\n0.11\\n0.11\\n0.61\\n0.04\\n0.04\\n0.06\\n0.18\\n0.07\\n0.13\\n0.14\\n0.34\\n0.13\\n0.14\\n0.12\\n0.17\\n0.14\\n0.18\\n0.27\\n0.17\\n0.07\\nExpert Routing Block 4\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nClipart\\nInfograph\\nPainting\\nQuickdraw\\nReal\\nSketch\\n0.14\\n0.1\\n0.18\\n0.11\\n0.19\\n0.29\\n0.13\\n0.09\\n0.43\\n0.11\\n0.07\\n0.16\\n0.31\\n0.11\\n0.17\\n0.15\\n0.13\\n0.13\\n0.07\\n0.29\\n0.08\\n0.32\\n0.13\\n0.11\\n0.34\\n0.11\\n0.15\\n0.13\\n0.11\\n0.16\\n0.14\\n0.12\\n0.15\\n0.14\\n0.12\\n0.33\\nExpert Routing Block 5\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\n0.33\\n0.09\\n0.16\\n0.15\\n0.16\\n0.11\\n0.15\\n0.06\\n0.29\\n0.12\\n0.05\\n0.33\\n0.27\\n0.1\\n0.12\\n0.27\\n0.11\\n0.15\\n0.15\\n0.37\\n0.1\\n0.06\\n0.23\\n0.09\\n0.17\\n0.1\\n0.13\\n0.29\\n0.13\\n0.18\\n0.41\\n0.09\\n0.17\\n0.16\\n0.09\\n0.09\\nExpert Routing Block 6\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\n0.08\\n0.35\\n0.14\\n0.12\\n0.22\\n0.1\\n0.03\\n0.34\\n0.23\\n0.19\\n0.09\\n0.12\\n0.08\\n0.18\\n0.32\\n0.25\\n0.11\\n0.07\\n0.46\\n0.08\\n0.06\\n0.03\\n0.05\\n0.31\\n0.07\\n0.17\\n0.24\\n0.2\\n0.22\\n0.09\\n0.08\\n0.2\\n0.2\\n0.25\\n0.18\\n0.09\\nExpert Routing Block 7\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\n0.23\\n0.1\\n0.13\\n0.17\\n0.3\\n0.08\\n0.3\\n0.21\\n0.2\\n0.1\\n0.15\\n0.03\\n0.15\\n0.05\\n0.32\\n0.14\\n0.3\\n0.05\\n0.06\\n0.32\\n0.04\\n0.08\\n0.06\\n0.44\\n0.23\\n0.07\\n0.28\\n0.2\\n0.17\\n0.06\\n0.18\\n0.08\\n0.17\\n0.19\\n0.31\\n0.06\\nExpert Routing Block 8\\nFigure 4: Routing distribution learnt by SMEAR in all routing blocks of ResNet-DomainNet\\n17\\nRouting\\nRTE\\nSST-2\\nMRPC\\nMRPC\\nSTS-B\\nSTS-B\\nQQP\\nQQP\\nMNLI\\nQNLI\\nCoLA\\nAverage\\nacc\\nacc\\nf1\\nacc\\npearson\\nspearman\\nf1\\nacc\\nacc\\nacc\\nmcc\\nSMEAR\\n69.92.6\\n90.90.8\\n90.51.5\\n86.92.2\\n87.00.7\\n86.60.8\\n86.90.3\\n90.10.2\\n84.90.5\\n90.20.6\\n33.86.4\\n81.61.0\\n1× parameters\\n72.32.1\\n92.10.5\\n89.90.5\\n86.00.8\\n85.50.8\\n85.30.9\\n87.00.3\\n90.20.2\\n84.10.5\\n89.90.7\\n20.18.1\\n80.20.8\\n1× compute\\n67.33.3\\n91.90.2\\n89.22.7\\n85.53.4\\n87.40.8\\n87.30.7\\n85.60.3\\n89.20.2\\n84.50.7\\n89.90.5\\n5.13.7\\n78.41.1\\nAdamix\\n70.23.2\\n92.40.6\\n87.41.4\\n83.31.2\\n86.70.6\\n86.60.7\\n85.70.2\\n89.40.1\\n85.40.3\\n90.50.4\\n5.11.1\\n78.40.4\\nHash\\n58.82.7\\n85.61.3\\n77.72.6\\n68.53.2\\n65.42.4\\n65.21.8\\n76.80.2\\n82.80.3\\n72.00.9\\n80.00.5\\n2.92.5\\n66.90.9\\nTag\\n71.72.9\\n90.30.5\\n85.40.7\\n79.50.8\\n82.21.1\\n81.51.2\\n86.20.4\\n89.50.3\\n84.40.8\\n87.90.9\\n25.18.3\\n78.51.2\\nLatent Skills\\n70.44.6\\n90.80.8\\n90.01.0\\n85.82.1\\n86.61.5\\n86.31.4\\n86.40.4\\n89.80.3\\n84.90.9\\n89.31.4\\n30.85.5\\n81.01.6\\nTop-k\\n68.22.3\\n92.50.4\\n88.60.7\\n84.71.7\\n87.71.6\\n87.41.7\\n85.30.4\\n89.00.5\\n84.90.9\\n90.10.9\\n2.02.0\\n78.20.9\\nST-Gumbel\\n67.62.3\\n92.10.7\\n88.81.0\\n84.81.7\\n86.91.0\\n86.80.8\\n85.70.1\\n89.20.2\\n84.50.3\\n89.10.5\\n1.31.7\\n77.90.4\\nREINFORCE\\n70.93.3\\n92.60.5\\n89.81.6\\n86.02.1\\n87.40.6\\n87.20.5\\n86.10.3\\n89.50.2\\n85.80.4\\n90.80.7\\n14.16.9\\n80.00.8\\nEnsemble\\n72.91.6\\n91.50.4\\n90.91.4\\n87.71.4\\n85.71.5\\n85.11.6\\n86.80.3\\n90.10.2\\n84.70.5\\n89.80.6\\n33.76.1\\n81.71.0\\nSMEAR 2×\\n70.93.1\\n90.90.7\\n89.51.1\\n85.81.0\\n86.90.9\\n86.51.0\\n86.80.4\\n90.10.3\\n84.40.5\\n89.60.8\\n33.16.5\\n81.31.1\\nTable 2: Full T5-GLUE results.\\n18\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.04 0.59 0.01\\n0.19\\n0.0\\n0.06 0.01 0.08\\n0.01 0.03\\n0.0\\n0.0\\n0.0\\n0.93\\n0.0\\n0.02\\n0.06\\n0.4\\n0.04 0.17\\n0.0\\n0.26 0.01\\n0.07\\n0.04 0.23 0.12\\n0.2\\n0.0\\n0.34 0.01 0.06\\n0.19\\n0.01\\n0.13\\n0.12\\n0.3\\n0.01\\n0.22 0.01\\n0.06 0.26 0.03 0.04\\n0.0\\n0.02\\n0.0\\n0.58\\n0.02 0.66 0.02 0.21\\n0.0\\n0.03 0.02 0.05\\n0.01\\n0.73 0.02 0.01\\n0.0\\n0.11\\n0.0\\n0.12\\nExpert Routing Block 1\\n0.14 0.05 0.12\\n0.14 0.32\\n0.0\\n0.16 0.06\\n0.01\\n0.01\\n0.01\\n0.0\\n0.01 0.02\\n0.1\\n0.84\\n0.03 0.04 0.03 0.05 0.09 0.03\\n0.4\\n0.34\\n0.03 0.01 0.04 0.01 0.08 0.02 0.55 0.26\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01 0.69 0.18\\n0.11\\n0.4\\n0.03 0.52 0.01\\n0.01\\n0.0\\n0.01\\n0.01\\n0.03 0.02 0.08 0.03 0.64 0.01\\n0.17\\n0.03\\n0.02 0.01 0.05\\n0.0\\n0.03 0.02 0.26 0.61\\nExpert Routing Block 2\\n0.2\\n0.19\\n0.0\\n0.03 0.47 0.09 0.01 0.02\\n0.03 0.47 0.01 0.03 0.07 0.06 0.03 0.29\\n0.06 0.55\\n0.0\\n0.05 0.26 0.06\\n0.0\\n0.01\\n0.06 0.35 0.03 0.04 0.29 0.16 0.02 0.06\\n0.02 0.03 0.41 0.23 0.03 0.02 0.03 0.23\\n0.25 0.02\\n0.0\\n0.01\\n0.07 0.32 0.23\\n0.11\\n0.19 0.09 0.02 0.03 0.45 0.08 0.02\\n0.11\\n0.11\\n0.11\\n0.03 0.04 0.09\\n0.11\\n0.15 0.36\\nExpert Routing Block 3\\n0.07 0.37 0.43 0.01\\n0.0\\n0.09 0.02 0.01\\n0.05\\n0.1\\n0.11\\n0.04\\n0.1\\n0.07 0.04 0.48\\n0.03 0.34 0.51\\n0.01\\n0.01 0.05 0.04 0.02\\n0.08 0.34 0.27 0.04 0.07 0.07 0.06 0.06\\n0.01\\n0.1\\n0.04 0.01 0.34 0.01 0.44 0.04\\n0.34 0.01 0.08 0.21\\n0.01 0.33 0.01\\n0.01\\n0.01 0.54 0.37\\n0.0\\n0.0\\n0.04 0.03\\n0.0\\n0.18\\n0.15 0.09\\n0.1\\n0.13\\n0.11\\n0.05 0.19\\nExpert Routing Block 4\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.03\\n0.0\\n0.51\\n0.18 0.03 0.01\\n0.14 0.09\\n0.01\\n0.0\\n0.01\\n0.0\\n0.96\\n0.0\\n0.01\\n0.01\\n0.0\\n0.0\\n0.0\\n0.96 0.01\\n0.0\\n0.01\\n0.01\\n0.01\\n0.0\\n0.0\\n0.98\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.71\\n0.0\\n0.0\\n0.0\\n0.29\\n0.0\\n0.0\\n0.23\\n0.0\\n0.0\\n0.12\\n0.01\\n0.01\\n0.37 0.26\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.95 0.01 0.02\\n0.0\\n0.0\\n0.02\\nExpert Routing Block 5\\n0.05 0.01\\n0.07\\n0.01\\n0.51\\n0.3\\n0.04 0.02\\n0.01 0.04 0.01\\n0.0\\n0.0\\n0.1\\n0.0\\n0.84\\n0.33 0.09 0.01 0.02\\n0.0\\n0.48 0.07\\n0.0\\n0.0\\n0.97\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.27 0.04 0.16 0.03\\n0.0\\n0.15 0.33 0.03\\n0.0\\n0.0\\n0.0\\n0.0\\n0.98 0.01\\n0.0\\n0.0\\n0.02 0.01\\n0.0\\n0.0\\n0.83\\n0.1\\n0.0\\n0.02\\nExpert Routing Block 6\\n0.16 0.02 0.03 0.15\\n0.25 0.28 0.05 0.06\\n0.88\\n0.0\\n0.0\\n0.05 0.02 0.01\\n0.01\\n0.01\\n0.01\\n0.0\\n0.01\\n0.0\\n0.85 0.07 0.02 0.03\\n0.02 0.29 0.13 0.04 0.05 0.06 0.04 0.37\\n0.01\\n0.15\\n0.28 0.05 0.13 0.02 0.34 0.03\\n0.01 0.08\\n0.1\\n0.14 0.04 0.37\\n0.01 0.25\\n0.42 0.02 0.01\\n0.07\\n0.21 0.24 0.01 0.02\\n0.81 0.03\\n0.0\\n0.07 0.02 0.01\\n0.01 0.05\\nExpert Routing Block 7\\n0.44 0.02\\n0.17\\n0.12 0.03 0.07\\n0.15 0.02\\n0.01\\n0.0\\n0.0\\n0.08 0.87\\n0.01 0.02\\n0.0\\n0.01\\n0.0\\n0.0\\n0.94 0.02 0.01 0.02\\n0.0\\n0.02 0.91\\n0.01\\n0.01\\n0.0\\n0.01\\n0.0\\n0.03\\n0.01\\n0.0\\n0.87\\n0.01\\n0.0\\n0.11\\n0.0\\n0.0\\n0.25 0.27 0.06 0.05\\n0.0\\n0.14 0.04 0.19\\n0.5\\n0.01 0.34 0.07\\n0.0\\n0.02 0.04 0.01\\n0.05\\n0.0\\n0.02 0.05 0.86 0.01\\n0.01\\n0.0\\nExpert Routing Block 8\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.0\\n0.77\\n0.0\\n0.01\\n0.21\\n0.0\\n0.0\\n0.0\\n0.97\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.01\\n0.97\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.01 0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.2\\n0.6\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.24\\n0.0\\n0.0\\n0.14\\n0.27 0.35\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 9\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.99\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\nExpert Routing Block 10\\n0.02 0.04 0.07 0.03 0.01\\n0.76 0.03 0.02\\n0.46 0.38 0.01\\n0.0\\n0.12\\n0.01\\n0.01\\n0.01\\n0.05 0.01\\n0.61 0.03 0.02 0.26 0.01 0.02\\n0.32 0.06 0.13 0.25 0.05 0.05 0.06 0.08\\n0.01\\n0.01 0.52\\n0.0\\n0.01\\n0.37 0.08\\n0.0\\n0.11\\n0.08 0.05\\n0.2\\n0.09 0.02 0.12 0.32\\n0.01\\n0.01 0.03 0.02 0.01\\n0.9\\n0.02 0.01\\n0.01\\n0.72 0.02\\n0.0\\n0.05 0.15 0.03 0.03\\nExpert Routing Block 11\\n0.0\\n0.04\\n0.0\\n0.62\\n0.0\\n0.0\\n0.34\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.14 0.06\\n0.0\\n0.8\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.98\\nExpert Routing Block 12\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.05\\n0.1\\n0.1\\n0.15 0.29 0.05 0.12\\n0.14\\n0.01\\n0.01\\n0.01 0.94 0.01\\n0.01 0.02 0.01\\n0.0\\n0.16\\n0.19\\n0.21\\n0.12\\n0.07\\n0.01 0.26\\n0.04 0.05 0.04 0.15 0.05 0.22 0.31\\n0.14\\n0.01\\n0.01\\n0.01 0.02 0.07 0.43 0.33 0.12\\n0.13\\n0.2\\n0.18\\n0.13\\n0.17\\n0.05 0.02 0.13\\n0.06 0.09 0.03 0.23 0.23 0.05 0.16\\n0.15\\n0.0\\n0.0\\n0.01 0.96 0.02\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 13\\n0.0\\n0.2\\n0.0\\n0.0\\n0.03\\n0.0\\n0.78\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.98\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.77 0.03\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.92\\n0.0\\n0.0\\n0.0\\n0.0\\n0.08\\n0.0\\nExpert Routing Block 14\\n0.04 0.07\\n0.4\\n0.06\\n0.17\\n0.06\\n0.1\\n0.09\\n0.0\\n0.04 0.02 0.02\\n0.0\\n0.0\\n0.92\\n0.0\\n0.0\\n0.01 0.05 0.65 0.05\\n0.0\\n0.01\\n0.22\\n0.01\\n0.01\\n0.27\\n0.13 0.03 0.01 0.48 0.07\\n0.0\\n0.0\\n0.86 0.05 0.01\\n0.01 0.06 0.01\\n0.12\\n0.16\\n0.01 0.03\\n0.2\\n0.18 0.02 0.28\\n0.01\\n0.01 0.92 0.02 0.02 0.01\\n0.0\\n0.02\\n0.0\\n0.85 0.02 0.01\\n0.0\\n0.0\\n0.01\\n0.1\\nExpert Routing Block 15\\n0.0\\n0.04\\n0.0\\n0.0\\n0.95 0.02\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.95 0.01\\n0.0\\n0.02\\n0.0\\n0.01\\n0.0\\n0.0\\n0.8\\n0.04 0.15\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.99 0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.03 0.96\\n0.0\\n0.0\\nExpert Routing Block 16\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.11\\n0.02\\n0.11\\n0.03 0.35 0.05\\n0.1\\n0.21\\n0.02 0.34 0.51\\n0.01 0.02 0.06 0.03 0.01\\n0.05 0.01 0.05 0.01\\n0.01\\n0.01 0.63 0.24\\n0.52 0.05 0.07\\n0.01\\n0.13 0.08 0.01\\n0.12\\n0.13 0.02 0.01\\n0.0\\n0.35 0.16\\n0.0\\n0.33\\n0.1\\n0.15\\n0.17\\n0.13 0.03 0.13\\n0.07 0.24\\n0.01\\n0.0\\n0.02 0.01 0.58 0.04 0.03\\n0.3\\n0.01 0.02 0.06 0.01\\n0.78 0.09 0.03 0.01\\nExpert Routing Block 17\\n0.0\\n0.01\\n0.2\\n0.02\\n0.0\\n0.76\\n0.0\\n0.0\\n0.0\\n0.0\\n0.97\\n0.01\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.94 0.03\\n0.0\\n0.01\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.62\\n0.0\\n0.37\\n0.0\\n0.0\\n0.0\\n0.1\\n0.0\\n0.0\\n0.62 0.01 0.03 0.22\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.01\\n0.0\\n0.99\\n0.0\\n0.0\\nExpert Routing Block 18\\n0.0\\n0.01\\n0.0\\n0.87\\n0.0\\n0.12\\n0.0\\n0.0\\n0.0\\n0.82\\n0.0\\n0.07\\n0.0\\n0.11\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.91\\n0.0\\n0.09\\n0.0\\n0.0\\n0.0\\n0.05 0.01 0.92\\n0.0\\n0.01\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.08\\n0.0\\n0.03\\n0.0\\n0.88\\n0.0\\n0.0\\nExpert Routing Block 19\\n0.01\\n0.0\\n0.08\\n0.0\\n0.0\\n0.89\\n0.0\\n0.01\\n0.0\\n0.0\\n0.96\\n0.0\\n0.0\\n0.04\\n0.0\\n0.01\\n0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.92 0.07\\n0.0\\n0.03\\n0.0\\n0.0\\n0.0\\n0.0\\n0.52 0.38 0.07\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.18\\n0.0\\n0.0\\n0.81\\n0.0\\n0.0\\nExpert Routing Block 20\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.0\\n0.0\\n0.01 0.06 0.09\\n0.0\\n0.83 0.01\\n0.0\\n0.0\\n0.01\\n0.01\\n0.97\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.46\\n0.0\\n0.0\\n0.36 0.18\\n0.0\\n0.01 0.26 0.22\\n0.3\\n0.01\\n0.21\\n0.0\\n0.0\\n0.0\\n0.14\\n0.2\\n0.0\\n0.0\\n0.62 0.04\\n0.02 0.03 0.19 0.46\\n0.0\\n0.01\\n0.3\\n0.0\\n0.0\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.96\\n0.0\\n0.0\\n0.0\\n0.02 0.01 0.95\\n0.0\\n0.02\\n0.0\\nExpert Routing Block 21\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.0\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.94 0.03\\n0.0\\n0.0\\n0.0\\n0.0\\n0.03\\n0.0\\n0.14\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.86\\n0.0\\n0.71\\n0.01\\n0.0\\n0.0\\n0.01\\n0.0\\n0.27\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\nExpert Routing Block 22\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.0\\n0.0\\n0.74\\n0.0\\n0.0\\n0.0\\n0.26\\n0.0\\n0.0\\n0.4\\n0.57\\n0.0\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.0\\n0.22\\n0.0\\n0.0\\n0.0\\n0.77\\n0.0\\n0.0\\n0.0\\n0.87\\n0.0\\n0.0\\n0.0\\n0.13\\n0.0\\n0.0\\n0.0\\n0.75\\n0.0\\n0.0\\n0.0\\n0.25\\n0.0\\n0.0\\n0.0\\n0.98\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.0\\n0.0\\n0.74\\n0.0\\n0.0\\n0.0\\n0.26\\n0.0\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.48 0.49\\n0.0\\nExpert Routing Block 23\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.74\\n0.0\\n0.0\\n0.0\\n0.04 0.01\\n0.21\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.96\\n0.0\\n0.03\\n0.0\\n0.77\\n0.0\\n0.0\\n0.0\\n0.17\\n0.0\\n0.05\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.01 0.98\\n0.0\\n0.6\\n0.0\\n0.2\\n0.0\\n0.0\\n0.19\\n0.01\\n0.0\\n0.0\\n0.01\\n0.0\\n0.01\\n0.01 0.89 0.07 0.02\\n0.96\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.04\\n0.0\\n0.08\\n0.0\\n0.0\\n0.0\\n0.59\\n0.2\\n0.11\\n0.02\\nExpert Routing Block 24\\nFigure 5: Routing distribution learnt by SMEAR in the encoder routing blocks (1-24) of T5-GLUE\\n19\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.25 0.06 0.02 0.05 0.03 0.34 0.22 0.05\\n0.02 0.07 0.02\\n0.3\\n0.1\\n0.31 0.06 0.14\\n0.03 0.09 0.02 0.08 0.06 0.57\\n0.1\\n0.05\\n0.26\\n0.21\\n0.28 0.06 0.02 0.08 0.04 0.04\\n0.03 0.04 0.02 0.54 0.18\\n0.12 0.05 0.03\\n0.06 0.14\\n0.16\\n0.17\\n0.06 0.16\\n0.1\\n0.15\\n0.12 0.04 0.02 0.27 0.04 0.33 0.13 0.04\\n0.05 0.05 0.01\\n0.12 0.04 0.03 0.24 0.45\\nExpert Routing Block 25\\n0.01 0.03 0.02 0.01\\n0.19 0.06 0.63 0.05\\n0.0\\n0.17\\n0.02 0.01 0.56 0.03 0.02 0.18\\n0.02 0.01 0.04 0.01 0.03 0.01 0.85 0.04\\n0.0\\n0.25\\n0.15\\n0.11\\n0.0\\n0.04 0.06 0.37\\n0.02 0.36 0.01\\n0.01\\n0.3\\n0.17\\n0.07 0.07\\n0.13 0.08 0.41\\n0.18\\n0.1\\n0.01 0.03 0.06\\n0.02 0.09 0.03 0.02 0.39 0.02 0.41\\n0.01\\n0.0\\n0.01\\n0.01\\n0.0\\n0.39 0.02 0.14 0.43\\nExpert Routing Block 26\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.0\\n0.79\\n0.0\\n0.01\\n0.18\\n0.0\\n0.01\\n0.0\\n0.21\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.79\\n0.9\\n0.0\\n0.01\\n0.0\\n0.08\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.2\\n0.2\\n0.0\\n0.0\\n0.6\\n0.0\\n0.0\\n0.01\\n0.0\\n0.53 0.14 0.32\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.2\\n0.0\\n0.6\\nExpert Routing Block 27\\n0.23\\n0.0\\n0.76\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.97\\n0.01\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.27\\n0.01\\n0.67\\n0.0\\n0.05\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.25\\n0.0\\n0.73\\n0.01\\n0.0\\n0.13\\n0.0\\n0.33\\n0.0\\n0.34\\n0.0\\n0.0\\n0.2\\n0.0\\n0.08\\n0.0\\n0.0\\n0.0\\n0.06 0.19 0.66\\n0.01\\n0.0\\n0.91\\n0.0\\n0.09\\n0.0\\n0.0\\n0.0\\n0.08\\n0.2\\n0.51\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\nExpert Routing Block 28\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.05 0.01\\n0.01 0.65 0.03 0.02\\n0.2\\n0.03\\n0.01\\n0.01\\n0.0\\n0.92 0.01 0.03 0.01\\n0.01\\n0.01\\n0.0\\n0.01\\n0.67\\n0.01 0.02 0.27\\n0.01\\n0.11\\n0.14 0.05\\n0.0\\n0.11\\n0.21\\n0.37\\n0.01\\n0.08 0.13\\n0.01 0.59 0.13\\n0.01\\n0.0\\n0.05\\n0.02 0.23\\n0.1\\n0.0\\n0.5\\n0.07\\n0.01\\n0.07\\n0.03\\n0.0\\n0.0\\n0.95 0.01\\n0.0\\n0.01\\n0.0\\n0.05 0.13\\n0.01 0.46 0.07\\n0.0\\n0.27\\n0.0\\nExpert Routing Block 29\\n0.02 0.01\\n0.0\\n0.77\\n0.0\\n0.19\\n0.0\\n0.0\\n0.91 0.08\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.19 0.02\\n0.0\\n0.74 0.03 0.02\\n0.0\\n0.0\\n0.11\\n0.28\\n0.0\\n0.29 0.29\\n0.0\\n0.03\\n0.0\\n0.0\\n0.0\\n0.0\\n0.36 0.32\\n0.2\\n0.11\\n0.0\\n0.21\\n0.11\\n0.05 0.01\\n0.16\\n0.0\\n0.41 0.06\\n0.01\\n0.0\\n0.01\\n0.8\\n0.18\\n0.0\\n0.0\\n0.0\\n0.21\\n0.01\\n0.0\\n0.59\\n0.0\\n0.0\\n0.19\\n0.0\\nExpert Routing Block 30\\n0.0\\n0.0\\n0.0\\n0.79\\n0.0\\n0.0\\n0.0\\n0.2\\n0.58 0.01\\n0.21\\n0.0\\n0.0\\n0.18\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.09 0.86\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.0\\n0.07\\n0.01\\n0.0\\n0.6\\n0.0\\n0.2\\n0.11\\n0.0\\n0.17\\n0.07\\n0.2\\n0.0\\n0.13\\n0.0\\n0.43\\n0.0\\n0.0\\n0.0\\n0.0\\n0.98\\n0.0\\n0.0\\n0.0\\n0.02\\n0.06\\n0.0\\n0.0\\n0.8\\n0.0\\n0.0\\n0.14\\n0.0\\nExpert Routing Block 31\\n0.0\\n0.09 0.01\\n0.12\\n0.0\\n0.08 0.69\\n0.0\\n0.0\\n0.6\\n0.0\\n0.01\\n0.0\\n0.13\\n0.19\\n0.07\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.19\\n0.8\\n0.0\\n0.0\\n0.27 0.23\\n0.5\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.6\\n0.2\\n0.26\\n0.0\\n0.07\\n0.0\\n0.0\\n0.0\\n0.0\\n0.67\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.0\\n0.2\\n0.56\\n0.2\\nExpert Routing Block 32\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.14\\n0.0\\n0.0\\n0.18\\n0.0\\n0.68\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.06\\n0.0\\n0.94\\n0.0\\n0.0\\n0.07\\n0.0\\n0.03 0.04\\n0.0\\n0.87\\n0.0\\n0.0\\n0.5\\n0.0\\n0.02\\n0.3\\n0.12\\n0.07\\n0.0\\n0.0\\n0.45\\n0.2\\n0.0\\n0.16\\n0.0\\n0.19\\n0.0\\n0.0\\n0.0\\n0.55\\n0.2\\n0.19\\n0.0\\n0.04\\n0.0\\n0.02\\n0.29\\n0.0\\n0.0\\n0.15\\n0.0\\n0.56\\n0.0\\n0.0\\n0.0\\n0.2\\n0.16\\n0.0\\n0.0\\n0.62 0.01\\n0.0\\nExpert Routing Block 33\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.96\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.0\\n0.01\\n0.0\\n0.19\\n0.0\\n0.96\\n0.0\\n0.01\\n0.0\\n0.01\\n0.0\\n0.0\\n0.01\\n0.0\\n0.6\\n0.14\\n0.0\\n0.0\\n0.0\\n0.2\\n0.06\\n0.06\\n0.0\\n0.3\\n0.07\\n0.0\\n0.33 0.01 0.24\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.6\\n0.2\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 34\\n0.0\\n0.2\\n0.77\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.0\\n0.03\\n0.17\\n0.0\\n0.0\\n0.0\\n0.79\\n0.01\\n0.0\\n0.17\\n0.82\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.63\\n0.0\\n0.0\\n0.0\\n0.0\\n0.37\\n0.0\\n0.2\\n0.0\\n0.6\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.07 0.07\\n0.0\\n0.13 0.53\\n0.0\\n0.0\\n0.2\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.58\\n0.0\\n0.2\\n0.0\\n0.22\\n0.0\\nExpert Routing Block 35\\n0.34\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.66\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.3\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.7\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.59\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.24\\n0.0\\n0.0\\n0.76\\n0.02\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.98\\n0.0\\n0.0\\n0.0\\n0.0\\n0.19\\n0.0\\n0.0\\n0.6\\n0.2\\nExpert Routing Block 36\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.0\\n0.27\\n0.1\\n0.05\\n0.0\\n0.57\\n0.0\\n0.0\\n0.0\\n0.03 0.07\\n0.0\\n0.88\\n0.0\\n0.02\\n0.0\\n0.0\\n0.02 0.21 0.49\\n0.0\\n0.28\\n0.0\\n0.0\\n0.0\\n0.02 0.98\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.12\\n0.18 0.69\\n0.0\\n0.0\\n0.0\\n0.33 0.26\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n0.0\\n0.01\\n0.01 0.24\\n0.0\\n0.74\\n0.0\\n0.0\\n0.0\\n0.32 0.01 0.35\\n0.0\\n0.31\\n0.0\\n0.0\\nExpert Routing Block 37\\n0.01\\n0.0\\n0.32\\n0.5\\n0.0\\n0.0\\n0.16\\n0.01\\n0.0\\n0.0\\n0.83\\n0.17\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.17\\n0.12\\n0.41\\n0.0\\n0.0\\n0.29\\n0.0\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.39\\n0.0\\n0.2\\n0.41\\n0.0\\n0.02\\n0.0\\n0.0\\n0.0\\n0.4\\n0.0\\n0.0\\n0.58\\n0.0\\n0.0\\n0.01\\n0.51\\n0.0\\n0.0\\n0.47\\n0.0\\n0.0\\n0.73 0.03 0.24\\n0.0\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 38\\n0.14 0.03\\n0.0\\n0.0\\n0.82\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.11\\n0.05\\n0.0\\n0.0\\n0.8\\n0.04\\n0.0\\n0.0\\n0.14\\n0.0\\n0.0\\n0.0\\n0.09\\n0.0\\n0.0\\n0.77\\n0.0\\n0.52\\n0.0\\n0.0\\n0.4\\n0.01\\n0.0\\n0.06\\n0.0\\n0.0\\n0.13\\n0.2\\n0.22\\n0.0\\n0.05 0.39\\n0.0\\n0.32\\n0.0\\n0.0\\n0.67\\n0.0\\n0.0\\n0.0\\n0.23\\n0.0\\n0.0\\n0.0\\n0.67\\n0.0\\n0.0\\n0.11\\nExpert Routing Block 39\\n0.0\\n0.14\\n0.0\\n0.58 0.05 0.22\\n0.0\\n0.0\\n0.18 0.04 0.01\\n0.75 0.02 0.01\\n0.0\\n0.0\\n0.0\\n0.12 0.02 0.52 0.01\\n0.19\\n0.14\\n0.0\\n0.01\\n0.01 0.66 0.02\\n0.0\\n0.3\\n0.0\\n0.0\\n0.0\\n0.02\\n0.0\\n0.39 0.01 0.58\\n0.0\\n0.0\\n0.01\\n0.6\\n0.0\\n0.0\\n0.13\\n0.0\\n0.19\\n0.07\\n0.0\\n0.01\\n0.0\\n0.5\\n0.0\\n0.49\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.78\\n0.0\\n0.0\\n0.01\\n0.0\\nExpert Routing Block 40\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.02\\n0.0\\n0.75\\n0.11\\n0.08 0.01\\n0.0\\n0.03\\n0.0\\n0.0\\n0.9\\n0.0\\n0.08\\n0.0\\n0.0\\n0.0\\n0.23\\n0.0\\n0.7\\n0.01 0.03\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.43\\n0.0\\n0.57\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.06\\n0.0\\n0.14\\n0.0\\n0.0\\n0.06\\n0.0\\n0.05\\n0.0\\n0.65\\n0.0\\n0.14\\n0.1\\n0.0\\n0.0\\n0.97\\n0.01\\n0.01\\n0.0\\n0.0\\n0.01\\n0.66\\n0.0\\n0.33 0.02\\n0.0\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 41\\n0.52 0.09 0.19\\n0.0\\n0.0\\n0.2\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.0\\n0.76\\n0.2\\n0.0\\n0.0\\n0.27 0.03 0.55\\n0.0\\n0.0\\n0.15\\n0.0\\n0.0\\n0.02\\n0.0\\n0.02\\n0.0\\n0.0\\n0.95\\n0.0\\n0.01\\n0.37\\n0.3\\n0.1\\n0.0\\n0.0\\n0.0\\n0.04 0.19\\n0.6\\n0.0\\n0.18\\n0.0\\n0.14\\n0.0\\n0.08\\n0.0\\n0.55 0.02 0.43\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.05 0.41\\n0.17\\n0.0\\n0.23\\n0.0\\n0.13\\n0.0\\nExpert Routing Block 42\\n0.13\\n0.07 0.28 0.13\\n0.01\\n0.14\\n0.2\\n0.04\\n0.02 0.38 0.01\\n0.0\\n0.01 0.42 0.15\\n0.0\\n0.11\\n0.08 0.12 0.03 0.01\\n0.17\\n0.45 0.04\\n0.1\\n0.4\\n0.12 0.06 0.05 0.13\\n0.1\\n0.03\\n0.35 0.13\\n0.07 0.05 0.09 0.06 0.24 0.02\\n0.21 0.02\\n0.1\\n0.07\\n0.18\\n0.18\\n0.15 0.09\\n0.31 0.05 0.06 0.05 0.05 0.08 0.36 0.04\\n0.07 0.25 0.05\\n0.17\\n0.02 0.35 0.05 0.04\\nExpert Routing Block 43\\n0.0\\n0.0\\n0.0\\n0.01 0.98\\n0.0\\n0.01\\n0.0\\n0.0\\n0.01\\n0.0\\n0.96\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.04\\n0.0\\n0.0\\n0.96\\n0.0\\n0.0\\n0.0\\n0.0\\n0.8\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.11\\n0.0\\n0.3\\n0.6\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.27 0.73\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.61\\n0.0\\n0.19\\n0.0\\nExpert Routing Block 44\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.89 0.02 0.01\\n0.07\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.99\\n0.9\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.0\\n0.07\\n0.0\\n0.0\\n0.02 0.02\\n0.0\\n0.09 0.07\\n0.8\\n0.29\\n0.0\\n0.0\\n0.71\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.37 0.06\\n0.0\\n0.28 0.13\\n0.16\\n0.0\\n0.58\\n0.0\\n0.0\\n0.41\\n0.0\\n0.0\\n0.0\\n0.0\\n0.96\\n0.0\\n0.0\\n0.03\\n0.0\\n0.0\\n0.0\\n0.01\\nExpert Routing Block 45\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.06 0.04 0.04 0.02 0.02 0.08 0.46 0.28\\n0.02 0.02 0.16 0.06 0.03 0.01 0.55 0.16\\n0.07 0.03 0.06 0.04 0.14 0.33 0.22\\n0.11\\n0.03 0.06 0.07\\n0.19 0.26\\n0.17\\n0.11\\n0.11\\n0.01 0.02 0.04 0.02 0.37 0.05 0.24 0.25\\n0.06 0.18 0.02 0.14 0.24\\n0.17\\n0.18 0.02\\n0.04 0.05 0.04 0.02\\n0.17\\n0.13 0.35 0.21\\n0.06 0.03 0.02\\n0.11\\n0.01\\n0.14 0.43 0.22\\nExpert Routing Block 46\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.0\\n0.65\\n0.2\\n0.0\\n0.0\\n0.0\\n0.14\\n0.0\\n0.57\\n0.0\\n0.21\\n0.0\\n0.0\\n0.0\\n0.02 0.19\\n0.0\\n0.15\\n0.85\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.93\\n0.0\\n0.07\\n0.0\\n0.0\\n0.0\\n0.0\\n0.65\\n0.0\\n0.0\\n0.35\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.2\\n0.6\\n0.0\\n0.0\\n0.99\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.79\\n0.2\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\nExpert Routing Block 47\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.79\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.84\\n0.0\\n0.16\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.38\\n0.0\\n0.0\\n0.14\\n0.0\\n0.06 0.42\\n0.0\\n0.0\\n0.02\\n0.0\\n0.0\\n0.14 0.06 0.59 0.18\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.5\\n0.0\\n0.32\\n0.0\\n0.0\\n0.0\\n0.18\\n0.0\\nExpert Routing Block 48\\nFigure 6: Routing distribution learnt by SMEAR in the decoder routing blocks (25-48) of T5-GLUE\\n20\\nRouting\\nClipart\\nInfograph\\nPainting\\nQuickdraw\\nReal\\nSketch\\nFinal Accuracy\\nSMEAR\\n64.20.1\\n31.20.3\\n57.80.3\\n62.30.1\\n74.30.1\\n56.00.2\\n62.00.1\\n1× parameters\\n63.30.3\\n29.80.3\\n56.40.3\\n61.50.1\\n72.90.1\\n54.90.4\\n60.80.1\\n1× compute\\n60.20.3\\n27.90.3\\n54.80.1\\n59.00.2\\n72.30.1\\n52.60.2\\n59.00.1\\nAdamix\\n58.90.2\\n27.00.2\\n54.10.2\\n57.20.3\\n72.10.1\\n51.20.2\\n58.00.2\\nHash\\n53.50.3\\n23.40.3\\n49.80.4\\n48.60.3\\n68.50.1\\n45.70.2\\n52.40.1\\nTag\\n62.80.4\\n30.20.3\\n58.00.2\\n61.70.2\\n74.10.1\\n55.10.3\\n61.40.1\\nLatent Skills\\n64.50.4\\n31.20.4\\n58.90.1\\n61.60.3\\n74.20.1\\n56.30.2\\n61.90.2\\nTop-k\\n61.60.2\\n29.60.2\\n55.80.4\\n60.20.3\\n73.00.2\\n53.50.1\\n60.00.1\\nST-Gumbel\\n59.90.3\\n27.60.4\\n54.50.3\\n58.10.5\\n72.10.2\\n51.90.4\\n58.50.2\\nREINFORCE\\n61.30.3\\n29.10.2\\n55.90.2\\n60.40.3\\n72.80.1\\n53.60.2\\n60.00.1\\nEnsemble\\n64.80.3\\n31.40.1\\n58.10.2\\n63.00.2\\n74.30.1\\n56.70.2\\n62.30.1\\nSMEAR 2×\\n65.30.1\\n31.80.1\\n58.40.5\\n63.30.3\\n74.90.2\\n57.30.1\\n62.80.1\\nTable 3: Full ResNet-DomainNet results.\\n21\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.02\\n0.01\\n0.82 0.03 0.02 0.09\\n0.01\\n0.01\\n0.44 0.02 0.04\\n0.01\\n0.01\\n0.04 0.36 0.07\\n0.01\\n0.0\\n0.58 0.04\\n0.11\\n0.07\\n0.18\\n0.0\\n0.43 0.02 0.06 0.05\\n0.13\\n0.07\\n0.11\\n0.13\\n0.1\\n0.0\\n0.42 0.08\\n0.0\\n0.15\\n0.0\\n0.25\\n0.04\\n0.11\\n0.0\\n0.29\\n0.15\\n0.18\\n0.03\\n0.2\\n0.01\\n0.0\\n0.78 0.08 0.02 0.07\\n0.0\\n0.04\\n0.02\\n0.01\\n0.52 0.07 0.06\\n0.2\\n0.13\\n0.0\\nExpert Routing Block 49\\n0.0\\n0.0\\n0.01\\n0.0\\n0.17\\n0.0\\n0.21\\n0.6\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.42\\n0.0\\n0.38\\n0.2\\n0.0\\n0.67\\n0.0\\n0.0\\n0.0\\n0.0\\n0.32\\n0.01\\n0.2\\n0.0\\n0.0\\n0.0\\n0.35\\n0.0\\n0.0\\n0.45\\n0.53\\n0.0\\n0.2\\n0.07\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.42\\n0.0\\n0.0\\n0.58\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.66\\n0.14\\nExpert Routing Block 50\\n0.1\\n0.01\\n0.0\\n0.69\\n0.0\\n0.0\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.22\\n0.16\\n0.58 0.03\\n0.41\\n0.1\\n0.0\\n0.37\\n0.0\\n0.0\\n0.0\\n0.11\\n0.0\\n0.0\\n0.0\\n0.0\\n0.16\\n0.0\\n0.84\\n0.0\\n0.26\\n0.0\\n0.0\\n0.74\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.18\\n0.06 0.53 0.06\\n0.13\\n0.0\\n0.05\\n0.31\\n0.0\\n0.0\\n0.69\\n0.0\\n0.0\\n0.0\\n0.0\\n0.06 0.22\\n0.0\\n0.71\\n0.0\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 51\\n0.03 0.29\\n0.01\\n0.18\\n0.15\\n0.31\\n0.01\\n0.02\\n0.12\\n0.42\\n0.19\\n0.11\\n0.01\\n0.12\\n0.02 0.02\\n0.02\\n0.15\\n0.01\\n0.08 0.45 0.26\\n0.01\\n0.01\\n0.05\\n0.01\\n0.08 0.03 0.08 0.62\\n0.12\\n0.03\\n0.01\\n0.12\\n0.01\\n0.12\\n0.24 0.49\\n0.01\\n0.01\\n0.16\\n0.03\\n0.11\\n0.13\\n0.16\\n0.11\\n0.21\\n0.08\\n0.01\\n0.15\\n0.0\\n0.11\\n0.36 0.36\\n0.0\\n0.01\\n0.08 0.36\\n0.15\\n0.19\\n0.11\\n0.04 0.04 0.03\\nExpert Routing Block 52\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.0\\n0.0\\n0.88\\n0.0\\n0.0\\n0.0\\n0.12\\n0.0\\n0.0\\n0.0\\n0.01\\n0.57\\n0.0\\n0.42\\n0.0\\n0.0\\n0.0\\n0.0\\n0.76\\n0.0\\n0.0\\n0.0\\n0.24\\n0.0\\n0.0\\n0.0\\n0.69 0.24\\n0.0\\n0.0\\n0.0\\n0.07\\n0.0\\n0.0\\n0.32\\n0.0\\n0.09\\n0.0\\n0.59\\n0.0\\n0.13\\n0.07\\n0.11\\n0.06\\n0.0\\n0.56\\n0.0\\n0.07\\n0.0\\n0.0\\n0.53\\n0.0\\n0.0\\n0.0\\n0.47\\n0.0\\n0.0\\n0.0\\n0.93\\n0.0\\n0.0\\n0.01\\n0.01\\n0.05\\nExpert Routing Block 53\\n0.38 0.03 0.03 0.09 0.05 0.04 0.02 0.35\\n0.03\\n0.17\\n0.02\\n0.01\\n0.01\\n0.47 0.27 0.02\\n0.13\\n0.06\\n0.01\\n0.09 0.22\\n0.11\\n0.14\\n0.24\\n0.1\\n0.25 0.03 0.07 0.05\\n0.19\\n0.27 0.03\\n0.18\\n0.33 0.02 0.07 0.04 0.09 0.03 0.24\\n0.02 0.03 0.22 0.33\\n0.13\\n0.06 0.03\\n0.18\\n0.27 0.08 0.03\\n0.15\\n0.07 0.04 0.03 0.34\\n0.4\\n0.0\\n0.01\\n0.01\\n0.04 0.04\\n0.16\\n0.34\\nExpert Routing Block 54\\n0.01\\n0.0\\n0.0\\n0.0\\n0.09\\n0.9\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.99\\n0.03\\n0.0\\n0.0\\n0.0\\n0.02 0.76\\n0.0\\n0.19\\n0.0\\n0.01\\n0.0\\n0.0\\n0.0\\n0.04\\n0.0\\n0.95\\n0.5\\n0.0\\n0.0\\n0.0\\n0.09\\n0.41\\n0.0\\n0.0\\n0.0\\n0.0\\n0.59\\n0.21\\n0.0\\n0.0\\n0.2\\n0.0\\n0.28\\n0.0\\n0.0\\n0.0\\n0.08 0.64\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.08\\n0.71\\n0.0\\n0.2\\nExpert Routing Block 55\\n0.0\\n0.64\\n0.0\\n0.2\\n0.0\\n0.14\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.01\\n0.64\\n0.0\\n0.2\\n0.0\\n0.15\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.31\\n0.2\\n0.0\\n0.0\\n0.29\\n0.0\\n0.0\\n0.0\\n0.0\\n0.67\\n0.0\\n0.07\\n0.0\\n0.07\\n0.2\\n0.0\\n0.49\\n0.0\\n0.0\\n0.0\\n0.51\\n0.0\\n0.0\\n0.0\\n0.59\\n0.2\\n0.2\\n0.0\\n0.0\\n0.0\\n0.0\\nExpert Routing Block 56\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\nRTE\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nCOLA\\n0.11\\n0.02 0.04 0.52 0.08\\n0.14\\n0.06 0.03\\n0.01\\n0.65 0.24 0.02\\n0.01\\n0.01\\n0.01\\n0.06\\n0.22 0.06\\n0.14\\n0.24\\n0.2\\n0.04 0.05 0.05\\n0.09\\n0.51\\n0.04\\n0.15\\n0.04 0.02\\n0.01\\n0.14\\n0.11\\n0.03\\n0.01\\n0.53 0.05 0.07 0.05\\n0.16\\n0.14\\n0.04\\n0.13\\n0.15\\n0.09\\n0.19\\n0.14\\n0.12\\n0.21\\n0.01\\n0.02 0.54 0.07 0.08 0.05 0.02\\n0.06 0.02 0.22\\n0.3\\n0.05 0.28 0.06\\n0.01\\nExpert Routing Block 57\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.01\\n0.03 0.28 0.44\\n0.01\\n0.01\\n0.2\\n0.01\\n0.33\\n0.01\\n0.14\\n0.08 0.08 0.26 0.09\\n0.01\\n0.03\\n0.17\\n0.15\\n0.23\\n0.11\\n0.01\\n0.29\\n0.01\\n0.18\\n0.07\\n0.19\\n0.04 0.07\\n0.31\\n0.09 0.04\\n0.06 0.06 0.38 0.35\\n0.0\\n0.02 0.09 0.04\\n0.06\\n0.12\\n0.08\\n0.12\\n0.18\\n0.07 0.23\\n0.12\\n0.0\\n0.1\\n0.23 0.42\\n0.01\\n0.0\\n0.23\\n0.01\\n0.02\\n0.01\\n0.07 0.53\\n0.18\\n0.01\\n0.17\\n0.01\\nExpert Routing Block 58\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.34\\n0.0\\n0.0\\n0.0\\n0.0\\n0.66\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.07\\n0.0\\n0.0\\n0.0\\n0.0\\n0.93\\n0.0\\n0.0\\n0.66\\n0.0\\n0.0\\n0.0\\n0.0\\n0.34\\n0.0\\n0.0\\n0.5\\n0.12\\n0.0\\n0.0\\n0.0\\n0.28\\n0.0\\n0.1\\n0.52\\n0.0\\n0.07\\n0.2\\n0.18\\n0.01\\n0.02\\n0.0\\n0.43\\n0.0\\n0.0\\n0.0\\n0.0\\n0.57\\n0.0\\n0.0\\n0.58\\n0.0\\n0.0\\n0.0\\n0.0\\n0.42\\n0.0\\n0.0\\nExpert Routing Block 59\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\nExpert 6\\nExpert 7\\nExpert 8\\n0.0\\n0.0\\n0.02\\n0.18\\n0.0\\n0.02 0.78\\n0.0\\n0.2\\n0.0\\n0.27\\n0.0\\n0.0\\n0.0\\n0.52\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.01\\n0.79\\n0.0\\n0.01\\n0.0\\n0.83\\n0.16\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n0.0\\n0.1\\n0.7\\n0.0\\n0.2\\n0.16\\n0.0\\n0.0\\n0.0\\n0.28\\n0.17\\n0.19\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.27 0.53\\n0.2\\nExpert Routing Block 60\\nFigure 7: Routing distribution learnt by SMEAR in the decoder routing blocks (49-60) of T5-GLUE\\n22\\n', 'source_name': 'Soft Merging of Experts with Adaptive Routing', 'source_url': 'https://arxiv.org/abs/2306.03745'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sparse_Upcycling_NOTES.pdf #31\n",
      "{'content': 'Sparse Upcycling: Training Mixture-of-Experts From Dense Checkpoints \\nMain Idea: the paper aims to provide an efficient way to train an MoE model from a dense \\ncheckpoint (a pre-trained dense transformer) to minimize training costs, that is, provide an MoE \\ntraining strategy that is cheaper than training from scratch. \\n- \\nThe paper shows that training a MoE from a dense checkpoint outperforms continued \\ndense training. \\n- \\nExpert-choice routing (with CF of 2) is generally used for the encoder and top-k (with k=2) \\nis used for the decoder. \\no The T5 encoder-decoder model is used as the dense checkpoint. \\n- \\nEach expert’s weights are initialized as the exact MLP of the dense checkpoint, and the \\nrouter needs to be trained from scratch. \\n- \\nThe layer-norm, attention, embedding and output layers are copied to the new model \\nfrom the dense checkpoint. \\nResults: \\n- \\nWhen continuing pre-training, the larger the training continues after the checkpoint, the \\nbigger the advantage obtained by the upcycle model vs a dense model. \\no The continued pre-training is referred to as sparse upcycling. \\n- \\nWhen sparse upcycling for language, there are two comparisons made: \\no Upcycle vs dense – upcycle performs better, with continued dense pre-training \\ngiving inconsistent results. \\no Upcycle vs MoE – upcycle generally performs better for small computational \\nbudgets. When enough computational budget is given (>100% of the initial pre-\\ntrained dense computational budget), MoE can catch up and perform better than \\nupcycled models. \\n- \\nSparse upcycling is also shown to perform better than warm starting (“dense upcycling”). \\nMy takeaways: \\n- \\nIt sounds like the approach studied takes T5 (encoder-decoder model) and stretches its \\nfeedforward layers horizontally (in other words, transforms them in MoE layers). All other \\nlayers remain static – assuming the sparse upcycling is only done on the new MoE layers \\nand routing mechanism, while other layers remain frozen during this process.  \\n- \\nThe main takeaway of this paper is that it indicates that with enough training computing \\nbudget, it is more efficient to train an MoE model than a dense one, and when not much \\ntraining computing budget is given, the best-performing approach is to train a sparse \\nupcycled model from a dense checkpoint. \\n \\n', 'source_name': 'Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Sparse_Upcycling_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MoE_meets_instruction_tuning_NOTES.pdf #32\n",
      "{'content': 'Mixture-of-Experts Meets Instruction Tuning: a Winning Combination for Large \\nLanguage Models \\nMain Idea: this study aims to measure the impact of instruction-tuning in MoE models compared \\nto its impact in dense models. \\nInstruction-tuning is related to fine-tuning as fine-tuning is training a pre-trained model on a \\nspecific task, while instruction-tuning consists of training a language model in a supervised \\nmanner to perform well in a dialogue setting. This means for the model to perform well on the \\ntask of predicting p(answer | question) instead of the pre-training objective of predicting p(word \\n| context). \\nThree different scenarios were evaluated: \\n- \\nDirect finetuning on individual tasks (no instruction tuning). \\n- \\nInstruction tuning followed by in-context learning (no direct fine-tuning) \\n- \\nInstruction tuning followed by further finetuning on individual tasks. \\nThe conclusion of this paper was that MoE models outperform dense models of equivalent \\ncomputational capacity on direct finetuning, but significantly outperform dense models on \\ninstruction tuning scenarios. Let’s understand how they reached this conclusion. \\n \\nSetup \\nTwo dense models were considered: T5 and PaLM. \\nFour MoE architectures were considered: \\n- \\nSwitch Transformers \\n- \\nGShard \\n- \\nExpert-Choice \\n- \\nST-MoE \\nAll instruction tuning was done using the FLAN dataset. \\n \\nResults \\n- \\nA base MoE architecture outperforms a dense architecture (T5) after instruction-tuning \\nacross all scales. \\n- \\nScaling the number of experts helps when fine-tuning on challenging tasks but saturates \\nwhen fine-tuning on easier tasks (more experts is not always better as it might confuse \\nthe gating algorithm). \\n- \\nAs expected, increasing k in top-k routing improves performance at an increase in the \\ninference cost. \\n- \\nOverperformance of MoE compared to dense models when instruction-tuning only \\nexacerbates with scale (the bigger the models, the bigger the performance gain of MoE \\nover dense). \\n- \\nExpert-choice outperforms GShard (token-choice) in an instruction-tuning scenario, \\nhowever, this difference is bridged by incorporating advanced auxiliary loss (router z-loss) \\nand pre-training strategy as employed in ST-MoE (also token-choice). \\n- \\nEven though FLAN-PaLM62B (dense instruction-tuned model) has 3x the number of FLOPs \\nper token than FLAN-ST32B (largest MoE instruction-tuned model trained for this work) \\nat inference, FLAN-ST32B significantly outperforms FLAN-PaLM (57.6 vs 63.6 average \\nscore). \\n- \\nDifferent auxiliary losses gave different results: \\no Z-loss worked better than balance-loss in FLAN-ST \\no Balance-loss worked better than z-loss in FLAN-EC \\n- \\nFreezing certain parts of the MoE layers during fine-tuning was evaluated to investigate \\nhow to prevent overfitting in MoE fine-tuning: \\no Freezing the gate led to small improvements. \\no Freezing any other areas resulted in worse performance. \\nMy takeaways: \\n- \\nFirst thought is that instruction-tuning should work better in dense models than in MoE \\nmodels based on the difficulties in obtaining good fine-tuning performance with MoE. \\nThis may not hold since the instruction-tuning process can be thought of a very specific \\ntype of fine-tuning. \\no This is shown to be false, as MoE significantly outperforms dense models when it \\ncomes to instruction-tuning. This is even more interesting when showed that this \\nadvantage of MoE over dense in the task of instruction-tuning only increases with \\nscale. \\no MoE results after instruction-tuning are quite promising. For some reason, MoE \\ncaptures the instruction-tuning task much more efficiently than dense models. \\n- \\nMore experts do not guarantee better performance with fine-tuning. In fact, on easier \\ntasks, more experts result in worse fine-tuning performance. \\no What was the size of the datasets used for fine-tuning? Perhaps easier tasks are \\nmore prone to overfitting, explaining the underperformance of fine-tuning MoE \\non easier datasets. If this was the case, these tasks would require more \\nregularization -> how much regularization to use might depend on the difficulty of \\nthe task. \\n▪ This makes sense to the overall MoE theory as easier tasks have less \\ncomplex data distributions The less complex data distribution will lead to \\nless of the experts being called consistently, causing them to overfit. In a \\ncomplex task, the data distribution will result in a more distributed load \\nbalancing due to more semantic/syntax patterns being in place, thus using \\nmore experts, preventing overfitting. \\no There might be router issues leading to this difficulty in fine-tuning on easier tasks \\nas well. \\n- \\nExpert-choice seems to be better than regular token-choice routing. However, ST-MoE, \\nwhich has improvements over traditional token-choice routing, surpasses expert-choice. \\no Why did Mixtral decide to not use Expert-Choice and seems to use a routing \\nstrategy that resembles GShard more, even though it underperforms both Expert \\nChoice and ST-MoE’s routing strategies? Maybe they started training before this \\npaper came out? (investigate if Mixtral’s routing strategy resembles more GShard \\nthan ST-MoE). \\n- \\nZ-loss is better for token-choice, but balance-loss is better for Expert-Choice? \\n- \\nThe routing learned during pre-training is thought to already have a good estimate of data \\ndistributions at a semantic and syntactic level, therefore more specialization is not \\nneeded during fine-tuning. The idea is that the semantics and syntax at fine-tuning \\ndomains are not new, what changes is their distribution. Therefore, the routing algorithm \\ndoes not to be updated -> gating/routing should be kept frozen during fine-tuning (this is \\nnot the first research work to come to this conclusion). \\n- \\nMoE models are prone to overfitting, so often underperform dense models on single-task \\nfine-tuning. MoE works better when scaling the number of tasks, that is, fine-tuning on \\nmore than just one domain. However, instruction-tuning seems to bring a reversal to this \\ntrend, with FLAN-MoE performing better than FLAN-T5 in single task fine-tuning.  \\no Perhaps a reason for this is how FLAN does not have a single task per-say, it instead \\nhas data from many different domains with the common aspect being the \\nstructure how it is presented (in a dialogue format). \\n \\n', 'source_name': 'Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Soft_Merging_of_Experts_NOTES.pdf #33\n",
      "{'content': 'Soft Merging of Experts with Adaptive Routing \\nMain Idea: develop a technique called SMEAR (Soft Merging of Experts with Adaptive Routing) – \\nsingle merged expert constructed via a weighted average of all the experts’ parameters - to \\naddress the non-differentiability issue of discrete routing in MoE, hypothesizing that this lack of \\ndifferentiability is what causes instabilities and underperformance in MoE. \\nPast research points that stable task/domain-level learned experts are possible (like in the DEMix \\nline of work), but this is harder to achieve at the token-level. A few works showing the challenges \\nof learned MoE at the token-level: \\n- \\nHash layer (random routing based on a fixed heuristic) achieves comparable results \\nthrough a fixed random strategy. \\n- \\nSwitch and the Scaling Laws paper find that increasing the active parameters and the \\nnumber of experts provides a predictable performance improvement, but this is not the \\nsame when just scaling the total number of parameters (this shows limited returns). \\no This can perhaps be explained by suboptimal routing. \\nWith SMEAR, the authors hypothesize that these inefficiencies in MoE are caused by gradient \\nestimation issues. First, they explore if fixed heuristic routing can overperform learned routing, \\nand then compare that to SMEAR (which is fully differentiable). \\n \\nSMEAR \\n- \\nIn traditional MoE routing, the router training needs to resort to gradient estimation \\ntechniques. The goal of SMEAR is to develop an architecture that enables end-to-end \\ngradient-based training (fully differentiable, no gradient estimation) without an increase \\nin computational costs. \\n- \\nEnsemble routing \\no Would allow for an end-to-end gradient-based training but with a significant \\nincrease in computational costs. \\n- \\nMerging of Experts \\no Recent work has shown that averaging the parameters of models that share a \\ncommon architecture can often produce an aggregate model that shares the \\ncapabilities of the individual models. \\n- \\nSMEAR \\no Constructs a single merged expert whose parameters are computed as the \\nweighted average of the experts within a routing block. \\n▪ Each expert’s set of weights is set by the corresponding routing probability \\ngenerated by the router. \\no Instead of only taking the top-k experts selected by the router, which is the \\ndiscrete step in the strategy, SMEAR weighs each expert’s parameters according \\nto the weight given by the router and merges them into a single expert. \\n▪ Allows updating each expert in each forward pass in a fully differentiable \\nmanner. \\n▪ Almost equivalent (slightly higher due to the cost of merging) cost of top-\\n1 routing at inference but more expensive training costs (due to having to \\nbackpropagate through each expert after each forward pass). \\n \\nExperimental Setup \\n- \\nMain question to be answered is if SMEAR can outperform heuristic routing strategies. \\n- \\nUse T5 fine-tuned on GLUE for NLP tasks, while also conducting computer vision \\nexperiments based on ResNet. \\n- \\nUsed a “tag routing” strategy as one of the baselines, which is a routing strategy based on \\nmetadata (oracle routing). \\n- \\nAdd experts to existing pre-trained network (models are not trained from scratch and are \\nbased off pre-trained dense models). \\no Similarly to adding adapters for fine-tuning (all pre-trained parameters are kept \\nfrozen). \\n- \\nRouter is a simple linear classifier. \\n- \\nEach layer has 8 experts. \\n- \\nNo balance loss was used. \\nResults \\n- \\nModels using learned routing strategies learned through gradient estimation (thus not \\nfully differentiable) often underperform heuristic routing strategies. \\n- \\nSMEAR outperforms every routing strategy (heuristic or learned) in both NLP and Vision \\nsettings, including tag routing (determined by metadata) and a parameter-matched (in \\nterms of total parameters) dense baseline. \\no Consistent with DEMix line of research, which says that a good learned routing \\nstrategy should be better than routing determined by metadata. \\n- \\nSMEAR performs comparably to a fully active MoE ensemble (especially in T5-GLUE), \\nwhich is seen as the upper bound of this approach. \\n- \\nIn terms of inference, SMEAR performs comparably to the top-1 routing strategy. \\n- \\nDoubling the number of experts (from 8 to 16) in SMEAR led to a slight performance boost \\nin Vision but no notable difference in T5-GLUE. \\n- \\nSignificant sparsity observed when visualizing the router’s distribution, suggesting expert \\nspecialization. \\n \\nMy takeaways: \\n- \\nSMEAR offers a novel training framework that might set a precedent for future MoE \\nmodels by mitigating the non-differentiability issue common in discrete routing decisions, \\nthereby leading to more stable and efficient learning. \\n- \\nThe gradual diversification from a single expert to a full MoE configuration in SMEAR could \\ninspire new initialization techniques for complex neural networks, ensuring a smoother \\ntransition to specialized expert utilization. \\n- \\nGiven SMEAR’s performance improvements and computational efficiency, it would be \\nworthwhile to investigate how it could be adapted to real-world tasks requiring \\nmodularity and efficiency, such as personalized recommendation systems or multi-\\ndomain language models. \\n \\n', 'source_name': 'Soft Merging of Experts with Adaptive Routing NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Soft_Merging_of_Experts_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "EvoMoE.pdf #34\n",
      "{'content': 'EvoMoE: An Evolutional Mixture-of-Experts\\nTraining Framework via Dense-To-Sparse Gate\\nXiaonan Nie†\\nXupeng Miao†$\\nShijie Cao‡\\nLingxiao Ma‡\\nQibin Liu†\\nJilong Xue‡\\nYoushan Miao‡\\nYi Liu#\\nZhi Yang†\\nBin Cui†§\\n†School of Computer Science & Key Laboratory of High Conﬁdence Software Technologies (MOE), Peking University\\n$Carnegie Mellon University\\n‡Microsoft Research\\n#Tencent Inc\\n§Institute of Computational Social Science, Peking University (Qingdao), China\\n{xiaonan.nie, xupeng.miao, 1700012767, yangzhi, bin.cui}@pku.edu.cn\\n{lingxiao.ma, shijiecao, jxue, yomia}@microsoft.com\\ncallbackliu@tencent.com\\nAbstract—Mixture-of-experts (MoE) is becoming popular due\\nto its success in improving the model quality, especially in\\nTransformers. By routing tokens with a sparse gate to a few\\nexperts (i.e., a small pieces of the full model), MoE can easily\\nincrease the model parameters to a very large scale while keeping\\nthe computation cost in a constant level. Most existing works\\njust initialize some random experts, set a ﬁxed gating strategy\\n(e.g., Top-k), and train the model from scratch in an ad-hoc\\nway. We identify that these MoE models are suffering from the\\nimmature experts and unstable sparse gate, which are harmful\\nto the convergence performance.\\nIn this paper, we propose an efﬁcient end-to-end MoE training\\nframework called EvoMoE. EvoMoE starts from training one\\nsingle expert and gradually evolves into a large and sparse\\nMoE structure. EvoMoE mainly contains two phases: the expert-\\ndiversify phase to train the base expert for a while and spawn\\nmultiple diverse experts from it, and the gate-sparsify phase to\\nlearn an adaptive sparse gate and activate a dynamic number of\\nexperts. EvoMoE naturally decouples the joint learning of both\\nthe experts and the sparse gate and focuses on learning the basic\\nknowledge with a single expert at the early training stage. Then\\nit diversiﬁes the experts and continues to train the MoE with\\na novel Dense-to-Sparse gate (DTS-Gate). Speciﬁcally, instead of\\nusing a permanent sparse gate, DTS-Gate begins as a dense gate\\nthat routes tokens to all experts, then gradually and adaptively\\nbecomes sparser while routes to fewer experts. Evaluations\\nare conducted on three popular models and tasks, including\\nRoBERTa for masked language modeling task, GPT for language\\nmodeling task and Transformer for machine translation task.\\nThe results show that EvoMoE outperforms existing baselines,\\nincluding Switch, BASE Layer, Hash Layer and StableMoE.\\nSpeciﬁcally, EvoMoE outperforms other MoE methods on GLUE\\nbenchmark up to 0.562 and 0.403 on average. Our code is\\navailable 1.\\nIndex Terms—Deep Learning, Transformer, Mixtures of Ex-\\nperts, Dense to Sparse.\\nI. INTRODUCTION\\nThe Transformer model architecture is becoming increas-\\ningly important in data mining and has achieved impressive\\nresults in a wide range of applications, such as natural lan-\\nguage processing [1], computer vision [2], graph learning [3],\\nand recommendation systems [4]. Recently, there is a trend of\\nimproving the capability of Transformer models through en-\\nlarging data and model scales [1]. Speciﬁcally, [5] explored the\\n1https://github.com/codecaution/EvoMoE\\nscaling law of transformer models which shows that the model\\nperformance scales as a power-law with data sizes, model\\nsizes and the computation. However, with the rapid increasing\\nof the model sizes, it is hard to further scale the model to\\nextremely large sizes due to the limited computation power\\nof available hardware devices. To address these challenges,\\nsparsely-gated Mixture-of-Experts (MoE), a popular form of\\nconditional computation, has been proposed to increase the\\nmodel size while without increasing the computational cost\\n(e.g., FLOPs) proportionally [6, 7, 8, 9, 10, 11]. Speciﬁcally,\\nthe input tokens are routed by a sparse gate to a few experts,\\nleading to lower computational costs compared to a dense\\nmodel with the same model size.\\nThe success of MoE model relies on both the large model\\ncapacity introduced by plenty of experts and the sophisticated\\nsparse routing connections learned by the gate network. Many\\nexisting works [8, 9, 10, 11] are exploring novel gating net-\\nworks to improve the model quality or the training efﬁciency.\\nThey typically adopt a pre-deﬁned sparse gate architecture\\n(e.g., Top-1 or Top-2 with a ﬁxed number of activated experts),\\nand then train the model parameters of both the gate and\\nexperts jointly from scratch. However, such joint-training over\\npre-deﬁned sparse architecture could severely limit the model\\nquality, and even the training efﬁciency. Particularly, at the\\nbeginning of training a MoE model, both the gate and the\\nexperts are randomly initialized. The gate does not have\\nevidence to decide which expert to process an input token,\\nand the experts also do not have experiences to process a\\nrandomly-assigned input token. Training all experts from a\\nrandom state with random routed samples requires a long\\nand duplicated warming-up process. Furthermore, these pre-\\ndeﬁned gates limit the MoE to explore only 1 or 2 experts at a\\ntime. But in the early stage with a immature gate, such small\\nopportunities could be easily inﬂuenced by the random routing\\nnoises, and the improper routing could even be reinforced for\\na long time. Our observation shows that such random routing\\nin the initial stage and long-distance reinforce-based model\\nupdating in existing approaches could affect both the training\\ntime and ﬁnal model quality.\\nIn this paper, to overcome the limitations in existing ap-\\nproaches, we revisit the learning process of MoE models and\\narXiv:2112.14397v2  [cs.LG]  9 Oct 2022\\n+\\nSparse Gate\\nExpert\\nExpert\\nExpert\\nToken\\n+\\nDense Gate\\nExpert\\nExpert\\nExpert\\nToken\\nExpert\\nToken\\n2\\nGate-Sparsify\\nthreshold\\nthreshold\\n1\\nExpert-Diversify\\ndegree = 3\\ndegree = 1\\nFig. 1. Illustration on the workﬂow of EvoMoE, which contains two phases: (1) expert-diversify phase and (2) gate-sparsify phase. In the ﬁrst stage, we train\\none shared-expert instead of N individual experts and then adopts diversify functions (i.e., random masking) to spawn multiple diverse experts from the\\nshared expert. In the second stage, we propose the Dense-to-Sparse gate, which starts as a dense gate that routes tokens to most experts and then gradually\\nbecomes sparser. Different from previous TopK-based gates, we propose the content-based gating mechanism, which activates experts whose weight is beyond\\nthe threshold.\\nadvocate a simple but effective end-to-end training paradigm,\\nnamed EvoMoE. Instead of directly training from a pre-deﬁned\\nsparse architecture, EvoMoE gradually evolves into a diverse\\nand sparse MoE architecture from an initial model in two\\nphases: expert-diversify and gate-sparsify. Speciﬁcally, we\\nﬁnd that both the gate and the experts are underachieving in\\nMoE training thus resulting in unstable routing performance.\\nMotivated by the successes of weight-sharing models, in the\\nﬁrst stage, we introduce the expert-diversify mechanism to\\nlearn the commonly shared knowledge across different experts.\\nOur proposed mechanism only trains one common experts\\nwith all input tokens at ﬁrst, which could be seen as sharing\\nthe model parameters across all the experts. To involve the\\ndiversity of these experts, we then randomly perturb each\\nexpert with different masks as the initial model state of the\\nfollowing training steps. In the gate-sparsify phase, the weight-\\nsharing constraint is released and the training of MoE turns\\nto the sparsely activated manner over these diverse experts.\\nUnlike the pre-deﬁned sparse gate in the previous works, we\\nintroduce DTS (Dense-to-Sparse) gate to decide the sparse\\ngate gradually for MoE models. We proposed DTS gate to\\nadaptively learn a better gating network from a dense one and\\ngradually route tokens to fewer experts, making the training\\nstructure sparser and continuously reducing the computation\\ncost, while keeping the model quality improving as usual. In\\nparticular, to implement the DTS gate, our idea is to carefully\\ncontrol the temperature of a softmax-based routing function,\\nso that to adjust the weights distribution among experts and\\ncontrol the sparsity of the MoE layer during training.\\nIn short, EvoMoE advances in two aspects. First, compared\\nto the joint-training of gate and experts from scratch, EvoMoE\\nsplits joint-training process and provides an opportunity to\\ntrain experts during a warm start. Such mechanism of training\\ngate after experts can reduce a lot of random error-trails\\nat the beginning. Second, compared to the reinforce-based\\nmodel updating, starting with a dense gate allows us to get\\ntraining feedback from all diverse experts and adjust the\\nrouting weights directly to the right direction, which not only\\nspeeds up the convergence of the gate, but also beneﬁts for\\nthe expert specialization.\\nWe evalute EvoMoE on three popular models and tasks,\\nincluding RoBERTa [12] (Encoder-Only) for masked language\\nmodeling (MLM) task, GPT [13] (Decoder-Only) for language\\nmodeling (LM) task and Transformer [14] (Encoder-Decoder)\\nfor machine translation (MT) task. The results show that\\nEvoMoE outperforms existing baselines, including Switch [8],\\nBASE Layer [15], Hash Layer [11] and StableMoE [16].\\nSpeciﬁcally, on MLM task, EvoMoE outperforms other MoE\\nmethods up to 0.562 GLUE score and 0.403 in average for\\nthe GLUE benchmark [17]; on LM task, EvoMoE outperforms\\nother MoE methods up to 0.88 ppl and 0.545 ppl on average;\\non translation task, EvoMoE can averagely achieve 1.0 BLEU\\nscore improvement as well as 1.33x speed-up that Switch\\nTransformer. Experiments also verify the ability of EvoMoE\\nfor scaling models with more experts or more MoE layers.\\nThe rest of the paper is organized as follows. We ﬁrst\\nintroduce the background of Transformers and MoEs in Sec-\\ntion II. And we identify two key defects in existing MoE\\ntraining process including the conformity and instability in\\nSection II-C. Motivated by these properties, we present our\\nEvoMoE design in Section III and introduce the expert-\\ndiversity stage and gate-sparsify stage respectively. Section IV\\ndescribes some implementation details. We provide the evalu-\\nation methodology and conduct substantial experiments under\\nvarious settings in section V to support our claims. More\\nrelevant approaches are introduced in Section VI. Finally, we\\nprovide some concluding remarks in section VII.\\nII. PRELIMINARY\\nA. Transformer\\nThe model architecture of Transformer [18] has demon-\\nstrated its superior performance in many sequence-to-sequence\\n2\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\ninput\\noutput\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\ninput\\nFeed\\nForward\\nFeed\\nForward\\noutput\\nGate\\n(a) Transformer Layer\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\ninput\\noutput\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\ninput\\nFeed\\nForward\\nFeed\\nForward\\noutput\\nGate\\n(b) Transformer Layer with Mixture-of-Expert\\nFig. 2. A brief architecture of Transformer Encoder Layer and Transformer\\nwith Mixture-of-Expert Layer. The Transformer encoder layer contains two\\nmain components: a Multi-Head Self-Attention Layer and a Position-wise\\nFeed-Forward Layer. Based on Transformer layer, the transformer with MoE\\nreplaces the FFN with a series of FFNs and introduce a gate network.\\nTABLE I\\nNOTATIONS\\nSymbols\\nDeﬁnitions\\nQ\\nQueries in self-attention module\\nK\\nKeys in self-attention module\\nV\\nValues in self-attention module\\ndk\\nFeature dimension of each Query/Key\\nS\\nA set of input tokens\\nE\\nAn series of experts\\nD\\nFeature dimension of each token\\nN\\nNumber of experts in each MoE layer\\nei(xs)\\nThe output of i-th Expert by taking input xs\\ng(xs)i\\nThe routing score for xs on i-th Expert\\nc\\nThreshold for expert selection\\nGS,E\\nThe routing score for S on E\\nIdS\\nThe set of selected expert id on S\\nnatural language processing (NLP) tasks, which contains sev-\\neral encoder layers and decoder layers. Each encoder layer is\\nstacked by a multi-head self-attention network and a position-\\nwise feed-forward network (FFN), which is illustrated in\\nFigure 2(a). It employs a residual connection on each of\\nthese two sub-layers, followed by a normalization layer [19].\\nFormally, each sub-layer, e.g., attention and FFN, produces\\nits output as LayerNorm(x + Sublayer(x)). The decoder is\\nsimilarly constructed, except for an additional cross-attention\\nmechanism between attention and FFN to introduce the output\\nof the encoder. For a sequence of input tokens (x1, ..., xs) ∈\\nRD, we formulate the function of each sub-layer in following:\\nAttention: The attention module [14] could capture the\\ndependencies between tokens in the sequence, and is effective\\nin sequence modeling. It performs as a ternary function, which\\nmaps the input queries (Q), keys (K) and values (V) to the\\noutput (O). Equation 1 represents the Scaled Dot-Product\\nFeed\\nForward\\nFeed\\nForward\\nFeed\\nForward\\nGate\\n𝑾𝒈:\\n𝒙𝒔:\\n2.01 2.64 1.8\\n𝑾𝒈· 𝒙𝒔:\\n0.35 0.65\\n0\\n𝒔𝒐𝒇𝒕𝒎𝒂𝒙(𝑻𝒐𝒑𝟐(𝑾𝒈· 𝒙𝒔)):\\n+\\n0.1\\n0.2\\n1.3\\n1.1\\n0.4\\n1.8\\n0.3\\n2.4\\n0.6\\n-0.2\\n0.4\\n1.5\\n𝒆𝟎(𝒙𝒔)\\n𝒆𝟏(𝒙𝒔)\\n𝒆𝟐(𝒙𝒔)\\n𝒚𝒔= 𝟎. 𝟑𝟓∗𝒆𝟎𝒙𝒔+ 𝟎. 𝟔𝟓∗𝒆𝟏(𝒙𝒔)\\nFig. 3. Illustration on the workﬂow of Mixture-of-Expert (MoE). The input\\ntoken xs is ﬁrst processed by the gate network to calculate similarities between\\nxs and each expert. Then, it performs Top-K operation on these similarities to\\ndetermine the target activated experts. Finally, ys is produced by the linearly\\nweighted combination of each expert’s output on the token by the gate’s\\noutput.\\nAttention [14], which performs dot products of each query\\nwith all keys, divides each by √dk and then adopts the softmax\\nfunction to get the weight of each value. In addition, dk is the\\ndimension of queries and keys.\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nMulti-Head Attention: Vaswani et al. [14] proposed the\\nmulti-head attention mechanism to jointly learn from differ-\\nent representation subspaces at different positions and thus\\nimproved the model performance. The multi-head attention\\nlinearly projected the queries, keys and values h times with\\nlearned linear projections to dk, dk and dv, dimensions,\\nrespectively.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n(2)\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nThe projections are the trainable parameter matrices, where\\nW Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv.\\nMeanwhile, h is the number of heads, and dk = dv =\\ndmodel/h. Because the dimension of each head is reduced from\\ndmodel to dmodel/h, the time cost of multi-head attention is\\nsimilar to that of the original attention. In addition, the decoder\\nemploys a masked self-attention, which only sees the tokens\\non the left of the sequence.\\nPosition-wise Feed-Forward Networks: Each transformer\\nlayer also includes a fully connected feed-forward network\\n(Equation 3), which consists of two fully connected networks\\nand a ReLU activation function.\\nFFN(xs) = W2 · ReLU(W1 · xs + b1) + b2\\n(3)\\n3\\n0 1 2 3 4 5 6 7 8 9 10 1112 13 14 15\\nExpert ID\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\nDuration ID\\nExpert Loads Distribution\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n(a) Expert Loads Distribution .\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\nDuration ID\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nExpert ID\\nSelected Expert in Top-1 Gate\\n(b) Unstable Routing Pattern for Token “the”\\nFig. 4.\\nThe observations of GPT-MoE with 16 experts and the Top-1 gate among 12 MoE-layers (totally 24 layer). Figure 4(a) shows the expert load\\ndistribution (deeper color represents heavier workload, i.e., more tokens to be processed) and Figure 4(b) shows the expert selection for a speciﬁc token “the”.\\nB. Mixture of Experts\\nBecause larger pretrained models always achieve better\\nmodel quality [5], the size of state-of-the-art NLP models has\\nbeen increasing 10× per year, e.g., BERT [20], GPT [21],\\nT5 [22], GPT-2 [13], GPT-3 [1], which require increasing\\ncompute budgets. To improve the model capacity without\\nincreasing computation budgets, researchers sparsely scale\\ntransformers recently as Figure 2(b) by replacing the feed-\\nforward network with the mixture of experts (MoE) architec-\\nture and activating only a subset of these experts for each\\ninput sample [7, 8, 9]. The main components of the MoE\\narchitecture include an expert network E for scaling model\\ncapacity and a sparse gate network G for introducing model\\nsparsity.\\nExpert Network: The expert network E includes a series\\nof experts {e1, ..., eN} to increase the model capacity, where\\neach expert ei represents a single neural network, e.g., FFN,\\nand contains its own parameters. In Figure 3, the MoE layer\\nconsists of three FFN networks. For each expert ei (ei : RD →\\nRD), it takes the token xs as an input to produce its own output\\nei(xs). The ﬁnal output of the expert network ys is the linearly\\nweighted combination of each expert’s output on the token by\\nthe gate’s output, formulated as Equation 4.\\nys =\\nN\\nX\\ni=1\\ng(xs)i · ei(xs)\\n(4)\\nIn Figure 3, the expert network takes the input token xs :\\n[−0.2, 0.4, 1.5] and produces the output of each individual\\nexpert on xs, e.g., e0(xs), e1(xs) and e2(xs). By combining\\nthe gate’s output, i.e., [0.35, 0.65, 0], the output of this MoE\\nlayer is ys = 0.35 ∗e0(xs) + 0.65 ∗e1(xs).\\nSparse Gate Network: The sparse gate network G is\\nthe key component to introduce model sparsity, which takes\\na batch of tokens {x1, ..., xs} as input and produces the\\nprobability of them with respective to all experts {e1, ..., eN}.\\nShazeer et al. [7] proposes the Top-K gating as Equation 5,\\nwhich keeps only the top k values before the softmax function.\\nIn addition, Wg is a trainable variable (Wg ∈RD×N) and\\ndetermine the targeted experts for each token.\\ng(xs) = softmax(TopK(xs · Wg))\\n(5)\\nWe illustrate the workﬂow of a MoE layer in Figure 3, where\\nk = 2 and Wg is a 3 × 3 (i.e., feature dimension × number of\\nexperts) matrix to represents the parameter of gate network.\\nWe ﬁrst perform a dot-product on xs and Wg to calculate\\nsimilarity between the input token and the experts. The result,\\n[2.01, 2.64, 1.8], indicates that the input prefers e1 > e0 > e2\\nand we only activate e0 and e1 as k = 2. Finally, we conduct\\na softmax function to get the weight score of each expert and\\nperform a weighted sum to get the ﬁnal output ys.\\nPrevious work mainly focuses on how to improve the quality\\nand efﬁciency of training such sparse gate network. Shazeer\\net al. [7] proposed the noisy Top-K gating on Long Short-\\nTerM memory (LSTM) kayers [23] and\\nLepikhin et al. [9]\\nintroduced MoE with Top-2 gate into Transformer.\\nLewis\\net al. [10] adopted the numerous solution for balanced token-\\nto-expert routing and Roller et al. [11] utilized the hash-based\\nrouting strategy.\\nDistributed Training of MoE Models: Expert parallel\\ntraining is a speciﬁc method of parallelism for MoE models,\\nwhich is ﬁrst proposed by GShard [9]. Experts are placed\\non different workers and each worker takes a different batch\\nof training samples. For non-MoE layers, expert parallelism\\nbehaves the same as data parallelism. In MoE layers, tokens\\nin the sequence are sent to workers where their desired experts\\nreside. Similar to model parallelism, the outputs of each MoE\\nlayer are exchanged again to be organized back into original\\nsequences for the computation of the next layer. As MoE\\nmodels often have numerous experts, expert parallelism can\\nscale up with model size better than model parallelism.\\nC. Observation and Motivation\\nIn this section, we revisit the learning process of MoE\\nmodels and introduce our two key ﬁndings in the following,\\nwhich motivates us to design our EvoMoE framework.\\nConformity in Mixture of Experts: One interesting ﬁnding\\nis conformity. During the early training stage, existing join-\\ntraining methods of sparse MoE make the routing decision\\n4\\nShared-Expert\\nExpert-0\\nExpert-1\\nExpert-2\\nFig. 5.\\nTo spawn multiple diverse experts from the shared-expert, EvoMoE\\nadopt the random masking technique. Speciﬁcally, part of the shared expert’s\\nweight are masked as 0.\\nto comply with most tokens. Here we train a GPT model\\nincluding 24 transformer layers, with every FFN layer replaced\\nby 16-expert MoE layer using the Top-1 gate.\\nFigure 4(a) shows that most tokens keep concentrating on\\nthe 8-th expert at ﬁrst, since it has been greedily reinforced.\\nAfter around hundreds of training steps (i.e., 1 duration equals\\n40 training steps), the other experts gradually catch up and\\nthe workload becomes balanced. Such phenomenon motivates\\nus to focus on training a common expert and utilize the\\ncomputational resources to accelerate the early stage.\\nInstability in Mixture of Experts: Another important\\nﬁnding is the instability. We take a single token “the” as an\\nexample and Figure 4(b) shows its expert selection results for a\\nlonger training process. As we can see, the selection is highly\\nunstable later since both the gate network and the experts are\\nnot knowledgeable enough to obtain a stable routing pattern,\\nespecially at the early stage of the training process. This\\nindicates that a pre-deﬁned gates (e.g., Top-K) in existing\\nworks, which assumes a ﬁxed number of activated experts,\\ncould limit the exploration of potential valuable experts. Ag-\\ngressively increasing the number of activated experts could\\nimprove the model capacity but inherently violates the original\\ndesign intention of sparse MoE. Such a dilemma motivates us\\nto design an adaptive solution to balance the trade-off between\\nthe convergence performance and computation costs.\\nIII. METHODS\\nThe observations in Section II-C motivates EvoMoE, a two-\\nphase framework that gradually and adaptively training MoE-\\nbased models, which is different from existing methods that\\njointly train the gate network and the expert network over a\\npre-deﬁned sparse (e.g., Top-1 or Top-2) gate and a series of\\nrandomly initialized experts. As shown in Figure 1, EvoMoE\\ncontains two phases: an expert-diversify phase and a gate-\\nsparsify phase. In the expert-diversify phase, EvoMoE shares\\nthe weights among experts in one MoE layer for several\\ntraining steps and then makes experts diverse by randomly\\nmasking. In the gate-sparsify phase, EvoMoE introduces the\\ndense-to-sparse (i.e., DTS) gate, which begins routing as a\\ndense gate that routes tokens to all experts and then adaptively\\nlearns the weights of routing to each expert and gradually\\nanneals to standard Top-1 gating.\\nAlgorithm 1: Training MoE in the EvoMoE Frame-\\nwork\\nData: xS: a group of tokens of size S,\\nE: expert network\\nTS: number of iterations for shared-expert,\\nTD: number of iterations for dense-gate,\\nT: number of training iterations.\\n1 for i ←1 to TS do\\n2\\nyS ←e(xS) ;\\n3 // Diversify experts from the shared ;\\n4 for ei ∈E do\\n5\\nei ←diversify(e, i) ;\\n6 for i ←TS to T do\\n7\\nτ ←temperature scheduler(i) ;\\n8\\n// Get selected expert ids and weights for each\\ntoken ;\\n9\\nGS, E, IdS ←DTS Gate(xS, τ, TD);\\n10\\nfor s ←1 to S do\\n11\\nys ←0 for id ∈ids do\\n12\\nys ←ys + Gs, id ∗eid(xs) ;\\nA. Problem Formulation\\nGiven an input token xs, a series of experts {e1, ..., eN}\\nand a learn-able gate with parameter Wg, Func is adopted\\nby the gate network to determine the targeted experts for it,\\ni.e., the token-to-expert assignment, formulated in Equation 6.\\ng(xs) is a 1×N vector, which represents the scores of xs with\\nrespect to experts. Meanwhile, each expert will process the\\ninput token separately as ei(xs) and combine their output as\\nEquation 7.\\ng(xs) = Func(xs · Wg)\\n(6)\\nys =\\nN\\nX\\ni=1\\ng(xs)i · ei(xs)\\n(7)\\nExisting work adopts a pre-deﬁned Top-K as Func, such as\\nTop-2 for GShard [9] and Top-1 for Switch-Transformer [8].\\nHowever, due to the non-derivability of Top-K, only the\\nselected experts would back-propagate their gradients to the\\ngate network and update their corresponding columns in Wg.\\nFor example, only 1 expert is selected and 1 column of the\\ngate would be updated in Switch-Transformer. So it is hard\\nfor Top-K gate to optimize this expert-selection problem.\\nMoreover, as observed in Figure 4(a), the loads of experts\\nare extremely imbalanced at the early stage of training and\\nthus most GPUs suffer from low utilization due to stragglers\\nin expert parallelism.\\nB. Stage 1: Expert-Diversify\\nAs the gate network and expert network are both randomly\\ninitialized, it requires a vast amount of computation budget\\nfor trial and errors, which is inefﬁcient for models’ training.\\nBased on the observation in Section II-C that most tokens are\\nprocessed by the same expert and other experts waste their\\n5\\ncomputation budget, we train one shared-expert instead of N\\nindividual experts in the early stage (illustrated as the left of\\nFigure 1). Because all experts within the same MoE layer\\nshare weights, the model is equal to its corresponding non-\\nMoE model as a small dense model.\\nAlgorithm 1 illustrates the MoE training process in our\\nEvoMoE framework. First, input tokens are processed by\\nthe shared expert e0 (line 1-2). Then EvoMoE switches the\\ntraining into standard MoE models’ training, by adding a gate\\nnetwork at each MoE layer and diversifying all experts from\\nthe shared expert (line 4-5). After this expert-diversify phase,\\nEvoMoE steps into the gate-sparsify phase, where it schedules\\nthe gate temperature coefﬁcients and then obtains the token-\\nto-expert routing relation from DTS-gate (line 7-9). Tokens\\nwill be dispatched to corresponding experts and aggregated\\ntogether by weighted sum operating (line 10-12).\\nMultiple diversify techniques can be adopted to spawn\\nmultiple diverse experts from one expert, such as noise, NAS,\\nrandom mask. EvoMoE adopts the random mask, which masks\\npart of the shared expert’s weights as 0 (shown as Figure 5).\\nFor example, expert-1 is initialized by masking the central\\nvalue. The proposed expert-diversify stage avoids joint training\\nfrom scratch and the well-trained diverse experts could be\\ntreated as a better initialization to beneﬁt the following model\\nconvergence.\\nC. Stage 2: Gate-Sparsify\\nAlthough sparse gating has demonstrated its superior model\\nefﬁciency in both training and inference, prior work tends to\\nconvergence to a sub-optimal model under the ﬁxed compu-\\ntation budget or the dataset size due to the jointly training\\nof the randomly initialized gate network and expert network.\\nIn this paper, we propose a new mechanism for training\\nthe gate network, named Dense-to-Sparse gate (DTS-\\nGate, as illustrated in Algorithm 2 ), which starts as a dense\\ngate that routes tokens to most experts and then gradually\\nbecomes sparser. DTS-Gate beneﬁts from the sufﬁcient train-\\ning of experts in the early stage and then make the experts\\nselection becomes sparser on the basis of specialized experts.\\nThis dense-to-sparse process only occupies a small fraction\\ncompared with the total training time, which usually takes\\ndays to weeks.\\nGate with Temperature: In order to control the sparsity\\nduring training, we adopt the softmax temperature to adjust\\nthe weights distribution among experts. Formulated as Equa-\\ntion 8, Wg is the parameter of gate, ζ is the extra noise and\\nsampled from Gumbel(0, 1) distribution [24], and τ is the\\nsoftmax temperature which controls the distribution. When the\\nτ increases, the distribution of g′(xs) becomes more uniform,\\nwhich evolves more experts into the computation of each\\ntoken. As the τ approaching 0, the distribution becomes one-\\nhot, which is more conﬁdent for the gate network.\\ng′(xs) =\\ne(xs·Wg+ζ)/τ\\nPN\\ns′=1 e(xs′·Wg+ζ)/τ\\n(8)\\nContent-based Sparsity: Different from existing static\\nTop-K based gate [8] [9], EvoMoE adopts the content-based\\nsparsity method to determine the number of activated experts,\\nwhich keeps the value beyond an threshold c. As formulated by\\nEquation 9, we drop the experts whose weights fall below the\\nthreshold c and no extra communication or computation will\\nbe wasted. It’s worth noting that the sum of selected experts’\\nscore can not be equal to 1 because we don’t normalize\\nthem after dropping. It is useful to remain the original score,\\nespecially only one expert is selected, which was veriﬁed in\\nSwitch. To meet the demand of above two designs, we enable\\neach expert with this content-based gate to make them well\\nspecialized. transformer [8].\\ng(xs)i =\\n(\\ng′(xs)i,\\nif\\ng(xs)i > c\\n0,\\nelse\\n(9)\\nSparsity Scheduler: With temperature τ increasing, the\\ndistribution tends to be uniform and more experts will be\\nselected. So the sparsity decreases and the training cost of\\nthe neural network would increases. On the opposite, less\\nexperts are involved into computation and thus the sparsity\\nincreases. By scheduling the temperature of Equation 8, we\\ncan control the sparsity of the MoE layer over different training\\nstages. There is a trade-off between model quality and training\\ncost for the selection of temperature. For example, when the\\ndistribution of experts is nearly one-hot, it would lead to\\nlarge variance of gradients between experts and thus make the\\nlearning of MoE learning difﬁcult. To optimize this problem,\\nour DTS-Gate starts at a large temperature that routes tokens\\nto most experts and then anneals to a small temperature that\\ngradually sparsiﬁes the MoE layer.\\nBalance Loss: Similar to Switch transformer [8], we utilize\\nthe balance loss Lbalance to avoid imbalanced assignments for\\ndifferent experts which would cause the straggler problem and\\nthus lead to low training efﬁciency.\\nLbalance = αN\\nN\\nX\\ni=1\\n(\\nP\\nxs∈B I{g(xs)i > 0}\\n|B|2\\nX\\nxs∈B\\ng′(xs)i)\\n(10)\\nAs formulated in Equation 10, α is a hyper-parameter and\\nB represents current batch of tokens. P\\nxs∈B I{g(xs)i > 0}\\nrepresents the number of tokens dispatch to expert i and\\nP\\nxs∈B g′(xs)i represents the gate probability allocated for\\nexpert i. Intuitively, the balance loss will reduce the amount\\nof data for overloaded-experts and move towards to balanced\\nloads at the batch data level.\\nTraining Objective: In the ﬁrst stage, the experts of each\\nMoE layer share same weights and thus the loads can be\\ndivided to them equally. The training objective is to optimize\\nthe model quality (i.e., Ltask). In the second stage, both the\\nmodel quality and training efﬁciency (i.e., balanced workloads\\nbetween experts) need to be considered.\\nL =\\n(\\nLtask,\\nif\\nstage = 1\\nLtask + Lbalance,\\nelse\\n(11)\\n6\\nAlgorithm 2: Dense-to-Sparse Gate Mechanism\\nData: xS: a group of tokens of size S, τ: temperature,\\nTD: number of iterations of dense-gate.\\nResult: GS, E: group combine weights, IdS: Index of\\nselected experts\\n1 Function DTS Gate(xS, τ, TD):\\n2\\ngS, E ←gumbel softmax(xS · Wg, τ) ;\\n3\\nif current iteration < TD then\\n4\\n//select experts for token, N ≥len(ids) ≥1 ;\\n5\\nIdS ←select gS, E>threshold ;\\n6\\nelse\\n7\\n//select Top-1 expert for token, len(ids) = 1 ;\\n8\\nIdS ←select Top1(gS, E);\\n9\\nfor s ←1 to S do\\n10\\nfor id ∈ids do\\n11\\nGs, id ←gs, id ;\\n12\\nReturn IdS, GS, E ;\\nIV. IMPLEMENTATION\\nEvoMoE is implemented by adding support for MoE mod-\\nels on FairSeq2 [25]. Meanwhile, EvoMoE proposes several\\nsystem optimizations, including:\\nTopology-Aware Hierarchical All-To-All Communica-\\ntion: In AllToAll operation, each GPU sends its data to all\\nGPUs (one-for-all) and receives data sent by all GPUs (all-\\nfor-one), where each data will be divided equally into n parts.\\nCurrent AllToAll operations implemented in NCCL and MPI\\nmay suffer from low utilization of network bandwidth because\\nof the small message size. We propose Topology-Aware\\nHierarchical AllToAll, which combines hierarchical\\nnetworks (intra-node and inter-node) and aggregates messages,\\nto optimize the communication between multi-nodes equipped\\nwith one NIC. It ﬁrst gathers the data of eight GPUs inside\\nthe same node into one GPU, and then performs a data\\nlayout transformation to orignize the placement of tokens.\\nAfterwards, it launches All-To-All communication between\\nnodes, and then performs the data layout transformation and\\nscatters tokens to its corresponding expert. In this way, the\\nsize of data transferred between nodes is #GPU2 times larger\\nthan before. Meanwhile, this two-level decoupled AllToAll\\nalso fully utilizes the intra-node (NvLink or PCIe) and inter-\\nnode bandwidth (Inﬁniband or Ethernet).\\nMoE-Aware Recomputation: Recomputation is a main-\\nstream techniques to reduce the memory footprint of training\\nmodels, which evicts feature map tensors in the forward pass\\nfor memory saving and then regenerates them for calculating\\ngradients by executing corresponding computation. Existing\\nsystems, e.g., DeepSpeed [26] and FairSeq [27], adopt recom-\\nputation as the recommended conﬁguration for training large\\nmodels, which only saves the input tensor of each Transformer\\nlayer and re-executes the whole Transformer in the backward.\\nAs the MoE layer involves two All-To-All communication in\\nthe forward, re-executing them may lead to large time cost. To\\nkeep the memory efﬁciency while improve training efﬁciency,\\n2https://github.com/facebookresearch/fairseq\\nwe propose the MoE-aware recomputation, which save three\\ntensors of each Transformer layer, including the input tensor\\nof Transformer layer, and the input tensor of two All-To-All\\noperations.\\nV. EXPERIMENTS\\nA. Experimental Setup\\n1) Machine Environment:\\nWe conduct experiments on\\nDGX-A100, where each server is equipped 2 AMD CPUs\\nand 8 NVIDIA Ampere A100 (40GB) GPUs, with Ubuntu\\n20.04, CUDA 11.3, CuDNN 8.2.0 and NCCL 2.12.7. GPUs\\ninside a server are connected via NVLink 3.0 with and\\nservers are connected with 8 InﬁniBand NICs via 8*200\\nGbps bandwidth totally. The RDMA is used by default and\\nthe PyTorch version is 1.11.\\n2) Baselines: To verify the effectiveness of our method, we\\ncompare it with the several representative baselines, including\\nSwitch-Transformer [8], BASELayer [15], HashLayer [11],\\nDSelectK [28] and StableMoE [16]. Switch-Transformer [8]\\nproposed to adopt Top-1 gate for the training of large-\\nscale models. BASELayer [10] formulates the token-expert\\nrouting as a linear assignment problem and guarantees bal-\\nanced compute loads by employing numerous algorithms.\\nHashLayer [11] replaces the gating network with a hash-\\nbased routing strategy (e.g., random hash, clustered hash).\\nDSelectK [28] involves sparse gates (Top-K) in the multi-\\ngate MoE (i.e., MMoE) architecture for better parameter\\nsharing among different tasks and trains gates from dense\\nto sparse for smoothness. StableMoE [16] also proposed two\\ntraining stages, which learn the gate as well as distill it into\\na lightweight one in the ﬁrst stage, and freezes the parameter\\nof gate for stable routing in the second stage. Our EvoMoE\\nmainly contains two phases: an expert-diversify phase\\nto spawn multiple diverse experts from one single well-trained\\nbase expert, and a gate-sparsify phase that gradually and\\nadaptively learns a increasingly sparse gate from a dense gate.\\n3) Benchmark and Datasets: We evaluate EvoMoE on three\\npopular tasks, including the machine translation (MT) task\\nfor domain-speciﬁc models, the Masked Language Modeling\\n(MLM) task and the language modeling (LM) task for pre-\\ntrained models.\\nWe adopt standard Transformer architecture [18] (Encoder-\\nDecoder) for the MT task and train models on four pop-\\nular translation datasets, WMT17 (English to German/Ger-\\nman to English)3, and WMT14 (English to French/French\\nto English)4. BLEU scores of the test sets are reported for\\ncomparison.\\nWe adopt RoBERTa architecture architecture [12] (Encoder-\\nonly) for the MLM task and train models on the combination\\nof datasets, including Wikipedia5, BooksCorpus6, OpenWeb-\\n3https://www.statmt.org/wmt17/\\n4https://www.statmt.org/wmt14/\\n5https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract.xml.gz\\n6https://battle.shawwn.com/sdb/books1/books1.tar.gz\\n7\\nTABLE II\\nEVALUATING THE PRE-TRAINED MODELS ON THE GLUE BENCHMARK. WE SCALE THE STANDARD TRANSFORMER(TRM) INTO THE MOE MODEL BY\\nREPLACING EVERY THE OTHER FFN WITH A MOE LAYER.\\nModels\\n#Shared Params.\\n#Expert Params.\\nFLOPs\\nMNLI\\nQNLI\\nQQP\\nRTE\\nSST-2\\nMRPC\\nCoLA\\nSTS\\nAvg\\nStandard TRM\\n355M\\n-\\n207B\\n88.2\\n93.2\\n92.1\\n85.1\\n95.8\\n88.6\\n84.5\\n90.5\\n89.750\\nLarger TRM\\n370M\\n-\\n220B\\n88.4\\n93.5\\n92.2\\n85.3\\n95.8\\n88.9\\n84.7\\n90.6\\n89.925\\nSwitch TRM\\n259M\\n1536M\\n220B\\n89.2\\n93.7\\n92.2\\n86.4\\n95.8\\n89.1\\n85.3\\n90.8\\n90.313\\nBASE Layer\\n259M\\n1536M\\n220B\\n89.5\\n93.9\\n92.4\\n87.3\\n96.0\\n89.4\\n85.5\\n91.2\\n90.650\\nHash Layer\\n259M\\n1536M\\n220B\\n89.4\\n93.9\\n92.2\\n87.1\\n95.9\\n89.2\\n85.5\\n90.9\\n90.513\\nStableMoE\\n259M\\n1536M\\n220B\\n89.3\\n93.8\\n92.1\\n86.7\\n95.8\\n89.2\\n85.4\\n91.0\\n90.413\\nEvoMoE\\n259M\\n1536M\\n220B\\n89.9\\n93.9\\n92.3\\n88.1\\n96.1\\n89.6\\n85.6\\n91.5\\n90.875\\nText7 and CC-1008. Moreover, these datasets are tokenized by\\nbyte-pair encoding with a vocabulary size of 50257. Models\\nare validated on the famous General Language Understanding\\nEvaluation(GLUE) benchmark [17] for comparison.\\nWe adopt GPT architecture architecture [13] (Decoder-only)\\nfor the LM task and train models on OpenWebText as Radford\\net al. [13]. We report train/valid/test perplexity (PPL) for\\ncomparison.\\nWe also report the inference FLOPs of each model, which\\nrepresents the speed of deploying this model at industry.\\nAll the training data are downloaded and pre-processed by\\nfollowing the example scripts from Fairseq9.\\n4) Hyper-Parameter Detail: We sparsely scale these mod-\\nels by replacing every other the feed-forward layer (FFN) with\\nMoE-FFN Layer, which contains a series of FFN experts. All\\nmodels use the GeLU activation functions [29], polynomial\\nlearning rate scheduler and Adam optimizer [30], where\\nβ1 = 0.9 and β2 = 0.98 . We set clip norm as 0.0, weight\\ndecay as 0.1 and dropout rate as 0.1. We use CrossEntropy\\nas the criterion and utilize the label smoothed technique with\\ncoefﬁcient of 0.1 for the MT task. The coefﬁcient of balance\\nloss is set as 0.1 in Switch-Transformer [8], StableMoE [16]\\nand our EvoMoE. We set the threshold c of our dense-to-sparse\\ngate as 0.001 over training steps, which determines how large\\nthe expert’s weight is important and is a trade-off between\\ntraining cost and model quality from our point of view.\\nB. GLUE Results\\nModel\\nArchitecture:\\nWe\\npretrain\\nthe\\nrepresentative\\nRoBERTa model for the masked language modeling task,\\nwhere we set standard Transformer(TRM) with 24 encoder\\nlayers, hidden dimension as 1024 and number of attention\\nheads as 16. We replace every other FFN layer in standard\\nTransformer with the MoE layer (16 experts per layers) to\\nconstruct the MoE models. The standard Transformer is a\\ndense model and contains 355M parameters totally, whose\\ninference FLOPs is 207B. Meanwhile, the sparse MoE model\\ncontains 1759M parameters totally, including 259M parame-\\nters for shared backbone and 1536M parameters for the expert\\n7https://zenodo.org/record/3834942/ﬁles/openwebtext.tar.xz\\n8https://data.statmt.org/cc-100/\\n9https://github.com/facebookresearch/fairseq/tree/main/examples\\nnetwork. In our setting that only 1 expert is active at a time,\\neach input token will activate 335M parameters of the sparse\\nMoE models, which is the same as standard Transformer\\nmodel except for the gate network. To exactly match the\\ninference speed (FLOPs) of MoE models, we slightly increase\\nthe FFN hidden size of standard TRM to construct the larger\\nTRM.\\nModel Performance: We pretrained each model for 100k\\nsteps totally, 5k of which was the warm-up phase. For our Evo-\\nMoE, we scheduled the ﬁrst 5k steps as the expert-diversify\\nstage and the following 5k steps for annealing temperature\\nfrom 2.0 to 0.3. After the pre-training stage, we ﬁnetune the\\npre-trained models on each GLUE task and summarized the\\nresults in Table II. As for RTE, we ﬁnetune it starting from the\\nMNLI model rather than the pretrained model as\\nLiu et al.\\n[12].\\nCompared with other baselines, EvoMoE achieves state-of-\\nthe-art results on 7 out of 8 tasks and the best averaged score.\\nThe MoE model is constructed by adding the gate network\\nand replacing the original FFN layer of standard Transformer,\\nwhich increases its model size and thus enlarges its capacity.\\nThus all MoE models outperform their backbone model (stan-\\ndard TRM), e.g., 89.750 for standard TRM and 90.313 (+\\n0.563) for Switch TRM with respect to the avg score. Larger\\nTRM slightly outperforms standard TRM because of its large\\nmodel size. As veriﬁed by Kaplan et al. [5], larger models tend\\nto be more sample-efﬁcient, which represents better model\\nquality with ﬁxed training data/steps.\\nCompared with other MoE methods, EvoMoE beneﬁts from\\nthe sufﬁcient training of experts in the early stage and then\\nmake the experts selection becomes sparser on the basis of\\nspecialized experts. Speciﬁcally, EvoMoE outperforms other\\nMoE methods on GLUE benchmark up to 0.562 and 0.403 on\\naverage. Switch TRM [8] jointly trains the randomly initialized\\nexperts and gates, which aims to learn better parameter as well\\nas balanced routing. It is hard to optimize them simultaneously\\nand thus performs bad among MoE models. To alleviate this\\nproblem, StableMoE [16] freezes the parameter of gate net-\\nwork after the early training stage and improves over Switch-\\nTRM. Hash Layer [11] utilizes the ﬁxed hash strategy to\\nroute tokens, which is based on the input embedding. Because\\nboth the hash strategy and input embedding is ﬁxed, Hash\\n8\\nTABLE III\\nPERPLEXITY RESULTS OF LANGUAGE MODELING TASK.\\nModels\\n#Shared Params.\\n#Expert\\n#Expert Params.\\nFLOPs\\nPerplexity(↓)\\nStandard TRM\\n345M\\n-\\n-\\n207B\\n15.14\\nLarger TRM (wider)\\n360M\\n-\\n-\\n220B\\n14.92\\nSwitch TRM\\n249M\\n192\\n1536M\\n220B\\n13.12\\nBASE Layer\\n249M\\n192\\n1536M\\n220B\\n12.45\\nHash Layer\\n249M\\n192\\n1536M\\n220B\\n12.87\\nStableMoE\\n249M\\n192\\n1536M\\n220B\\n12.91\\nEvoMoE\\n249M\\n192\\n1536M\\n220B\\n12.24\\nTABLE IV\\nBLEU SCORE ON EACH MACHINE TRANSLATION DATASETS\\nModels\\nEn-De\\nDe-En\\nEn-Fr\\nFr-En\\nTRM-Base\\n28.1\\n34.8\\n39.2\\n38.1\\nSwitch-TRM\\n28.4\\n34.6\\n39.1\\n38.2\\nEvoMoE\\n29.6\\n36.7\\n40.3\\n39.2\\nw/o stage 1\\n29.6\\n36.5\\n40.2\\n39.3\\nw/o stage 2\\n28.7\\n35.2\\n39.4\\n38.3\\nLayers only need to learn the parameter of experts. However,\\nit may lead to sub-optimal because the hash strategy is selected\\nbased on human knowledge and may be inappropriate. BASE\\nLayer [15] enforces a balanced token-to-expert assignment\\nthrough a linear assignment problem, which simplify the\\ntraining in another way. All these work ﬁnd the problem of\\njointly training and targeting at alleviate it.\\nC. Language Modeling Results\\nModel Architecture: We pretrain the representative GPT\\nmodel for the language modeling task, where we set standard\\nTransformer(TRM) with 24 decoder layers, hidden dimension\\nas 1024 and number of attention heads as 16. Every other FFN\\nlayer is replaced by in standard Transformer with the MoE\\nlayer (16 experts per layers) to construct the MoE models.\\nThere totally exists 12 MoE layers and thus 192 experts (i.e.,\\n12 × 16). Meanwhile, larger TRM is scaled by increasing its\\nFFN hidden size.\\nModel Performance: We pretrained each model on the\\nOpenWebText dataset for 200k steps totally, 10k of which was\\nthe warm-up phase. For our EvoMoE, we scheduled the ﬁrst\\n10k steps as the expert-diversify stage and the following 5k\\nsteps for annealing temperature from 2.0 to 0.3. We report the\\nperplexity on the test set. Results are summarized in Table III.\\nCompared with other baselines, our EvoMoE achieves the\\nbest result among all baselines. Speciﬁcally, the perplexity\\nof EvoMoE is 12.24, which achieves a 2.90 improvement\\ncompared with 15.14 of standard TRM. Meanwhile, all MoE\\nmodels outperform their backbone model (standard TRM)\\nbecause of their large model capacity. Larger TRM slightly\\noutperforms standard TRM because of its large model size,\\nwhich demonstrates the sample-efﬁcient of large models.\\nCompared with other MoE methods, EvoMoE beneﬁts from\\nthe sufﬁcient training of experts in the early stage and then\\nmake the experts selection becomes sparser on the basis of\\nTABLE V\\nBLEU PERFORMANCE OF MOE MODELS WITH DIFFERENT EXPERT\\nNUMBER.\\nNumber of Experts\\n4\\n8\\n16\\nSwitch\\n28.4\\n28.6\\n28.7\\nEvoMoE\\n29.6\\n29.9\\n30.0\\nw/o stage 1\\n29.6\\n29.9\\n30.1\\nw/o stage 2\\n28.7\\n28.9\\n28.9\\nTABLE VI\\nEFFICIENCY OF MOE MODELS WITH DIFFERENT EXPERT NUMBER, AND\\nTHE RESULTS ARE NORMALIZED OVER SWITCH.\\nNumber of Experts\\n4\\n8\\n16\\nSwitch\\n1\\n1\\n1\\nEvoMoE\\n0.82\\n0.78\\n0.75\\nw/o stage 1\\n0.86\\n0.82\\n0.81\\nw/o stage 2\\n0.95\\n0.93\\n0.92\\nspecialized experts. Speciﬁcally, EvoMoE outperforms other\\nMoE methods up to 0.88 ppl and 0.545 ppl on average. The\\nanalysis between different methods is same as that in GLUE\\nresults.\\nD. Machine Translation Results\\nModel Architecture: We pretrain the representative T5\\nmodel for the machine translation task, where we set standard\\nTransformer(TRM) with 12 encoder-decoder layers, hidden\\ndimension as 768 and number of attention heads as 12. Every\\nother FFN layer is replaced by in standard Transformer with\\nthe MoE layer (4 experts per layers) to construct the MoE\\nmodels.\\nModel Performance: We compare EvoMoE with Trans-\\nformer and Switch-Transformer on four language-pair datasets,\\nincluding English to German, German to English, English\\nto French and French to English. Results are shown in\\nTable IV, V, VI. We remark that these models all have\\nthe same inference speed even if MoE models enlarge the\\nparameter size. We show the BLEU score on the test set of\\neach dataset in Table IV. EvoMoE outperforms other methods\\nby 1 BLEU score on average. Although Switch-Transformer\\nscale the model size, it still achieves a similar performance\\nwith Transformer-Base, which is parameter-efﬁcient. Table V\\n9\\n2x Speedup\\n(a) Validation PPL over steps\\n1.25x Speedup\\n(b) Validation PPL over FLOPs\\nFig. 6.\\nEnd-to-end performance comparison between GPT-ori, GPT-Switch and GPT-DTS. Figure 6(a) and Figure 6(b) represent the curve of PPL over\\niterations and FLOPs, where GPT-DTS can obtain 2.0x speed-up to reach the same validation perplexity, as well as higher FLOPs-efﬁciency of a 1.42x\\nspeed-up.\\nshows the BLEU performance of different expert number on\\nthe English-German datasets. EvoMoE can still outperform\\nthe Switch-Transformer about 1.3 BLEU with the increasing\\nnumber of experts. Because of the datasets’ quality, the effect\\nof increasing expert number is limited.\\nModel-Efﬁcency: Table VI show the model efﬁciency of\\nEvoMoE and Switch Gate on the English-German datasets.\\nEvoMoE is efﬁcient at the speed of converge. For exam-\\nple, EvoMoE need only 75% compute budget of Switch-\\nTransformer to reach the same PPL. It is worth noting that\\nthe speedup over Switch-Transformer improves as the expert\\nnumber increases.\\nAblation Study: We present a ablation study on EvoMoE\\nto show the inﬂuence of two stages by removing the expert-\\ndiversify phase and the gate-sparsify phase respectively. Re-\\nsults are summarized in Table IV V VI. As for the model\\nperformance metric, it will lead to performance degrada-\\ntion when EvoMoE removes the gate-sparsify stage, such as\\n38.3/39.2 in Fr-En of Table IV. Meanwhile, it is worth noting\\nthat inﬂuence is little as for w/ and w/o the expert-diversify\\nstage, which encourages us to involve this stage for saving\\ncomputation budget. As for the FLOPs-efﬁciency metric, the\\ngate-sparsify phase can improve the FLOPs-efﬁciency by 17%.\\nBy introducing the expert-diversify stage, EvoMoE can obtain\\nan extra 4% improvement.\\nIn summary, the gate-sparsify phase can both improve the\\nmodel performance and FLOPs-efﬁciency signiﬁcantly and the\\nexpert-diversify phase can introduce extra FLOPs-efﬁciency\\nwithout performance degradation. In the following sections,\\nwe will detail analyze the gate-sparsify phase and evaluate it\\nat large scale.\\nE. Breakdown on the Gate-Sparsify Phase\\nModel Architecture: We pretrain the representative GPT\\nmodel for the language modeling task, where we set GPT-ori\\nwith 24 decoder layers, hidden dimension as 1024 and number\\nof attention heads as 16. Every other FFN layer is replaced\\nby in standard Transformer with the MoE layer (16 experts\\nper layers) to construct the GPT-MoE model. GPT-Switch\\nrepresents training MoE models with Switch gate, which keeps\\nTop-1 selection. GPT-DTS represents training MoE models\\nwith the dense-to-sparse gate, which starts as a dense\\ngate that routes tokens to most experts and then gradually\\nbecomes sparser.\\nWe compare the required FLOPs to train models to show the\\nFLOPs-efﬁciency of different methods. The FLOPs-efﬁciency\\nis deﬁned as the best model performance (PPL) can be\\nachieved given the ﬁxed number of ﬂoating-point operations\\n(computation budget). Because the actual training time could\\nbe affected by the system engineering efforts on the implemen-\\ntation details, which are not our focus in this approach. Instead,\\nin our experiments, we prefer to choose the computation\\ncomplexity for fair comparisons.\\nModel Performance: We pretrained each model on the\\nOpenWebText dataset for 300k steps totally, 10k of which\\nwas the warm-up phase. For our EvoMoE, we scheduled the\\nﬁrst 10k steps for annealing temperature from 2.0 to 0.3. We\\nreport the perplexity on the validation set. Results are shown\\nin Figure 6. To improve the computation efﬁciency, only part\\nparameters are used for each token in sparse models with the\\ncost of model performance. DTS-Gate aims to shift the model\\ntraining from dense to sparse, and keep the inference cost same\\nas sparse models. Experiments show that compared with the\\nstate-of-the-art Switch-Transformer in GPT-MoE model with\\nOpenWebText dataset, GPT-DTS can obtain 2.0x speed-up to\\nreach the same validation perplexity (Figure 6(a)), as well as\\nhigher FLOPs-efﬁciency of a 1.42x speed-up (Figure 6(b)).\\nExperiments also verify the ability of dense-to-sparse\\ngate for scaling models with more experts or more MoE layers.\\nComparison with Sparse Models. MoE-Switch pre-deﬁnes\\nits static Top-1 gating network and jointly training the gate\\nand experts networks. Different from GPT-Switch, GPT-DTS\\nutilizes temperature to adjust the distribution of the token-to-\\nexperts (one-hot or uniform) and threshold to remove compu-\\ntation of experts with low weights. EvoMoE performs better\\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nFLOPs\\n1e12\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\nValidation PPL\\nScale with more experts\\n1-expert\\n4-experts\\n8-experts\\n16-experts\\n(a) More Experts\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nFLOPs\\n1e12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nValidation PPL\\nScale with more MoE layers\\n0-MoE-Layer\\n1-MoE-Layer\\n4-MoE-Layer\\n6-MoE-Layer\\n(b) More MoE Layers\\nFig. 7. Scalability for DTS gate. It shows that more experts or more MoE-layers (larger models with constant FLOPs), will lead to better FLOPs-efﬁciency.\\nthan GPT-Switch in sample-efﬁciency because of more experts\\ninvolved in training and updates at the beginning, shown as\\nFigure 6(a). As for FLOPs-efﬁciency, DTS-Gate ﬁrst involves\\nmore experts into warm-up training, which is poor FLOPs-\\nefﬁcency. But with the training going on, GPT-DTS can obtain\\ngreater than 25% improvements in FLOPs-efﬁciency com-\\npared with the state-of-the-art Switch-Transformer in GPT-\\nMoE model with OpenWebText dataset.\\nF. Scalability\\nIn this subsection, we investigate different experiment set-\\ntings to validate the scalability of our DTS-Gate.\\nModel Architecture: We choose the GPT-small as the\\nbackbone model for the language modeling task, where we\\nset the model with 12 decoder layers, hidden dimension as\\n768 and number of attention heads as 12.\\nIncrease the Expert Number: Based on GPT-Small model\\nwith 117M parameters, we replace the 7-th FFN layer by one\\nMoE layer and vary its experts number within {1, 4, 8, 16}.\\nAs shown by Figure 7(a), with increasing expert numbers,\\nEvoMoE keeps consistent improvements (i.e., lower PPL)\\nduring training.\\nIncrease the MoE layer number: Similarly, we also vary\\nthe number of MoE layers to validate the performance of\\nDTS gate. We insert k MoE layers in GPT-Small, where\\nk ∈{0, 1, 4, 6} and each MoE layer contains 8 experts.\\nFigure 7(b) shows that by increasing MoE layers, EvoMoE\\ncan achieve better model performance with same FLOPs.\\nG. Effect of Sparsity Scheduler\\nIt is worth noting that several hyper-parameters are intro-\\nduced in Dense-To-Sparse gate, such as max/min temperature\\nand decay iterations. In this section, we analyze the effect of\\ndifferent hyper-parameter setting by conducting experiments\\nof various settings. The training model is GPT-MoE, 24-layer\\ndecoder with 12 MoE-layer (16 experts per layer) and the\\ndataset is OpenWebText.\\nWe decay temperature τ from max value to min value in\\nthe ﬁrst 15000 iterations and switch to Top1 then. Experiments\\nwith different max value to min value are evaluated, and\\nthe results are shown in Figure 8.\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e13\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\nValidation PPL\\nDifferent Temperature Scheduler\\n1 to 0.1\\n2 to 0.1\\n2 to 0.3\\n2 to 0.5\\nFig. 8. Effect of different temperature scheduler.\\nTABLE VII\\nTHE MOST FREQUENT TOKENS ASSIGNED TO EACH EXPERT IN THE\\nVALIDATION SET, WHICH SHOWS THAT SOME EXPERTS ASSIGNMENT\\nDECISIONS ARE MADE BASED ON LOCAL CONTEXTS. FOR MANY OTHER\\nEXPERTS, THE ASSIGNMENT DECISION DEPENDS ON LONGER CONTEXT,\\nAND IS HARDER TO VISUALIZE.\\nExpert\\nTop5 Proceeding Tokens\\nDescriptions\\n1\\nis\\nwas\\nbe\\nare\\nhave\\nauxiliary verbs\\n3\\n.\\n\\\\n\\n/\\n(\\n;\\npunctuations\\n4\\nin\\n,\\nof\\nand\\nfrom\\nprepositions\\n6\\nand\\nI\\nit\\nthat\\nthey\\npossessive cases\\n12\\nout\\nup\\ngo\\nback\\ndown\\ndirectional prepositions\\nMax/Min Temperature Under small temperatures, the\\nweight distribution of experts is close to one-hot, which\\nleads to the one-token-one-expert distribution and low training\\ncost, but the variance of gradients is large. In contrast, large\\ntemperatures result in nearly uniform distribution gate weights,\\nwhich evolves more experts into training but the variance of\\ngradients is small. As shown in Figure 8, we ﬁnd the these\\ntwo hyper-parameters have low inﬂuence on the model quality\\nunder the same training budget, except for the extrame value,\\ne.g., 1.0 for max value and 0.1 for min value.\\nH. Visualization of Expert Specialization\\nWe visualize the routing strategy of the pre-trained GPT\\nmodel by EvoMoE in Table VII through its corresponding\\n11\\ninput embedding, where each MoE layer contains 16 experts.\\nFor each expert, we present the Top5 proceeding tokens\\nassigned and give descriptions for explanations from our points\\nof view. For example, Expert 1 captures the auxiliary verbs and\\nExpert 6 captures possessive cases. These experts can capture\\nlocal contexts of each embedding well. For other experts, it is\\ndiff cult to visualize because of the long contexts’ inﬂuence.\\nVI. RELATED WORK\\na) Static Sparse Neural Networks: Exploiting the spar-\\nsity in deep neural networks can reduce both storage and com-\\nputation requirements for model training and inference. One of\\nthe most widely used sparsiﬁcation methods is weight pruning\\n[31, 32]. Previous studies proposed to prune away redundant\\nor less useful weights based on various pruning criteria (e.g.,\\nthe importance of individual weights [32] or groups of weights\\n[33, 34, 35]) and then ﬁne-tune remaining weights to regain the\\nlost accuracy. After pruning and ﬁne-tuning, parts of weights\\nare permanently removed, inducing a static sparsity pattern in\\nDNNs. The sparsity pattern/structure is a trade-off between\\nmodel effectiveness and hardware efﬁciency [36]. Early work\\nattempts to increase the sparsity ratio or model accuracy by\\nemploying unstructured sparsiﬁcation methods, while recent\\nwork focuses more on structured sparsity for practical speedup\\non hardware. Interestingly, [37] points out training a sparse\\nnetwork from scratch is superior or comparable to pruning-\\nbased methods. Our EvoMoE adopts a dense-to-sparse gate,\\nwhich is analogous to pruning-based methods that train all\\nexperts ﬁrst and then learning the sparse gate routing.\\nb) Conditional Computation with MoE: Different from\\nprevious static sparse neural networks that permanently re-\\nmove partial weights, conditional computation [6] only acti-\\nvates the relevant parameter of the model on a per-sample\\nbasis, which can be regarded as a dynamic sparsity structure\\nthat remains all model weights but brings sparsity into the\\ncomputation. The mixture-of-expert (MoE) architecture [7],\\nas a speciﬁc form of conditional computation, contains a\\nseries of experts and a trainable gating network which routes\\neach input sample to its corresponding experts. Conditional\\ncomputation is capable of reducing inference cost (without\\nreducing model capacity) or increasing model capacity (with-\\nout increasing inference cost) from a model acceleration or\\nscaling perspective. On the other hand, the sparsely activated\\nparts (i.e., MoE in models) can be regarded as structured\\nsparse blocks, which does not introduce additional compu-\\ntational overhead. However, conditional computation models\\nare often difﬁcult to train, since they require learning discrete\\nrouting decisions from individual examples to experts and\\nthe gating network tends to converge to a state that only\\nselects the same few experts [38]. LSTM-MoE [7], GShard [9]\\nand Switch-Transformer [8] utilize auxiliary load-balancing\\nlosses to mitigate this self-reinforcement phenomenon and\\nthus improve training efﬁciency. In such MoE models, the\\ngating network and experts, as two critical components, are\\njointly trained which may interfere with each other. In Evo-\\nMoE, we consider decoupling the training of experts and the\\ngating network by involving all experts starting with a high\\ntemperature in Gumbel-Softmax and then training the gating\\nnetwork to be sparser and select the best expert through decay-\\ning this temperature. BASELayer [10] formulates the token-\\nexpert routing as a linear assignment problem and guarantees\\nbalanced compute loads by employing numerous algorithms.\\nHashLayer [11] replaces the gating network with a hash-based\\nrouting strategy (e.g., random hash, clustered hash dispersed\\nhash). MoEﬁcation [39] utilizes the sparse activation in feed-\\nforward networks (FFNs) of a Transformer model and divides\\neach large dense FFN into several experts to accelerate model\\ninference, while EvoMoE evolves a small dense model into a\\nlarge and sparse MoE model.\\nc) Multi-Task Learning with MoE: The multi-task learn-\\ning (MTL) adopts a shared architecture to learn multiple tasks,\\nwhich exploits relationships among tasks and achieve better\\ngeneralization performance [40]. However, sharing parameters\\nbetween unrelated tasks can potentially degrade performance.\\nThe multi-gate MoE (i.e., MMoE) architecture is introduced\\nas an effective way to exploit both the commonalities and\\ndifferences among tasks, where each task has its own gate\\nthat adaptively controls the extent of parameter sharing [41].\\nDSelect-K [28] involves sparse gates (Top-K) for better param-\\neter sharing and trains gates from dense to sparse for smooth-\\nness, Regarding DSelect-K and the DTS gate in our EvoMoE\\nboth propose the dense-to-sparse mechanism, they are trying\\nto address two totally different problems, though both two\\npieces of work show SparseMoE is better than DenseMoE by\\ncoincidence. For our DTS work, because DenseMoE performs\\nwell but cost for large model pretraining, we tried to ﬁnd\\na more efﬁcient solution (SparseMoE). While for the multi-\\ntask work, because DenseMoE performs badly for multi-task\\nlearning, they tried to ﬁnd a better solution to deal with various\\ntasks, i.e., DSelectK. Therefore, this two pieces of work have\\nclearly different motivations.\\nVII. CONCLUSION AND FUTURE WORK\\nMoE models suffer from the training efﬁciency challenge\\ndue to the difﬁculty of training many experts and the gate\\nnetwork jointly. In this work, we presented an MoE training\\nframework EvoMoE that decouples the training of experts\\nand the gate network by ﬁrst spawning multiple diverse\\nexperts from one single well-trained base expert and then\\nlearning a increasingly sparse gate from a dense gate. Our\\nevaluations show that EvoMoE can not only achieve better\\nmodel quality in Transformers with given computation budget\\nbut also achieve better FLOPs-efﬁciency when comparing with\\nprevious works in MoE training. On the other hand, EvoMoE\\nopens challenges for system execution due to the computation\\nin the early stage and the adaptive capacity of experts. In\\nthe future, we would like to design and implement system-\\nlevel optimizations to achieve efﬁcient training in both model\\nquality and system execution.\\n12\\nREFERENCES\\n[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\\nA. Radford, I. Sutskever, and D. Amodei, “Language\\nmodels are few-shot learners,” in NeurIPS, 2020.\\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\\nderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,\\n“An image is worth 16x16 words: Transformers for\\nimage\\nrecognition\\nat\\nscale,”\\nin\\n9th\\nInternational\\nConference on Learning Representations, ICLR 2021,\\nVirtual Event, Austria, May 3-7, 2021, 2021. [Online].\\nAvailable: https://openreview.net/forum?id=YicbFdNTTy\\n[3] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen,\\nand T.-Y. Liu, “Do transformers really perform bad for\\ngraph representation?” arXiv preprint arXiv:2106.05234,\\n2021.\\n[4] Q. Chen, H. Zhao, W. Li, P. Huang, and W. Ou, “Be-\\nhavior sequence transformer for e-commerce recommen-\\ndation in alibaba,” in Proceedings of the 1st Interna-\\ntional Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data, 2019, pp. 1–4.\\n[5] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\\nD. Amodei, “Scaling laws for neural language models,”\\nCoRR, vol. abs/2001.08361, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2001.08361\\n[6] Y. Bengio, N. L´\\neonard, and A. C. Courville, “Estimating\\nor propagating gradients through stochastic neurons for\\nconditional computation,” CoRR, vol. abs/1308.3432,\\n2013. [Online]. Available: http://arxiv.org/abs/1308.3432\\n[7] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V.\\nLe, G. E. Hinton, and J. Dean, “Outrageously large\\nneural networks: The sparsely-gated mixture-of-experts\\nlayer,” in 5th International Conference on Learning\\nRepresentations, ICLR 2017, Toulon, France, April 24-\\n26, 2017, Conference Track Proceedings, 2017. [Online].\\nAvailable: https://openreview.net/forum?id=B1ckMDqlg\\n[8] W. Fedus, B. Zoph, and N. Shazeer, “Switch transform-\\ners: Scaling to trillion parameter models with simple and\\nefﬁcient sparsity,” CoRR, vol. abs/2101.03961, 2021.\\n[Online]. Available: https://arxiv.org/abs/2101.03961\\n[9] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat,\\nY. Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard:\\nScaling giant models with conditional computation and\\nautomatic sharding,” in 9th International Conference on\\nLearning Representations, ICLR 2021, Virtual Event,\\nAustria, May 3-7, 2021, 2021. [Online]. Available:\\nhttps://openreview.net/forum?id=qrwe7XHTmYb\\n[10] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and\\nL. Zettlemoyer, “Base layers: Simplifying training of\\nlarge, sparse models,” arXiv preprint arXiv:2103.16716,\\n2021.\\n[11] S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston,\\n“Hash layers for large sparse models,” arXiv preprint\\narXiv:2106.04426, 2021.\\n[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\\nO. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov,\\n“Roberta: A robustly optimized bert pretraining ap-\\nproach,” arXiv, 2019.\\n[13] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\\nI. Sutskever et al., “Language models are unsupervised\\nmultitask learners.”\\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\\n“Attention is all you need,” in NeurIPS, 2017, pp. 5998–\\n6008.\\n[15] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and\\nL. Zettlemoyer, “BASE layers: Simplifying training\\nof\\nlarge,\\nsparse\\nmodels,”\\nin\\nProceedings\\nof\\nthe\\n38th International Conference on Machine Learning,\\nICML 2021, 18-24 July 2021, Virtual Event, ser.\\nProceedings of Machine Learning Research, M. Meila\\nand T. Zhang, Eds., vol. 139.\\nPMLR, 2021, pp.\\n6265–6274. [Online]. Available: http://proceedings.mlr.\\npress/v139/lewis21a.html\\n[16] D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang,\\nand F. Wei, “Stablemoe: Stable routing strategy for\\nmixture\\nof\\nexperts,”\\nin\\nProceedings\\nof\\nthe\\n60th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2022, Dublin,\\nIreland, May 22-27, 2022, S. Muresan, P. Nakov, and\\nA. Villavicencio, Eds.\\nAssociation for Computational\\nLinguistics, 2022, pp. 7085–7095. [Online]. Available:\\nhttps://doi.org/10.18653/v1/2022.acl-long.489\\n[17] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and\\nS. R. Bowman, “GLUE: A multi-task benchmark and\\nanalysis platform for natural language understanding,” in\\nProceedings of the Workshop: Analyzing and Interpreting\\nNeural Networks for NLP, BlackboxNLP@EMNLP 2018,\\nBrussels,\\nBelgium,\\nNovember\\n1,\\n2018,\\nT.\\nLinzen,\\nG. Chrupala, and A. Alishahi, Eds.\\nAssociation for\\nComputational Linguistics, 2018, pp. 353–355. [Online].\\nAvailable: https://doi.org/10.18653/v1/w18-5446\\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\\n“Attention is all you need,” Advances in neural informa-\\ntion processing systems, vol. 30, 2017.\\n[19] L. J. Ba, J. R. Kiros, and G. E. Hinton, “Layer\\nnormalization,”\\nCoRR,\\nvol.\\nabs/1607.06450,\\n2016.\\n[Online]. Available: http://arxiv.org/abs/1607.06450\\n[20] J. Devlin, M. Chang, K. Lee, and K. Toutanova,\\n“BERT: pre-training of deep bidirectional transformers\\nfor language understanding,” in Proceedings of the\\n2019\\nConference\\nof\\nthe\\nNorth\\nAmerican\\nChapter\\nof\\nthe\\nAssociation\\nfor\\nComputational\\nLinguistics:\\nHuman Language Technologies, NAACL-HLT 2019,\\n13\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\\n(Long\\nand\\nShort\\nPapers),\\nJ.\\nBurstein,\\nC.\\nDoran,\\nand T. Solorio, Eds.\\nAssociation for Computational\\nLinguistics, 2019, pp. 4171–4186. [Online]. Available:\\nhttps://doi.org/10.18653/v1/n19-1423\\n[21] A.\\nRadford,\\nK.\\nNarasimhan,\\nT.\\nSalimans,\\nand\\nI.\\nSutskever,\\n“Improving\\nlanguage\\nunderstanding\\nby generative pre-training,” 2018.\\n[22] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\\nM. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring\\nthe limits of transfer learning with a uniﬁed text-\\nto-text transformer,” J. Mach. Learn. Res., vol. 21,\\npp.\\n140:1–140:67,\\n2020.\\n[Online].\\nAvailable:\\nhttp:\\n//jmlr.org/papers/v21/20-074.html\\n[23] S. Hochreiter and J. Schmidhuber, “Long short-term\\nmemory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780,\\n1997. [Online]. Available: https://doi.org/10.1162/neco.\\n1997.9.8.1735\\n[24] E.\\nJang,\\nS.\\nGu,\\nand\\nB.\\nPoole,\\n“Categorical\\nreparameterization\\nwith\\ngumbel-softmax,”\\nin\\n5th\\nInternational Conference on Learning Representations,\\nICLR\\n2017,\\nToulon,\\nFrance,\\nApril\\n24-26,\\n2017,\\nConference\\nTrack\\nProceedings,\\n2017.\\n[Online].\\nAvailable: https://openreview.net/forum?id=rkE3y85ee\\n[25] M. Baines, S. Bhosale, V. Caggiano, N. Goyal, S. Goyal,\\nM. Ott, B. Lefaudeux, V. Liptchinsky, M. Rabbat,\\nS. Sheiffer, A. Sridhar, and M. Xu, “Fairscale: A\\ngeneral purpose modular pytorch library for high per-\\nformance and large scale training,” https://github.com/\\nfacebookresearch/fairscale, 2021.\\n[26] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He,\\n“Deepspeed: System optimizations enable training deep\\nlearning models with over 100 billion parameters,” in\\nKDD ’20: The 26th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, Virtual Event,\\nCA, USA, August 23-27, 2020.\\nACM, 2020, pp.\\n3505–3506. [Online]. Available: https://doi.org/10.1145/\\n3394486.3406703\\n[27] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,\\nD. Grangier, and M. Auli, “fairseq: A fast, extensible\\ntoolkit for sequence modeling,” in Proceedings of\\nthe 2019 Conference of the North American Chapter\\nof\\nthe\\nAssociation\\nfor\\nComputational\\nLinguistics:\\nHuman Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Demonstrations,\\nW. Ammar, A. Louis, and N. Mostafazadeh, Eds.\\nAssociation for Computational Linguistics, 2019, pp.\\n48–53. [Online]. Available: https://doi.org/10.18653/v1/\\nn19-4009\\n[28] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoor-\\nthy, Y. Chen, R. Mazumder, L. Hong, and E. H. Chi,\\n“Dselect-k: Differentiable selection in the mixture of\\nexperts with applications to multi-task learning,” in Ad-\\nvances in Neural Information Processing Systems 34:\\nAnnual Conference on Neural Information Processing\\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin,\\nP. Liang, and J. W. Vaughan, Eds., 2021, pp. 29 335–\\n29 347.\\n[29] D. Hendrycks and K. Gimpel, “Gaussian error linear\\nunits (gelus),” arXiv preprint arXiv:1606.08415, 2016.\\n[30] D. P. Kingma and J. Ba, “Adam: A method for\\nstochastic optimization,” in 3rd International Conference\\non Learning Representations, ICLR 2015, San Diego,\\nCA, USA, May 7-9, 2015, Conference Track Proceedings,\\nY. Bengio and Y. LeCun, Eds., 2015. [Online]. Available:\\nhttp://arxiv.org/abs/1412.6980\\n[31] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain\\ndamage,” in Advances in neural information processing\\nsystems, 1990, pp. 598–605.\\n[32] S. Han, H. Mao, and W. J. Dally, “Deep compres-\\nsion: Compressing deep neural networks with pruning,\\ntrained quantization and huffman coding,” arXiv preprint\\narXiv:1510.00149, 2015.\\n[33] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning\\nstructured sparsity in deep neural networks,” Advances\\nin neural information processing systems, vol. 29, pp.\\n2074–2082, 2016.\\n[34] J.-H. Luo, J. Wu, and W. Lin, “Thinet: A ﬁlter level\\npruning method for deep neural network compression,”\\nin Proceedings of the IEEE international conference on\\ncomputer vision, 2017, pp. 5058–5066.\\n[35] Y. He, X. Zhang, and J. Sun, “Channel pruning for\\naccelerating very deep neural networks,” in Proceedings\\nof the IEEE international conference on computer vision,\\n2017, pp. 1389–1397.\\n[36] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang,\\nand W. J. Dally, “Exploring the granularity of sparsity\\nin convolutional neural networks,” in Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern\\nRecognition Workshops, 2017, pp. 13–20.\\n[37] J. Frankle and M. Carbin, “The lottery ticket hypothesis:\\nFinding sparse, trainable neural networks,” in Interna-\\ntional Conference on Learning Representations, 2018.\\n[38] D. Eigen, M. Ranzato, and I. Sutskever, “Learning fac-\\ntored representations in a deep mixture of experts,” arXiv\\npreprint arXiv:1312.4314, 2013.\\n[39] Z.\\nZhang,\\nY.\\nLin,\\nZ.\\nLiu,\\nP.\\nLi,\\nM.\\nSun,\\nand\\nJ. Zhou, “Moeﬁcation: Conditional computation of trans-\\nformer models for efﬁcient inference,” arXiv preprint\\narXiv:2110.01786, 2021.\\n[40] S. Ruder, “An overview of multi-task learning in deep\\nneural networks,” CoRR, vol. abs/1706.05098, 2017.\\n[Online]. Available: http://arxiv.org/abs/1706.05098\\n[41] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H.\\nChi, “Modeling task relationships in multi-task learning\\nwith multi-gate mixture-of-experts,” in Proceedings of\\nthe 24th ACM SIGKDD International Conference on\\nKnowledge Discovery & Data Mining, KDD 2018,\\nLondon, UK, August 19-23, 2018, Y. Guo and F. Farooq,\\nEds.\\nACM, 2018, pp. 1930–1939. [Online]. Available:\\nhttps://doi.org/10.1145/3219819.3220007\\n14\\n', 'source_name': 'EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate', 'source_url': 'https://arxiv.org/abs/2112.14397'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BlackMamba_NOTES.pdf #35\n",
      "{'content': 'BlackMamba: Mixture of Experts for State-Space Models \\nMain Idea: this work looks to combine the Mamba with the MoE architecture. Each of \\nthese architectures have unique advantages: Mamba has linear time and memory \\ncomplexity to increases in context length (is robust to long-range context), while MoE has \\nthe advantage of allowing for scaling model’s parameters while keeping inference costs \\nfixed at the expense of a larger memory footprint. BlackMamba (Mamba + MoE fusion) is \\nthen expected to have the long-range context robustness of Mamba while having the \\ninference efficiency of MoE. \\nThe models experimented with are larger than the previous work done (Mamba-MoE) but \\ncould be scaled much more (the models are, in [ACTIVE PARAMETERS/TOTAL \\nPARAMETERS], 340M/1.5B and 630M/2.8B) \\n \\nExpected Advantages (Synergies) of BlackMamba vs Dense Transformer \\n- \\nFrom Mamba \\no Linear computational complexity with respect to input sequence length for \\nboth training and inference. \\no Autoregressive generation in constant time and memory. \\n- \\nFrom MoE \\no Inference latency and training FLOPs of the equivalent smaller dense base \\nmodel, while preserving model quality close to an equivalent dense model \\nin terms of total parameters. \\n \\nMoE Details \\n- \\nMoE top-k routing is used. \\n- \\nMoE is compared/evaluated based on: \\no (Forward pass or active parameters) / total parameters ratio \\n- \\nSimilarly to Mixtral8x7B, a relatively small number of experts is used in \\nBlackMamba (even though scaling laws show promise in having many experts) to \\nbalance the inference FLOPs and memory cost of MoE (more experts = more \\nmemory costs). \\n \\nArchitecture \\n- \\nBlackMamba consists of replacing a few layers in the Transformer architecture: \\no The MLP/FF layers are replaced by sparse MoE layers. \\no The Attention layers are replaced by Mamba layers. \\n- \\nBlackMamba was trained on 300B tokens. This is consistent with the scaling done \\nin this paper compared to the previous work trying to combine these architectures \\n(MoE-Mamba was trained on 10B tokens and had significantly smaller model size). \\no 340M/1.5B and 630M/2.8B sized models trained (active parameters/total \\nparameters). \\no 8 experts used per MoE layer. \\no Found a slight advantage in using sequential versus parallel blocks, so \\nprioritized a sequential setup. \\n▪ This is equivalent to depth vs width. \\no Used top-1 routing with the Sinkhorn algorithm to ensure load balancing \\nbetween experts. \\n▪ Sinkhorn was the same algorithm used in BASE routing. It makes \\nrouting more efficient in accelerated hardware (GPUs). \\n▪ A novel version of Sinkhorn was developed, which has faster \\nconvergence. \\no Used the Megatron-LM framework for distributed training. \\no Trained using bf16 precision. \\n \\nResults \\n- \\nFor the same number of active parameters (equal at inference) and the same \\namount of training FLOPs (equal amount of training), BlackMamba performed \\nsignificantly better than the Transformer, Transformer-MoE and Mamba \\nequivalents. \\n- \\nAs expected, BlackMamba also showed significant latency improvements over the \\nother architectures. These latency improvements increase with an increase in \\ncontext length. \\no This indicates that the synergy between Mamba and MoE works. \\n- \\nIn terms of expert balance, most layers show this happens successfully. However, \\nlater layers show a clear transition towards expert imbalance. \\no Perhaps this is due to numerical instabilities that show as we get deeper \\ninto the network? \\no This pattern of instability in later MoE layers was also shown in the “Faster-\\nMoE” paper. \\n- \\nBlackMamba leaves room for future work in terms of the Mamba + MoE fusion: \\no Few-shot performance. \\no Quantization and PEFT performance. \\no Fine-tuning, instruction tuning and DPO performance. \\no Are the expert’s specialization dynamics in BlackMamba the same as in \\nTransformer MoEs? \\n \\nMy takeaways: \\n- \\nThe checkpoints of BlackMamba were released, so perhaps some investigation \\ncan be done in terms of exploring the expert’s specialization dynamics in the \\nBlackMamba architecture and compare it to regular Transformer MoEs. \\n', 'source_name': 'BlackMamba: Mixture of Experts for State-Space Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/BlackMamba_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "EvoMoE_NOTES.pdf #36\n",
      "{'content': 'EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-\\nSparse Gate \\nMain Idea: EvoMoE is a proposed end-to-end framework for training MoE models. The focus of \\nEvoMoE is to deal with the issues of immature experts and unstable sparse gates (instabilities \\nrelated to early stages of training, the same issue explored in StableMoE), which come from the \\ntraditional MoE framework and are harmful to convergence performance. This issue from \\ntraditional MoE is thought to come from training a sparse gate from scratch, with randomly \\ninitialized weights for both experts and router – impossible to not have router instabilities with \\nthis setup. To solve this, EvoMoE proposes starting training with a single expert, and gradually \\nevolving that into a large and sparse MoE structure. \\nIn sum: \\n- \\nEvoMoE allows the model to warm-up before dividing it into experts. \\n- \\nThe gate starts as dense and gradually sparsifies, allowing it to better understand how to \\nroute inputs to experts before it reaches a high degree of sparsity. \\n \\nMethod \\n2 stages: \\n1. Expert-Diversify – can be seen as an improved initialization technique. \\na. Start by training a single expert (so the early stages of training are the equivalent \\nof training a dense Transformer architecture). \\nb. After T training steps, the single expert is replicated N times to initialize all experts. \\nThe initialization of experts from the initial expert can be done in multiple ways: \\nadding random noise to each expert, randomly masking the initial expert’s \\nweights, etc. \\ni. EvoMoE adopts the random masking strategy for initializing experts from \\nan original warmed-up expert. \\nc. Once all experts are initialized, EvoMoE goes into a standard MoE period with a \\nDense-to-Sparse (DTS) Gate. \\ni. The training of the DTS Gate is what the next stage is all about. \\n2. Gate-Sparsify – training the router. \\na. The router starts as a dense gate which routes the input to most experts. The idea \\nis that at early stages of routing, the gate is not so good at its task, so would benefit \\nfrom more dense routing so it can analyze the relevant experts more thoroughly, \\ngaining more information about which experts work better from each input, \\ninstead of just using 1 or 2 experts at a time. \\nb. As more training steps are done with the router, the better it becomes, so the \\nsparser it can be. So DTS-Gate gradually becomes sparser. \\nc. This stage uses an auxiliary load balancing loss. \\n \\nExperiments \\n- \\nBaselines \\no Switch - top-1 routing. \\no BASE – linear assignment routing. \\no Hash Layer – hashing-based routing. \\no DSelectK – differentiable routing achieved through smoothing techniques. \\no StableMoE – gate distillation and freezing for routing consistency during training. \\n- \\nEvaluated on (all with 355M active parameters) \\no Machine translation - encoder-decoder setup. \\no Masked language modeling - encoder-only setup. \\no Language modeling - decoder-only setup. \\n- \\nEvery other FFN layer is replaced by an MoE layer (EvoMoE alternates between dense and \\nsparse FFNs. \\n- \\nEvoMoE beats other variants on all architectures (encoder, decoder, encoder-decoder) \\nand provides training speedups. \\n- \\nBoth the expert-diversify and gate-sparsify stages are shown to be useful, per ablation \\nstudies. \\n- \\nCompared to GPT-MoE, EvoMoE can provide a 2x training speedup (2x less training \\nsamples needed to achieve the same perplexity) as well as a 1.42x speedup in terms of \\nFLOPs efficiency (1.42x less training FLOPs needed to achieve the same perplexity). \\no The sample efficiency and FLOPs efficiency speedups are different because \\nEvoMoE’s routing is dense during some of the gate-sparsify stage, which requires \\nmore FLOPs per training sample. \\n- \\nWith increasing number of experts per layer, EvoMoE shows consistent improvements. \\n- \\nWith increasing number of MoE layers (replacing denser FFNs by MoE layers than in the \\ninitial setup), EvoMoE shows better performance while maintaining inference FLOPs \\n(although with a higher memory cost – more total parameters). \\n \\nMy takeaways: \\n- \\nResearch Idea – it might not be efficient to enforce load balancing due to some areas of \\nthe input space being more common than others (load balancing could cause undesired \\noverlap in clusters at the token-level). Perhaps there could be some synergy between \\nearly-stage stability and MegaBlocks (for stable gating + no necessary load balancing at \\nthe batch level). Could also explore how custom compute depending on the complexity \\nof the input could be implemented, and how this would perform. \\n- \\nOverall, EvoMoE shows promising results. The challenge to this framework is the dense \\nrouting stage of training, which incurs high compute costs, but is a part of the trade-off \\nfor achieving better routing stability and sample efficiency. \\n \\n', 'source_name': 'EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/EvoMoE_NOTES.docx'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "PE_SparsityCrafting_NOTES.pdf #37\n",
      "{'content': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for \\nInstruction Tuning on General Tasks \\nMain Idea: introduces parameter-efficient sparsity crafting (PESC), a technique consisting of \\ntransforming a dense LLM into an MoE architecture for added model capacity and making use of \\nadapters to differentiate experts without altering their original weights. This work focuses \\nspecifically on instruction-tuning. \\nThe motivation for this work comes mainly from leveraging the idea of sparse upcycling \\n(converting dense into MoE) to improve LLMs’ performance on instruction-tuning, since \\n“Mixture-of-experts meets instruction-tuning: A winning combination for large language models” \\nshowed how the MoE architecture is highly effective for instruction-tuning tasks. \\n \\nMethod: \\n- \\nSparse upcycling: \\no From a dense pre-trained LLM, transform the FFN layers in each Transformer block \\ninto a mixture-of-experts by replicating (copying) the FFN layer n times (n being \\nthe number of experts per layer). \\no The other layers of the Transformer block (embedding, attention, normalization) \\nremain the same. \\no Continue pre-training on this sparse architecture. \\n- \\nPESC: \\no Generally the same as sparse upcycling with a few caveats. \\n▪ The dense to sparse transformation is similar, but instead of replicating the \\nactual FFN layer, PESC initializes an adapter to represent each expert, while \\nthe FFN remains the same for each expert. \\n▪ PESC does not continue normal pre-training as sparse upcycling, it only \\nperforms instruction-tuning. \\n▪ PESC does not update all experts’ parameters/weights, but only each \\nexpert’s adapter instead for parameter-efficiency. \\n• This means that we don’t need n copies of the FFN parameters, but \\ninstead the equivalent of n copies of the adapter. \\n▪ For constructing the adapter, PESC uses QLoRA. \\n▪ Top-2 routing and auxiliary load balancing loss were used. \\nParameter Efficiency Gains \\nWhile in sparse upcycling we are trying to optimize Fi (Theta(o)), where this represents the \\nobjective function in respect to all experts’ parameters, in PESC we are optimizing expert \\nadapters to approximate Fi(Theta(o)) through ~Fi(Theta(o), w(o)), where w(o) represents the \\nadapters’ weights. \\nThis provides more efficiency in: \\n- \\nTraining costs, since w(o) is significantly smaller than Theta(o). \\n- \\nMemory costs, since instead of replicating a full FFN layer for each expert, we are \\nreplicating an adapter for each expert, which is significantly smaller. \\no Original FFN weights are shared between experts, so only one copy per MoE layer \\nis needed. \\n \\nExperiments \\n- \\nThe largest PESC model trained was Camelidae-8x34B-pro (38B total parameters, ~34B \\nactivated parameters). \\n- \\nStrong performance of Camelidae-8x34B-pro on benchmarks analyzed when compared \\nto other SOTA chat models (Mixtral-8x7B-Instruct, GPT 3.5, Llama-2-70B-Chat). \\no Especially strong in knowledge and reasoning, math and coding. \\no Comparable overall performance to GPT 3.5. \\n- \\nDense vs sparse variations \\no Significant advantage of Camelidae-8x7B over Llama2-7B-Chat and Vicuna-7B, \\nespecially in more complex areas (coding and math). \\no Advantages are only amplified in the 10-20B range with Camelidae-8x13B. \\no Strong performance continues in the 30-50B range, with Cameliade-8x34B-pro \\noutperforming the leading sparse model Mixtral-8x7B-Instruct (47B total \\nparameters, 13B active parameters). \\no PESC effectively mitigates the knowledge forgetting issue observed in the \\ninstruction-tuning process of Camelidae’s dense counterpart Camel. \\no Increasing the number of experts in the MoE layers significantly improves the \\nmodel’s performance. \\n▪ Experimented with relatively low number of experts per MoE layer, from \\n4 to 16. \\no Increasing the number of experts in this approach seems way less costly than with \\na regular MoE, since we would need to add m more adapters and not m more \\nFFNs. \\n \\nMy takeaways: \\n- \\nThe sparsity crafting idea seems to pretty much be parameter-efficient sparse upcycling \\napplied to instruction-tuning. \\n- \\nSounds possible to practically apply this to TinyLlama-1B? Or another model in the 1-3B \\nrange. \\n \\n', 'source_name': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_SparsityCrafting_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MegaBlocks.pdf #38\n",
      "{'content': 'MEGABLOCKS: EFFICIENT SPARSE TRAINING WITH MIXTURE-OF-EXPERTS\\nTrevor Gale 1 Deepak Narayanan 2 Cliff Young 3 Matei Zaharia 1\\nABSTRACT\\nWe present MegaBlocks, a system for efﬁcient Mixture-of-Experts (MoE) training on GPUs. Our system is\\nmotivated by the limitations of current frameworks, which restrict the dynamic routing in MoE layers to satisfy\\nthe constraints of existing software and hardware. These formulations force a tradeoff between model quality and\\nhardware efﬁciency, as users must choose between dropping tokens from the computation or wasting computation\\nand memory on padding. To address these limitations, we reformulate MoE computation in terms of block-sparse\\noperations and develop new block-sparse GPU kernels that efﬁciently handle the dynamism present in MoEs. Our\\napproach never drops tokens and maps efﬁciently to modern hardware, enabling end-to-end training speedups\\nof up to 40% over MoEs trained with the state-of-the-art Tutel library and 2.4× over DNNs trained with the\\nhighly-optimized Megatron-LM framework.\\n1\\nINTRODUCTION\\nExploiting sparsity in the weights, activations and input data\\nof deep neural networks (DNNs) is an effective technique\\nfor reducing the amount of computation that is needed to\\nachieve a given model quality (Han et al., 2015; Gale et al.,\\n2019). The past decade has seen signiﬁcant progress in\\nalgorithms and high-performance software to make sparsity\\npractically useful (Gray et al., 2017; Narang et al., 2017;\\nKalchbrenner et al., 2018; Elsen et al., 2020; Gale et al.,\\n2020). One area that remains a challenge for sparsity is\\nmodel training on accelerators. DNNs are most commonly\\ntrained on hardware accelerators like GPUs (NVIDIA, 2020)\\nand TPUs (Jouppi et al., 2017), which exploit the regularity\\nof dense computation to deliver high performance. Con-\\nsequently, ﬁne-grained sparse computation is less efﬁcient\\non these processors. To enable efﬁcient computation on ac-\\ncelerators, structure can be enforced on the sparse matrices\\n(Narang et al., 2017; Gray et al., 2017; Yao et al., 2019).\\nAn emerging class of models with underlying structured\\nsparsity is Mixture-of-Experts (MoEs) (Shazeer et al., 2017).\\nEach layer in an MoE is a collection of experts, which are\\nthemselves small DNNs. As data is passed through the MoE\\nlayers, each token is dynamically routed to a subset of the\\nexperts for computation. By exploiting this sparse computa-\\ntion, MoEs have reduced training times by as much as 4× for\\napplications in natural language processing and computer\\nvision (Artetxe et al., 2021; Riquelme et al., 2021). These\\n1Stanford University, Stanford, California, USA 2Microsoft\\nResearch, Redmond, Washington, USA 3Google Research, Moun-\\ntain View, California, USA. Correspondence to: Trevor Gale\\n<tgale@cs.stanford.edu>.\\ngains have translated to new levels of scale for model train-\\ning, pushing model sizes past 1 trillion parameters (Artetxe\\net al., 2021; Du et al., 2021; Fedus et al., 2022).\\nThe challenge in computing MoEs efﬁciently is handling\\nthe dynamic routing and load-imbalanced computation that\\nare fundamental to these architectures. However, existing\\nhardware and software for deep learning make it difﬁcult\\nto meet this challenge. For example, TPUs and their XLA\\ncompiler require all tensor shapes to be known statically\\nand often struggle with ﬁne-grained operations like scatters\\nand gathers (Fedus et al., 2022). These constraints make it\\ndifﬁcult to implement MoEs directly on TPUs. While GPUs\\nare more ﬂexible, the sparse computation in MoEs does not\\nmap cleanly to the software primitives supported in major\\nframeworks and libraries.\\nState-of-the-art frameworks for MoE training sidestep these\\nchallenges by placing rigid constraints on MoE routing. In\\norder to remove the dynamism from the computation, the\\nset of tokens mapped to each expert are trimmed or padded\\nto a user-speciﬁed size (Lepikhin et al., 2020; Fedus et al.,\\n2022; Hwang et al., 2022). This procrustean formulation\\nintroduces a tradeoff between model quality and hardware\\nefﬁciency, as users must decide whether to drop tokens or\\nwaste computation and memory on padding. This decision is\\noften made through hyperparameter tuning, which increases\\nthe complexity of using MoEs.\\nTo address these challenges, we develop an approach for\\nMoE routing and computation based on sparse primitives.\\nOur approach never drops tokens and maps efﬁciently to\\nmodern GPUs, enabling end-to-end training speedups of up\\nto 40% and 2.4× over state-of-the-art frameworks for MoE\\nand DNN training, respectively. We make the following\\narXiv:2211.15841v1  [cs.LG]  29 Nov 2022\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\n.54\\n.91\\n.75\\n.37\\n.43\\n.66\\n1\\n2\\n0\\n2\\n2\\n1\\nPermutation\\n“the”\\nhidden_size\\ntokens\\nRouter\\n(1) Routing\\nAssign token feature vectors to experts \\nbased on probabilities.\\nProbabilities\\n“quick”\\n“brown”\\n“fox”\\n“jumped”\\n“over”\\nExpert Indices\\n(2) Permutation\\nGroup tokens by expert. Drop tokens that \\nexceed expert capacity.\\nExpert-2\\nExpert-1\\nExpert-0\\n(3) Computation\\nCompute the expert layers for the set of \\ntokens they were assigned.\\n(4) Un-Permutation\\nUn-permute the results and scale each by \\nits expert probability.\\ncapacity_factor=1\\n“brown”\\n(unused)\\n“the”\\n“jumped”\\n“quick”\\n“fox”\\n(dropped)\\nPermutation\\nExpert-1(“the”)\\nExpert-0(“quick”)\\nExpert-2(“brown”)\\nExpert-0(“fox”)\\nExpert-1(“jumped”)\\n0\\nScale\\nFigure 1. A Mixture-of-Experts Layer. Shown for num experts=3, top k=1 and capacity factor=1 with the prevalent, token dropping\\nformulation. First (1), tokens are mapped to experts by the router. Along with expert assignments, the router produces probabilities that\\nreﬂect the conﬁdence of the assignments. Second (2), the feature vectors are permuted to group tokens by expert assignment. If the\\nnumber of tokens assigned to an expert exceeds its capacity, extra tokens are dropped. Third (3), the expert layers are computed for the\\nset of tokens they were assigned as well as any padding needed for unused capacity. Lastly (4), the results of the expert computation are\\nun-permuted and weighted by the router probabilities. The outputs for dropped tokens are shown here set to zero.\\nspeciﬁc contributions:\\n• We show how the computation in an MoE layer can be\\nexpressed as block-sparse operations to accommodate\\nimbalanced assignment of tokens to experts. We use\\nthis formulation to train dropless-MoEs (dMoEs).\\n• We develop high-performance GPU kernels for block-\\nsparse matrix products that efﬁciently handle dynamic\\nMoE computation. Our kernels use two techniques,\\nblocked-CSR-COO encoding and transpose indices, to\\nenable efﬁcient matrix products with sparse inputs and\\noutputs in transposed or non-transposed order.\\nWe have implemented these techniques in a system called\\nMegaBlocks, which builds on the state-of-the-art Megatron-\\nLM library for training Transformer models (Shoeybi et al.,\\n2019). We evaluate our system through both microbench-\\nmarks and end-to-end training of Transformer language\\nmodels.\\n2\\nBACKGROUND: MOE LAYERS\\nMoE layers are made up of many experts, which are them-\\nselves small neural networks. Each token1 is dynamically\\nrouted to a subset of the experts for computation based on\\nscores computed by a router. The experts are commonly\\ndeﬁned to be small multi-layer perceptrons (MLPs). It is\\ntypical for tokens to be sent to a small number of experts,\\noften between 1 and 4 (Fedus et al., 2022).\\nMoE layers are often interleaved with other DNN layers and\\nare most commonly used to replace the feed-forward net-\\n1For natural language, data is commonly called tokens. For\\nvision, the data is typically pixels or patches (Dosovitskiy et al.,\\n2021). For simplicity, we use the term token throughput this paper.\\nwork (FFN) layers in Transformers (Shazeer et al., 2017; Fe-\\ndus et al., 2022). This hybrid architecture has demonstrated\\nstrong results on both natural language and vision tasks (Du\\net al., 2021; Riquelme et al., 2021). It is conjectured that\\nthese improvements are a result of experts specializing to\\ndifferent parts of the data distribution (Shazeer et al., 2017).\\nWe illustrate an MoE layer in Figure 1 and describe it in\\ndetail in the remainder of this section.\\n2.1\\nRouting\\nThe ﬁrst stage of an MoE layer is the router, which is respon-\\nsible for determining the assignment of tokens to experts. In\\naddition to expert assignments, MoE routers also produce\\nprobabilities for each assignment that reﬂect the conﬁdence\\nof the mapping. These weights are encoded as a matrix of\\nscores for each token-expert pair, which are used to linearly\\ncombine the top k expert outputs for each token (see §2.4).\\nThe most common style of MoE routing is the learned router\\nproposed by Shazeer et al. (2017). In this router, the tokens\\nare projected from hidden size elements to num experts\\nscores by multiplying with a weight matrix that is learned\\njointly with the other model parameters. The scores are\\nnormalized with a softmax and the routing decisions are\\nmade by greedily selecting the top k scoring experts for\\neach token.\\n2.2\\nPermutation\\nState-of-the-art MoE implementations aim to compute all\\nexpert layers in parallel in order to make effective use of the\\nparallelism available on GPUs and TPUs (Lepikhin et al.,\\n2020; Fedus et al., 2022; Hwang et al., 2022). The stan-\\ndard primitive used by implementations is batched matrix\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\n1.0\\n1.5\\n2.0\\nmax\\nCapacity Factor\\n2.20\\n2.25\\n2.30\\n2.35\\n2.40\\n2.45\\n2.50\\n2.55\\n2.60\\nValidation Loss\\nTransformer-Small\\nTransformer-Medium\\nTransformer-Large\\nMoE-small (e64, top-1)\\nFigure 2. MoEs Trained on The Pile with Different Capacity\\nFactors. The loss reached by the MoE models decreases signiﬁ-\\ncantly as expert capacity is increased, but at the cost of additional\\ncomputation. The lowest loss is achieved by the “max” capacity\\nfactor model, which avoids dropping tokens through the dynamic\\ncapacity factor mechanism proposed by Hwang et al. (2022).\\nmultiplication, which computes a set of matrix products of\\nthe same shape (see Figure 3A). However, mapping MoE\\ncomputation to this primitive is non-trivial. In order to re-\\nspect the shape constraints of batched matrix multiplication,\\nthe experts must be constrained to have weight matrices of\\nthe same shape and the number of tokens assigned to each\\nexpert must be equal. The latter constraint is particularly\\nproblematic because the learned routing algorithm described\\nabove provides no guarantees of a load balanced assignment\\nof tokens to experts.\\nIn order to satisfy this constraint, prior work has deﬁned a\\nﬁxed expert capacity, which is the number of tokens that\\neach expert can be assigned (Lepikhin et al. (2020); Fedus\\net al. (2022)). If the number of tokens assigned to an expert\\nexceeds its capacity, the extra tokens are dropped. That is\\nto say, they are not passed to any expert for computation\\nand the model relies on a residual connection to reintroduce\\nthe dropped tokens’ representation after the MoE layer. If\\nan expert layer is not assigned enough tokens to ﬁll its\\ncapacity, its set of tokens is padded to ﬁll the remaining\\nspace. Expert capacity is typically speciﬁed in terms of a\\ncapacity factor hyperparameter, which is a multiplier on the\\nexpected number of tokens that would be assigned to each\\nexpert under a perfect uniform distribution:\\nexpert capacity = num tokens\\nnum experts × capacity factor\\nThe capacity factor can be thought of as a parameter that\\nreduces the chance of dropping a token. This hyperparam-\\neter represents a tradeoff between additional computation\\nand model quality. As such, it is desirable to minimize the\\namount of load imbalance in the assignment of tokens to ex-\\nperts. The typical mechanism for doing so is auxiliary load\\nTable 1. Transformer Model Conﬁgurations.\\nThese models\\nare based on those used by Vaswani et al. (2017) and Brown\\net al. (2020). FLOPs were calculated using the expression from\\nNarayanan et al. (2021b) with a single sequence. All models use\\nffn hidden size=4×hidden size.\\nTransformer\\nhidden size\\nnum layers\\nWeights (M)\\nGFLOPs\\nXS\\n512\\n6\\n46\\n316\\nSmall\\n768\\n12\\n125\\n879\\nMedium\\n1024\\n24\\n356\\n2487\\nLarge\\n1536\\n24\\n760\\n5122\\nXL\\n2048\\n24\\n1316\\n8684\\nbalancing losses, which incentivize the router to produce a\\nbalanced assignment (Shazeer et al., 2017; Lepikhin et al.,\\n2020; Fedus et al., 2022). These losses additionally help to\\nensure that all experts see a similar number of tokens during\\ntraining. This is thought to be important to avoid degenerate\\nstates where some experts are assigned zero tokens and stop\\nreceiving gradient updates (Zhou et al., 2022).\\nIn addition to enabling batched computation of the expert\\nlayers, these constraints allow all tensor shapes to be known\\nstatically, which is required by TPUs and XLA.\\n2.3\\nComputation\\nOnce the data has been permuted, the experts can be com-\\nputed in parallel. For models where the experts are MLPs,\\nthis entails computing each layer for all experts using\\nbatched matrix multiplication. For convolutional experts,\\nthe layers can be computed with grouped convolutions.\\n2.4\\nUn-permutation\\nAfter the experts are computed, the resulting feature vectors\\nare un-permuted such that their ordering matches that of the\\ninput to the layer. The last step in MoE computation is to\\nscale the output tokens by the scores with which they were\\nassigned to their respective experts. When tokens are routed\\nto more than one expert, these weighted results are summed\\nto produce the ﬁnal layer output for each token.\\n3\\nMOTIVATION: TOKEN DROPPING IN\\nMOES\\nDespite the use of load balancing losses, prior work has\\nshown that token routing is still highly imbalanced (Hwang\\net al., 2022). To quantify the effect of token dropping on\\nmodel quality, we trained MoE language models on The\\nPile (Gao et al., 2020) with a range of capacity factors. We\\ntrain Transformer MoEs similar to those used by Fedus et al.\\n(2022), where each model is a Transformer with the FFN\\nlayers replaced with 64-expert MoE layers where each ex-\\npert is a 2-layer MLP matching the original FFN dimensions.\\nWe used top-1 routing and based our MoE model dimen-\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\n(A) Batched Matrix Multiplication\\nCompute a set of independent matrix multiplications of \\nthe same size in parallel.\\nT\\n(B) Block Diagonal Matrix Multiplication\\nExpert computation can equivalently be computed using block diagonal matrix \\nproducts with equal sized blocks along the diagonal.\\nT\\n(C) Block Sparse Matrix Multiplication\\nWe can enable load imbalanced routing and variable sized experts by \\nexpressing expert computation as block sparse matrix multiplication.\\nExpert-0\\nExpert-1\\nExpert-2\\nExpert-0\\nExpert-1\\nExpert-2\\nExpert-0\\nExpert-1\\nExpert-2\\nexpert_capacity\\nhidden_size\\nffn_hidden_size\\nFigure 3. Expert Computation in an MoE Layer. Shown with num expert=3. (A) State-of-the-art MoE implementations use batched\\nmatrix multiplication to compute all experts within a layer in parallel. This introduces the constraints that all experts are assigned the same\\nnumber of tokens and that all experts have the same shape. (B) Expert computation can be analogously posed in terms of block diagonal\\nmatrix multiplication with identically sized blocks. (C) In order to relax these constraints, we can construct a block diagonal matrix with\\nvariable sized blocks made up of many smaller blocks. We can compute this matrix efﬁciently using block-sparse matrix multiplication.\\nsions on the Transformer-Small model described in Table\\n1. All models were trained using the tokenization from\\nGPT2 (Radford et al., 2019) for 10B tokens with sequence\\nlength 1024, the Adam optimizer, and the learning rate and\\ngradient clipping settings from Shoeybi et al. (2019). We\\ntrained all models on a single A100 GPU with a batch size\\nof 512 sequences. We trained MoEs with capacity factor\\n1, 1.5, and 2 as well as the dynamic capacity factor tech-\\nnique proposed by Tutel (Hwang et al., 2022), where the\\ncapacity factor is set dynamically to the minimum value\\nthat would avoid token dropping. As a baseline, we trained\\nstandard Transformer models across a range of sizes. All\\nTransformer and MoE models have vocabulary size 51200,\\nsequence length 1024 and an attention head size of 64. Our\\nmodel conﬁgurations are summarized in Table 1 and the\\nresults of the experiments are shown in Figure 2.\\nFor these models, we observed that the impact of token\\ndropping is signiﬁcant. While the MoE with capacity factor\\nof 1 achieved a 0.15 reduction in validation loss, the MoE\\nthat avoided dropping tokens provided a reduction of 0.26,\\n1.73× larger than the gain of the former model and enough\\nto exceed the quality of Transformer-Medium.\\nWhile dropping tokens reduces model quality, increasing\\ncapacity factor comes at the cost of additional computation\\nand memory. In this example, MoE-layer math operations\\nincreased by over 2× in order to avoid dropping tokens.\\nHwang et al. (2022) showed that some MoEs require capac-\\nity factors as high as 11 in order to avoid dropping tokens,\\nand other models where the necessary capacity factor to\\navoid dropping tokens spiked unpredictably during training.\\nIn addition to the computational overhead of increasing the\\ncapacity factor, having to tune an additional hyperparameter\\ncan signiﬁcantly increase the number of models that need to\\nbe trained for a target task. This is particularly cumbersome\\nfor large neural networks, where the cost to train a single\\nmodel can run into the hundreds of thousands of dollars\\n(MosaicML, 2022). Possibly as a result of this, some large\\nstudies on MoEs have declined to explore different capacity\\nfactors at all (Artetxe et al., 2021; Clark et al., 2022).\\n4\\nNO-TOKEN-LEFT-BEHIND WITH BLOCK\\nSPARSITY2\\nThis section describes how we formulate MoE layer com-\\nputation in terms of block-sparse computation in order to\\navoid dropping tokens. The motivation for using block-\\nsparse primitives to express MoE computation is manifold.\\nFirst, as we show below, block-sparse matrices are a nat-\\nural and ﬂexible way of describing the dynamic and load\\nimbalanced computation in MoEs. Second, block sparsity\\nmaps efﬁciently to hardware accelerators built around sys-\\ntolic array matrix multipliers like GPUs and TPUs. Because\\nof the coarse granularity of MoE experts, we can select a\\nblock size for our implementation that is large enough to\\nenable the computation to realize high fractions of peak\\ndevice throughput. Lastly, block-sparse kernels like matrix\\nmultiplication and convolution are general-purpose primi-\\ntives that are useful across a range of applications (Narang\\net al., 2017; Gray et al., 2017; Child et al., 2019; Elsen et al.,\\n2020). This makes investment in high-performance ker-\\nnels more practical, as work can be amortized across target\\ntasks. We could similarly invest in variable sized batched\\nmatrix multiplication kernels, but the utility of this would\\nbe limited to MoE architectures as they are designed today.\\nIn addition to these considerations, the block-sparse formu-\\nlation of MoEs exposes a new perspective on these algo-\\nrithms as a form of dynamic, structured, activation sparsity.\\n2The name No-Token-Left-Behind references the technique\\nbrieﬂy discussed by Fedus et al. (2022), which was an unsuccessful\\nattempt to regain the quality lost from dropping tokens.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nThis perspective draws parallels to much of the literature on\\nsparse training algorithms and opens up the opportunity to\\nfurther improve MoEs with insights from this adjacent ﬁeld.\\nPreliminaries: Sparse Matrix Product Notation. In the\\nremainder of this paper we often refer to matrix multiplica-\\ntion where one of the three matrices (the two inputs and one\\noutput) is sparse and the others are dense. We borrow the\\nnotation from Triton (Tillet et al., 2019) to describe these\\ndifferent operations. Each operation is described with a\\nthree character string where each character is either “S” for\\nsparse or “D” for dense. The order of characters is output,\\nfollowed by the left input followed by the right input. For\\nexample, the product of two dense matrices with a sparse\\noutput is “SDD”, which is also referred to as sampled dense-\\ndense matrix multiplication (SDDMM). This notation is\\nuseful to distinguish operations like DSD and DDS, which\\nare different forms of sparse matrix-dense matrix multipli-\\ncation (SpMM). Superscript “T” indicates transposition of\\nthe input arguments. For example, SDDT indicates an SDD\\nwhere the right-hand input matrix is transposed.\\n4.1\\nExpert Computation With Block Sparsity\\nThe key insight behind our method is shown in Figure 3.\\nRather than the prevailing approach of computing the ex-\\nperts within an MoE layer using batched matrix multiplica-\\ntion, we could equivalently compute the experts as an SDD\\nwhere the output sparse matrix has block diagonal structure,\\nas shown in Figure 3B. In this formulation, allowing for a\\nload-imbalanced assignment of tokens to experts is analo-\\ngous to allowing for the blocks in the block diagonal matrix\\nto have a variable number of rows. To achieve this, we\\npropose to compute each block as many smaller ﬁxed size\\nblocks using block-sparse matrix multiplication, as shown in\\nFigure 3C. To construct multi-layer experts, we can iterate\\nbetween SDD and DSD operations (see Figure 6).\\nIn this formulation, we could also relax the constraint on the\\nnumber of columns in each block to build MoE layers with\\nvariable sized experts, as is shown in Figure 3C. While this\\nis an interesting direction for future work, we did not explore\\nthese conﬁgurations as more research is needed to identify\\nhow this capability can be used to increase efﬁciency.\\nWith sufﬁciently large blocks, block-sparse matrix multipli-\\ncation is capable of reaching high fractions of peak through-\\nput on modern GPUs (Gray et al., 2017; NVIDIA, 2021).\\nThe coarse-grained sparsity in MoEs lends itself to this re-\\nquirement - in Transformer models using MoE FFN layers,\\nthe number of columns in the blocks shown in Figure 3B\\ncorresponds to ffn hidden size, which is commonly between\\n1024 and 8192 (Vaswani et al., 2017; Radford et al., 2019;\\nBrown et al., 2020). The number of rows in these blocks\\ncorresponds to the number of tokens assigned to each expert,\\nwhich is expected to be equal to the number of tokens di-\\n512\\n1024\\n2048\\n4096\\n8192\\n16384\\nSquare Matrix Side Length\\n0%\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\nPercent of Peak Throughput\\n64x64\\n128x64\\n128x128\\n256x64\\n256x128\\nFigure 4. Matrix Multiplication Throughput with Different\\nTile Dimensions. Benchmarked on an A100 SXM4 80GB GPU\\nwith CUDA 11.5 and all tile dimensions supported by CUTLASS\\n2.5. We observe that 128x128 tiles perform consistently on-par or\\nbetter than other conﬁgurations.\\nvided by the number of experts under a uniform distribution.\\nThis can range from a few thousand to tens of thousands of\\ntokens per expert (Lepikhin et al., 2020; Artetxe et al., 2021;\\nFedus et al., 2022). These coarse-grained blocks are many\\ntimes larger than the largest tile dimensions used for dense\\nmatrix multiplication kernels, which give us the ﬂexibility\\nto select a block size that can match their throughput.\\n5\\nMEGABLOCKS: A FRAMEWORK FOR\\nEFFICIENT MOE TRAINING\\nWe implemented our techniques in a system called\\nMegaBlocks, which builds on Megatron-LM (Shoeybi et al.,\\n2019) and PyTorch (Paszke et al., 2019). In addition to\\nhigh-performance dropless-MoE (dMoE) layers, our system\\nsupports distributed training of MoEs with both data and\\nexpert model parallelism (Fedus et al., 2022).\\nThis section discusses the design of our dMoE implementa-\\ntion, including our block-sparse kernels, and other consid-\\nerations for building an efﬁcient system. §5.1.1 discusses\\nthe limitations of existing block-sparse kernels. §5.1.2 ana-\\nlyzes the effects of the block size on block-sparse product\\nperformance. §5.1.3 describes our hybrid blocked-CSR-\\nCOO sparse matrix format, which enables efﬁcient matrix\\nproducts with sparse input and output operands. §5.1.4\\nintroduces transpose indices as a mechanism for efﬁcient\\niteration over block-sparse matrices in transposed order.\\nLastly, §5.2 discusses efﬁcient routing and permutation for\\ndMoEs.\\nPreliminaries: Matrix Multiplication on GPUs. Matrix\\nmultiplication kernels on GPUs exploit tiling, where the out-\\nput matrix is broken up into statically sized two-dimensional\\nblocks of values (NVIDIA, 2022c). The computation of\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nthese tiles can be parallelized, and the individual tiles can be\\nsized to tradeoff arithmetic intensity and parallelism. The\\ngroup of threads assigned to a tile is called a threadblock.\\n5.1\\nEfﬁcient Block-Sparse Kernels for MoEs\\nTo train MoEs with block-sparse kernels we need primitives\\nfor the forward and backward passes. Consider an MoE\\nFFN layer where each expert is a 2-layer MLP. For this\\nconﬁguration, the forward pass requires an SDD operation\\nfollowed by a DSD (Figure 6). For the backward pass, we\\ncompute SDDT and DSTD for the second layer data gradient\\nand weight gradient, respectively, followed by DSDT and\\nDDTS for the ﬁrst layer data gradient and weight gradient,\\nrespectively.\\n5.1.1\\nExisting Block-Sparse Primitives\\nWe considered two existing libraries for block-sparse matrix\\nmultiplication on GPUs: NVIDIA cuSPARSE (NVIDIA,\\n2022b) and Triton Blocksparse (Tillet et al., 2019). cuS-\\nPARSE supports the blocked-ELL sparse matrix format for\\nDSD. However, as of CUDA 11.8 this operation does not\\nsupport transposition of the sparse matrix input. cuSPARSE\\nalso provides no SDD primitive with a blocked-ELL matrix.\\nIn addition to these limitations, the blocked-ELL format\\nrequires that all rows in the sparse matrix have the same\\nnumber of nonzeros, which would defeat our goal of sup-\\nporting load imbalanced matrices. Blocksparse supports\\nSDD, DSD, and DDS as well as all combinations of trans-\\nposed and non-transposed inputs. However, these primitives\\nassume that the topology of the sparse matrices does not\\nchange between invocations3. The library API takes a bit-\\nmask describing the sparse operand and then pre-computes\\nlook-up tables and block groupings to accelerate computa-\\ntion. For our use case, the sparse matrix topology varies\\nacross every iteration of training and every MoE layer in the\\nmodel. In order to use Blocksparse, we would have to pay\\nthe cost of these preprocessing steps repeatedly.\\nBased on this analysis, we opted to write our own block-\\nsparse primitives in order to tailor them to the dynamism\\nof MoE expert computation. We implemented SDD, DSD,\\nand DDS operations targeting NVIDIA GPUs. Our kernels\\nsupport all combinations of transposed and non-transposed\\ninputs. The remainder of this section details the design and\\nimplementation of our kernels.\\n5.1.2\\nSelecting Block Size for MoEs\\nIn order to efﬁciently use modern GPUs, we want to use\\nsparse blocks that have sufﬁcient arithmetic intensity to\\n3This is likely because they were written for applications like\\nsparse attention where the sparse matrix topology is determined\\nprior to training (Child et al., 2019).\\nA\\nC\\nB\\nD\\nBlock Data\\n0\\n2\\n0\\n3\\n2\\nColumn Indices\\n0\\n1\\n4\\n4\\n5\\nRow Indices\\n0\\n1\\n2\\n2\\n2\\n4\\nRow Offsets\\n0\\n2\\n2\\n4\\n5\\nColumn Offsets\\n0\\n2\\n1\\n4\\n3\\nTranspose Indices\\nA\\nB\\nC\\nD\\nE\\nBCSR\\n+BCOO\\n+Transposed Iteration\\nE\\n5\\nBlock-Sparse Matrix\\n(A)\\n(B)\\n(A)\\n(C)\\n(B)\\n(E)\\n(D)\\nFigure 5. Block-Sparse Matrix Format used in MegaBlocks.\\nPane (B) shows the encoding for the sparse matrix in pane (A).\\nIndices and offsets in our encoding are block-wise. We use blocked\\ncompressed sparse row (BCSR) as our primary sparse matrix for-\\nmat. We additionally store the row indices of each nonzero block\\n(§5.1.3) and a secondary index of transpose indices (§5.1.4).\\nkeep matrix multiplication units busy. Large blocks are also\\ndesirable to amortize the cost of storing and operating on\\nsparse matrix metadata, since metadata like column indices\\nonly need to be kept for each block of nonzeros.\\nTo select our target block size, we studied the performance\\nof dense matrix multiplication kernels from NVIDIA CUT-\\nLASS (NVIDIA, 2022c) with different tile dimensions. We\\nbenchmarked mixed-precision (FP16 + FP32 accumulation)\\nmatrix multiplication on square matrices with power of two\\nside lengths from 512 to 16384 and every set of tile di-\\nmensions supported in CUTLASS. For rectangular tiles,\\nwe show only the conﬁgurations where the ﬁrst tile dimen-\\nsion is larger as we found these to slightly outperform the\\nalternative ordering for these problems. We ran all bench-\\nmarks on an A100 SXM4 80GB GPU with CUDA 11.5 and\\nCUTLASS 2.5. These benchmarks are shown in Figure 4.\\nAcross these benchmarks, we observed that 128x128 tiles\\nconsistently perform on-par or better than other conﬁgura-\\ntions. Anecdotally, we observe that this same conﬁgura-\\ntion is commonly selected by NVIDIA cuBLAS (NVIDIA,\\n2022a) for the dense Transformer models we studied. Based\\non this analysis, we opted to use 128x128 block sparsity.\\nWhile the tile dimensions of a block-sparse matrix multipli-\\ncation and the block size in the sparse matrix do not need\\nto be equal, we found that for 128x128 blocks the high-\\nest performing tile dimensions in our workloads were also\\n128x128.\\nTo implement our kernels, we extended CUTLASS\\n(NVIDIA, 2022c) to support block-sparse matrices and\\nreused their machinery for high-performance matrix multi-\\nplication with different data types and GPU architectures.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\n5.1.3\\nComputing Sparse Outputs With Hybrid\\nBlocked-CSR-COO\\nWe use blocked compressed sparse row (BCSR) as our pri-\\nmary sparse matrix format. BCSR makes it simple to iterate\\nacross the nonzeros in a row, which is necessary for op-\\nerations like DSD and DDST. Iterating over blocks also\\nhas minimal overhead with BCSR, as identifying a block’s\\nposition in the matrix only requires a single load of its col-\\numn index. We discuss our approach for efﬁciently iterating\\nacross the nonzeros in a column with this format in §5.1.4.\\nOne challenge with BCSR sparse matrices is efﬁciently\\ncomputing SDD operations in parallel. On kernel launch,\\neach threadblock needs to identify the row and column of\\nits output block so that it knows which rows and columns of\\nthe input matrices are needed to compute it. Because BCSR\\nonly encodes column indices for each block, identifying\\nthe row index of a nonzero block requires a search through\\nthe row offsets. One solution to this problem is to launch\\nthe maximum number of threadblocks that could be needed\\nto compute each row of the output if it were fully dense.\\nOn startup, each threadblock can check whether its column\\noffset is out of range for the number of nonzeros in its row\\nand return if there is no work to do. Gale et al. (2020)\\nshowed that the overhead introduced by launching extra\\nthreadblocks was negligible for moderately sparse matrices\\n(50 - 90% zeros). We experimented with this approach but\\nobserved that for MoEs the cost of launching these unused\\nthreadblocks was signiﬁcant, particularly for models with\\nhigh expert counts where the level of sparsity in the block-\\nsparse matrices is very high.\\nTo efﬁciently parallelize SDD, we additionally materialize\\nthe row indices for each nonzero block so that threadblocks\\ncan trivially look up the coordinates of sparse blocks in\\nthe output matrix. The storage required for this additional\\nmetadata is negligible since we only need to store one index\\nper 16384 nonzero values in a 128x128 block. Even with\\nthis additional metadata, we maintain the row-wise ordering\\nof nonzero blocks so the matrix can be operated on as either\\nBCSR or blocked coordinate format (BCOO). We illustrate\\nthis hybrid blocked-CSR-COO encoding in Figure 5.\\n5.1.4\\nBlock-Sparse Transposition With Transpose Indices\\nComputing forward and backward passes for model training\\nrequires sparse matrix transposition. However, iterating\\nover BCSR matrices in transposed order requires searching\\nthrough each row to identify if the block in the target column\\nis nonzero (Buluc\\n¸ et al., 2009). We could materialize a\\ntransposed version of the sparse matrix explicitly, but this\\nwould incur runtime and storage costs as all of the nonzero\\nvalues in the matrix would need to be copied. To enable\\nefﬁcient iteration over BCSR matrices in transposed order,\\nwe construct the metadata for the transposed matrix but do\\n1 # x.shape: (num_tokens, hidden_size)\\n2 def dmoe_forward(self, x):\\n3\\n# (1) Assign tokens to experts.\\n4\\n#\\n5\\n# indices.shape: (num_tokens)\\n6\\n# weights.shape: (num_tokens)\\n7\\nindices, weights = router(x)\\n8\\n9\\n# (2) Create the sparse matrix topology.\\n10\\n#\\n11\\n# This describes the matrix in Figure 3C.\\n12\\ntopology = make_topology(indices)\\n13\\n14\\n# (3) Permute the tokens to group by expert.\\n15\\nx = padded_gather(x, indices)\\n16\\n17\\n# (4): Compute the expert layers.\\n18\\n#\\n19\\n# inner_dim = ffn_hidden_size * num_experts\\n20\\n# self.w1.shape: (hidden_size, inner_dim)\\n21\\n# self.w1.shape: (inner_dim, hidden_size)\\n22\\nx = sdd(x, self.w1, topology)\\n23\\nx = dsd(x, self.w2)\\n24\\n25\\n# (5) Un-permute the tokens and scale.\\n26\\nx = padded_scatter(x, indices)\\n27\\nreturn x * weights\\nFigure 6. Pseudo-Code for a dMoE. The code follows Figure 1\\nwith three changes. First, we construct the sparse matrix topology\\nfrom Figure 3C from expert assignments (line 12). Second, we pad\\neach expert batch to a multiple of the block size during permutation\\n(line 15, §5.2). Lastly, we compute the experts in parallel by\\niterating between SDD and DSD operations (lines 22-23, §4.1).\\nnot explicitly transpose the nonzero values. Instead, we\\nconstruct an array of indices, one for each nonzero block,\\nwhich are stored in transposed order and contain the offset\\nof each nonzero block in memory. This additional metadata\\nallows efﬁcient iteration through the matrix in transposed\\norder with a layer of indirection, as shown in Figure 5.\\nThis idea is similar to a secondary index in a database, which\\nallows efﬁcient access to entries in a different order than the\\nprimary index. Similar to our hybrid Blocked-CSR-COO\\nencoding, this technique relies on the fact that storage and\\ncomputation is many times cheaper for metadata than it is\\nfor nonzero values thanks to our large block sizes.\\n5.2\\nEfﬁcient Routing and Permutation\\nAs currently implemented, our block-sparse matrix multipli-\\ncation kernels require the number of tokens assigned to each\\nexpert to be a multiple of the block size. In order to respect\\nthis constraint, we pad each group of tokens with zeros to\\nthe nearest multiple of 128 and fuse this operation into cus-\\ntom permutation kernels. We could remove this constraint\\nby supporting partial blocks at the fringes of the problem\\nsimilar to how matrix multiplication handles matrices that\\nare not divisible by the tile dimensions. However, the per-\\nformance impact of this feature would be minimal given we\\nexpect the number of tokens assigned to each expert to be\\nthousands or tens of thousands.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nTable 2. MoE Model Conﬁgurations. These models correspond\\nto the Transformer conﬁguration of the same size, but with each\\nFFN layer replaced with a 64-expert MoE layer.\\nMoE\\nnum experts\\ntop k\\nWeights (M)\\nGFLOPs\\nXS\\n64\\n1\\n839\\n316\\nSmall\\n64\\n1\\n3,693\\n879\\nMedium\\n64\\n1\\n13,041\\n2487\\nTable 3. Micro Batch Sizes Used for Model Training. We used\\nthe largest micro batch size that ﬁt in memory for all experiments.\\nModel\\nmicro batch size\\nMegatron-LM\\nTransformer-XS\\n64\\nTransformer-Small\\n32\\nTransformer-Medium\\n16\\nTransformer-Large\\n16\\nTransformer-XL\\n8\\nMegaBlocks\\ndMoE-XS\\n64\\ndMoE-Small\\n32\\ndMoE-Medium\\n8\\nTutel\\ndMoE-XS\\n32\\ndMoE-Small\\n8\\ndMoE-Medium\\n1\\nOnce the expert assignments have been computed by the\\nrouter, we create the metadata for the block-sparse matrix\\nusing a custom CUDA kernel. We additionally construct the\\ntransposed metadata at this time to amortize the cost over\\nthe multiple block-sparse matrix multiplications that use it\\nacross forward and backward computation.\\n6\\nEXPERIMENTS\\nThis section analyzes the performance of our system com-\\npared to state-of-the-art libraries, Microsoft Tutel (Hwang\\net al., 2022) and NVIDIA Megatron-LM (Shoeybi et al.,\\n2019), for training Transformer MoEs and standard Trans-\\nformers respectively. In order to ensure fair comparisons,\\nwe extended Megatron-LM to additionally support MoE\\ntraining using Tutel’s MoE layer. All experiments were con-\\nducted on NVIDIA A100 SXM4 80GB GPUs with CUDA\\n11.5, CUTLASS 2.5 and used mixed-precision training (Mi-\\ncikevicius et al., 2018) as implemented in Megatron-LM.\\n6.1\\nMoE Training Without Dropping Tokens\\nTo assess the efﬁciency of our technique for avoiding token\\ndropping, we compared to the dMoE method proposed by\\nHwang et al. (2022) where the capacity factor is set dynami-\\ncally to the minimum value that avoids token dropping.\\nWe trained decoder-only Transformer language models on\\nThe Pile (Gao et al., 2020) with the same hyperparameters\\ndescribed in §3. For Transformer MoEs, we trained models\\nscaled from our XS, Small, and Medium models with each\\nFFN layer replaced with 64-expert MoE layers using top-1\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nTraining Time (days)\\n2.2\\n2.3\\n2.4\\n2.5\\n2.6\\n2.7\\n2.8\\n2.9\\nValidation Loss\\nTransformer (Megatron-LM)\\ndMoE (Tutel, e64, top-1)\\ndMoE (MegaBlocks, e64, top-1)\\nFigure 7. MegaBlocks dMoEs, Tutel dMoEs and Megatron-\\nLM Transformers Trained on The Pile. MegaBlocks uses block-\\nsparse operation to handle the dynamic and load imbalanced com-\\nputation in MoEs, which enables 1.38×, 2.0× and 4.35× end-\\nto-end training speedups for MoE-XS, MoE-Small, and MoE-\\nMedium respectively compared to the padding-based approach\\nused by Tutel. The advantage of our approach increases with the\\nsize of the model, as the memory requirements of padding ex-\\npert batches forces Tutel to use smaller micro batch sizes which\\ndecreases hardware efﬁciency. Compared to dense Transformer\\nlanguage models, MegaBlocks achieves 1.8× - 2.4× end-to-end\\ntraining speedups for the same validation loss across these models.\\nrouting. We also trained standard Transformer models from\\n46M to 1.3B parameters, equivalent to Transformer-Base\\n(Vaswani et al., 2017) up to GPT3-XL (Brown et al., 2020),\\nas a dense baseline. We trained all models on 8 A100 SXM4\\n80GB GPUs using 8-way expert model parallelism for MoE\\nlayers and data parallelism for all other layers. We use\\ngradient accumulation for all models and train with a batch\\nsize of 512 sequences and the largest micro batch size that\\ndoes not run out of memory (Narayanan et al., 2021a). Our\\nmodel conﬁgurations are summarized in Tables 1 and 2. For\\neach model, we report the end-to-end training time and ﬁnal\\nloss achieved on a validation set in Figure 7.\\nCompared to the prevalent padding-based approach for\\navoiding token dropping, our technique for adaptive MoE\\ncomputation with block sparsity enables end-to-end training\\nspeedups of 1.38×, 2.0× and 4.35× for MoE-XS, MoE-\\nSmall, and MoE-Medium, respectively. In addition to com-\\nputational overhead, the padding-based approach imple-\\nmented in Tutel signiﬁcantly increases the amount of mem-\\nory required to store activations in the MoE layers. This\\nis particularly problematic because MoEs already require\\nmany times more storage for their large weight matrices\\ncompared to standard Transformers. For these models, we\\nobserved this increase in memory usage reduced the maxi-\\nmum micro batch size that Tutel could use by 2×, 4×, and\\n8× compared to MegaBlocks for MoE-XS, MoE-Small, and\\nMoE-Medium, respectively. This in turn increases training\\ntime because of reduced hardware efﬁciency. As a result,\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nTraining Time (days)\\n2.2\\n2.3\\n2.4\\n2.5\\n2.6\\n2.7\\n2.8\\n2.9\\nValidation Loss\\nTransformer (Megatron-LM)\\nMoE (Tutel, e64, top-1, cp=1)\\nMoE (Tutel, e64, top-1, cp=1.5)\\nMoE (Tutel, e64, top-1, cp=2)\\ndMoE (MegaBlocks, e64, top-1)\\nFigure 8. MegaBlocks dMoEs, Tutel MoEs and Megatron-LM\\nTransformers Trained on The Pile. Even with the most efﬁcient\\ncapacity factor for each MoE, MegaBlocks reduces the training\\ntime required to reach a given validation loss by 1.38×, 1.37× and\\n1.18× for MoE-XS, MoE-Small and MoE-Medium respectively.\\nIn addition to these speedups, our approach reduces the cost of\\nusing MoEs by decreasing the number of hyperparameters that\\nneed to be re-tuned for each model and task.\\nwe observe that the advantage of MegaBlocks over Tutel\\ngrows with model size. The micro batch size used for each\\nmodel conﬁguration are shown in Table 3.\\nCompared to Transformer models trained with Megatron-\\nLM, dMoEs trained with MegaBlocks reduce the training\\ntime required to reach a given validation loss by 1.8× - 2.4×.\\nThe variation in this comparison is primarily a result of the\\nincreased weight memory usage of MoE models, which\\nforced MegaBlocks to use a 2x smaller micro batch size\\nfor MoE-Medium than the analogous Transformer model.\\nThese results highlight the importance of reducing memory\\nusage in MoEs as a direction for future research.\\nFor these Transformer models, we observed that Megatron-\\nLM sustains between 21% and 48% of the 2.5 petaFLOP\\npeak throughput of this 8-GPU system with efﬁciency\\nincreasing with model size. The speedups achieved by\\nMegaBlocks over this state-of-the-art framework demon-\\nstrates the efﬁciency of our system and the efﬁcacy of MoEs.\\n6.2\\nMoE Training With Token Dropping\\nWe additionally compare our dMoE models to token-\\ndropping MoEs trained with Tutel. In order to ﬁnd the\\nmost efﬁcient conﬁgurations, we trained MoE-XS, MoE-\\nSmall and MoE-Medium models with capacity factors of\\n1×, 1.5×, and 2× for a total of 9 additional models. For\\nthese conﬁgurations, all token-dropping MoE models were\\nable to use the same micro batch size as the analogous\\ndMoE without running out of GPU memory. We report\\nthe end-to-end training time and validation loss for these\\nmodels along with our dMoE and standard Transformer re-\\nsults in Figure 8. Comparing MoEs and dMoEs for the same\\naccuracy is non-trivial because token dropping degrades\\nmodel quality. For each dMoE, we estimated the runtime\\nof the MoE that would achieve the same validation loss by\\ncomparing to the loss-equivalent point on the MoE Pareto\\nfrontier.\\nEven with the most efﬁcient capacity factor for each MoE,\\ndMoEs trained with MegaBlocks reduce the training time\\nrequired to reach a given validation loss by 1.38×, 1.37×\\nand 1.18× for MoE-XS, MoE-Small and MoE-Medium,\\nrespectively. In addition to signiﬁcant reductions in end-\\nto-end training time, our system reduces the cost of using\\nMoEs by decreasing the number of hyperparameters that\\nneed to be re-tuned for each model and task. These compu-\\ntational savings could in turn be applied to exploring other\\nparameters to further improve model quality.\\nFor MoE-Medium, we observe some loss of efﬁciency in our\\nimplementation due to the relatively small micro batch size\\nthat could be used while ﬁtting in limited GPU memory. For\\nsmall batch sizes, smaller tile dimensions (e.g., 64x128 or\\n64x64) in our block-sparse kernels could improve perfor-\\nmance by reducing the amount of wasted computation when\\nthe problem dimensions are not divisible by 128. Another\\ndirection for increasing efﬁciency is to reduce the memory\\nusage per device such that larger batch sizes can be used, ei-\\nther through parallelization over more devices or techniques\\nlike selective recomputation (Korthikanti et al., 2022).\\n6.3\\nBlock-Sparse Matrix Multiplication Performance\\nTo assess the quality of our block-sparse matrix multiplica-\\ntion kernels, we benchmarked the problem conﬁgurations\\nused in training MoE-XS, MoE-Small and MoE-Medium\\nmodels and compared to cuBLAS batched matrix multipli-\\ncation. This includes the forward pass, backward weights,\\nand backward data operations for the two layers in each FFN\\nlayer. In total, we benchmark 18 problems - 6 problems for\\neach of the 3 models. To allow for comparison with batched\\nmatrix multiplication, we benchmarked each problem with\\na uniform distribution of tokens to experts and the same\\nmicro batch size listed in Table 3. These benchmarks can\\nbe viewed as an ablation assessing the overhead that would\\nbe introduced if one were to use our block-sparse kernels\\nto implement a standard, token-dropping MoE. For each\\nproblem we averaged throughput over 100 executions. We\\ndo not include the time taken to construct the sparse matrix\\nmetadata in these benchmarks as these operations amortize\\nover all 6 problems within an FNN layer. The results of\\nthese benchmarks are shown in Figure 9.\\nOn these problems, we observe that our block-sparse kernels\\nare able to realize 98.6% of the throughput of cuBLAS\\nwith a standard deviation of 4%. The maximum relative\\nthroughput was 104% and the minimum was 91%. Overall,\\nour kernels slightly outperformed cuBLAS on half of the\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nlayer0:fwd:sdd:nt\\nlayer0:gradw:dsd:tn layer0:gradx:dsd:nn\\nlayer1:fwd:dsd:nn\\nlayer1:gradw:dsd:tn layer1:gradx:sdd:nt\\n85%\\n90%\\n95%\\n100%\\n105%\\nThroughput Relative to cuBLAS\\ncuBLAS\\nMoE-XS\\nMoE-Small\\nMoE-Medium\\nFigure 9. Block-Sparse Matrix Multiplication Throughput Compared to cuBLAS Batched Matrix Multiplication. Benchmarked\\nfor the problem conﬁgurations used in training MoE-XS, MoE-Small and MoE-Medium models. For these problems, our block-sparse\\nmatrix multiplication kernels realize 98.6% of the throughput achieved by cuBLAS on average with a standard deviation of 4% and a\\nmaximum and minimum relative throughput of 104% and 91% respectively.\\nproblems and slightly underperformed on the other half.\\nWhile benchmarking CUTLASS, we observed that altering\\nthe order in which tiles of the output matrix are computed\\ncan change the throughput of the operation by as much as\\n10% due to L2 caching effects. We believe that most of the\\nperformance discrepancy in these results can be attributed to\\nthe re-ordering of computation that occurs with block-sparse\\nmatrices, although further investigation is needed.\\nOne case where we note additional overhead is in the DSTD\\noperations used to compute weight gradients. Because we\\nuse a secondary index to iterate over the sparse operand in\\ntransposed order the access patterns when iterating through\\nthis matrix exhibit little spatial locality which in turn reduces\\nthe throughput of the overall operation. While this is an\\ninteresting problem for further study, the overall impact\\non model performance is minimal because of the limited\\nopportunity for improvement (<10%) combined with the\\nrelatively small amount of end-to-end runtime that these\\ntwo operations represent.\\n7\\nRELATED WORK\\nMoE Routing. Improved routing algorithms for MoEs is\\nan active area of research. BASE layers formulate MoE\\nrouting as a linear assignment problem trying to maximize\\nthe aggregate token-expert afﬁnities under the constraint of\\na perfectly balanced assignment (Lewis et al., 2021). This\\nmethod guarantees no tokens are dropped by re-routing to-\\nkens to different experts as needed. Clark et al. (2022) found\\nthat BASE layers can incur signiﬁcant runtime overhead and\\nproposed an approximate version using the Sinkhorn algo-\\nrithm. Because their approximation is no longer guaranteed\\nto avoid token dropping, Clark et al. (2022) use a capacity\\nfactor of 2 for all experiments. Other techniques have been\\nproposed to statically decide tokens to expert mappings\\nahead of time based on hash functions (Roller et al., 2021).\\nHowever, Clark et al. (2022) observed that this approach\\ndid not perform as well as the other routing algorithms they\\nstudied. More recently, Zhou et al. (2022) proposed to re-\\nverse the routing problem such that each expert selects its\\ntop k scoring tokens. While this guarantees a load balanced\\nassignment of tokens to experts, this method still suffers\\nfrom token dropping because the same token can be selected\\nby multiple experts. We expect that improved routing al-\\ngorithms complement our method for efﬁcient and ﬂexible\\nexpert computation. Exploring how these methods could be\\ncombined is an interesting direction for future research.\\nHigh-Performance MoEs. To scale MoE training, Tutel\\nimplements optimized distributed communication primitives\\nfor MoEs and techniques for hiding the communication\\ncosts of expert model parallelism (Hwang et al., 2022). He\\net al. (2022) proposed FasterMoE, a system for distributed\\ntraining of MoEs based on efﬁcient communication strate-\\ngies and changes to the MoE routing algorithm to avoid\\nnetwork congestion. Our implementation could additionally\\nbeneﬁt from these techniques, particularly for large-scale\\ndistributed training.\\nSparse Kernels. Sparse matrix formats that allow for efﬁ-\\ncient transposed access are well studied (Buluc\\n¸ et al., 2009;\\nSmith & Karypis, 2015; Li et al., 2018). Exploring how\\nthese formats can be adapted to large block sparsity on\\nmodern GPUs is an interesting direction for future research.\\n8\\nCONCLUSION\\nWe introduced MegaBlocks, a system for efﬁcient MoE\\ntraining on GPUs. Our system is based on a reformulation\\nof MoEs in terms of block-sparse operations and new, block-\\nsparse GPU kernels that efﬁciently handle the dynamism\\npresent in MoEs. Our approach never drops tokens and maps\\nefﬁciently to modern hardware accelerators, enabling end-\\nto-end training speedups of up to 40% over MoEs trained\\nwith the state-of-the-art Tutel library and 2.4× over DNNs\\ntrained with the highly-optimized Megatron-LM framework.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nREFERENCES\\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,\\nShleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R.,\\nAnantharaman, G., Li, X., Chen, S., Akin, H., Baines, M.,\\nMartin, L., Zhou, X., Koura, P. S., O’Horo, B., Wang, J.,\\nZettlemoyer, L., Diab, M. T., Kozareva, Z., and Stoyanov,\\nV. Efﬁcient large scale language modeling with mixtures\\nof experts. CoRR, abs/2112.10684, 2021.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\\nS., Radford, A., Sutskever, I., and Amodei, D. Language\\nmodels are few-shot learners. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on\\nNeural Information Processing Systems 2020, NeurIPS\\n2020, December 6-12, 2020, virtual, 2020.\\nBuluc\\n¸, A., Fineman, J. T., Frigo, M., Gilbert, J. R., and\\nLeiserson, C. E.\\nParallel sparse matrix-vector and\\nmatrix-transpose-vector multiplication using compressed\\nsparse blocks.\\nIn SPAA 2009: Proceedings of the\\n21st Annual ACM Symposium on Parallelism in Algo-\\nrithms and Architectures, Calgary, Alberta, Canada,\\nAugust 11-13, 2009, pp. 233–244. ACM, 2009.\\ndoi:\\n10.1145/1583991.1584053.\\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gener-\\nating long sequences with sparse transformers. CoRR,\\nabs/1904.10509, 2019.\\nClark, A., de Las Casas, D., Guy, A., Mensch, A., Paganini,\\nM., Hoffmann, J., Damoc, B., Hechtman, B. A., Cai, T.,\\nBorgeaud, S., van den Driessche, G., Rutherford, E., Hen-\\nnigan, T., Johnson, M., Millican, K., Cassirer, A., Jones,\\nC., Buchatskaya, E., Budden, D., Sifre, L., Osindero, S.,\\nVinyals, O., Rae, J. W., Elsen, E., Kavukcuoglu, K., and\\nSimonyan, K. Uniﬁed scaling laws for routed language\\nmodels. CoRR, abs/2202.01169, 2022.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.\\nAn image is worth 16x16 words: Transformers for image\\nrecognition at scale. In 9th International Conference\\non Learning Representations, ICLR 2021, Virtual Event,\\nAustria, May 3-7, 2021. OpenReview.net, 2021.\\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu,\\nY., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B.,\\nFedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E.,\\nWebster, K., Pellat, M., Robinson, K., Meier-Hellstern,\\nK., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y.,\\nChen, Z., and Cui, C. GLaM: Efﬁcient scaling of lan-\\nguage models with mixture-of-experts, 2021.\\nElsen, E., Dukhan, M., Gale, T., and Simonyan, K. Fast\\nsparse convnets. In 2020 IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, CVPR 2020, Seat-\\ntle, WA, USA, June 13-19, 2020, pp. 14617–14626. Com-\\nputer Vision Foundation / IEEE, 2020. doi: 10.1109/\\nCVPR42600.2020.01464.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers:\\nScaling to trillion parameter models with simple and ef-\\nﬁcient sparsity. Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\nGale, T., Elsen, E., and Hooker, S. The state of sparsity in\\ndeep neural networks. CoRR, abs/1902.09574, 2019.\\nGale, T., Zaharia, M., Young, C., and Elsen, E. Sparse\\nGPU kernels for deep learning. In Proceedings of the\\nInternational Conference for High Performance Comput-\\ning, Networking, Storage and Analysis, SC 2020, Virtual\\nEvent / Atlanta, Georgia, USA, November 9-19, 2020.\\nIEEE/ACM, 2020.\\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\\nPresser, S., and Leahy, C. The Pile: An 800GB dataset\\nof diverse text for language modeling. arXiv preprint\\narXiv:2101.00027, 2020.\\nGray, S., Radford, A., and Kingma, D. P.\\nBlock-\\nsparse GPU kernels. https://blog.openai.com/\\nblock-sparse-gpu-kernels/, 2017.\\nHan, S., Pool, J., Tran, J., and Dally, W. J. Learning both\\nweights and connections for efﬁcient neural network. In\\nAdvances in Neural Information Processing Systems 28:\\nAnnual Conference on Neural Information Processing\\nSystems 2015, December 7-12, 2015, Montreal, Quebec,\\nCanada, 2015.\\nHe, J., Zhai, J., Antunes, T., Wang, H., Luo, F., Shi, S.,\\nand Li, Q. Fastermoe: Modeling and optimizing training\\nof large-scale dynamic pre-trained models. In Proceed-\\nings of the 27th ACM SIGPLAN Symposium on Princi-\\nples and Practice of Parallel Programming, PPoPP ’22,\\npp. 120–134, New York, NY, USA, 2022. Association\\nfor Computing Machinery. ISBN 9781450392044. doi:\\n10.1145/3503221.3508418.\\nHwang, C., Cui, W., Xiong, Y., Yang, Z., Liu, Z., Hu, H.,\\nWang, Z., Salas, R., Jose, J., Ram, P., Chau, J., Cheng,\\nP., Yang, F., Yang, M., and Xiong, Y. Tutel: Adaptive\\nmixture-of-experts at scale, 2022.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\\nA., Boyle, R., Cantin, P.-l., Chao, C., Clark, C., Coriell, J.,\\nDaley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami,\\nT. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, C. R.,\\nHogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey,\\nA., Jaworski, A., Kaplan, A., Khaitan, H., Killebrew, D.,\\nKoch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le,\\nD., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean,\\nG., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R.,\\nNarayanaswami, R., Ni, R., Nix, K., Norrie, T., Omer-\\nnick, M., Penukonda, N., Phelps, A., Ross, J., Ross, M.,\\nSalek, A., Samadiani, E., Severn, C., Sizikov, G., Snel-\\nham, M., Souter, J., Steinberg, D., Swing, A., Tan, M.,\\nThorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan,\\nV., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H.\\nIn-datacenter performance analysis of a tensor processing\\nunit. SIGARCH Comput. Archit. News, 45(2):1–12, jun\\n2017. ISSN 0163-5964. doi: 10.1145/3140659.3080246.\\nKalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\\nCasagrande, N., Lockhart, E., Stimberg, F., van den Oord,\\nA., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neu-\\nral audio synthesis. In Proceedings of the 35th Inter-\\nnational Conference on Machine Learning, ICML 2018,\\nStockholmsm¨\\nassan, Stockholm, Sweden, July 10-15, 2018,\\n2018.\\nKorthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch,\\nM., Shoeybi, M., and Catanzaro, B. Reducing Activa-\\ntion Recomputation in Large Transformer Models. arXiv\\npreprint arXiv:2205.05198, 2022.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\\ngiant models with conditional computation and automatic\\nsharding. CoRR, abs/2006.16668, 2020.\\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettle-\\nmoyer, L. BASE layers: Simplifying training of large,\\nsparse models. In Proceedings of the 38th International\\nConference on Machine Learning, ICML 2021, 18-24\\nJuly 2021, Virtual Event, volume 139 of Proceedings of\\nMachine Learning Research. PMLR, 2021.\\nLi, J., Sun, J., and Vuduc, R. Hicoo: Hierarchical stor-\\nage of sparse tensors.\\nIn SC18: International Con-\\nference for High Performance Computing, Network-\\ning, Storage and Analysis, pp. 238–252, 2018.\\ndoi:\\n10.1109/SC.2018.00022.\\nMicikevicius, P., Narang, S., Alben, J., Diamos, G. F., Elsen,\\nE., Garc´\\nıa, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\\nVenkatesh, G., and Wu, H. Mixed precision training.\\nIn 6th International Conference on Learning Representa-\\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 - May\\n3, 2018, Conference Track Proceedings. OpenReview.net,\\n2018.\\nMosaicML.\\nMosaic LLMs (part 2):\\nGPT-3 quality\\nfor <$500k. https://www.mosaicml.com/blog/\\ngpt-3-quality-for-500k, 2022.\\nNarang, S., Undersander, E., and Diamos, G. F. Block-\\nsparse recurrent neural networks. CoRR, abs/1711.02782,\\n2017.\\nNarayanan, D., Phanishayee, A., Shi, K., Chen, X., and Za-\\nharia, M. Memory-efﬁcient pipeline-parallel dnn training.\\nIn International Conference on Machine Learning, pp.\\n7937–7947. PMLR, 2021a.\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\\nwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti,\\nP., Bernauer, J., Catanzaro, B., Phanishayee, A., and Za-\\nharia, M. Efﬁcient large-scale language model training on\\nGPU clusters using megatron-lm. In de Supinski, B. R.,\\nHall, M. W., and Gamblin, T. (eds.), SC ’21: The Inter-\\nnational Conference for High Performance Computing,\\nNetworking, Storage and Analysis, St. Louis, Missouri,\\nUSA, November 14 - 19, 2021, pp. 58:1–58:15. ACM,\\n2021b. doi: 10.1145/3458817.3476209.\\nNVIDIA. NVIDIA A100 Tensor Core GPU Architecture.\\nhttps://www.nvidia.com/content/dam/\\nen-zz/Solutions/Data-Center/nvidia-\\nampere-architecture-whitepaper.pdf,\\n2020.\\nNVIDIA.\\nAccelerating matrix multiplication with\\nblock\\nsparse\\nformat\\nand\\nnvidia\\ntensor\\ncores.\\nhttps://developer.nvidia.com/blog/\\naccelerating-matrix-multiplication-\\nwith-block-sparse-format-and-nvidia-\\ntensor-cores/, 2021.\\nNVIDIA.\\nNVIDIA cuBLAS Library.\\nhttps://\\ndeveloper.nvidia.com/cublas, 2022a.\\nNVIDIA.\\nNVIDIA cuSPARSE Library.\\nhttps://\\ndeveloper.nvidia.com/cusparse, 2022b.\\nNVIDIA.\\nNVIDIA CUTLASS Library.\\nhttps://\\ngithub.com/NVIDIA/cutlass, 2022c.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., K¨\\nopf, A., Yang, E. Z., DeVito, Z., Rai-\\nson, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,\\nL., Bai, J., and Chintala, S. Pytorch: An imperative style,\\nhigh-performance deep learning library. In Advances\\nin Neural Information Processing Systems 32: Annual\\nConference on Neural Information Processing Systems\\n2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\\nBC, Canada, 2019.\\nMegaBlocks: Efﬁcient Sparse Training with Mixture-of-Experts\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. 2019.\\nRiquelme, C., Puigcerver, J., Mustafa, B., Neumann, M.,\\nJenatton, R., Pinto, A. S., Keysers, D., and Houlsby, N.\\nScaling vision with sparse mixture of experts. In Ran-\\nzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and\\nVaughan, J. W. (eds.), Advances in Neural Information\\nProcessing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, De-\\ncember 6-14, 2021, virtual, pp. 8583–8595, 2021.\\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. Hash\\nlayers for large sparse models. In Advances in Neural In-\\nformation Processing Systems 34: Annual Conference on\\nNeural Information Processing Systems 2021, NeurIPS\\n2021, December 6-14, 2021, virtual, 2021.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\\nQ. V., Hinton, G. E., and Dean, J. Outrageously large\\nneural networks: The sparsely-gated mixture-of-experts\\nlayer. In 5th International Conference on Learning Rep-\\nresentations, ICLR 2017, Toulon, France, April 24-26,\\n2017, Conference Track Proceedings. OpenReview.net,\\n2017.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\\nJ., and Catanzaro, B.\\nMegatron-lm: Training multi-\\nbillion parameter language models using model paral-\\nlelism. CoRR, abs/1909.08053, 2019.\\nSmith, S. and Karypis, G. Tensor-matrix products with\\na compressed sparse tensor. In Proceedings of the 5th\\nWorkshop on Irregular Applications: Architectures and\\nAlgorithms, IA¡sup¿3¡/sup¿ ’15, New York, NY, USA,\\n2015. Association for Computing Machinery.\\nISBN\\n9781450340014. doi: 10.1145/2833179.2833183.\\nTillet, P., Kung, H., and Cox, D. D. Triton: an interme-\\ndiate language and compiler for tiled neural network\\ncomputations.\\nIn Proceedings of the 3rd ACM SIG-\\nPLAN International Workshop on Machine Learning and\\nProgramming Languages, MAPL@PLDI 2019, Phoenix,\\nAZ, USA, June 22, 2019, pp. 10–19. ACM, 2019. doi:\\n10.1145/3315508.3329973.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in Neural Information\\nProcessing Systems 30: Annual Conference on Neural In-\\nformation Processing Systems 2017, 4-9 December 2017,\\nLong Beach, CA, USA, 2017.\\nYao, Z., Cao, S., Xiao, W., Zhang, C., and Nie, L. Balanced\\nsparsity for efﬁcient DNN inference on GPU. In The\\nThirty-Third AAAI Conference on Artiﬁcial Intelligence,\\nAAAI 2019, The Thirty-First Innovative Applications of\\nArtiﬁcial Intelligence Conference, IAAI 2019, The Ninth\\nAAAI Symposium on Educational Advances in Artiﬁcial\\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan-\\nuary 27 - February 1, 2019, pp. 5676–5683, 2019.\\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao,\\nV. Y., Dai, A. M., Chen, Z., Le, Q., and Laudon, J.\\nMixture-of-experts with expert choice routing. CoRR,\\nabs/2202.09368, 2022.\\n', 'source_name': 'MegaBlocks: Efficient Sparse Training with Mixture-of-Experts', 'source_url': 'https://arxiv.org/abs/2211.15841'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "QMoE.pdf #39\n",
      "{'content': 'QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nElias Frantar 1 Dan Alistarh 1 2\\nAbstract\\nMixture-of-Experts (MoE) architectures offer a\\ngeneral solution to the high inference costs of\\nlarge language models (LLMs) via sparse routing,\\nbringing faster and more accurate models, at the\\ncost of massive parameter counts. For example,\\nthe SwitchTransformer-c2048 model has 1.6 tril-\\nlion parameters, requiring 3.2TB of accelerator\\nmemory to run efficiently, which makes practi-\\ncal deployment challenging and expensive. In\\nthis paper, we present a solution to this mem-\\nory problem, in form of a new compression\\nand execution framework called QMoE. Specif-\\nically, QMoE consists of a scalable algorithm\\nwhich accurately compresses trillion-parameter\\nMoEs to less than 1 bit per parameter, in a cus-\\ntom format co-designed with bespoke GPU de-\\ncoding kernels to facilitate efficient end-to-end\\ncompressed inference, with minor runtime over-\\nheads relative to uncompressed execution. Con-\\ncretely, QMoE can compress the 1.6 trillion pa-\\nrameter SwitchTransformer-c2048 model to less\\nthan 160GB (20x compression, 0.8 bits per pa-\\nrameter) at only minor accuracy loss, in less than\\na day on a single GPU. This enables, for the first\\ntime, the execution of a trillion-parameter model\\non affordable commodity hardware, like a single\\nserver with 4x NVIDIA A6000 or 8x NVIDIA\\n3090 GPUs, at less than 5% runtime overhead\\nrelative to ideal uncompressed inference. The\\nsource code and compressed models are available\\nat github.com/IST-DASLab/qmoe.\\n1. Introduction\\nGenerative large language models (LLMs), e.g. (Radford\\net al., 2019; Brown et al., 2020; Touvron et al., 2023a;b),\\nhave garnered significant industrial and popular attention\\ndue to their surprising performance across many practical\\nlanguage and reasoning tasks. Yet, a major obstacle to\\nbroad deployment is given by their extremely high inference\\ncosts. One particularly promising approach for reducing\\nthese costs is the use of Mixture-of-Experts (MoE) architec-\\n1Institute of Science and Technology Austria (ISTA) 2Neural\\nMagic Inc. Corresponding author: elias.frantar@ist.ac.at\\ntures, e.g. (Fedus et al., 2022; Artetxe et al., 2022), whose\\ngeneral idea is to replicate certain model components many\\ntimes while routing each input only to a small subset of\\nthose replicas. Through expert “specialization” to input\\nsubsets, MoEs achieve faster inference for the same model\\nquality, but with significantly higher memory costs due to\\ncomponents being replicated hundreds or even thousands of\\ntimes, for the largest and best-performing models.\\nFor example, the popular SwitchTransformer family (Fe-\\ndus et al., 2022), which we focus on in this study, uses\\nbetween 128 and 2048 experts (layer replicas) to signifi-\\ncantly outperform standard dense T5 models (Raffel et al.,\\n2020b) in terms of inference and training costs, at equiv-\\nalent model accuracy. Artetxe et al. (2022) report similar\\nimprovements, on different tasks, for 512 experts. However,\\nthese results come at the cost of dramatic increases in model\\nsize: the largest SwitchTransformer has 1.6 trillion parame-\\nters, requiring 3.2TB of storage in standard half-precision,\\nand correspondingly requires a hundred or more expensive\\n(GPU or TPU) accelerators for efficient usage. This not only\\nmakes practical deployment costly and challenging, but also\\nstrongly limits research on such models.\\nChallenges. It is natural to ask whether the truly mas-\\nsive memory costs of such MoEs can be reduced via stan-\\ndard techniques for model compression, such as quantiza-\\ntion (Gholami et al., 2021) or sparsity (Hoefler et al., 2021),\\nwithout significant accuracy loss. Achieving this would\\nrequire overcoming conceptual and technical barriers:\\n1. Conceptually, existing post-training compression meth-\\nods, whose costs would be affordable enough to exe-\\ncute on such models, are currently only able to reduce\\nprecision to 3 or 4 bits per parameter (Frantar et al.,\\n2022; Dettmers & Zettlemoyer, 2022; Wu et al., 2023)\\nor around 50% sparsity (Frantar & Alistarh, 2023),\\nbefore significant accuracy loss occurs. Yet, making\\ntrillion-parameter MoEs practical would require com-\\npression rates between 10× and 20× relative to 16-bit\\nprecision, i.e., on average less than 1 bit per parameter.\\n2. A key practical issue is scaling: applying state-of-the-\\nart compression methods, designed for large dense\\nmodels, to MoEs that are an order of magnitude larger,\\nwhile maintaining affordability, runs into a plethora of\\nmemory, performance and reliability roadblocks.\\n3. Actually achieving sub-1-bit compression would re-\\narXiv:2310.16795v1  [cs.LG]  25 Oct 2023\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nquire a non-trivial custom compression format. Such a\\nformat would also need to come with decoding algo-\\nrithms that are highly-efficient on accelerators such as\\nGPUs, in order to run inference on compressed models\\nwithout major processing slowdowns.\\nContribution. In this paper, we overcome these challenges,\\nand introduce QMoE, a framework for accurate compres-\\nsion and fast compressed inference of massive MoEs, re-\\nducing model sizes by 10–20×, to less than 1 bit per pa-\\nrameter. QMoE is specifically designed to compress and\\nsubsequently inference with models like the 1.6 trillion\\nparameter SwitchTransformer-c2048, using only modest\\ncomputational resources.\\nOur key technical contributions are a highly scalable com-\\npression algorithm implementation and a customized com-\\npression format designed together with bespoke GPU-\\nkernels for fast on-the-fly decoding. Further, we show for\\nthe first time that accurate sub-1-bit compression of tril-\\nlion parameter MoEs is feasible and can be achieved via\\naffordable retraining-free compression techniques.\\nConcretely, we reduce the size of SwitchTransformer-c2048,\\nthe largest openly-available model, from 3.2TB in bfloat16\\nto less than 160GB in our customized compressed format,\\nthat is, ≈0.8 bits per parameter, at only a minor increase in\\nloss on pretraining validation and zero-shot data. Using our\\nQMoE kernels, this compressed model can then be executed\\nfully, without any slow offloading, on commodity hardware\\nsuch as 8× NVIDIA RTX 3090 or 4× NVIDIA A6000\\nGPUs, with < 5% runtime overhead relative to an idealized\\nversion of uncompressed execution, which would require\\n≈20× more GPUs.\\nIn summary, our work enables, for the first time, the perfor-\\nmant execution of massive-scale MoE models on commod-\\nity hardware. This is illustrated by the fact that we are able\\nto efficiently run the trillion-parameter SwitchTransformer-\\nc2048 model on a single commodity GPU server, with minor\\naccuracy loss. This addresses one of the key limitations be-\\nhind MoE architectures, and should improve their practical\\nadoption as well as facilitate further research on understand-\\ning and improving such models.\\n2. Background\\n2.1. Mixture of Expert Models (MoEs)\\nThe core idea behind Mixture of Expert models (MoEs)\\nis to increase the number of parameters, and thus the net-\\nwork’s modelling power, while at the same time keeping\\ncompute costs near-constant, relative to a standard feed-\\nforward architecture. This is typically achieved by creating\\nmany copies of certain model components, each of which\\nis responsible for processing only a subset of all input to-\\nkens. The corresponding input-to-component assignments\\nare generally decided by a “router” layer. Probably the most\\ncommon MoE design (Fedus et al., 2022; Artetxe et al.,\\n2022), which we also focus on in this paper, is to replicate\\nthe fully-connected module of a Transformer and route to-\\nkens to the replica, referred to as an expert, with the highest\\nassignment score predicted by a linear routing layer; see\\nFigure 1 for an illustration. This design enables efficient\\ntraining and inference of extremely large models, using 100s\\nor even 1000s of experts/, since each token is processed only\\nby a small subset of the massive overall network.\\nAttention Block\\nRouter\\nFC Block 1\\nFC Block 2\\nFC Block 3\\nMoE Layer\\nTokens\\nFigure 1. Example of an MoE Transformer block. Each token is\\nrouted to a different fully-connected (FC) block.\\nMoEs have been shown to bring substantial accuracy and\\ntraining speed improvements for equivalent inference speed\\n(Clark et al., 2022; Du et al., 2022; Zoph et al., 2022). How-\\never, their current practicality is limited since they are ex-\\ntremely large in size and thus require massive amounts of\\naccelerator memory to be executed efficiently.\\n2.2. Data-dependent Quantization\\nThe currently most effective strategy for reducing model\\nsize and corresponding memory costs is quantization, i.e.,\\nconverting model weights to lower numerical precision. On\\nlarge models (Dettmers et al., 2022; Dettmers & Zettle-\\nmoyer, 2022), in particular also MoEs (Kim et al., 2022b;\\nYi et al., 2023), just simple rounding can decrease precision\\nto 8 or even 4 bits per weight, at minimal accuracy loss\\nrelative to the standard half (16-bit) precision employed\\nfor these models. However, some MoEs are so large that\\nreduction rates significantly higher than 4× (accomplished\\nby 4-bit) would be required to render them practical. Accu-\\nrately quantizing models to extremely low precision (e.g.,\\nlower than 3 bits per parameter) typically requires more\\nsophisticated data-dependent methods (Nagel et al., 2020;\\nWang et al., 2020; Hubara et al., 2021).\\nSuch data-dependent quantization methods use a small set\\nof calibration data, which is passed through the model. As\\nthis happens, for each linear layer ℓwith weights Wℓ, quan-\\ntized weights Qℓare determined one-by-one. Specifically,\\none approach to do this is by solving a layer-wise quantiza-\\ntion problem, stated with respect to Wℓand the observed\\ncalibration data inputs Xℓat the current layer:\\nargminQℓ||QℓXℓ−WℓXℓ||.\\n(1)\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nVarious solvers for Equation (1) have been proposed, with\\nsome optimized, in terms of speed and accuracy, particularly\\nfor extremely large models, like GPTQ (Frantar et al., 2022)\\nor ZeroQuant (Yao et al., 2022; Wu et al., 2023). The former\\nperforms quantization using second-order information in the\\nlayer-wise Hessian matrix XℓX⊤\\nℓ, while the latter applies\\nSGD-optimization with straight-through gradient estimation\\n(Bengio et al., 2013).\\nAnother noteworthy characteristic of many such methods is\\nthat per-layer quantization can be performed sequentially,\\nusing the input from the already partially quantized model\\nup to layer ℓ−1, when quantizing layer ℓ, serving to re-\\nduce error accumulation. Concretely, this can be efficiently\\nimplemented by using Xℓto find Qℓbefore passing on\\nXℓ+1 = QℓXℓto the next layer.\\n2.3. MoE Quantization\\nThere are several aspects which make very-low-bit, e.g.\\nternary (3 values) quantization promising for MoE models:\\n• In many architectures, almost all parameters are lo-\\ncated in the experts, as they are 1000s of them. This\\nmeans that, for size reduction, it suffices to focus on\\ncompressing just those experts and leave other layers\\nin standard precision. This reduces error accumulation\\nsince only a subset of modules involved in a forward\\npass are actually quantized.\\n• Previous work has observed that extremely large dense\\nmodels are more resistant to quantization noise than\\nsmaller ones (Frantar et al., 2022; Chee et al., 2023).\\nLarge MoEs can be much larger than some of these\\nmassive dense models, and are thus a prime target for\\naccurate quantization.\\n• MoE training involves additional stochasticity through\\nrouting instabilities and strategies like token drop-\\nping (Lepikhin et al., 2020), which may inherently\\nencourage high resistance to noise. Finetuning is also\\noften performed with high dropout (Fedus et al., 2022).\\nOur experiments in Section 5.2 confirm that MoEs are in-\\ndeed highly robust to extreme levels of quantization.\\n3. Scaling Data-dependent Quantization to\\nTrillion Parameter MoEs\\n3.1. Challenges\\nWhile data-dependent quantization techniques have already\\nbeen used to successfully compress large dense models up to\\n176 billion parameters (Frantar et al., 2022; Wu et al., 2023),\\napplying them to sparse mixture-of-expert models another\\norder of magnitude larger brings several new challenges.\\nMemory Costs. The first major problem we encounter is\\na large increase in the memory required to apply such tech-\\nniques. Not only are the original model weights nearly 10×\\nlarger, but the quantization process itself also needs > 100×\\nmore data. The latter constraint is because accurate data-\\ndependent quantization methods require a sufficient number\\nof input samples for each layer that is being compressed.\\nFor very large dense models, a few hundreds of thousands\\nof “calibration tokens” typically suffice (Frantar et al., 2022;\\nYao et al., 2022). However, in MoEs with thousands of\\nlayers, a single expert processes only a small subset of all\\ninputs, hence we need much more tokens overall to achieve\\ngood coverage of all experts. Further, in encoder-decoder\\narchitecture models, like SwitchTransformers, each token\\nis processed only by half of the model, again increasing\\ndata requirements. For fast compression, we must maintain\\nintermediate results for the full calibration dataset, which\\nrequires 100s of GBs of memory for the largest models.\\nGPU Utilization. The next significant challenge is that\\nexisting large-scale quantization implementations, in par-\\nticular for GPTQ and related methods (Frantar et al., 2022;\\nChee et al., 2023), are designed to be fast and memory ef-\\nficient for the massive individual layers occurring in dense\\nmodels. Meanwhile, MoEs typically have smaller layers,\\nbut 100× to 1000× more of them. Current implementations\\nhave poor GPU utilization in this case, and consequently\\nbad performance. A similar issue occurs if activations and\\nweights have to be transferred between CPU and GPU with\\nhigh frequency, which may be required to cope with the\\nmassive memory requirements discussed previously.\\nReliability Requirements. Finally, another issue when\\ncompressing models with tens of thousands of layers is that\\nrunning into rare edge cases, which may break the process, is\\nhighly likely. This is includes numerical problems like non-\\ninvertible layer-wise Hessians, as well as model-specific\\nones, e.g., extreme routing patterns on particular layers.\\n3.2. System Design & Optimizations\\nIn this section, we describe system-level design and opti-\\nmizations to address the challenges in Section 3.1. This\\nallows us to apply data-dependent compression to massive\\nMoEs, while preserving the key feature of post-training\\ncompression techniques: the ability to perform effective\\ncompression using only modest computational resources,\\ne.g., a single NVIDIA A6000 GPU and less than one day of\\ncompute. Although we focus on scaling the popular GPTQ\\nmethod, most techniques described below will generalize\\nto other data-dependent quantization approaches, like Zero-\\nQuant (Yao et al., 2022), as well.\\n3.2.1. OPTIMIZED ACTIVATION OFFLOADING\\nAs discussed in Section 3.1, a key challenge in compressing\\nMoEs is that we need to maintain massive activation sets.\\nYet, it is possible to carefully orchestrate model execution in\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nQ\\nToken Hidden States\\nQuantized Expert\\nGPU\\n1\\n2\\n2\\n2\\n1\\n1\\n2\\nExpert Assignments\\nFigure 2. Illustration of the offloading execution for the sparse part of a Transformer block. An expert E2 and its corresponding input\\ntokens XE are fetched to GPU memory to produce E′\\n2, which together with the corresponding outputs YE are written back to CPU again.\\nsuch a way that we only ever need to perform computation\\non a small subset of the intermediate data. This allows us\\nto offload main storage from GPU, to much less expensive\\nand plentiful CPU memory.\\nConcretely, we maintain a single large buffer B which we\\nupdate as follows, for the dense part of a Transformer block:\\n1. Fetch one “sample” X, containing a few hundreds of\\ntokens, from CPU to GPU.\\n2. Pass it through the corresponding dense layers to obtain\\nthe result Y .\\n3. Calculate and store expert assignment for tokens in Y .\\n4. Send Y back to CPU and overwrite X in B.\\nand respectively for the sparse part, looping over experts:\\n1. Fetch all individual tokens in B that have been assigned\\nto expert E, denoted by XE, from CPU to GPU.\\n2. Use them to produce compressed expert E′ (for exam-\\nple, with GPTQ).\\n3. Run XE through E′ to get YE′.\\n4. Send YE′ back to CPU and overwrite XE in B.\\nThis process, which is visualized in Figure 2, minimizes\\nboth memory consumption and transfer cost: we need only\\na single copy of B and each token is only read and written\\ntwice per Transformer block.\\n1\\n3\\n4\\n7\\n8\\n8\\nBounds\\nTokens\\nFigure 3. List buffer example with 3 samples, indicated by hue.\\n3.2.2. LIST BUFFER\\nTo efficiently support per-sample access for evaluating dense\\nmodel components, as well as fully-vectorized querying of\\nexpert tokens, we store B as a list buffer data structure. This\\ncan be seen as a huge contiguous buffer of all token hidden\\nstates, together with delimiter indices denoting boundaries\\nbetween individual samples. Figure 3 illustrates this storage\\nformat. This datastructure is crucial for efficiency; naively\\niterating over samples and fetching relevant tokens via mask-\\ning is unusably slow for large sample counts.\\n3.2.3. LAZY WEIGHT FETCHING\\nSince the weights of the 1.6 trillion parameter model con-\\nsume > 3 TB of storage, they cannot even be stored in\\nCPU RAM. Thus, we lazily fetch them directly from disk\\nstorage as they are required. If we follow the inference\\nprocedure outlined in Section 3.2.1, this would be exactly\\nonce. Afterwards, their memory is released again.\\n3.2.4. EXPERT GROUPING\\nAdditionally, in order to avoid GPU underutilization (see\\nSection 3.1), we group multiple experts together and apply\\na joint batched variant of the GPTQ algorithm. Concretely,\\nwe extract the inputs XE corresponding to all experts E ∈E\\nin group E (the XE will generally have different sizes) and\\ncompute Hessians HE. These matrices, together with the\\nweight matrices WE, are then stacked to 3-dimensional ten-\\nsors, on which our modified GPTQ algorithm operates, com-\\npressing all experts simultaneously. We can also compute\\nHE = XEX⊤\\nE directly with a single matmul as the XE are\\ngenerally small enough, avoiding the slow per-sample accu-\\nmulation employed by prior implementations. Our default\\nexpert groupsize |E| is 16, which brings a good trade-off\\nbetween GPU memory consumption and utilization.\\nTable 1 demonstrates the impact of expert grouping via\\nGPTQ batching, when compressing a sparse encoder layer\\nof switch-base-128 using 10k samples; |E| = 16 yields\\nabout ≈6× speedup over standard per-expert computation.\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\n|E| = 1\\n|E| = 4\\n|E| = 16\\n174.1s\\n54.4s\\n28.8s\\nTable 1. Sparse layer compression time for different |E|.\\n3.2.5. ROBUSTNESS MODIFICATIONS\\nTo achieve sufficiently high robustness for successfully\\nquantizing trillion parameter models with tens of thousands\\nof layers, we need to employ various numerical and memory\\nadjustments. The most important are listed below:\\n• We use 10× higher relative Hessian dampening δ =\\n0.1, avoiding breakdowns with inf-values.\\n• Very few layer Hessians are not invertible even after\\nhigh dampening; we skip GPTQ for those and simply\\nperform vanilla rounding.\\n• Sometimes an expert receives a number of tokens that\\nis much larger than average, leading to out-of-memory\\nsituations when these are fetched to GPU. We avoid\\nthis by capping the maximum number of tokens used\\nfor compression at 4× the mean and use multiple itera-\\ntions for computing and updating YE in such cases.\\n3.3. Accuracy Improvements\\nIn addition to implementing a highly efficient compression\\nsystem, we also make new discoveries about applying GPTQ\\nin our particular context, i.e., for models trained for masked-\\nlanguage-modelling, MoEs and ternary quantization.\\nPremasking Special Tokens. First, we find that results\\ncan be improved if the various special separator tokens\\ninserted by the masked-language-modelling task (Raffel\\net al., 2020b) are excluded from the calibration data used for\\ncompression. Conretely, in the encoder, we mask out those\\n“mask-tokens” during the Hessian computation. Meanwhile,\\nin the decoder, we skip the token directly before such a\\nspecial token as this is the one used to predict the latter.\\nAs shown in Table 2 for switch-base-128 with 10k samples,\\nthis brings noticeably lower loss at no additional compute\\ncost. We think that because those tokens are very common\\nduring training, the model is so robust in their prediction\\nthat any error compensation on them during quantization is\\nunnecessary, while worsening correction for other tokens.\\nmask\\nBF16\\n2bit\\ntern\\nno\\n1.73\\n1.86\\n2.16\\nyes\\n1.73\\n1.76\\n1.99\\nTable 2. Impact of special token masking; validation loss.\\nIneffective Heuristics. We also evaluate two more recently\\nproposed GPTQ enhancement heuristics: activation reorder-\\ning and true sequential execution (Frantar et al., 2023). How-\\never, as shown in Table 3 for ternary quantization of switch-\\nbase-128, we find the former to be actually harmful and the\\nlatter to be more or less quality neutral, for our particular\\nuse-case. We suspect that, in this highly aggressive setting,\\nquantizing all the most sensitive columns first, leads to large\\nchanges of the entire weight matrix, and thus to overfitting.\\nGPTQ\\nact\\nseq\\nact + seq\\n1.99\\n2.23\\n1.99\\n2.28\\nTable 3. Activation reordering (act) and sequential execution (seq).\\n4. Realizing Sub-1-Bit Compression\\nUsing our system discussed in Section 3, we can accurately\\nquantize extremely large SwitchTransformers to very low\\nbit-widths: 2-bit and even ternary (3 possible values). Yet, in\\npractice, this falls still short of our compression goal of less\\nthan 1 bit per parameter. We find that compression rates can\\nbe pushed significantly further by taking advantage of the\\nlow entropy in the quantized weights. Next, we co-design\\nan encoding scheme and a CUDA kernel which realize sub-\\n1-bit per weight compression in practice, at minimal cost in\\nterms of GPU execution overhead for inference.\\n4.1. Natural Sparsity\\nWe pick quantization grids in standard fashion: row-wise\\naround the min and max weights values (Dettmers et al.,\\n2022; Frantar et al., 2022), e.g., for ternary: {wmin, 0, wmax}.\\nThese rather wide grids combined with the fact that weights\\nare typically close to normally distributed, naturally lead to\\nhigh sparsity after quantization, i.e., a large number of zeros.\\nWe demonstrate this in Table 4, averaged over all layers. For\\nternary weights, the largest model achieves close to 90%\\nnatural sparsity; the standard deviation is also quite low, at\\n< 5%. Seen another way, the quantized weights have low\\nentropy, meaning that, on average, significantly less bits per\\nweight should be required for lossless storage.\\nmodel\\n2-bit\\nternary\\nbase128\\n72.2%\\n85.7%\\nlarge128\\n73.1%\\n86.4%\\nc2048\\n76.5%\\n88.6%\\nTable 4. Natural sparsity for different compressed models.\\n4.2. From Sparsity to Entropy\\nThe direct way of utilizing these high zero proportions\\nwould be in form of a joint sparse & quantized represen-\\ntation (Kurtic et al., 2022; Yu et al., 2023): storing only\\nthe quantized values of non-zero weights, together with\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nnecessary position metadata. However, as our base quantiza-\\ntion levels are already very low, standard sparsity metadata\\nformats (Elsen et al., 2020; Lin et al., 2023) would only\\nallow limited additional compression. A bitmask indicating\\nnon-zero locations requires 1 bit per weight, while 10-13\\nbit (depending on layer size) column indices are even less\\nmemory efficient at the sparsity levels we encounter. There-\\nfore, we take a different approach: we do not utilize sparsity\\ndirectly but rather the low entropy, which is implied by the\\nfact that a single value (0) occurs very frequently, using an\\nappropriate encoding scheme.\\n4.2.1. FAST GPU DECODING CHALLENGES\\nIn principle, we could group multiple consecutive ternary\\nweights into super-symbols and then apply a code which\\nassigns variable length codewords to those super-symbols,\\nbased on their probability of occurrence, for example, via a\\nHuffman approach (Huffman, 1952). If the quantized weight\\nvalues were close to independent, this would achieve strong\\ncompression rates; in fact, for actual independence, they\\nwould be essentially Shannon-optimal (MacKay, 2003).\\nAt the same time, our primary goal is to use compressed\\nmodels for fast and space-efficient inference. Thus, it is\\ncritical not only that our encoding scheme achieves good\\ncompression, but also that it can be decoded fast on GPU\\nhardware. This is challenging for a number of reasons:\\nChallenge 1: Entropy-based codes generally possess se-\\nquential decoding dependencies: symbol i can only be de-\\ntermined if the length, which is variable, of all (i −1) prior\\nsymbols is known. Hence, processing consecutive symbols\\nsimultaneously leads to high synchronization overhead.\\nChallenge 2: Binary words in storage (e.g., INT32 blobs)\\nmay contain different numbers of decoded symbols. Conse-\\nquently, even if rows/blocks are encoded independently, par-\\nallel decoding will happen non-uniformly, while all threads\\nin a GPU-warp must always execute the same instruction.\\nThis would result in many wasted operations.\\nChallenge 3: Variable-length low-bit decoding involves a\\nlarge number of binary operations like shifts, which are not\\nparticularly efficient on GPUs.\\nChallenge 4: Individual matrices of MoEs are typically not\\nvery large, making it difficult to split them into enough sep-\\narately decoded segments to achieve good GPU utilization\\nwithout having to store additional data to break sequential\\ndependencies, which would harm compression rates.\\nIn contrast, uncompressed half-precision matrix-vector prod-\\nucts, which are the primary operation underlying generative\\ninference, easily achieve close to ideal memory-bandwidth\\nutilization and thus present a very strong baseline.\\n4.3. Compression Scheme & Kernel Co-design\\nTo achieve our goal, we need to design a compression\\nscheme and its GPU decoding kernel jointly, and poten-\\ntially trade off compression for faster decoding. We begin\\nwith an overview of the main ideas behind our approach,\\nfollowed by an in-depth discussion of key details.\\n4.3.1. OVERVIEW\\nInstead of a code with variable length codewords (see Sec-\\ntion 4.2.1) mapping to fixed length data, we will use a\\ndictionary-based code with fixed length codewords mapping\\nto a variable number of symbols. Such LZW-based schemes\\n(Welch, 1984) are popular for general purpose compression\\nlike ZIP, as they are particularly effective for text data with\\nlong repeated segments. While a dictionary code is not ideal\\nin terms of compression rate for the case of almost-random\\ndata in our application, it will be key for fast GPU decoding.\\nFirst, our kernel design uses one warp, that is 32 consecutive\\nthreads, to handle a row of a weight matrix, each of which is\\nencoded independently. This addresses Challenge 4 in Sec-\\ntion 4.2.1, yielding reasonable GPU utilization for relevant\\nmatrix sizes, with negligible metadata overhead. Further,\\nwe use a fixed-to-variable code with a large dictionary. This\\nallows us to use a full warp to process one codeword at-a-\\ntime, extracting all data, while maintaining good efficiency,\\nthus working around Challenges 1 and 2. This way, slow\\nbit and base-3 operations (for ternary) can also be kept at a\\nminimum, resolving Challenge 3.\\n4.3.2. DICTIONARY DESIGN AND IMPLEMENTATION\\nIn general, assume that the values of a ternary weight matrix\\n(denoted by 0, 1, 2) are distributed close to independently\\naccording to the distribution:\\nP(0) = p0,\\nP(1) = P(2) = 1 −p0\\n2\\n,\\n(2)\\nwhere p0 denotes the probability of sampling 0, e.g., 0.885\\nas per Table 4. Since we plan to use a rather large dictionary,\\nit should be shared between many weight matrices, in or-\\nder for the dictionary itself not to cause substantial storage\\noverheads. We find that such a static dictionary works well\\nenough, while simplifying memory efficient compression\\n(see Section 3.2) as we do not have to collect statistics over\\nmany yet uncompressed experts.\\nNext, we consider pairs of ternary values t = (t1, t2), whose\\ncorresponding probability is P(t) = P(t1)P(t2). We gen-\\nerate the 216 highest probability sequences containing at\\nmost 14 such pairs. This dictionary can be generated using a\\nmax-priority queue on probability, as shown by Algorithm 1.\\nTo briefly understand the procedure, notice that upon the\\nfirst iteration, it will push all individual pairs t = (t1, t2) to\\nthe priority queue, sorting them by decreasing probability,\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nAlgorithm 1 Generate decoding dictionary sequences.\\nQ ←max priority queue containing (1.0, ())\\nwhile |D| < 216 do\\np, s ←pop(Q)\\nappend s to dictionary if 0 < |s| < 28\\nfor t ∈{(t1, t2) | t1, t2 ∈{0, 1, 2}} do\\npush((p · P(t), cat(s, t)), Q)\\nend for\\nend while\\nafter which they will be expanded in this order.\\nWe have exactly 216 codewords as this allows us to store\\nthem in the native UINT16 datatype, avoiding any slow bit-\\nextractions at this decoding level. Each of those codewords\\nmaps to two consecutive UINT32 values containing up to 7\\npairs each, stored using 2 bits per ternary value, followed by\\nthe total number of pairs in the sequence; see also Figure 4.\\nThis format dictates our maximum chosen pair count of 14.\\nFurther, we consider pairs, rather than individual weights, to\\nfit the maximum count into 4 bits. The 2-bit-per-weight for-\\nmat is used as there is enough space, while a more compact\\nternary encoding would involve slow modulo and division\\noperations for extraction. We store the pair-count twice so\\nthat each thread can work with only half of the data, stored\\nin a fast INT32 type.\\n01\\n10\\n00\\n00\\n01\\n00\\n00\\n00\\n01\\n10\\n00\\n00\\n10\\n00\\n1100\\n2 bits\\n4 bits\\n1 weight\\n1 pair\\npair count\\n4 bits\\n01\\n10\\n00\\n00\\n01\\n00\\n00\\n00\\n10\\n10\\n00\\n00\\n00\\n00\\n1100\\nunfilled\\nFigure 4. Data format of a dictionary entry; here of 24 weights.\\nOverall, mapping 16-bit codewords to 64-bit data blobs\\nstrikes a good balance between several goals: (a) Having\\ncodewords map to, on average, more uncompressed values\\nthan their bitwidth, a necessary condition for achieving < 1-\\nbit compression. (b) Minimizing the overall storage cost of\\nthe dictionary to fit into the L2-cache of the GPU, which\\nis critical for good decoding performance. (c) Utilizing\\nas many threads in a warp as possible for simultaneously\\nextracting plain weights from the decoded data; usually,\\n> 16 will do useful work and only 4 out of 32 threads are\\nnever active in this step. (d) Avoiding as many conditionals\\nand extra operations necessary for dealing with non-uniform\\ndata storage as possible, which slow down parallelization.\\nFinally, we note that while dictionary lookups are in princi-\\nple random access, keeping it sorted from highest to lowest\\nprobability ensures very favorable caching behavior. Since\\neach lookup also automatically prefetches several subse-\\nquent elements, and most lookups are for frequently occur-\\nring codewords, there are many fast L1-cache hits.\\nValidation. To assess the effectiveness of our scheme, we\\ncompute achieved compression rates, both on a real ternary\\nquantized c2048 model as well as on weight matrices sam-\\npled directly from distribution (2), yielding 20.07× and\\n21.11×, respectively. This gap of only ≈5% suggests that\\nour simplifying independence assumption is indeed quite\\nclose for large models. We also note that our rates are only\\n≈20% away from the distribution’s (with p = 0.885) theo-\\nretical compression limit of 25.40×, which we consider a\\nreasonable trade-off for enabling fast GPU decoding.\\n4.3.3. GPU KERNEL\\nHaving defined the dictionary format, we can now discuss\\nthe design of the actual decoding kernel in detail. We focus\\non the most important operation for inference, decompres-\\nsion fused with a matrix-vector-product. However, our tech-\\nniques can easily be adapted to other use-cases, e.g., pure\\ndecompression.\\nListing 1 provides CUDA-like pseudocode for our kernel,\\ncomputing the matrix-vector-product of compressed matrix\\nw comp (with metadata row off and ter minmax, using\\ndictionary dec) and BF16 vector x, into output buffer y. The\\nhandling of various edge cases and some index calculations\\nhave been removed for readability. Please see our repository\\nfor the fully functional implementation.\\n1\\ntemplate <int num_warps, int w_width>\\n2\\n__global__ void Sub1MatVec(\\n3\\nint* dec,\\n4\\nushort* w_comp, int* row_off, __nv_bfloat162* ter_minmax,\\n5\\n__nv_bfloat16* x, __nv_bfloat16* y\\n6\\n) {\\n7\\n__shared__ float x_shared[w_width];\\n8\\nfor (int i = thread; i < w_width; i += 32 * num_warps)\\n9\\nx_shared[i] = __bfloat162float(x[i]);\\n10\\n11\\n__shared__ float deq[3][32 * num_warps];\\n12\\ndeq[0][thread] = 0;\\n13\\ndeq[1][thread] = __bfloat162float(ter_minmax[row].x);\\n14\\ndeq[2][thread] = __bfloat162float(ter_minmax[row].y);\\n15\\n16\\n__syncthreads();\\n17\\n__shared__ w_comp_block[32][num_warps];\\n18\\n19\\nfloat res = 0;\\n20\\nint idx = 0;\\n21\\n22\\nfor (int i = 0; i < row_off[row + 1] - row_off[row]; i += 32) {\\n23\\nw_comp_block[warp][lane] = w_comp[i + lane];\\n24\\n25\\nif (lane < 28) {\\n26\\nfor (int j = 0; j < 32; j++) {\\n27\\nint enc = w_comp_block[warp][j];\\n28\\nint wx14 = dec[2 * enc + (lane / 14)];\\n29\\nint ter = (wx14 >> (4 + 2 * (lane % 14))) & 0x3;\\n30\\nfloat w = deq[ter][thread];\\n31\\nres += w * x_shared[idx + lane];\\n32\\nidx += 2 * (wx14 & 0xf);\\n33\\n}\\n34\\n}\\n35\\n}\\n36\\n37\\nfor (int i = 16; i > 0; i /= 2)\\n38\\nres += __shfl_down_sync(0xffffffff, res, i);\\n39\\nif (lane == 0)\\n40\\ny[row] += __float2bfloat16(res);\\n41\\n}\\nListing 1. Simplified kernel pseudocode for a fused decompress +\\nmatrix-vector-product operation.\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nParallelization. Overall, each threadblock will handle mul-\\ntiple consecutive rows, each of which is processed by a sin-\\ngle warp. We use exactly one thread-block per GPU Stream-\\ning Multiprocessor (SM) with min(#rows in block, 32)\\nwarps; if there are more than 32 rows in a block, (some)\\nwarps sequentially process multiple rows (note that this part\\nis omitted in Listing 1 for simplicity). This avoids any bad\\nwave quantization effects. We find this strategy to be an ef-\\nfective heuristic that yields good performance for all matrix\\nshapes we consider.\\nExecution. Our kernel starts by loading the entire input\\nvector to shared memory (x shared, lines 7-9), using all\\nwarps in a threadblock. This enables fast element access in\\nthe subsequent per-row product-sum accumulations.\\nNext, each warp processes its corresponding row by\\nfirst fetching (up to) 32 codewords into shared memory\\n(w comp block, line 23) using a single coalesced transac-\\ntion. It then loops over those symbols, processing one-at-\\na-time (lines 26-33). First, using 28 of its 32 threads (line\\n25), it fetches the corresponding decoding data from the\\ndictionary where the first UINT32 is assigned to threads\\n0-13 and the second to threads 14-27 (wx14, line 27). Then,\\neach thread extracts its corresponding ternary weight (lines\\n29-30) and adds the corresponding input product into its\\nown partial result accumulator (res, line 31). We note that\\nthe input reads from shared memory are contiguous and do\\nnot cause bank conflicts. Afterwards, each thread advances\\nthe offset index (idx, line 32) into the input vector by the\\ntotal number of weights encoded in the current symbol.\\nFinally, after the full row has been scanned, a warp-\\nreduction (lines 37-38) over the partial results of each thread\\nyields the output (y, lines 39-40).\\nTernary decoding. Another relevant detail is that ternary\\nweights are stored as 0, 1, 2 (line 29) but need to be dequan-\\ntized to 0, wmin, wmax for multiplication with inputs. We\\nfound that the most efficient way of performing this con-\\nversion is via a shared memory lookup table (lines 11-14).\\nCrucially, this table needs to be replicated 32 times across\\nthe column-dimension to avoid very frequent bank conflicts,\\nwhich would otherwise occur every time not all 28 threads\\ndequantize the same value (line 30). Fortunately, there are\\nonly 3 input values and so its overall size is tolerable.\\nEncoding. So far, we have only focused on the decoding\\noperation, but we also have to encode matrices with reason-\\nable efficiency. In general, this is done by building a trie\\ndatastructure (of the dictionary discussed in Section 4.3.2)\\nmapping sequences to codewords. Then, we iterate through\\nthe input while simulatenously traversing the trie to find\\nlongest prefix matches, yielding the corresponding code-\\nwords. Finally, we densely pack rows of different lengths\\ninto a contiguous buffer and record corresponding row off-\\nsets. Unlike decoding, encoding is not very latency critical\\nand a straight-forward GPU kernel using one thread per row\\nof the matrix to compress suffices.\\n5. Experiments\\n5.1. General Setup\\nModels. We focus our experiments on the SwitchTrans-\\nformer (Fedus et al., 2022) family of models. Our primary\\ntarget is the very largest variant, c2048, with around 1.6\\ntrillion parameters, but we also consider the comparatively\\nsmall base128 (7B params) and large128 (26B params) ver-\\nsions for testing and ablations. We chose the SwitchTrans-\\nformer family as it contains the largest publicly-available\\nmodel, which also features a similar or higher number of\\ntraining tokens to parameters ratio than potential alterna-\\ntives like Artetxe et al. (2022). Further, those models are\\nalso among the most popular massive MoEs, with several\\nimplementations across frameworks.\\nFramework. As accessibility is a major goal of our work,\\nwe build our code-base around the PyTorch-backend of the\\nhighly popular HuggingFace (Wolf et al., 2019) framework,\\nrather than on the SwitchTransormer’s original training en-\\nvironment MeshTensorflow (Shazeer et al., 2018) or its\\nJAX-based successor T5X (Google, 2023). This brings a\\nnumber of additional challenges.\\nFirst, we find that the largest model variants require a hand-\\nful of bugfixes, primarily configuration and model setup\\nchanges, in order to run properly. We suspect that this is\\nbecause their enormous sizes have rendered extensive test-\\ning very difficult. Second, we observed a major inefficiency\\nin the context of generative inference for models with a\\nlarge number of experts: the HuggingFace implementation\\nwill perform several (empty) CUDA calls for potentially\\n1000s of experts to which no token is routed, accumulating\\nlarge overheads. We modify the implementation (also for\\nbaselines) to skip such unnecessary calls, leading to > 10×\\nspeedup for large models. We apply all changes to the Hug-\\ngingFace framework only dynamically at runtime, so that\\nour code can be run directly with an official installation.\\nHuggingFace prioritizes ease-of-use and flexibility over\\nhigh performance. For that reason, we conduct inference\\nmeasurements not only end-to-end, including all Hugging-\\nFace overheads, but also in isolated fashion, comparing\\nuncompressed and compressed matrix operations directly.\\nThis is to demonstrate that our GPU kernels would also yield\\nlow overhead in more optimized inference environments.\\nDatasets.\\nSwitchTransformers have been trained for a\\nMasked-Language-Modelling (MLM) objective (Raffel\\net al., 2020b) on the C4 dataset (Raffel et al., 2020a). Similar\\nto most works in the area of LLM quantization (Yao et al.,\\n2022; Frantar et al., 2022; Dettmers & Zettlemoyer, 2022),\\nwe focus on general upstream compression directly on this\\npretraining task/dataset combination. Consequently, our\\nevaluation focuses on validation performance for C4/MLM,\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nwhere we use the public reproduction of C4 on HuggingFace\\nas well as their replication of the original masking proce-\\ndure. Calibration data for compression is taken, in order,\\nfrom the first two shards of the training set. For efficiency,\\nwe primarily evaluate on 128 samples (corresponding to the\\naverage loss over > 10K tokens, which is quite stable) from\\nthe first shard of the validation set, but we also perform\\nsome evaluations other datasets.\\nHardware. All compression experiments, including those\\nfor the very largest models, can be performed in less than a\\nday on a single NVIDIA A6000 with 48GB of GPU memory.\\nHowever, efficiently compressing trillion parameter models\\nusing a large number of calibration samples requires a few\\n100GBs of (CPU) RAM; the original 1.6T model itself also\\noccupies > 3 TB disk storage. We highlight that our work is\\nperformed in a highly constrained environment for models\\nof this size, for example, it is already infeasible to load the\\nentire (uncompressed) 1.6T model into RAM, let alone into\\nGPU memory. For inference on compressed models, we\\nwill also consider running on multiple NVIDIA 3090 GPUs,\\nwith 24GB of memory each, in addition to A6000s.\\n5.2. Compression Results\\nAccuracy. We begin by quantizing all SwitchTransformer\\nmodels to 2-bit and ternary precision, and evaluating their\\nvalidation loss. Our default number of calibration samples is\\n10K for 128 experts and 160K for 2048, but we also consider\\nusing 0.5× and 2× as many samples. In addition to using\\nour efficient QMoE framework discussed in Section 3, we\\nalso consider a standard round-to-nearest (RTN) baseline\\n(Dettmers et al., 2022). We simulate the latter by fixing\\nHessians to the identity matrix, thus applying precisely the\\nsame quantization settings and evaluation protocol. Table 5\\nsummarizes our results.\\nmethod\\nbase128\\nlarge128\\nc2048\\n2bit\\ntern\\n2bit\\ntern\\n2bit\\ntern\\nBF16\\n1.73\\n1.55\\n1.18\\nRTN\\n2.27\\n4.54\\n1.96\\n2.79\\n1.33\\n2.15\\nQMoE 0.5x\\n1.78\\n2.11\\n1.54\\n1.70\\n1.22\\n1.27\\nQMoE 1.0x\\n1.76\\n1.99\\n1.56\\n1.69\\n1.20\\n1.26\\nQMoE 2.0x\\n1.76\\n1.93\\n1.57\\n1.64\\n1.21\\n1.26\\nTable 5. Comparing C4 validation losses for 2-bit and ternary (tern)\\nquantized SwitchTransformers. “QMoE 0.5x” indicates that only\\nhalf of the default number of calibration samples are used.\\nPerhaps surprisingly, vanilla rounding (RTN) does not lead\\nto a complete model collapse even at ternary precision, em-\\nphasizing the high robustness of large MoEs to quantization.\\nNevertheless, the loss increases are quite significant for\\nsmaller models at 2-bit and far too large to be useful at\\nternary precision. In contrast, using data-dependent quanti-\\nzation, 2-bit is achievable at minimal loss (1.7% relative on\\nc2048) and ternary at only a small increase (6.7% relative\\non c2048). This demonstrates not only the effectiveness\\nof such advanced quantization methods in this context, but\\nalso shows that extremely low-bit compression is indeed\\npractical for massive MoEs.\\nAdditionally, we conduct evaluations on Arxiv, GitHub,\\nStackeEchange and Wikipedia data sampled from RedPa-\\njama (Computer, 2023). Even though only < 0.01% of our\\nC4 calibration data originates from those websites, the com-\\npressed model still preserves performance almost as well as\\non the core of the distribution (see Table 6).\\nbits\\narxiv\\ngithub\\nstackexch.\\nwiki\\nBF16\\n1.31\\n0.99\\n1.15\\n1.20\\n2-bit\\n1.34\\n1.05\\n1.17\\n1.24\\ntern\\n1.42\\n1.13\\n1.22\\n1.32\\nTable 6. Additional evaluations for the c2048 model.\\nIn terms of calibration data, we see that increasing the\\namount of samples generally improves performance slightly,\\nmost noticeably for ternary quantization, but there is also\\nsome noise in the process, especially at 2-bit.\\nCompression. Next, we investigate the actual compression\\nrates that are achieved by further compressing ternary mod-\\nels using our scheme introduced in Section 4. We consider\\nboth compression relative to just the MoE modules (the\\nmodel parts we quantize) as well as to the full model and all\\nits metadata. The compression rates and overall checkpoint\\nsizes are listed in Table 7.\\nmodel\\nmoe-only\\nfull\\nsize [GB]\\nbf16\\nours\\nbase128\\n17.06×\\n11.76×\\n14.9\\n1.27\\nlarge128\\n18.34×\\n13.32×\\n52.7\\n3.96\\nc2048\\n20.07×\\n19.81×\\n3142\\n158.6\\nTable 7. Compression rates and sizes for ternary models.\\nIn general, measuring only relative to parts we compress\\n(moe-only), all sizes achieve > 16× compression rate and\\nthus < 1 bits per parameter storage. On c2048, even the\\noverall rate, including all uncompressed dense layers, re-\\nmains at 19.81×, corresponding to 0.807 bits per parameter,\\nreducing the checkpoint size from 3142GB to 158.6GB. One\\ncan also observe that compression rates increase with model\\nsize, which is for two reasons: (a) natural sparsity increases\\nwhile our encoding dictionary is also optimized for c2048\\n(see Section 4), and (b) weight distributions become closer\\nto independent for larger layer sizes.\\nRuntime. Finally, we evaluate how long it takes to produce\\ncompressed models on a single A6000 GPU, for different\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\n768 × 3072\\n3072 × 768\\n1024 × 4096\\n4096 × 1024\\n2080 × 6144\\n6144 × 2080\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nTime relative to BF16\\nPer-layer performance of compressed matrix-vector kernels\\nBF16\\nRTX3090\\nA6000\\nbase128\\nlarge128\\nc2048\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nSeconds to generate 128 tokens\\nEnd-to-end performance of compressed models\\n3090-BF16*\\n3090\\nA6000-BF16*\\nA6000\\nFigure 5. (Left) Per-layer compressed kernel performance relative to uncompressed execution. (Right) End-to-end runtimes of compressed\\nmodels and estimates (∗, would require 65/130 GPUs) for bloat16 baselines. c2048 is run on 4×A6000 and 8×3090 GPUs, respectively.\\namounts of calibration data. The results are shown in Ta-\\nble 8. Smaller models can be compressed in less than an\\nhour and even c2048 in less than a day, confirming the high\\nefficiency of QMoE. The runtime increase from large128\\nto c2048 is roughly proportional to the difference in size,\\ndespite the latter using 16× more samples. This is because\\nthe number of samples per expert stays constant and the\\nexpert size increases only slightly. Finally, we note that\\nsimply (iteratively) loading the original 1.6T model into\\nRAM takes close to 5 hours on our slow disk storage.\\nmodel\\n5K/80K\\n10K/160K\\n20K/320K\\nbase128\\n8.4min\\n14.0min\\n21.6min\\nlarge128\\n22.0min\\n30.2min\\n45.2min\\nc2048\\n13.3h\\n16.0h\\n20.8h\\nTable 8. Compression runtime for different calibration data size.\\n5.3. Runtime Results\\nIndividual Layers. Our kernel performance evaluation\\nstarts with a direct (isolated) comparison of our compressed\\nmatrix-vector product kernels (see Section 4) against Py-\\nTorch’s standard (uncompressed) bfloat16 cuBLAS kernels.\\nFigure 5 (Left) shows the time taken by our compressed\\nkernels relative to bfloat16, for the matrix shapes found in\\nour MoEs, on two different GPUs. While our kernels have\\nto perform a lot less slow (global) memory reads than the\\nbfloat16 baseline due to lower storage costs, they need to\\nspend much more compute for complex unpacking of the\\nheavily-compressed weights. Nevertheless, executing our\\ncompressed kernels takes less time than the close to ideal\\nbfloat16 baseline in all cases, with up to 35% speedup on\\nspecific matrix shapes. We note that these are very low-\\nlatency operations, with the smallest matrix taking < 0.02\\nmilliseconds and the largest < 0.05.\\nEnd-to-End Execution. Finally, we also benchmark our\\nkernels end-to-end in HuggingFace on the real weights of\\nour compressed MoE models. We consider an individual\\nuser application, like (Frantar et al., 2022; Leviathan et al.,\\n2023; Park et al., 2022), where a single prompt (sampled\\nfrom C4) should be processed to generate a 128-token re-\\nsponse. As actually running the bfloat16 version of the\\nc2048 model would require > 65 A6000 and > 130 3090\\nGPUs (versus 4 and 8, respectively, for sub-1-bit com-\\npressed weights) we have to estimate its runtime. We do this\\nby having all experts in a layer point to the same weight data\\n(completely resolving memory issues), which allows us to\\ncollect timings with precisely the same overheads as for our\\ncompressed models. However, this is a highly optimistic\\nestimate since real execution would require close to 20×\\nmore GPUs, with corresponding communication overheads,\\nand our numbers should thus be viewed only as a lower\\nbound.\\nThe results, shown in Figure 5 (Right), demonstrate that\\nend-to-end execution of compressed models is only < 5%\\nslower than standard (uncompressed) execution. This slight\\nslow-down despite faster per-layer timings is due to the fact\\nthat the encoder may sometimes route multiple tokens to\\nthe same expert. Our current implementation naively exe-\\ncutes a separate matrix-vector product for each token, while\\nthe baseline performs a much more efficient joint matrix\\nmultiplication. For applications where this is a significant\\nbottleneck, one could easily introduce an inner loop over to-\\nkens into our kernel (Listing 1, line 30), or fully decompress\\nfirst, followed by a standard matmul, for large token counts.\\n6. Related Work\\nMixture-of-Expert (MoE) Models.\\nMixture-of-expert\\nmodels are a popular research direction aimed at creating\\nsignificantly more efficient large-scale models (Fedus et al.,\\n2022; Artetxe et al., 2022; Clark et al., 2022). At the core\\nof MoEs lie (sparse) routing mechanisms, of which many\\nvariants have been proposed. Those range from static as-\\nsignment based on input tokens IDs (Roller et al., 2021),\\nover dynamic token-to-expert matching (Zhou et al., 2022),\\nto “soft” routing of linear input combinations (Puigcerver\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\net al., 2023). Since MoEs can feature rather different com-\\nputational profiles from standard dense models, there is also\\nsignificant research on optimizing inference and training\\nsystems (Barham et al., 2022; Gale et al., 2023; Hwang\\net al., 2023). Among the most critical problems in this area\\nare data-exchanges between accelerators during routing and\\ndealing with uneven compute-loads for different experts.\\nLLM Quantization. Quantization is a very popular com-\\npression technique, which has seen a vast amount of\\nwork (Gholami et al., 2021), especially in the context\\nof LLMs.\\nSpecifically, the ability to perform accurate\\nweight quantization for billion-parameter models has greatly\\nboosted their accessibility: it has been shown that extremely\\nlarge dense models can be quantized to 8- or even 4-bit\\nprecision at little accuracy loss (Dettmers et al., 2022; Yao\\net al., 2022; Frantar et al., 2022; Dettmers & Zettlemoyer,\\n2022). Pushing towards even lower bitwidths via more\\nsophisticated compression formats, like multi-level group-\\ning coupled with higher-precision outliers (Dettmers et al.,\\n2023b), or new quantization techniques, like incoherence\\npreprocessing (Chee et al., 2023), is an active area of re-\\nsearch. Currently, accurate quantization to 2 or less bits per\\nparameter appears to be a major barrier for post-training\\nquantization of standard LLMs. By contrast, in this work we\\nshow that massive MoE models appear to be significantly\\nmore compressible, as we achieve sub-1-bit compression at\\ncomparable loss increases to 3-bit or 4-bit quantization of\\nstandard LLMs with advanced techniques.\\nMoE Compression. There has also been work on com-\\npressing MoE models in particular. Chen et al. (2022) and\\nKoishekenov et al. (2022) perform compression via spe-\\ncialization of MoEs to specific “downstream” finetuning\\ndatasets by pruning components not relevant to the par-\\nticular task. In contrast, we focus on general “upstream”\\ncompression of the pretrained model, via extremely low-bit\\nquantization. Other works (Kim et al., 2022b; Yi et al., 2023;\\nKim et al., 2023) also perform MoE quantization, but focus\\non noticeably higher bit-widths, like 8 or 4 bits per weight.\\nThis is accomplished primarily via simple rounding, which,\\nas shown by our experiments, is not accurate enough for full\\n2-bit or lower compression. Kim et al. (2022a) achieve 2-bit\\nquantization on a 5 billion parameter MoE, which is con-\\nsidered relatively small in this area, by further optimization\\nof the model via Quantization-Aware Training (Nagel et al.,\\n2021). Applying such an approach for trillion-scale models\\nwould be extremely resource intensive. They also do not\\nprovide any mechansims for exploiting low-bit quantization\\nand its corresponding natural sparsity in practice, which is\\nchallenging and constitutes a key contribution of our work.\\nWe are particularly focused on scalabilty and practicalty.\\nWhile existing works study models with at most tens of\\nbillions of parameters, we demonstrate the effectiveness\\nand efficiency of our techniques at trillion parameter scale,\\nboth for the quantization process itself as well as for actual\\ninference of compressed models.\\n7. Discussion and Limitations\\nWe have presented QMoE, an end-to-end compression and\\ninference framework for addressing the massive memory\\ncosts of MoE inference. We showed, for the first time, that\\nmodels such as the trillion-parameter SwitchTransformer-\\nc2048 can be accurately compressed to less than 1 bit per\\nparameter, close to 20× compression rate, in a custom for-\\nmat that enables the first efficient end-to-end execution of\\nsuch a model on a single commodity GPU server. QMoE is\\nfully open-source and built around the popular HuggingFace\\nframework, making deployment and research for massive\\nMoEs significantly cheaper and more accessible.\\nOur study is confined to a limited set of models, as only\\nvery few massive and accurate MoEs are available publicy.\\nAdditionaly, due to their size, most MoEs are trained and de-\\nployed in different bespoke framework, requiring complex\\nmanual integrations to use for further research. Neverthe-\\nless, we have covered some of the largest and most accurate\\navailable MoEs, specifically SwitchTransformers (Fedus\\net al., 2022). A natural extension of our work would be to\\napply our QMoE techniques to other MoE models and vari-\\nants, such as Artetxe et al. (2022) or the recently-proposed\\nSoftMoEs (Puigcerver et al., 2023).\\nAdditionally, we have focused on direct compression of the\\npretrained base model. However, it would also be interesting\\nto further finetune a compressed model for specialized down-\\nstream tasks, similar to QLoRA (Dettmers et al., 2023a).\\nZoph et al. (2022) report strong results when finetuning\\nonly non-expert layers, which QMoE leaves uncompressed,\\nsuggesting that this application could be promising. We\\nhope to explore this in future work.\\nReferences\\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,\\nShleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al.\\nEfficient large scale language modeling with mixtures\\nof experts. In Empirical Methods in Natural Language\\nProcessing (EMNLP), 2022.\\nBarham, P., Chowdhery, A., Dean, J., Ghemawat, S., Hand,\\nS., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., et al.\\nPathways: Asynchronous distributed dataflow for ml. In\\nConference on Machine Learning and Systems (MLSys),\\n2022.\\nBengio, Y., L´\\neonard, N., and Courville, A. Estimating or\\npropagating gradients through stochastic neurons for con-\\nditional computation. arXiv preprint arXiv:1308.3432,\\n2013.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nIn Conference on Neural Information Processing Systems\\n(NeurIPS), 2020.\\nChee, J., Cai, Y., Kuleshov, V., and De Sa, C. Quip: 2-bit\\nquantization of large language models with guarantees.\\narXiv preprint arXiv:2307.13304, 2023.\\nChen, T., Huang, S., Xie, Y., Jiao, B., Jiang, D., Zhou, H.,\\nLi, J., and Wei, F. Task-specific expert pruning for sparse\\nmixture-of-experts. arXiv preprint arXiv:2206.00277,\\n2022.\\nClark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini,\\nM., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,\\nBorgeaud, S., et al. Unified scaling laws for routed lan-\\nguage models. In International Conference on Machine\\nLearning (ICML), 2022.\\nComputer, T.\\nRedPajama:\\nAn open source recipe\\nto reproduce llama training dataset, 2023.\\nURL\\nhttps://github.com/togethercomputer/\\nRedPajama-Data.\\nDettmers, T. and Zettlemoyer, L. The case for 4-bit pre-\\ncision: k-bit inference scaling laws.\\narXiv preprint\\narXiv:2212.09720, 2022.\\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\\nLLM.int8(): 8-bit matrix multiplication for transformers\\nat scale. arXiv preprint arXiv:2208.07339, 2022.\\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\\nL. QLoRA: Efficient finetuning of quantized llms. arXiv\\npreprint arXiv:2305.14314, 2023a.\\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,\\nD., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,\\nand Alistarh, D. SpQR: A sparse-quantized representation\\nfor near-lossless llm weight compression. arXiv preprint\\narXiv:2306.03078, 2023b.\\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D.,\\nXu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O.,\\net al. GLaM: Efficient scaling of language models with\\nmixture-of-experts. In International Conference on Ma-\\nchine Learning (ICML), 2022.\\nElsen, E., Dukhan, M., Gale, T., and Simonyan, K. Fast\\nsparse convnets. In Conference on Computer Vision and\\nPattern Recognition (CVPR), 2020.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\\ners: Scaling to trillion parameter models with simple\\nand efficient sparsity. The Journal of Machine Learning\\nResearch, 23(1):5232–5270, 2022.\\nFrantar, E. and Alistarh, D. SparseGPT: Massive language\\nmodels can be accurately pruned in one-shot. In Interna-\\ntional Conference on Machine Learning (ICML), 2023.\\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh,\\nD.\\nGPTQ: Accurate post-training compression for\\ngenerative pretrained transformers.\\narXiv preprint\\narXiv:2210.17323, 2022.\\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.\\nGPTQ code, 2023.\\nURL https://github.com/\\nIST-DASLab/gptq.\\nGale, T., Narayanan, D., Young, C., and Zaharia, M.\\nMegaBlocks: Efficient sparse training with mixture-of-\\nexperts. In Conference on Machine Learning and Systems\\n(MLSys), 2023.\\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,\\nand Keutzer, K.\\nA survey of quantization methods\\nfor efficient neural network inference. arXiv preprint\\narXiv:2103.13630, 2021.\\nGoogle.\\nT5x, 2023.\\nURL https://github.com/\\ngoogle-research/t5x.\\nHoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and\\nPeste, A. Sparsity in deep learning: Pruning and growth\\nfor efficient inference and training in neural networks.\\narXiv preprint arXiv:2102.00554, 2021.\\nHubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,\\nD. Accurate post training quantization with small cal-\\nibration sets. In International Conference on Machine\\nLearning (ICML), 2021.\\nHuffman, D. A. A method for the construction of minimum-\\nredundancy codes. Proceedings of the IRE, 40(9):1098–\\n1101, 1952.\\nHwang, C., Cui, W., Xiong, Y., Yang, Z., Liu, Z., Hu, H.,\\nWang, Z., Salas, R., Jose, J., Ram, P., et al. Tutel: Adap-\\ntive mixture-of-experts at scale. In Conference on Ma-\\nchine Learning and Systems (MLSys), 2023.\\nKim, Y. J., Fahim, R., and Awadalla, H. H. Mixture of\\nquantized experts (MoQE): Complementary effect of low-\\nbit quantization and robustness. OpenReview, 2022a.\\nKim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.\\nWho says elephants can’t run: Bringing large scale\\nmoe models into cloud scale production. arXiv preprint\\narXiv:2211.10017, 2022b.\\nKim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.\\nFinequant:\\nUnlocking efficiency with fine-grained\\nweight-only quantization for llms.\\narXiv preprint\\narXiv:2308.09723, 2023.\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nKoishekenov, Y., Nikoulina, V., and Berard, A. Memory-\\nefficient NLLB-200: Language-specific expert pruning\\nof a massively multilingual machine translation model.\\narXiv preprint arXiv:2212.09811, 2022.\\nKurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz,\\nM., Fineran, B., Goin, M., and Alistarh, D. The Op-\\ntimal BERT Surgeon: Scalable and accurate second-\\norder pruning for large language models. arXiv preprint\\narXiv:2203.07259, 2022.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang,\\nY., Krikun, M., Shazeer, N., and Gshard, Z. Scaling\\ngiant models with conditional computation and automatic\\nsharding. arXiv preprint arXiv:2006.16668, 2020.\\nLeviathan, Y., Kalman, M., and Matias, Y. Fast inference\\nfrom transformers via speculative decoding. In Interna-\\ntional Conference on Machine Learning (ICML), 2023.\\nLin, B., Zheng, N., Wang, L., Cao, S., Ma, L., Zhang,\\nQ., Zhu, Y., Cao, T., Xue, J., Yang, Y., et al. Efficient\\nGPU kernels for n:m-sparse weights in deep learning. In\\nConference on Machine Learning and Systems (MLSys),\\n2023.\\nMacKay, D. J. Information theory, inference and learning\\nalgorithms. Cambridge University Press, 2003.\\nNagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and\\nBlankevoort, T. Up or down? Adaptive rounding for\\npost-training quantization. In International Conference\\non Machine Learning (ICML), 2020.\\nNagel, M., Fournarakis, M., Amjad, R. A., Bondarenko,\\nY., van Baalen, M., and Blankevoort, T. A white pa-\\nper on neural network quantization.\\narXiv preprint\\narXiv:2106.08295, 2021.\\nPark, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,\\nD. nuQmm: Quantized matmul for efficient inference of\\nlarge-scale generative language models. arXiv preprint\\narXiv:2206.09557, 2022.\\nPuigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N.\\nFrom sparse to soft mixtures of experts. arXiv preprint\\narXiv:2308.00951, 2023.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. OpenAI blog, 1(8):9, 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research, 21\\n(140):1–67, 2020a.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the\\nlimits of transfer learning with a unified text-to-text trans-\\nformer. Journal of Machine Learning Research (JMLR),\\n21(1):5485–5551, 2020b.\\nRoller, S., Sukhbaatar, S., Weston, J., et al. Hash layers for\\nlarge sparse models. In Conference on Neural Informa-\\ntion Processing Systems (NeurIPS), 2021.\\nShazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A.,\\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\\nC., et al. Mesh-tensorflow: Deep learning for supercom-\\nputers. Conference on Neural Information Processing\\nSystems (NeurIPS), 2018.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`\\nere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation lan-\\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288,\\n2023b.\\nWang, P., Chen, Q., He, X., and Cheng, J. Towards accurate\\npost-training network quantization via bit-split and stitch-\\ning. In International Conference on Machine Learning\\n(ICML), 2020.\\nWelch, T. A. A technique for high-performance data com-\\npression. Computer, 17(06):8–19, 1984.\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\\net al. Huggingface’s transformers: State-of-the-art natural\\nlanguage processing. arXiv preprint arXiv:1910.03771,\\n2019.\\nWu, X., Yao, Z., and He, Y. ZeroQuant-FP: A leap forward\\nin llms post-training w4a8 quantization using floating-\\npoint formats. arXiv preprint arXiv:2307.09782, 2023.\\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\\nHe, Y. ZeroQuant: Efficient and affordable post-training\\nquantization for large-scale transformers. arXiv preprint\\narXiv:2206.01861, 2022.\\nYi, R., Guo, L., Wei, S., Zhou, A., Wang, S., and Xu,\\nM. Edgemoe: Fast on-device inference of moe-based\\nlarge language models. arXiv preprint arXiv:2308.14352,\\n2023.\\nYu, C., Chen, T., and Gan, Z. Boost transformer-based\\nlanguage models with gpu-friendly sparsity and quanti-\\nzation. In Findings of the Association for Computational\\nLinguistics: ACL 2023, 2023.\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,\\nDai, A. M., Le, Q. V., Laudon, J., et al. Mixture-of-\\nexperts with expert choice routing. Conference on Neural\\nInformation Processing Systems (NeurIPS), 2022.\\nZoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J.,\\nShazeer, N., and Fedus, W. ST-MoE: Designing stable\\nand transferable sparse expert models. arXiv preprint\\narXiv:2202.08906, 2022.\\n', 'source_name': 'QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models', 'source_url': 'https://arxiv.org/abs/2310.16795'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "QMoE_NOTES.pdf #40\n",
      "{'content': 'QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models \\nMain Idea: this paper presents a technique to quantize/compress large MoE models. More \\nspecifically, it compresses Switch Transformer (1.6T parameter model, 3.2 TBs of memory \\nneeded) 20x, to only 160 GB (0.8 bits per parameter compared to 16 without any compression). \\nThis quantization technique at the Switch scale can be done in less than a day on a single GPU \\nand results in a minor accuracy loss. \\n \\nMoE: faster inference with the tradeoff of higher memory cost. \\nUsually, post-training compression techniques only reduce parameters to around 3 or 4 bits. This \\nwould not be enough to make MoE practical – giving inspiration to QMoE. \\n \\nMoE Quantization \\n- \\nIt might suffice only quantizing MoE layers (not FFs). \\n- \\nLarge dense models are more resistant to quantization, so MoE models can be a good \\ntarget for it (due to increase in scale seeming to relate to better teachers). \\n- \\nMoE training might be highly resistant to noise. \\nMain Challenges: \\n- \\nMemory \\no The quantization process requires data. In the case of MoEs, the data needed is \\nmuch larger than with dense models, due to the potential large number of experts. \\nThis means that it is even more important to have data that represents different \\nparts of the distribution, so all experts are represented. \\n- \\nGPU utilization \\no Large-scale quantization had previously been applied to dense models, which \\nconsists of applying it to single massive individual layers, which is fast and efficient \\non GPUs. This can be challenging for MoEs as instead of single massive layers there \\ncan potentially be many experts. \\n \\nQMoE Method \\nFor dense part of the model: \\n- \\nFetch one sample X, containing a few hundreds of tokens, from CPU to GPU. \\n- \\nPass it through the corresponding dense layers to obtain the result Y. \\n- \\nCalculate and store expert assignments for tokens in Y. \\n- \\nSend Y back to CPU and overwrite X in B (large buffer). \\nFor sparse part of the model (expert FFs): \\n- \\nFetch all individual tokens in B that have been assigned to expert E, denoted by Xe, from \\nCPU to GPU. \\n- \\nUse these tokens to produce compressed expert E’ (for example, with GPTQ). \\n- \\nRun Xe through E’ to get Ye’. \\n- \\nSend Ye’ back to CPU and overwrite Xe in B. \\nIn sum: \\n- \\nThe dense part consists of passing a set of samples X from CPU to GPU, performing a \\nforward pass through only the dense layers, calculating the expert assignments of each \\ninput in X to know which experts were used and then storing the outputs of the forward \\npass and expert assignments in Y, passing it back to the CPU (in QMoE, the dense parts of \\nthe model are left uncompressed). \\n- \\nThe sparse part consists of performing a loop through each expert. For each expert, from \\nbuffer B, all the tokens in X assigned to that expert are taken, forming Xe. A quantized \\nversion of the expert is also formed (through a technique like GPTQ), resulting in E’. Then, \\nXe is passed through E’, with the outputs forming Ye’. Finally, the results Ye’ are stored in \\nthe buffer B, replacing the input tokens Xe. \\nThe method described provides the main compression gains from QMoE but is not sufficient to \\nachieve the goal of 1 bit per parameter established. To achieve this, the authors adopt GPTQ \\noptimizations for the MoE case, GPU decoding optimizations, and more. \\n \\nResults \\n- \\nMoEs are shown to be highly robust to quantization as vanilla rounding with ternary \\nprecision does not lead to a model collapse. \\n- \\nUsing data-dependent quantization in MoE (method explained) allows 2-bit and ternary \\nquantization with minimal accuracy loss. \\n \\nMy takeaways: \\n- \\nDue to my lack of expertise in quantization methods like GPTQ, I did not find it relevant \\nto go into this topic more in-depth. \\n \\n', 'source_name': 'QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/QMoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FastInferenceMoE.pdf #41\n",
      "{'content': \"Fast Inference of Mixture-of-Experts Language\\nModels with Offloading\\nArtyom Eliseev\\nMoscow Institute of Physics and Technology\\nYandex School of Data Analysis\\nlavawolfiee@gmail.com\\nDenis Mazur\\nMoscow Institute of Physics and Technology\\nYandex\\nResearchcore\\ndenismazur8@gmail.com\\nAbstract\\nWith the widespread adoption of Large Language Models (LLMs), many deep\\nlearning practitioners are looking for strategies of running these models more\\nefficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) — a\\ntype of model architectures where only a fraction of model layers are active for\\nany given input. This property allows MoE-based language models to generate\\ntokens faster than their “dense” counterparts, but it also increases model size\\ndue to having multiple “experts”. Unfortunately, this makes state-of-the-art MoE\\nlanguage models difficult to run without high-end GPUs. In this work, we study\\nthe problem of running large MoE language models on consumer hardware with\\nlimited accelerator memory. We build upon parameter offloading algorithms and\\npropose a novel strategy that accelerates offloading by taking advantage of innate\\nproperties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with\\nmixed quantization on desktop hardware and free-tier Google Colab instances.\\n1\\nIntroduction\\nMany recent advances in natural language processing rely on large pre-trained language models, such\\nas GPT-3 and 4 Brown et al. (2020); OpenAI (2023), Palm & Gemini Chowdhery et al. (2022); Team\\net al. (2023) and many others. However, the rapid scientific progress in this area would be impossible\\nwithout open-access LLMs such as LLaMA 1 and 2 (Touvron et al., 2023), Falcon (TII UAE, 2023),\\nBLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), or NeoX/Pythia (Biderman et al., 2023). The\\nkey advantage of open-access LLMs is that researchers can deploy them locally and modify them in\\nways that would be impossible with proprietary APIs.\\nEven though LLM parameters are openly available, it is still difficult to use these models due to their\\nsheer size. State-of-the-art open-access language models require multiple high-end GPUs 1 even for\\nbasic inference workloads. To use these LLMs on more affordable hardware setups, one must either\\ncompress model parameters (Dettmers et al., 2022; Frantar et al., 2022) or offload parameters to a\\ncheaper storage, be it RAM or SSD (Pudipeddi et al., 2020; Sheng et al., 2023).\\nSeveral recent works modify transformer architecture by introducing sparse Mixture-of-Experts\\nblocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple “experts” (layers),\\nas well as a “gating function” that selects which experts are used on a given input. As a result, the\\nMoE block uses a small portion of all “experts” for any single forward pass, allowing for more\\ncompute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the\\nlargest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While\\nMixture-of-Experts models can be more efficient than their dense counterparts, many techniques for\\nefficient LLM inference were not designed with MoE in mind and perform suboptimally on modern\\nlarge language models that use mixture-of-experts layers.\\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires\\n140GB of combined accelerator memory.\\narXiv:2312.17238v1  [cs.LG]  28 Dec 2023\\nIn this work, we systematically develop techniques for running large MoE language models with\\nlimited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B-\\nInstruct — a MoE-based chat assistant — on a desktop-grade hardware where only a fraction of\\nexperts fit into the accelerator memory. To that end:\\n• we observe how MoE language model accesses its experts between tokens, and find several\\nregularities: i) some experts are reused between adjacent tokens and ii) the model hidden\\nstates of early layers already “know” which experts are to be used at subsequent layers.\\n• we design a MoE-specific offloading strategy that takes advantage of these regularities: i)\\nit uses LRU cache to significantly reduces GPU-RAM communication, leading to faster\\ngeneration and ii) it guesses which experts are needed ahead of time to better overlap expert\\nloading with computation.\\n• we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060\\nand RTX 3080 Mobile and develop a practical combination of mixed quantization and\\nthe proposed offloading algorithm to run this model interactively at 2-3 tokens per second\\ndepending on the hardware. The source code with our implementation is available online2\\n2\\nBackground & Related Work\\n2.1\\nMixture-of-Experts\\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan &\\nJacobs, 1994) of training ensembles of specialized models (“experts”) and a gating function to select\\nthe right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously\\ni) training the gating function to choose the best experts and ii) training the experts themselves on\\nsamples assigned to them by the gating function. Since then, many different MoE variants emerged,\\nincluding mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal,\\n2009) and various neural networks.\\nShazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as\\na language model. The full model consists of a recurrent neural network backbone and a MoE\\nmodule with up to 131072 experts. When processing a given token, a linear gating function select\\n4 most suitable experts based on the latest hidden state. The resulting model (including the gating\\nfunction and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to\\npromote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves\\nperplexity, but also learns interpretable expert specializations: some experts would “specialize” on\\nprepositions, while others learn to express a particular concept (e.g. speed).\\nSince then, several lines of work explore Mixture-of-Experts with Transformer-based language\\nmodels for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al.\\n(2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional\\n(dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the\\nfeedforward (MLP) blocks and use a linear token-level gating function. A common observation\\nacross most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021);\\nLepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity.\\nPre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these\\nmodels seem to have gained less traction than equivalent dense models, arguable because their\\nsheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI\\nreleased a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art\\nperformance Mixtral AI team (2023). This model has already inspired several follow-up works and\\npractical applications, but it still requires a high-end GPU accelerator.\\n2.2\\nPost-training Quantization of LLMs\\nA natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020;\\nGholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023),\\n2https://github.com/dvmazur/mixtral-offloading\\n3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\\n2\\nfactorization Hsu et al. (2022), or a combination thereof. These compression types are not specific to\\nLLMs and are based on much older methods outside the scope of our work4. However, recent works\\nfound that there are unique challenges to quantizing very large transformer-based language models\\ndue to emergent outliersDettmers et al. (2022); Lin et al. (2023); Dettmers et al. (2023).\\nGenerally speaking, the optimal compression rate for most LLMs is 4 bits per parameter Dettmers &\\nZettlemoyer (2022). While there are more extreme algorithms for 3- and even 2-bit compression Chee\\net al. (2023); Lin et al. (2023); Dettmers et al. (2023), they are typically inferior to choosing a smaller\\nmodel and quantizing it to around 4 bits. Most recently, there has been several concurrent works for\\nquantizing Mixture-of-Experts models (Kim et al., 2023; Frantar & Alistarh, 2023b).\\n2.3\\nInference with Parameter Offloading\\nA recent line of work explores inferencing and training large models with limited accelerator mem-\\nory by “offloading” their parameters to another, cheaper memory, such as system RAM or even\\nSSD (Pudipeddi et al., 2020; Ren et al., 2021). This technique works by loading model parameters\\njust-in-time when they are needed for computation. Since most deep learning models use layers in a\\nfixed order, offloading can pre-dispatch the next layer parameters in the background, ahead of time.\\nThis technique works particularly well when processing large batches of data, during train-\\ning Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al.\\n(2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded\\nfrom RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works\\nsignificantly slower than on-device inference. This is because interactive inference generates tokens\\nautoregressively, from left to right. This way, the inference system processes one or few tokens at a\\ntime, and therefore spends most of the time waiting for next layer’s parameters to be loaded.\\n2.4\\nHardware Setup\\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of\\ncheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam\\n(2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii)\\nhost-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE\\nmodels (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available\\nGPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even\\nwith quantization. Finally, even if we could fit the model parameters in memory, running generative\\ninference requires additional memory for layer activations and past attention keys & values.\\n3\\nMethod\\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts\\nLLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating\\ntokens interactively, i.e. generate multiple tokens per second at batch size 15.\\nThe generative inference workload consists of two phases: 1) encoding the input prompt and 2)\\ngenerating tokens conditioned on that prompt. The key difference between these two phases is that\\nprompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially\\n(token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture-\\nof-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when\\ngenerating tokens, one must load layer once per each token generated. In practice, this means that\\ninference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit\\nthese patterns to speed up inference time.\\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021)\\n5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\\n3\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nLayer 0 expert #\\nSelected experts for Mixtral-8x7B-Instruct layers 0 (top) and 15 (bottom)\\nAs\\nan\\nopen\\nsource\\nalternative\\nto\\nChat\\nG\\nPT\\n,\\nI\\ndo\\nnot\\nhave\\npersonal\\nopinions\\n.\\nHowever\\n,\\nI\\ncan\\nprovide\\nobjective\\ninformation\\nabout\\nChat\\nG\\nPT\\n'\\ns\\ncapabilities\\nand\\nlimitations\\nbased\\non\\nits\\narchitecture\\nand\\ntraining\\ndata\\n.\\nChat\\nG\\nPT\\nis\\na\\npowerful\\nlanguage\\nmodel\\nbased\\non\\nthe\\nG\\nPT\\n(\\nGener\\native\\nPre\\n-\\ntrained\\nTrans\\nformer\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nLayer 15 expert #\\nFigure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells\\nindicate that a certain expert was active when encoding a certain token; deeper blue indicates higher\\ngating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\\n3.1\\nExpert Locality and LRU caching\\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to\\nassign individual experts to distinct sub-tasks. However, this does not mean that the model uses the\\nsame expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4\\ntokens, while others are often used with “gaps”, as shown in Figure 1.\\nTo take advantage of this pattern, we can keep active experts in GPU memory as a “cache” for\\nfuture tokens. If the same experts are activated again in future, they will be available instantaneously.\\nNaturally, the number of experts that can be stored this way if very limited by the available GPU\\nmemory. For simplicity, we choose to always keep k least recently used experts as a type of LRU\\ncache. If k is greater than the number of active experts, the cache will save experts from multiple\\nprevious tokens. For simplicity, we keep the same number of cached experts for each MoE layer.\\nWe illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very\\nsimple strategy that does not consider factors like expert activation frequencies, varying cache size\\nbetween MoE layers, or any sequential patterns in expert activation. However, we found that even\\nthis simple strategy can significantly speed up inference for modern Mixture-of-Experts models such\\nas Mixtral-8x7B (see Section 4 for detailed evaluation).\\n3.2\\nSpeculative Expert Loading\\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still\\nspent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense\\nmodels, MoE offloading cannot effectively overlap expert loading with computation. To understand\\nthis problem, let us zoom into the process of generating a single token, layer-by-layer. The full\\ncompute workload starts by embedding the previous token via look-up, then alternates between\\nrunning self-attention and MLP for each transformer block in the model. Finally, the outputs from\\nthe last transformer block are used to predict next token logits with a linear projection.\\nFor regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads\\nthe next transformer layer ahead of time, while the previous layer is still running. Unfortunately,\\nthis schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose\\nwhich experts to load just-in-time for computation. This is because the system cannot pre-fetch\\nthe next layer until it learns which experts should be loaded. Modern open-access MoE language\\nmodels choose active experts using the final outputs of the previous layer, which means they cannot\\nbe pre-fetched them in parallel with previous layer.\\nWhile it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could\\nstill try to guess the likely next experts and load them speculatively, while processing the previous\\nlayer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual\\nnext layer’s experts later. In other words, this type of speculative loading does not change the final\\nmodel predictions, but may reduce latency if the guess is accurate enough.\\n6More specifically, not possible without changing the model architecture, which would require re-training\\n4\\nWhile analyzing modern MoE models, we found that it is possible to get an accurate guess of next\\nlayer’s experts by applying next layer’s gating function to previous layer’s hidden states — or,\\nmore specifically, to the same hidden states that are used by previous MoE layer’s gating function.\\nThis heuristic relies on the fact that transformer layers are residual, i.e. each layer adds to the previous\\nhidden states instead of re-computing them from scratch. This architecture introduces an inductive\\nbias such that any layer’s hidden states into a decent estimate of next layer’s hidden states.\\n3.3\\nSystem Design & Implementation Details\\nIn this section, we describe practical design considerations and implementation details that we used\\nfor inferencing MoE language models on consumer and low-end cloud hardware. Our system design\\ncombines the caching & prefetching techniques and a mixed MoE quantization scheme .\\nMoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization\\nalgorithms optimized for LLMs. Model compression has a natural synergy with offloading because\\ncompressed models take less time to load onto GPU. In our experitments, we also observed that\\nMoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while\\nkeeping all non-expert layers at 4-bit.\\nWe use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) — a data-free quantization\\nalgorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience,\\nbecause it was already well tested for Mixtral models. Since our analysis does not rely on any\\nspecific choice of quantization, we believe that if we chose another quantization algorithm (e.g.\\nGPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the\\nsub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048\\nmodel. However, we found that sub-1-bit compression caused too significant a loss in perplexity for\\nMixtral-8x7B models.\\nExpert Offloading. As described earlier, we use LRU cache with an equal number k of cached\\nexperts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We\\ntrigger speculative expert loading immediately after the system finished loading all experts for the\\ncurrent layer. The speculative expert loading fetches 1 −2 most likely experts. The newly loaded\\nexperts do not replace the currently cached experts. If a speculatively loaded expert was later used\\nduring next layer inference, it will replace the least recently used expert from the next layer’s cache.\\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire\\nmodel7. In these cases, the experts must be split between host and device memory. To support this,\\nour implementation of expert LRU cache splits experts between host and GPU devices. When loading\\nand expert to the GPU cache, the system also offloads the least recently used on-device expert back\\nto RAM so as to preserve memory parity.\\nTo speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer\\nthat can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this\\nmemory buffer for faster communication. Our implementation additionally allocates b=4 on-device\\nbuffers used to copy and prefetch experts asynchronously, without modifying existing experts. These\\nbuffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires\\nnum_layers × num_experts expert memory buffers split between host and device memory and b=4\\ntemporary buffers, the size of each buffer being equal to a single expert.\\n4\\nExperiments\\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference\\nlatency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct\\nmodels since they represent the current state of the art among open-access MoE models. We organize\\nthis section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in\\nisolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses\\nfrom Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.\\n7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate\\n8This corresponds to tensor.pin_memory() command in PyTorch.\\n5\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nLayer #\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nCache hit rate\\ncache_size = 1\\ncache_size = 2\\ncache_size = 3\\ncache_size = 4\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nLayer #\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nPrediction recall\\nprefetch 1 experts\\nprefetch 2 experts\\nprefetch 3 experts\\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when\\npre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line\\nstands for 2 layers ahead; dotted line is 10 layers ahead.\\n4.1\\nExpert LRU Cache and Speculative Loading\\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching\\nand and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we\\nmeasure “expert recall” — the fraction of times when an expert needed for inference was already\\navailable on GPU.\\nFor this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (Köpf et al.,\\n2023). We test LRU caching by running the model on recorded conversations and measuring the\\nrecall (aka “hit ratio” from caching perspective) for different cache sizes k. Next, we test speculative\\nloading in isolation by “guessing” which experts should be loaded (by applying the next layer’s\\ngating function on current layer activations), then measuring how often the actual next experts get\\nloaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were\\npre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\\n4.2\\nMixed MoE Quantization\\nNext, we test how different Quantization schemes affect MoE performance and size. We also use\\nMixtral-8x7B, but this time, we use non-instruction-tuned variant since it fits better with the available\\nbenchmarks. We measure WikiText2 perpliexity Merity et al. (2016), C4 perplexity Raffel et al.\\n(2020), as well as 5-shot MMLU accuracy Hendrycks et al. (2021). Our objective for this section is\\nto find the best trade off between size and performance for offloading with the target setups. Note\\nthat out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%).\\nThe rest of the model parameters are allocated to embeddings, self-attention layers, MoE gates and\\nminor layers such as LayerNorm.\\nAttn\\nquant\\nExperts\\nquant\\nModel\\nsize, GB Wiki2\\nC4\\nMMLU\\nFP16\\nFP16\\n86.99\\n3.59\\n6.52 70.51%\\n4-bit\\n25.82\\n3.67\\n6.58\\n70.3%\\n3-bit\\n23.21\\n3.96\\n6.78 69.32%\\n2-bit\\n19.33\\n4.52\\n7.31 66.66%\\n4-bit\\nFP16\\n85.16\\n3.68\\n6.59\\n—\\n4-bit\\n23.99\\n3.76\\n6.66 69.11%\\n3-bit\\n21.37\\n4.05\\n6.87 68.47%\\n2-bit\\n17.54\\n4.61\\n7.42 65.58%\\nAttn\\nquant\\nExperts\\nquant\\nModel\\nsize, GB Wiki2\\nC4\\nMMLU\\n3-bit\\nFP16\\n85.08\\n3.99\\n6.90\\n—\\n4-bit\\n23.92\\n4.06\\n6.97 66.54%\\n3-bit\\n21.31\\n4.34\\n7.21 65.79%\\n2-bit\\n17.46\\n4.90\\n7.82 61.83%\\n2-bit\\nFP16\\n84.96\\n4.98\\n7.92\\n—\\n4-bit\\n23.79\\n5.08\\n8.06\\n59.0%\\n3-bit\\n21.18\\n5.36\\n8.34 57.67%\\n2-bit\\n17.30\\n5.97\\n9.11 55.26%\\nTable 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared\\nattention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit\\nquantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section\\n4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\\n6\\nAlgorithm\\n2-bit Experts\\n3-bit Experts\\nA100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud)\\nFull algorithm\\n3.061\\n2.655\\n2.278\\n2.092\\n2.845\\n2.475\\n2.038\\n1.603\\nW/o expert pre-loading\\n2.918\\n2.227\\n2.051\\n1.567\\n2.683\\n2.024\\n1.857\\n1.365\\nW/o LRU cache & pre-loading\\n2.265\\n1.758\\n1.547\\n1.168\\n2.055\\n1.595\\n1.346\\n1.061\\nNaive offloading (accelerate) 1.392\\n1.059\\n0.919\\n0.661\\n1.246\\n0.914\\n1.791\\n0.580\\nTable 2: Inference speed for Mixtral-8x7B in low-tier , measured in tokens per second.\\nAs discussed earlier, we use HQQ Badri & Shaji (2023) data-free quantization algorithm and consider\\nthe following quantization schemes:\\n1. FP16 (no quantization)\\n2. HQQ 4-bit with group size 64, scale group size 256\\n3. HQQ 3-bit with group size 64, scale group size 128\\n4. HQQ 2-bit with group size 16, scale group size 128\\nNote that the actual model size with n-bit quantization is larger than n bits per parameter. This is\\nbecause the quantized data format also stores quantization scale and zero point for each group of\\nweights. Notably, the above 2-bit quantization scheme uses, on average, 2.6 bits per parameter\\ndue to a large number of quantization schemes. We also keep embeddings, logits, MoE gates and\\nnormalization layers in 16-bit format.\\nTable 1 summarizes our results: overall, it seems advantageous to quantize experts to 3 or 2 bits while\\nkeeping attention layers to a higher bitwidth (16 or 4 bits). Based on these evaluations, we chose two\\nquantization schemes (highlighted in green) that offer favourable performance-size trade-offs within\\nthe target hardware constraints.\\n4.3\\nPractical offloading performance\\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech-\\nniquesproposed throughout this report. Based on the perplexity evaluations from the previous section,\\nwe chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for\\nexperts. We evaluate this system by generating tokens via sampling on OpenAssistant (Köpf et al.,\\n2023) conversations and measuring the average number of tokens generated per second with batch\\nsize 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without\\ntemperature or nucleus sampling.\\nWe consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM,\\nPCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid-\\nrange gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with\\nA100-80GB-SXM. Note that the A100 server could run the model without offloading. We use\\noffloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit\\nmodels, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have\\nenough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other\\nGPUs.\\nAs shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm.\\nUsing pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size.\\nCursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We\\nattribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by\\nthe PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the\\nentire MoE layer.\\n5\\nConclusion and Future Work\\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on\\nconsumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\\n7\\nand explore how mixed quantization affects perplexity and performance on language understanding\\ntasks. We evaluate the proposed strategies and show that they produce a significant increase in\\ngeneration speed compared to na¨\\nve approaches on consumer-grade hardware, including free-tier\\nGoogle Colab.\\nOur method provides a practical solution for inferencing large MoE language models on resource-\\nconstricted hardware, enabling broader access to these powerful models for research and development.\\nAs future work, we plan to explore further offloading strategies, based on speculative expert predic-\\ntion.\\nAcknowledgements\\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\\nReferences\\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith,\\nS., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of\\ntransformer models at unprecedented scale. In Proceedings of the International Conference on\\nHigh Performance Computing, Networking, Storage and Analysis, SC ’22. IEEE Press, 2022. ISBN\\n9784665454445.\\nBadri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, November\\n2023. URL https://mobiusml.github.io/hqq_blog/.\\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A.,\\nPurohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373, 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Conference on Neural\\nInformation Processing Systems (NeurIPS), 2020.\\nChee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit quantization of large language models with\\nguarantees, 2023.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\\nH. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311, 2022.\\nCollobert, R., Bengio, S., and Bengio, Y. A parallel mixture of svms for very large scale problems.\\nIn Advances in Neural Information Processing Systems, pp. 633–640, 2002.\\nDettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720, 2022.\\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for\\ntransformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference\\non Neural Information Processing Systems 2022, NeurIPS 2022, 2022.\\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov,\\nA., Hoefler, T., and Alistarh, D. Spqr: A sparse-quantized representation for near-lossless llm\\nweight compression. arXiv preprint arXiv:2306.03078, 2023.\\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat,\\nO., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M.,\\nRobinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z.,\\nand Cui, C. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\n8\\nFrantar, E. and Alistarh, D. SparseGPT: Massive language models can be accurately pruned in\\none-shot. arXiv preprint arXiv:2301.00774, 2023a.\\nFrantar, E. and Alistarh, D. Qmoe: Practical sub-1-bit compression of trillion-parameter models,\\n2023b.\\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for\\ngenerative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization\\nmethods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\\nGoogle. Google colaboratory, 2023. URL https://colab.research.google.com/.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring\\nmassive multitask language understanding. Proceedings of the International Conference on\\nLearning Representations (ICLR), 2021.\\nHsu, Y.-C., Hua, T., Chang, S., Lou, Q., Shen, Y., and Jin, H. Language model compression with\\nweighted low-rank factorization. arXiv preprint arXiv:2207.00112, 2022.\\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts.\\nNeural Computation, 3(1):79–87, March 1991. ISSN 0899-7667. doi: 10.1162/neco.1991.3.1.79.\\nURL https://doi.org/10.1162/neco.1991.3.1.79.\\nJordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural\\ncomputation, 6(2):181–214, 1994.\\nKim, Y. J., Fahim, R., and Awadalla, H. H. Mixture of quantized experts (moqe): Complementary\\neffect of low-bit quantization and robustness, 2023.\\nKöpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A.,\\nDuc, N. M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A.,\\nSchuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations – democratizing large\\nlanguage model alignment, 2023.\\nLample, G., Sablayrolles, A., Ranzato, M. A., Denoyer, L., and Jegou, H. Large memory layers\\nwith product keys. In Wallach, H., Larochelle, H., Beygelzimer, A., dÁlché-Buc, F., Fox, E., and\\nGarnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8546–8557. Curran\\nAssociates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-\\nwith-product-keys.pdf.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen,\\nZ. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv\\npreprint arXiv:2006.16668, 2020.\\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying\\ntraining of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.\\nLiang, T., Glossner, J., Wang, L., and Shi, S. Pruning and quantization for deep neural network accel-\\neration: A survey. CoRR, abs/2101.09671, 2021. URL https://arxiv.org/abs/2101.09671.\\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization\\nfor llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models,\\n2023.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint\\narXiv:1609.07843, 2016.\\nMixtral AI team. Mixtral of experts a high quality sparse mixture of experts, 2023. URL https:\\n//mistral.ai/news/mixtral-of-experts/.\\n9\\nNagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive\\nrounding for post-training quantization. In International Conference on Machine Learning (ICML),\\n2020.\\nOpenAI. Gpt-4 technical report. arXiv, 2023.\\nPudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S. Training large neural networks\\nwith constant memory using a new execution algorithm. CoRR, abs/2002.05645, 2020. URL\\nhttps://arxiv.org/abs/2002.05645.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.\\nExploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine\\nLearning Research, 21(140):1–67, 2020.\\nRen, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y.\\nZero-offload: Democratizing billion-scale model training. CoRR, abs/2101.06840, 2021. URL\\nhttps://arxiv.org/abs/2101.06840.\\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´\\nc, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv\\npreprint arXiv:2211.05100, 2022.\\nShahbaba, B. and Neal, R. Nonlinear models using dirichlet process mixtures. Journal of Machine\\nLearning Research, 10(Aug):1829–1850, 2009.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outra-\\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\\narXiv:1701.06538, 2017.\\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Ré, C., Stoica, I., and\\nZhang, C. Flexgen: High-throughput generative inference of large language models with a single\\ngpu. In International Conference on Machine Learning, pp. 31094–31116. PMLR, 2023.\\nSteam. Steam hardware & software survey: October 2023, accessed on 2023.11.02, 2023. URL\\nhttps://store.steampowered.com/hwsurvey/videocard/.\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai,\\nA. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser,\\nJ., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M.,\\nBarham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E.,\\nMeyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M.,\\nBarr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati,\\nL., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L.,\\nProleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Rrustemi, A., Clay, N., Crone,\\nP., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L.,\\nMaggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., Barth-Maron, G.,\\nWong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R., Li, Y., Cogan, S., Chen, J.,\\nJia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X.,\\nSenter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C.,\\nBlanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita,\\nG., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki,\\nR., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer,\\nJ., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S.,\\nKannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman,\\nP., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys,\\nP., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S.\\nM. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H.,\\nHodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz,\\nA., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozi´\\nnska, D., Nikolaev,\\nV., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C.,\\nNewlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani,\\nS., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn,\\n10\\nD., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S.,\\nLandon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Giménez, M., Yeung,\\nL., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K.,\\nQin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A.,\\nAgrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek,\\nP., McIlroy, R., Luˇ\\nci´\\nc, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y.,\\nBansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal,\\nS., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P.,\\nWang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya,\\nY., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang,\\nZ., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S.,\\nBortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia,\\nP., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M.,\\nKaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A.,\\nTomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta,\\nM., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir,\\nC., Cohen, V., Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang,\\nM., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjösund, L. L., Cevey, S., Gleicher, Z.,\\nAvrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B.,\\nBaumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A.,\\nFrye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala,\\nA., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z.,\\nCaelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., Ça˘\\nglar\\nÜnlü, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas,\\nK., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K.,\\nDafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V.,\\nCobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao,\\nY., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan,\\nS., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A.,\\nSheahan, H., Misra, V., Li, C., Raki´\\ncevi´\\nc, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S.,\\nSezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., Chi, E., Mahdieh, M., Tenney,\\nI., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A.,\\nBunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu,\\nI., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stanczyk, P., Zhang, Y., Steiner, D., Naskar,\\nS., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F.,\\nMiao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon,\\nD., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J.,\\nLevskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider,\\nJ., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I.,\\nChen, N., Shafran, I., Fink, M., Castaño, A., Giannoumis, I., Kim, W., Rybi´\\nnski, M., Sreevatsa,\\nA., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J.,\\nWright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu,\\nC., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo,\\nK., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J.,\\nLi, S., Dousse, O., Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz, N., Vijayakumar, A.,\\nThiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S.,\\nNguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K.,\\nFeng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q., Abellan, E. A., Du, D., McKinnon,\\nD., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R.,\\nHawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R.,\\nYadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C.,\\nLin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Liu, J., Omernick,\\nM., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron,\\nG., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J.,\\nPolozov, A., Kushman, N., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri,\\nM., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo,\\nR., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor,\\nJ., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D.,\\nGreig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korchemniy, A., Tsai, T., Jasarevic,\\n11\\nM., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J.,\\nGladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C.,\\nWoodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert,\\nA., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J.,\\nFatehi, S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., Héliou, A., Niu, N., Gu, S.,\\nPang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., Santamaria-Fernandez, R., Goenka,\\nS., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee,\\nH., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O.,\\nYue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., Põder, S., Zheng,\\nS., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Kotikalapudi, R., Liu,\\nZ., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S.,\\nFan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu,\\nT., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy,\\nD., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy,\\nA., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Rivière, M., Walton, A., Crepy, C., Parrish, A.,\\nLiu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A.,\\nScellato, S., Latorre-Chimoto, E., Klimczak-Pluci´\\nnska, H., Bridson, D., de Cesare, D., Hudson,\\nT., Mendolicchio, P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A.,\\nOdoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez,\\nA., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb, L., Dua, S., Li, D., Lahoti, P.,\\nBhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R.,\\nWang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S.,\\nNayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev,\\nE., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier,\\nF., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang,\\nH., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H.,\\nChen, W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller, C., Li, X., Wang, W., Wiesinger, J.,\\nKoukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H.,\\nUrbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N.,\\nRahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng,\\nG., Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J.,\\nGreen, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly\\ncapable multimodal models, 2023.\\nTII UAE. The Falcon family of large language models. https://huggingface.co/tiiuae/\\nfalcon-40b, May 2023.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,\\nN., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin,\\nX. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,\\n2022.\\n12\\n\", 'source_name': 'Fast-Inference of Mixture-of-Experts Language Models with Offloading', 'source_url': 'https://arxiv.org/abs/2312.17238'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ExtremelyPE_MoE_for_InstructionTuning.pdf #42\n",
      "{'content': 'Pushing Mixture of Experts to the Limit:\\nExtremely Parameter Efficient MoE for\\nInstruction Tuning\\nTed Zadouri\\nCohere for AI\\nted@cohere.com\\nAhmet Üstün\\nCohere for AI\\nahmet@cohere.com\\nArash Ahmadian†\\nCohere for AI\\narash@cohere.com\\nBeyza Ermiş\\nCohere For AI\\nbeyza@cohere.com\\nAcyr Locatelli\\nCohere\\nacyr@cohere.com\\nSara Hooker\\nCohere for AI\\nsarahooker@cohere.com\\nAbstract\\nThe Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized\\nsub-models optimizes overall performance with a constant computational cost. However, conventional\\nMoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we\\npush MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE\\narchitecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient\\nfine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight\\nexperts – less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen\\ntasks as it does not depend on any prior task knowledge. Our research underscores the versatility\\nof the mixture of experts architecture, showcasing its ability to deliver robust performance even\\nwhen subjected to rigorous parameter constraints. Our code used in all the experiments is publicly\\navailable here: https://github.com/for-ai/parameter-efficient-moe.\\n1\\nIntroduction\\nA conventional training paradigm is to apply the weights of a model to each input. Arguably, this is\\nnot efficient since a given input may not need all of a model’s capacity. In contrast, MoEs build\\non the premise that sub-modular components – so called experts – can specialize to different types\\nof inputs. This emphasis on conditional computation has important efficiency side-effects such as\\nconstant inference cost. This has made MoEs an area of significant research and widespread adoption\\nin the era of large-scale Transformers where scaling has increased deployment and latency costs\\n(Shazeer et al., 2018; Riquelme et al., 2021; Du et al., 2022; Fedus et al., 2022).\\nWhile the majority of work to-date has focused on MoEs as a pretraining strategy,the inherent\\nmotivation of MoEs is not confined solely to pretraining. In fact, the merits of MoEs are arguably\\nwell suited to an instruction fine-tuning setting where the data is often deliberately structured to\\n†Also affiliated with the University of Toronto & the Vector Institute for Artificial Intelligence.\\nReleased as a preprint on September 12, 2023\\n1\\narXiv:2309.05444v1  [cs.CL]  11 Sep 2023\\n0.01%\\n0.6%\\n3%\\n5%\\n% of Parameters Updated\\n54.0\\n56.0\\n58.0\\n60.0\\n62.0\\nAverage Median Accuracy\\n(IA)3\\nMoV-10\\nMoV-30\\nLoRA\\nMoLoRA-10\\nMoLoRA-15\\nFull fine-tuning\\nPerformance vs Parameter Budget\\n770M\\n3B\\n11B\\nBase Model Parameters\\n50.0\\n52.0\\n54.0\\n56.0\\n58.0\\n60.0\\n62.0\\n64.0\\n66.0\\nAverage Median Accuracy\\nPerformance vs Base Model Size\\nFull Fine-Tuning\\nMoV-60\\nIA3\\nFigure 1: Left: Our mixture of PEFT experts outperforms SOTA single PEFT methods using\\na comparable amount of parameters demonstrated for T5-XL (3B). Right: Mixture of PEFT\\napproach scales up to 11B; with tiny parameter updates, it approximates or matches full fine-tuning\\nperformance.\\nrepresent a diverse set of tasks, often referred to as multi-task finetuning (Chung et al., 2022; Wei\\net al., 2022; Sanh et al., 2022; Longpre et al., 2023; Muennighoff et al., 2023).\\nIn this work, we pose the question can we leverage MoEs for instruction fine-tuning? One of the main\\ndrawbacks of MoEs paradigm is that it introduces an extreme amount of total parameters (Fedus\\net al., 2022). Despite the conditional computation, fully fine-tuning MoE architecture is extremely\\ncomputationally demanding given the need to update all the parameters. For most practitioners,\\ngiven the scale of modern LLMs (Brown et al., 2020; Touvron et al., 2023; Kaplan et al., 2020; Anil\\net al., 2023) this is an infeasible computational cost.\\nThus, we focus on a more realistic setting for everyday practitioners – can we successfully apply\\nMoEs to parameter-efficient fine-tuning (PEFT) methods such as (IA)3 (Liu et al., 2022) or LORA\\n(Hu et al., 2021) which only fine-tune a far smaller number of parameters. This is a significant\\nchallenge not only since our aim is to update only a small percentage of all parameters but as we\\nalso navigate the optimization challenges inherent to MoEs already noted by prior work (Chen et al.,\\n2022) in a more constrained environment.\\nIn this work, we propose a new framework that leverages the benefits of MoE in a severely constrained\\ncomputational environment. We introduce Mixture of Vectors (MoV) and Mixture of LORA\\n(MoLORA), a parameter-efficient adaptation of the Mixture of Experts approach. Unlike the\\nstandard MoE, our framework can be utilized in a parameter-limited setting due to its lightweight\\nnature. Remarkably, our method achieves performance parity with full fine-tuning on unseen tasks\\nby updating less than 1% of the parameters. It also easily outperforms base parameter-efficient\\ntechniques like (IA)3 or LORA.\\nWe achieve consistent results across T5 models (Raffel et al., 2020) ranging from 770M to 11B across\\n12 different tasks from 55 datasets P3 (Sanh et al., 2022). In summary, our contributions are as\\nfollows:\\n(i) We present extremely parameter-efficient MoEs. This architecture leverages MoEs in a more\\n2\\nrealistic setting using modular and lightweight experts. Our MoEs can be used to fine-tune a\\ndense model by updating less than 1% of its parameters.\\n(ii) Instruction fine-tuning with our proposed methods consistently outperforms traditional pa-\\nrameter efficient methods on unseen tasks, while maintaining high parameter efficiency across\\ndifferent scales. The mixture of (IA)3 vectors (MoV) achieves up to 14.57% and 8.39% im-\\nprovements over the standard (IA)3 at 3B and 11B model sizes respectively. This superiority\\nholds across different model sizes, types of experts and trainable parameter budgets.\\n(iii) We show that our recipe can match the performance of full fine-tuning at large scales while\\nupdating a tiny fraction of the model parameters. Our results across 8 unseen tasks show\\nthat our MoV which updates just 0.32% and 0.86% of the parameters in the 3B and 11B\\nmodels achieves higly competitive performance to full fine-tuning with a significantly reduced\\ncomputational cost.\\n(iv) Finally, we present an extensive set of ablation studies that systematically evaluate the\\nefficacy of various MoE architectures and PEFT strategies at various model sizes, different\\nadapter types, the number of experts, routing mechanisms, and the importance of optimizing\\nhyper-parameters, especially given the sensitivity of MoE.\\n2\\nMethodology\\nThe instruction tuning setup is formulated as such where there are set of tasks which are divided\\ninto training and held-out evaluation tasks, T = Ttrain ∪Teval. The base pretrained model is first\\nfine-tuned on Ttrain and then evaluated in a zero-shot manner on each unseen task from Teval. The\\nstandard approach is fine-tuning all model parameters that cause high compute and memory costs.\\nOur method offers an efficient alternative using parameter-efficient mixture of experts. In this section,\\nwe describe our framework in detail.\\n2.1\\nParameter-efficient Fine-tuning with (IA)3 and LORA Adapters\\nIn this work, we push the mixture of expert (MoE) architecture to an extreme degree of parameter\\nefficiency using parameter-efficient fine-tuning (PEFT) methods.\\nPEFT methods address the\\nchallenges associated with updating a large number of parameters – especially emerging at scale\\nwhen fully fine-tuning an LLM – by restricting weight updates to a limited number of parameters.\\nTo show how our method scales with different PEFT techniques, we experiment with both (IA)3\\nand LORA. These methods add a small number of parameters to the existing pre-trained model.\\nWe briefly introduce each PEFT method below:\\n(IA)3 introduces three new vectors, lk ∈Rdk, lv ∈Rdv, lff ∈Rdff which re-scale key and value\\nactivations in self-attention, and intermediate activations in position-wise feed-forward layers:\\nsoftmax\\n\\x12Q(lk ⊙KT )\\n√dk\\n\\x13\\n(lv ⊙V );\\n(lff ⊙γ (W1x))W2\\n((IA)3)\\nwhere Q, K, V are query, key, and value projection matrices for self-attention, and W1, W2 are\\nfrozen weights of the feed-forward layers in the pretrained model. Since (IA)3 only updates lk, lv, lff\\n3\\nQ\\nK\\nV\\nW1\\nRouter\\nvec1\\nvec2\\nvec3\\nvec4\\nMixture of Vectors\\n(MOV)\\nxinput\\nW2\\nxhidden\\n1 class\\nMOV_Layer(nn.module):\\n2\\nn_experts: int # number of experts\\n3\\n4\\ndef call(self , inputs):\\n5\\n# inputs\\nshape: [batch , seq , h_dim]\\n6\\nbatch , seq , h_dim = inputs.shape\\n7\\n8\\n# MOV\\nscaling\\nvectors: [n_experts , h_dim]\\n9\\nmov_vectors = self.param(’mov_scalers ’,\\n10\\nnn.init.ones (), (self.n_experts , h_dim))\\n11\\n12\\n# router\\nprobs: [batch , seq , n_experts]\\n13\\nrouter_probs = self.router(inputs ,\\n14\\nself.n_experts , dtype=’float32 ’)\\n15\\n16\\n# combined\\nvector: [batch , seq , h_dim]\\n17\\nmov_combined = jnp.einsum(’...e,ed - >...d’,\\n18\\nrouter_probs ,\\n19\\nmov_vectors)\\n20\\n21\\nreturn\\ninputs * mov_combined\\nFigure 2: Left: Overview of the MoV architecture highlighting soft-merging where only the vectors\\nand router are updated for each multi-head attention block, as denoted by color. Right: JAX-like\\npseudo-code illustrating the core implementation of a MoV layer.\\nrescaling vectors for each Transformer layer∗, it is extremely parameter-efficient. For the 3 billion\\nparameter T5 model (Raffel et al., 2020), it only updates 0.018% of the total parameters.\\nNote that, unlike adapters (Houlsby et al., 2019) or prompt-tuning (Lester et al., 2021), the number\\nof new parameters inserted by (IA)3 is determined by the architecture as the scaling vectors need to\\nbe the same size with the corresponding activation dimensions.\\nLow-Rank adaptation (LORA; Hu et al., 2021) optimizes low-rank decomposition of dense layers\\nin LLMs. For a pre-trained weight matrix W0 ∈Rdm×dp and input activation x ∈Rdm, LORA\\ndecomposes W0 into two low-rank matrices:\\nh = W0 + ∆Wx = W0 + BAx\\n(LORA)\\nwhere B ∈Rdp×r A ∈Rr×dm, and the rank r = min(dm, dp). During fine-tuning, all pretrained\\nweights are frozen, and only A and B matrices are updated.\\nLORA adaptation can be used for all the linear layers in each Transformer block including query Q,\\nkey K, value V , and output O of the self-attention and the feed-forward layers W1 and W2. Unlike\\n(IA)3, LORA adaptation offers more flexibility in terms of the parameters used. We can adjust\\nthe capacity by incrementing the rank r of the matrix decomposition until it reaches its maximum,\\ndetermined by r = min(dm, dp). To illustrate its parameter efficiency, for a T5 3B model, LORA\\nwith a rank of 4, updates 0.3% of the model parameters.\\n∗For an encoder-decoder model with L number of layers in both sides, (IA)3 only introduces L(dk + dv + dff) new\\nparameters for encoder and L(2dk + 2dv + dff) for decoder, due to the additional encoder-decoder attention block.\\n4\\n2.2\\nExtremely Parameter Efficient Mixture of Experts\\nWe propose an extremely parameter-efficient Mixture of Experts (MoE) framework that leverages\\nlightweight “adapters” as experts on top of a pretrained dense model. Concretely, the MoE is a\\nfamily of neural network architecture that enables conditional computation through multiple experts\\nthat are activated based on a gating mechanism (router). An MoE layer consists of a router network\\nR and a set of n experts E1, ..., En where each expert Ei is a parameterized function. Following\\nFedus et al. (2022), our router network commonly consists of a dense layer with trainable weights\\nWg ∈Rdm×n followed by a softmax function which takes an intermediate token representation x as\\ninput and combines the output of each expert based on the gating scores s1, ..., sn:\\nsi = R(x)i = softmax(W T\\ng x)\\n(Router)\\ny =\\nn\\nX\\ni=1\\nsi · Ei(x)\\n(MoE)\\nFor Transformer models (Vaswani et al., 2023), dense feed-forward layers are replaced by MoE layers\\nwhere each expert Ei corresponds to an independent dense feed-forward network. This multiplies the\\ntotal number of model parameters as each expert size and number of experts increase. However, in\\nour parameter-efficient MoE architecture, we replace each expert with a lightweight PEFT adapter\\nsuch as (IA)3 vectors or LORA adapters. During fine-tuning, pretrained weights of dense layers\\nremain fixed, while experts and router layers are trained from scratch. Unlike the standard MoE, our\\nlightweight experts learn to adapt the pretrained Transformer layers in the fine-tuning time. In this\\nway, our MoE framework requires a limited number of parameter updates and does not introduce a\\nhuge model size in total.\\nIn addition to parameter efficiency, our selection of PEFT adapters enables routing computation\\nwith soft merging. Concretely, since both (IA)3 vectors and LORA adapters are linear functions,\\nwe compute a weighted average of experts first and then apply a PEFT transformation using the\\ncombined expert Emix similar to Muqeeth et al. (2023):\\nEmix =\\nn\\nX\\ni=1\\nsi · Ei;\\ny = Emix(x)\\n(Soft Merging)\\nWe call the variants of our method as Mixture of Vectors (MoV) and Mixture of LORA (MoLORA)\\nthat leverage (IA)3 vectors or LORA adapters as experts respectively, both demonstrating consistent\\ngains over the corresponding PEFT method. Figure 2 shows the architecture of a MoV layer together\\nwith the corresponding pseudo-code. Only updating a small fraction of parameters through MoV\\nand MoLORA has multiple practical benefits not only to training but to inference time, with the\\nlatter being unique to MoE architectures. We provide a brief overview of these gains below:\\nEfficiency in training Our extremely parameter-efficient MoE formulation leads to a significant\\nreduction in memory. The freezing of most parameters during training reduces the computational\\n5\\noverhead of calculating gradients for model parameters but also reduces the memory requirements\\nof storing the optimizer states for the model. The latter can be quite significant depending on the\\nchoice of the optimizer, for instance, variants of Adam (Kingma & Ba, 2017) including AdamW\\n(Loshchilov & Hutter, 2019), require twice the memory required for each parameter, to store the\\noptimizer states (estimates for first and second moments) whereas Adafactor (Shazeer & Stern, 2018)\\nreduces this overhead roughly by half through factored estimation of the second-order parameter\\nmoments.\\nEfficiency at inference The inherent structural modularity of our MoV and MoLORA methods\\nallows for significant memory gains at inference time. For traditional MoE models, many copies\\nof the full-fledged feed-forward blocks (or even complete replicas of the model based on specific\\narchitecture) need to be stored in memory at inference time which is an expensive undertaking. With\\nour methods, regardless of the exact type, only a single copy of the model backbone needs to be\\nstored in memory in addition to lightweight parameter-efficient experts. This leads to a significant\\nreduction in the memory requirements at inference time.\\n3\\nExperiments\\nDataset\\nWe conduct instruction-tuning experiments using a comprehensive set of prompt in-\\nstructions from the Public Pool of Prompts (P3) dataset Sanh et al. (2022). We follow the same\\nprocedure as Raffel et al. (2020) where each task is converted into the format provided templates in\\n(Sanh et al., 2022). P3 is a collection of 62 datasets covering a wide variety of tasks.\\nExperimental Setup\\nFor the base pretrained models, we use T5 v1.1+LM adaptation (Lester\\net al., 2021) that includes T5 models of different sizes ranging from 770M to 11B parameters. For\\nall experiments, we fine-tune using Adafactor optimizer (Shazeer & Stern, 2018) with a learning rate\\nof 3e−4. We set the sequence length to 1024 for the input and 256 for the target following to Sanh\\net al. (2022). For all parameter-efficient MoE variants, we fine-tune T5 models using a batch size of\\n32 over 500K steps.\\nBaselines\\nWe compare our mixture of parameter-efficient experts against both T0 baseline as the\\nfully fine-tuned model, and the standard parameter-efficient fine-tuning methods (IA)3 and LORA.\\nFor T0 baselines, based on our experiments with different hyperparameters, we find that a larger\\nbatch size and learning rate result in better performance, thus, we replicated T0 by fine-tuning for\\n10k steps with a batch size of 256, and a learning rate of 1e−3, following Phang et al. (2023) – these\\nhyperparameters achieve significantly higher results as shown in Table 1. For (IA)3 and LORA with\\nrank=4, we use the same training hyper-parameters such as learning rate of 3e−4 and batch of 32\\nover 500k steps.\\nMetrics\\nFollowing the zero-shot evaluation presented in T0 Sanh et al. (2022), we test our method\\nand the baselines on 8 held-out (unseen during training) datasets – ANLI (Nie et al., 2020), HellaSwag\\n(Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), and 5 Super Glue datasets Wang et al.\\n(2020). These datasets cover different tasks ranging from coreference resolution, natural language\\ninference, multiple-choice question answering, story completion, and word sense disambiguation. We\\ncalculate the median accuracy for each evaluation dataset across different prompt templates and\\nthen report the per-dataset result together with an average across all datasets. We also include the\\nmean accuracy for all evaluation datasets in the Appendix.\\n6\\nInfrastructure\\nAll experiments were conducted on TPU v4 machines up to 256 pod slices. For\\ntraining, evaluation, and inference of all the models experimented, we used SeqIO and T5X (Roberts\\net al., 2022) frameworks that enable data and model parallelization across TPU cores with integrated\\nsequential data processing.\\n3.1\\nAblations\\nGiven no work to-date has studied MoE in extremely parameter-efficient settings, we also seek to\\nunderstand key characteristics of our proposed methdology by running rigorous ablations. We detail\\nboth briefly, along with the experimental set-up below:\\nRouting Input: Token vs Sentence Embeddings How does a pronounced inductive bias for task\\nrepresentations in the form of instruction embedding affect routing and downstream generalization?\\nIn our main MoV and MoLORA methods, router layers take intermediate embeddings of input tokens\\nas input similar to other MoE architectures (Shazeer et al., 2017; Fedus et al., 2022). However, as an\\nalternative, a sentence embedding can be computed for each instruction (prompt with corresponding\\ninput) and be used as input for the router (Ye et al., 2022). To compare both – sentence embeddings\\nfor each instruction were derived using the Sentence-T5 encoder (Ni et al., 2022), trained with the\\nT5-XL retrieval model (Ni et al., 2021). This encoder was initialized from the pretrained T5 and\\ntrained on diverse data sources as outlined in Ni et al. (2022). Without additional fine-tuning, each\\ninstruction sequence which consists of a prompt template and the input sentence, was passed to\\nretrieve the embeddings with a dimension of 768.\\nRouting Strategy: Soft vs Discrete\\nWhat is the best routing strategy in parameter-efficient\\nMoEs? In our MoE framework, we use soft merging of experts as routing strategy. Soft merging\\nrefers to a weighted average of all the experts computed within a specified routing block. As an\\nalternative, discrete top-k routing strategy as used in standard MoE architectures introduces the\\nsparsity and decreases the amount of computation (Shazeer et al., 2018; Fedus et al., 2022). In the\\ntop-k routing approach, rather than considering all experts for a decision, only the top ’k’ experts,\\ndetermined by the router, are chosen for the computation. Note that, although the computation is\\nconditional to the top-k experts, the required memory depends on the total number of experts.\\nWe evaluate top-k selection with k = {1, 2} as they were proposed by previous work (Shazeer et al.,\\n2017; Fedus et al., 2022). Results for these strategies are elaborated in Section 4.4. Additionally, we\\nassess discrete routing with top-k using load balancing following to Fedus et al. (2022) which promotes\\nbalanced top-k selection through an auxiliary loss, aiming for an equitable workload distribution\\namong experts.\\n4\\nResults and Discussion\\nParameter efficient MoEs vs PEFTs\\nHow does our MoE recipe compare to a single expert\\nPEFT? Table 1 compares zero-shot performance of PEFTs methods ((IA)3 and LORA), and our\\nvariants of parameter-efficient MoE (MoV and MoLORA), using T5-3B as the base model. We\\nobserve that our MoE variants (MoV and MoLORA) present a significant performance boost over\\nthe standard (IA)3 vectors and LORA adapters.\\nMoV using 30 experts achieves a 14.57% performance improvement compared to its dense counterpart\\n7\\nZero-shot Results at 3B Scale\\nModel\\n% Params.\\nANLI\\nCB\\nRTE\\nWSC\\nWIC\\nCopa\\nWNG\\nHS\\nAverage\\nFull-FT\\nT0-3B (Sanh et al., 2022)\\n100%\\n33.46\\n50.0\\n64.08\\n64.42\\n50.39\\n74.92\\n50.51\\n27.51\\n51.91\\nT0-3B (our replication)\\n100%\\n41.08\\n80.36\\n76.17\\n53.37\\n53.92\\n88.94\\n57.46\\n29.19\\n60.06\\nPEFT\\n(IA)3\\n0.018%\\n34.08\\n50.0\\n66.43\\n56.25\\n55.41\\n79.08\\n52.09\\n29.91\\n52.90\\nLORA\\n0.3%\\n37.5\\n75.57\\n73.53\\n61.02\\n51.25\\n83.6\\n54.33\\n25.32\\n57.51\\nOur Method\\nMoV-10\\n0.32%\\n38.92\\n75.0\\n78.88\\n62.5\\n52.19\\n85.77\\n55.96\\n30.24\\n59.93\\nMoV-30\\n0.68%\\n38.7\\n78.57\\n80.87\\n63.46\\n51.1\\n87.25\\n56.27\\n28.63\\n60.61\\nMoV-60\\n1.22%\\n38.83\\n76.79\\n74.55\\n60.1\\n52.66\\n89.79\\n55.49\\n30.47\\n59.83\\nMoLORA-10\\n3.18%\\n38.5\\n78.57\\n78.16\\n63.46\\n50.86\\n86.5\\n55.41\\n26.72\\n59.77\\nMoLORA-15\\n4.69%\\n40.0\\n80.36\\n80.51\\n62.98\\n50.86\\n89.0\\n55.33\\n27.3\\n60.79\\nTable 1: Average median results on unseen tasks for full model fine-tuning (T0), parameter-efficient\\nfine-tune methods ((IA)3 and LORA) and our mixture of parameter-efficient experts (MoV and\\nMoLORA), using T5-3B base model (Raffel et al., 2020). Note that our replication of T0 performs\\nsignificantly higher than the original T0 confirming previous work (Phang et al., 2023; Ivison et al.,\\n2023).\\n(IA)3. This improvement is consistent across all unseen tasks and is achieved at a marginal increase\\nin the number of updated parameters – only an additional 0.018% parameters per expert. In the\\ncontext of LORA, our MoLORA equipped with 15 experts, achieves an average median score increase\\nof 5.70%. This improvement is notably less significant when compared to MoV. We attribute this\\ndisparity to the difference in updated parameter count in LORA adapters and (IA)3 vectors (0.3%\\nvs 0.018%). Overall, learning a mixture for both MoV and MoLORA as opposed to a single dense\\nmodel leads to notable gains in zero-shot performance.\\nMoV outperforms MoLORA given same parameter budget\\nBetween our methods, MoV\\nachieves a better performance-parameter cost trade-off at 3B parameters base model. As shown in\\nthe left plot in figure 1 MoV with 30 experts, only updating 0.68% of all parameters, achieves nearly\\nthe same performance as MoLORA with 15 experts that updates 4.69% of parameters. This shows\\nthe effectiveness of our MoE approaches even with tiny experts at a large base model scale.\\nParameter efficient MoEs vs full fine-tuning How does MoE compare to updating all parameters\\nduring finetuning? As shown in Table 1 when compared to fully fine-tuned T0-3B, our proposed\\nmethods, MoV and MoLORA both with 10 experts, are on par with full fine-tuning. This is\\nimpressive as MoV-10 only updates 0.32% of all model parameters. Furthermore, when increasing\\nthe number of experts from 10 to 15 and 30 for MoV and MoLORA respectively, our both methods\\noutperform the full fine-tuning by a small margin.\\n4.1\\nHow do parameter-efficient MoEs scale with base model size?\\nFigure 1 (right) shows the scaling characteristic of MoV with 60 experts compared with (IA)3 and\\nfull fine-tuning for 770M, 3B and 11B parameters base models. We find that across all model sizes\\nwe evaluate, our parameter-efficient MoEs consistently maintain higher performance compared to\\nstandard PEFTs and achieve comparable results with full fine-tuning.\\nMoV benefits from scaling At all model sizes, MoV-60 significantly outperforms standard (IA)3.\\nIt is also far closer in performance to full fine-tuning than a single expert. For example, at 770M\\n8\\n(IA)3\\nMoV-60\\nLoRA\\nMoLoRA-10\\n50\\n52\\n54\\n56\\n58\\n60\\n62\\nAverage Median Accuracy\\nFull Fine-Tuning\\n770M Model Size\\n(IA)3\\nMoV-30\\nLoRA\\nMoLoRA-15\\n50\\n52\\n54\\n56\\n58\\n60\\n62\\nAverage Median Accuracy\\nFull Fine-Tuning\\n3B Model Size\\nOur Methods vs Standard PEFTs\\nFigure 3: Comparison of the top-performing variants from our proposed mixture of PEFT experts\\nversus their dense counterparts across T5-Large (Left) and T5-XL (Right).\\nparameters, there is a 12.34% performance gap between (IA)3 and full fine-tuning vs 5.56% for\\nMoV-60. As the base model scales up, MoV becomes more competitive with full fine-tuning. For\\n3B and 11B parameter models, MoV-60 achieves performance approximately on par with the full\\nfine-tuning, despite updating less than 1.3% of the total parameters.\\nMoLORA outperforms MoV in smaller model size regimes As discussed in the main results,\\nat larger model sizes MoV achieves a better performance-parameter efficiency trade-off compared\\nto MoLORA. Conversely, at the 770M scale, MoLORA with 10 experts that updates 3.18% of\\ntotal parameters, performs better compared to MoV-60 and nearly matches the performance of full\\nfine-tuning (Figure 3). Finally, similar to MoV, MoLORA archives higher performance than LORA\\nat both 770M and 3B scales.\\n4.2\\nHow does the number of experts impact the downstream performance?\\nThe center plot of Figure 4 shows the performance of MoV with different numbers of experts at\\nall model sizes. We find that increasing the number of experts generally improves unseen task\\nperformance. However, this improvement is contingent upon the specific number of experts and the\\nbase model size. For both 770M and 11B parameter base models, our MoV method achieves its best\\nperformance by using 60 experts. To illustrate, when number of experts is increased from 10 to 60,\\nthe average median accuracy improves from 52.47 to 53.63 for the 770M model and from 62.3 to\\n64.08 for the 11B model. However, for the 3B model, using just 30 experts, updating 0.68% of the\\nparameters, reaches peak accuracy with a score of 60.61 at this scale, as performance stagnates when\\n60 experts are used.\\nThis trend of performance improvement by scaling more experts is further corroborated in the context\\nof MoLORA; when scaling experts from sets of (5, 10, 15), there was a corresponding elevation in\\nthe average median score, registering at 58.6, 59.77, and 60.79, respectively.\\n4.3\\nWhat is the best routing strategy in parameter-efficient MoEs?\\nIn Figure 4, the rightmost plot shows the overall unseen task performance when using different\\nrouting strategies for MoV. Specifically, we compare the soft merging of 10 experts (dashed line) with\\n9\\n770M\\n3B\\n11B\\nBase Model Components\\n52\\n54\\n56\\n58\\n60\\n62\\n64\\n66\\n68\\nAverage Median Accuracy\\nToken vs Sentence Embeddings for Routing\\ntoken\\nsentence embedding\\n1\\n10\\n30\\n60\\nNumber of Experts\\n50\\n52\\n54\\n56\\n58\\n60\\n62\\n64\\nAverage Median Accuracy\\nEffectiveness of Expert Count vs. Model Sizes\\n11B\\n3B\\n770M\\nRouting Strategies\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\nMedian Accuracy\\nMoV-10\\nTop-K Selection\\nMoV-10 (top-2)\\nMoV-10 (top-1)\\nMoV-2\\nMOV-1 = (IA)3\\nFigure 4: Left: Zero-shot performance of passing embedding of the token sequence to the router\\nvs. passing tokens to the router. Middle: Zero-shot performance across T5 model sizes (Large, XL,\\nXXL) as the number of experts increases. Right: The effectiveness of activating top-k experts.\\ndiscrete top-2 and top-1 routing. We observe that soft merging significantly outperforms discrete\\nrouting in the MoV-10 setting. Specifically, for discrete routing with top-k experts, where k is 1 and\\n2, the MoE achieves an average median accuracy of 54.92 and 57.45 respectively. In contrast, using\\nthe soft merging approach, where all experts are activated, we observe an accuracy of 59.93.\\nFurthermore, to understand if we recover the performance loss of top-k routing by using load\\nbalancing, we integrated the loss balancing following to Fedus et al. (2022). However, we find that\\nthe top-k selection of k = 2 with load balancing loss leads to a further decrease in performance 1.5\\naverage median score.\\nTogether, these results show that in extremely parameter-efficient MoE settings, soft merging enables\\nsuperior performance. Note that top-2 and top-1 routing strategies (among 10 experts) perform\\nbetter than MoV with only 2 experts and a single expert (IA)3 respectively, showing that soft\\nmerging performs better when a larger number of experts are used.\\n4.4\\nDoes a pronounced task information in routing lead to higher performance?\\nTo understand the effects of a pronounced inductive bias towards task representations in our MoE\\nframework, we compare using sentence embeddings of instructions with token embeddings for the\\nrouting input. These sentence embeddings are obtained offline using an external sentence embedding\\nmodel. Here, we aim to evaluate how pronounced task information affects the router’s decision and\\nthe subsequent generalization capabilities of the model in downstream tasks. Figure 4 leftmost plot\\nshows performances of token routing and sentence routing at all model sizes. We find that the token\\nrouting exhibits superior performance with 3.03%, 8.86%, and 0.94% improvement for 770M, 3B,\\nand 11B base model sizes respectively. These results suggest that a higher degree of inductive bias\\nfor task datasets is not necessarily beneficial as our approaches can acquire a diverse set of task\\nknowledge directly from the hidden representations of tokens. Furthermore, token routing enables\\nthe use of learned experts and routing layers without any prior task information for unseen tasks.\\n4.5\\nDo experts specialize in diverse knowledge across different tasks?\\nTo understand how expert routing differs for different tasks, we take a closer look at how experts\\nare activated for a variety of tasks. Figure 5 shows the mean expert probabilities for MoV with\\n5 experts that are located in feed-forward layers in the last decoder block at 770M parameter T5\\n10\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nAvg. Routing Probability\\nTraining Tasks\\nquail\\nrotten_tomatoes\\ncommon_gen\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nExpert 5\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nEvaluation Tasks\\nsuper_glue_cb\\nsuper_glue_wic\\nwinogrande\\nFigure 5: Mean expert routing probabilities for intermediates activations at the last feedforward\\nlayer. Values are averaged across tokens and batch. Experts are weighted differently in soft merging\\ndepending on the task. Left: Measured on tasks seen during training. Right: Measured on unseen\\nevaluation tasks.\\nmodel. We selected the last decoder block as it has been shown deeper layers learn more task-specific\\ninformation (Rogers et al., 2020). We plot the mean routing probabilities for both training tasks and\\nevaluation tasks that are unseen during training, to understand cross-task generalization through\\nthe lens of experts if skills learned at training time generalize to unseen tasks at evaluation time.\\nIntuitively, if experts have indeed learned different skills, we expect that they contribute in different\\ndegrees to tasks that are different in nature. The amount of contribution is directly reflected in\\nthe routing probability of each expert since we use soft merging i.e. summation of expert vectors\\nweighted by the routing probability as described in Figure 2. As such, the mean routing probabilities\\nplotted in Figure 5 provide an overall picture of the contribution of each expert, depending on the\\ndownstream task.\\nSpecialization across unseen vs seen tasks As depicted in Figure 5, both evaluation and\\ntraining tasks lead to the activation of experts at different magnitudes. For example, both quail and\\nsuper_glue_cb activate Expert 3 the most out of the 5 experts, followed by Expert 4 but are different\\nboth in terms of the relative contribution of each expert and the ordering of the remaining 3 experts\\nbased on routing probability. A similar pattern can be observed for common_gen & winogrande as\\nthey both activate Expert 2 the most but are otherwise different. Overall, the fact that routing\\nspecialization seems to occur regardless of whether the downstream task was trained on, suggests\\nthat expert specialization is inherent and transferable from seen tasks to unseen tasks.\\n4.6\\nHyper-parameters Sensitivity\\nGiven the widely documented sensitivity of MoE-style architecture to hyperparameters (Fedus et al.,\\n2022; Shazeer et al., 2017), we ran extensive ablation studies to uncover the idiosyncrasy of PEFT\\nmethods in the context of MoE. We experimented with batch sizes of 32, 128, 256, and 2048 and we\\nfound that the larger the batch size, the more likely our MoEs to collapse to a single expert. Our\\nempirical finding resonates with Shen et al. (2023) which also finds that a small batch is necessary\\nfor stable training. For instance, by experimenting with a batch size of 2048 and evaluating every\\n5K steps up to 20K, we observed that the performance of our parameter-efficient MoEs deteriorates\\nafter 5K steps, converging to performance levels akin to their dense counterparts. Additionally, we\\n11\\nexperimented with varying learning rates from 3e−3 to 6e−4 where we discovered for our methods, a\\nsmaller learning rate of 3e−4 leads to higher performance relative to their dense PEFT counterpart\\nand full fine-tuning. Smaller learning rates stabilize training in parameter-efficient experts by\\npreventing rapid, imbalanced updates that can suppress diversity and lead to suboptimal solutions.\\n5\\nRelated Work\\nMixture-of-Experts\\nThe Mixture-of-Experts (MoE) has been investigated thoroughly in Natural\\nLanguage Processing (Lou et al., 2022; Mustafa et al., 2022; Shazeer et al., 2017; Lepikhin et al., 2020;\\nFedus et al., 2022; Du et al., 2022; Zoph et al., 2022; Clark et al., 2022; Zhou et al., 2022; Komatsuzaki\\net al., 2023; Kudugunta et al., 2021; Zuo et al., 2022) as an effective way of increasing the model’s\\ncapacity in parameter size where certain parts of the model are activated while computation is kept\\nthe same or close to its dense counterpart. In the context of MoE, there is a body of work focusing on\\nimproving the routing (Hazimeh et al., 2021; Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022)\\nincluding random routing (Zuo et al., 2022) activating all expert through weighted average (Eigen\\net al., 2014) to sparsely select a single or k experts (Fedus et al., 2022; Du et al., 2022). MoE has\\nalso been invested in multi-task settings including multilingual neural machine translation(Hazimeh\\net al., 2021; Kudugunta et al., 2021). Unlike these studies, our research addresses MoE by scaling\\nboth the volume of data and the number of tasks, aiming to mitigate the instability inherent in\\ntraining the MoE models. But our primary emphasis remains on achieving efficient fine-tuning.\\nRecently, Shen et al. (2023) highlighted how instruction fine-tuning with scaled tasks can counteract\\nthe generalization challenges tied to MoE models. In distinction from this, our study scrutinizes\\nthe efficacy of instruction fine-tuning in the MoE domain, specifically concentrating on a unique\\nensemble of the PEFT components, considering the memory cost of the traditional MoE can be\\nprohibitive for many practitioners. Similar to the aforementioned work, Ye et al. (2022) utilized MoE\\nin a multi-task context, employing BART Lewis et al. (2019) as their pre-trained model. However,\\nthey limited their experimental scope to a smaller scale and used replicas of each transformer layer\\nas experts, simply multiplying the model by the number of experts. Our work, on the other hand,\\npresents an extreme parameter efficiency with small experts at a large scale up to 11B parameter\\nbase model.\\nInstruction Tuning Instruction tuning, as elucidated in (Sanh et al., 2022; Wei et al., 2022;\\nMishra et al., 2022), is a technique where a language model is fine-tuned over a collection of tasks\\nusing paired prompts and responses. The primary goal of this technique is to enable the model\\nto predict responses accurately based on the provided prompts, thereby augmenting its ability to\\nunderstand and execute instructions effectively. The method has gained considerable attention due\\nto its pronounced success in enhancing zero-shot performance on tasks to which the model has not\\nbeen previously exposed. Additionally, instruction tuning has led to breakthroughs such as Chain of\\nThought Prompting (Wei et al., 2023) where a breakdown of complex problems into smaller steps to\\nproduce intermediate reasoning along with the final solution, PaLM (Chowdhery et al., 2022), FLAN\\n(Wei et al., 2022). In our work, we explore the use of instruction fine-tuning with the intention\\nof harnessing its benefits that enable the model to learn from a diverse set of inputs where the\\nmixture of expert style models suits well, for enhanced evaluation performance on unseen tasks. Our\\nobjective remains to optimize computational efficiency without compromising zero-shot performance.\\nParameter-Efficient Fine-tuning. Houlsby et al. (2019) established \"adapters\" in the NLP\\ndomain to fine-tune BERT. There are many variants of adapters with different design choices (Bapna\\n12\\net al., 2019; Pfeiffer et al., 2021). Li & Liang (2021) proposed updating soft prompts concatenated\\nto embeddings or layer outputs instead of adapters. Zaken et al. (2022) show that just updating\\nonly a small subset of parameters during fine-tuning (e.g. just biases) is very effective. Hu et al.\\n(2021) proposed LORA based on low-rank decomposition matrices of transformer layers. They show\\nsuperior performance with a smaller parameter budget and no inference cost as LORA parameters\\ncan be applied offline to the baseline model. Liu et al. (2022) proposed (IA)3, task-specific vectors\\nto modify attention activation. Instead of using feedforward layers inserted in transformer layers\\nas adapters, they learn vectors to update (by broadcast multiplication) key, value, and linear layer\\nweight matrices. Unlike the other PEFT methods, (IA)3 does not induce any additional inference cost\\nand enables mix-batches (from different datasets). The multiplicative nature of the (IA)3 creates an\\ninteresting opportunity for the mixture-of-expert type of modeling without parallelization overhead.\\nChen et al. (2023) experiment with different design spaces (essentially a hyperparameter search)\\nfor PEFT. They suggest four phases: 1) grouping layers into different sets; 2) adding trainable\\nparameters towards each group; 3) deciding which group should be trained; 4) assigning groups with\\ndifferent training strategies. Their finding is that different architectures have different best settings.\\nWe have chosen (IA)3 and LORA as our PEFT components because they offer an optimal balance\\nbetween performance and parameter efficiency (Mahabadi et al., 2021; Liu et al., 2022).\\nSeveral studies have explored PEFT in the context of MoE or in a similar fashion, albeit with\\ncertain distinctions. For instance, Wang et al. (2022) focused on single-task fine-tuning employing a\\nmixture of adapters for BERTbase with 110M parameters (Devlin et al., 2019) and RoBERTalarge\\nwith 355M parameters (Liu et al., 2019), incorporating random routing, and adopting a few-shot\\nevaluation. In divergence from this, our work centers on instruction-tuning with multiple tasks\\npresent during fine-tuning. We underscore the efficacy of this approach by rigorously testing up\\nto 11B parameter text-to-text model Raffel et al. (2020), implementing token routing, and strictly\\nemphasizing evaluation on a set of unseen (held-out) tasks to underscore the potential of instruction\\ntuning. In another work, Ponti et al. (2022) introduced Polytropon, which involves learning adapters\\n(termed as ’skills’) specific to each task and employing a task-skills binary matrix to determine\\nthe skill set associated with each task. In their method, input examples dictate the selection of\\nadapters. These adapters are then aggregated, and the resultant single adapter is integrated into the\\noverall architecture. Extending upon the Polytropon framework, Caccia et al. (2023) implemented a\\ndistinct skill set for every layer in their variant named Polytropon-S. They introduce a deterministic\\nrouting function, delve into supplementary inductive biases, show effectiveness up to 3B models,\\nand they don’t employ MoE style architecture. Our research presents a departure from these two\\nstudies. Specifically, our primary experimental setup employs MoEs that do not require any specific\\ntask identifier during fine-tuning by the use of their token routing strategy. In this way, we can\\nevaluate our instruction-tuned MoEs on unseen tasks without any further task-specific few-shot\\nfine-tuning. We showed the scaling property of our MoEs in this setting by fine-tuning models up to\\n11B parameters.\\n6\\nConclusion\\nThis work introduces MoEs in an extremely computationally limited environment. We propose\\nintroduce the Mixture of Vectors (MoV) and Mixture of LoRA (MoLORA) to mitigate the challenges\\nassociated with scaling instruction-tuned LLMs at scale. Our method outperforms parameter-efficient\\ntechniques and achieves performance parity with full fine-tuning on unseen tasks by updating less\\nthan 1% of the 3B and 11B model parameters. This percentage may vary depending on the base\\n13\\nmodel size and the number of experts involved. Our extensive experiments, including rigorous\\nablations across model sizes, representation of tokens vs embeddings, soft vs top-k routing, confirm\\nthe effectiveness of our approach across diverse unseen tasks, highlighting its superior accuracy and\\ncomputational efficiency. Furthermore, our framework’s versatility seamlessly integrates with other\\nparameter-efficient strategies and remains compatible with efficiency-enhancing techniques such as\\nquantization.\\nLimitations A primary constraint of our experimental framework is its focus on text-to-text models,\\nsuch as T5, without extending the evaluation to decoder-only such as GPT style models. We leave\\nthis as the subject of future work. Additionally, our assessment is exclusively within the context of\\nfine-tuning. Exploration of its efficacy during the pre-training phase remains an avenue for future\\nresearch.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James\\nBradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,\\nChristopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy\\nGur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy\\nHurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,\\nMaxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,\\nWei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello\\nMaggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado,\\nJohn Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\\nZhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural machine\\ntranslation, 2019.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. Language models are few-shot learners, 2020.\\nLucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni.\\n14\\nMulti-head adapter routing for cross-task generalization, 2023.\\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient\\nfine-tuning design spaces, 2023.\\nZixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the\\nmixture-of-experts layer in deep learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\\nhttps://openreview.net/forum?id=MaYzugDmQV.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra,\\nKevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\\nSaeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\\nJeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat,\\nKevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping\\nHuang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,\\nDenny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche,\\nEliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones,\\nElena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich\\nElsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models,\\n2022.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,\\nKathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng\\nChen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts, 2014.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efficient sparsity, 2022.\\n15\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\\nHamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew E Peters. Hint:\\nHypernetwork instruction tuning for efficient zero-and few-shot generalisation. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pp. 11272–11288, 2023.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\\n2020.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-\\nexperts from dense checkpoints, 2023.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference,\\n2021.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning, 2021.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for\\nnatural language generation, translation, and comprehension, 2019.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. In Marina Meila and Tong Zhang (eds.), Proceedings\\nof the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine\\nLearning Research, pp. 6265–6274. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.pr\\ness/v139/lewis21a.html.\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning,\\n2022.\\n16\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods\\nfor effective instruction tuning, 2023.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Cross-token modeling with conditional\\ncomputation, 2022.\\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-\\nefficient multi-task fine-tuning for transformers via shared hypernetworks, 2021.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\\nvia natural language crowdsourcing instructions, 2022.\\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2023.\\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing,\\n2023.\\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal\\ncontrastive learning with limoe: the language-image mixture of experts, 2022.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\\nretrievers, 2021.\\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei\\nYang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings\\nof the Association for Computational Linguistics: ACL 2022, pp. 1864–1874, 2022.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\\nnli: A new benchmark for natural language understanding, 2020.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nJason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hypertuning: Toward adapting large\\nlanguage models without back-propagation. In International Conference on Machine Learning, pp.\\n27854–27875. PMLR, 2023.\\nEdoardo M. Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining modular skills\\nin multitask learning, 2022.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer, 2020.\\n17\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André\\nSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\nAdvances in Neural Information Processing Systems, 34:8583–8595, 2021.\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares,\\nHaitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian,\\nXavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan\\nGarrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre\\nPassos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi,\\nAlexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x\\nand seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/abs/2203.17189.\\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about\\nhow bert works, 2020.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large\\nsparse models, 2021.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale, 2019.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen\\nXu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\\nSheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,\\nTali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted\\ntraining enables zero-shot task generalization, 2022.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and\\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman.\\nMesh-tensorflow: Deep learning for supercomputers, 2018.\\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret\\nZoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan\\nLi, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-experts\\nmeets instruction tuning:a winning combination for large language models, 2023.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\\nmodels, 2023.\\n18\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is all you need, 2023.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems, 2020.\\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan\\nAwadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model\\ntuning, 2022.\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\\nQinyuan Ye, Juan Zha, and Xiang Ren. Eliciting and understanding cross-task skills with task-level\\nmixture-of-experts, 2022.\\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning\\nfor transformer-based masked language-models, 2022.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\\nreally finish your sentence?, 2019.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng\\nChen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and\\nWilliam Fedus. St-moe: Designing stable and transferable sparse expert models, 2022.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and\\nJianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2022.\\n19\\nA\\nFull Experimental Results\\nA.1\\nZero-Shot Evaluation for P3 dataset\\nIn our study, we conducted a comprehensive evaluation of the variants of our proposed methods in\\ncomparison to our established baselines. This evaluation encompassed various sizes of the T5 model,\\nspecifically 770M, 3B, and 11B. Both mean and median scores were reported for every evaluation set\\nderived from the P3 dataset, which covers a range of tasks. For further details and a more in-depth\\nexploration, please refer to the following URL: https://huggingface.co/datasets/bigscience/P\\n3.\\nT5-Large (770M)\\nModel\\n% Params.\\nMetric\\nANLI\\nCB\\nRTE\\nWSC\\nWIC\\nCopa\\nWNG\\nHS\\nAverage\\nFull-FT\\nT0-770M (ours)\\n100%\\nmedian\\n35.6\\n71.43\\n75.63\\n57.21\\n51.41\\n77.0\\n53.04\\n26.78\\n56.01\\nmean\\n35.57\\n57.74\\n75.88\\n52.31\\n52.52\\n74.6\\n52.93\\n26.74\\n53.54\\nPEFT\\n(IA)3\\n0.036%\\nmedian\\n33.5\\n42.86\\n67.87\\n62.02\\n52.35\\n67.0\\n51.22\\n26.33\\n50.39\\nmean\\n33.27\\n45.12\\n67.08\\n58.17\\n52.74\\n66.63\\n51.35\\n26.32\\n50.09\\nLoRA\\n0.497%\\nmedian\\n35.0\\n55.36\\n57.4\\n63.46\\n50.24\\n77.0\\n53.28\\n26.67\\n52.3\\nmean\\n35.26\\n51.67\\n59.35\\n62.98\\n50.66\\n76.5\\n52.41\\n27.24\\n52.0\\nOur Method\\nMOV-5\\n0.27%\\nmedian\\n33.6\\n41.07\\n71.48\\n61.54\\n50.86\\n76.5\\n51.46\\n26.02\\n51.57\\nmean\\n33.51\\n42.62\\n71.26\\n60.96\\n51.14\\n73.8\\n51.55\\n26.01\\n51.36\\nMoV-10\\n0.55%\\nmedian\\n33.9\\n42.86\\n74.19\\n62.5\\n50.31\\n77.0\\n52.64\\n26.34\\n52.47\\nmean\\n33.68\\n42.38\\n74.51\\n59.23\\n50.74\\n74.82\\n52.2\\n26.72\\n51.78\\nMoV-20\\n1.10%\\nmedian\\n33.7\\n41.07\\n73.83\\n63.46\\n50.94\\n75.46\\n51.14\\n25.48\\n51.89\\nmean\\n33.98\\n45.12\\n73.36\\n59.13\\n51.33\\n73.47\\n51.3\\n25.45\\n51.64\\nMoV-30\\n1.66%\\nmedian\\n33.75\\n41.07\\n72.92\\n55.77\\n51.25\\n77.0\\n51.46\\n26.55\\n51.22\\nmean\\n33.81\\n44.88\\n72.56\\n56.15\\n51.29\\n77.43\\n51.81\\n26.52\\n51.81\\nMoV-60\\n3.32%\\nmedian\\n34.0\\n53.57\\n75.81\\n57.69\\n50.55\\n77.96\\n53.12\\n26.33\\n53.63\\nmean\\n34.24\\n52.26\\n75.02\\n58.37\\n50.78\\n77.06\\n52.87\\n26.74\\n53.42\\nMoLoRA-10\\n5.60%\\nmedian\\n33.2\\n67.86\\n68.41\\n64.9\\n50.39\\n80.0\\n52.64\\n52.64\\n55.52\\nmean\\n33.37\\n56.31\\n68.88\\n63.37\\n51.55\\n79.35\\n52.31\\n52.31\\n53.99\\nTable 2: Zero-shot evaluation of the 770M parameter model across all unseen tasks, comparing\\ndifferent numbers of experts for both MoV and MoLoRA.\\n20\\nT5-XL (3B)\\nModel\\n% Params.\\nMetric\\nANLI\\nCB\\nRTE\\nWSC\\nWIC\\nCopa\\nWNG\\nHS\\nAverage\\nFull-FT\\nT0-3B (Sanh et al., 2022)\\n100%\\nmedian\\n33.46\\n50.0\\n64.08\\n64.42\\n50.39\\n74.92\\n50.51\\n27.51\\n51.91\\nmean\\n33.42\\n45.36\\n64.55\\n65.10\\n50.69\\n72.40\\n50.97\\n27.29\\n51.22\\nT0-3B (our replication)\\n100%\\nmedian\\n41.08\\n80.36\\n76.17\\n53.37\\n53.92\\n88.94\\n57.46\\n29.19\\n60.06\\nmean\\n40.73\\n74.52\\n76.82\\n52.21\\n53.84\\n88.99\\n56.83\\n29.2\\n59.14\\nPEFT\\n(IA)3\\n0.018%\\nmedian\\n34.08\\n50.0\\n66.43\\n56.25\\n55.41\\n79.08\\n52.09\\n29.91\\n52.90\\nmean\\n34.56\\n51.07\\n68.38\\n54.9\\n55.61\\n78.23\\n52.14\\n28.97\\n52.98\\nLoRA\\n0.3%\\nmedian\\n37.5\\n75.57\\n73.53\\n61.02\\n51.25\\n83.6\\n54.33\\n25.32\\n57.51\\nmean\\n37.85\\n66.9\\n77.04\\n56.73\\n52.29\\n82.83\\n55.64\\n26.79\\n57.01\\nOur Method\\nMoV-2\\n0.18%\\nmedian\\n34.7\\n46.43\\n66.06\\n56.25\\n54.86\\n85.42\\n53.75\\n29.25\\n53.34\\nmean\\n35.14\\n50.36\\n69.31\\n56.15\\n54.4\\n83.79\\n53.69\\n28.47\\n53.91\\nMoV-5\\n0.23%\\nmedian\\n37.1\\n76.79\\n78.16\\n57.69\\n52.27\\n86.77\\n53.99\\n29.31\\n59.01\\nmean\\n37.66\\n62.14\\n78.3\\n58.46\\n53.54\\n86.52\\n54.54\\n28.3\\n57.43\\nMoV-10\\n0.32%\\nmedian\\n38.92\\n75.0\\n78.88\\n62.5\\n52.19\\n85.77\\n55.96\\n30.24\\n59.93\\nmean\\n38.83\\n63.45\\n79.49\\n60.19\\n53.04\\n86.41\\n56.27\\n29.11\\n58.35\\nMoV-20\\n0.50%\\nmedian\\n39.2\\n75.0\\n76.71\\n57.69\\n53.45\\n89.0\\n55.64\\n30.89\\n59.7\\nmean\\n39.25\\n64.05\\n76.53\\n56.63\\n53.45\\n86.93\\n56.24\\n29.36\\n57.81\\nMoV-30\\n0.68%\\nmedian\\n38.7\\n78.57\\n80.87\\n63.46\\n51.1\\n87.25\\n56.27\\n28.63\\n60.61\\nmean\\n38.9\\n67.5\\n81.23\\n59.9\\n52.43\\n86.28\\n56.39\\n27.57\\n58.77\\nMoV-60\\n1.22%\\nmedian\\n38.83\\n76.79\\n74.55\\n60.1\\n52.66\\n89.79\\n55.49\\n30.47\\n59.83\\nmean\\n38.97\\n63.93\\n75.38\\n57.79\\n53.5\\n86.04\\n55.88\\n29.28\\n57.59\\nMoV-10 (top-1)\\n0.32%\\nmedian\\n33.9\\n75.0\\n71.12\\n61.06\\n50.71\\n70.0\\n51.7\\n25.89\\n54.92\\nmean\\n34.31\\n60.6\\n71.41\\n58.94\\n51.24\\n68.39\\n51.79\\n25.98\\n52.82\\nMoV-10 (top-2)\\n0.32%\\nmedian\\n38.7\\n82.14\\n75.63\\n48.08\\n53.68\\n79.88\\n54.14\\n27.37\\n57.45\\nmean\\n38.89\\n69.76\\n74.95\\n47.69\\n53.51\\n79.89\\n53.83\\n26.91\\n55.67\\nMoLORA-2\\n0.75%\\nmedian\\n39.2\\n82.14\\n80.32\\n62.5\\n50.39\\n80.58\\n57.38\\n28.47\\n60.12\\nmean\\n38.86\\n65.71\\n80.0\\n60.0\\n50.8\\n82.17\\n56.51\\n28.03\\n57.76\\nMoLORA-5\\n1.66%\\nmedian\\n36.75\\n71.43\\n79.96\\n56.25\\n55.17\\n85.81\\n55.8\\n27.63\\n58.6\\nmean\\n37.52\\n62.14\\n80.22\\n52.6\\n55.34\\n84.05\\n56.04\\n26.62\\n56.82\\nMoLORA-10\\n3.18%\\nmedian\\n38.5\\n78.57\\n78.16\\n63.46\\n50.86\\n86.5\\n55.41\\n26.72\\n59.77\\nmean\\n38.49\\n66.43\\n77.44\\n59.9\\n51.63\\n84.96\\n56.1\\n26.7\\n57.71\\nMoLORA-15\\n4.69%\\nmedian\\n40.0\\n80.36\\n80.51\\n62.98\\n50.86\\n89.0\\n55.33\\n27.3\\n60.79\\nmean\\n39.73\\n69.52\\n80.97\\n60.67\\n51.54\\n86.5\\n55.03\\n27.25\\n58.9\\nTable 3: In our most comprehensive experimental setup, we conducted a zero-shot evaluation across\\nall unseen tasks using a 3B parameter model. We compared varying numbers of experts for both\\nMoV and MoLoRA and experimented with a top-k selection routing strategy\\nT5-XXL (11B)\\nModel\\n% Params.\\nMetric\\nANLI\\nCB\\nRTE\\nWSC\\nWIC\\nCopa\\nWNG\\nHS\\nAverage\\nFull-FT\\nT0-11B (Sanh et al., 2022)\\n100%\\nmedian\\n42.17\\n78.57\\n81.23\\n64.42\\n57.21\\n90.79\\n60.46\\n33.65\\n63.56\\nmean\\n41.16\\n70.12\\n80.83\\n61.45\\n56.58\\n90.02\\n59.94\\n33.58\\n61.70\\nT0-11B (our replication)\\n100%\\nmedian\\n47.1\\n80.36\\n81.41\\n60.1\\n56.27\\n96.08\\n67.32\\n31.61\\n65.03\\nmean\\n45.83\\n72.62\\n81.52\\n58.17\\n56.66\\n96.0\\n66.77\\n30.95\\n63.57\\nPEFT\\n(IA)3\\n0.0098%\\nmedian\\n42.3\\n73.21\\n75.99\\n58.65\\n52.04\\n86.27\\n54.3\\n30.27\\n59.12\\nmean\\n42.1\\n63.27\\n75.31\\n55.49\\n52.27\\n85.74\\n55.06\\n30.09\\n57.41\\nOur Method\\nMoV-10\\n0.143%\\nmedian\\n45.83\\n76.79\\n78.52\\n53.85\\n51.88\\n94.23\\n63.77\\n33.5\\n62.3\\nmean\\n44.73\\n70.12\\n78.88\\n54.23\\n53.26\\n93.64\\n63.57\\n33.59\\n61.5\\nMoV-20\\n0.287%\\nmedian\\n44.58\\n76.79\\n73.83\\n55.77\\n52.98\\n95.0\\n62.27\\n32.92\\n61.77\\nmean\\n43.54\\n69.17\\n74.4\\n52.88\\n54.5\\n93.93\\n62.95\\n32.85\\n60.53\\nMoV-30\\n0.431%\\nmedian\\n43.6\\n76.79\\n77.62\\n56.73\\n53.84\\n93.62\\n64.25\\n31.34\\n62.22\\nmean\\n43.32\\n69.29\\n77.22\\n53.56\\n56.03\\n93.65\\n63.52\\n31.32\\n60.99\\nMoV-60\\n0.862%\\nmedian\\n45.17\\n75.0\\n83.03\\n60.1\\n53.68\\n95.42\\n65.82\\n34.38\\n64.08\\nmean\\n43.9\\n69.88\\n83.07\\n56.54\\n54.51\\n94.01\\n64.56\\n34.17\\n62.58\\nTable 4: We evaluated the largest available model size from the original T5 pre-trained checkpoint,\\nT5-XXL with 11B parameters, to demonstrate the efficacy of our proposed mixture of PEFT experts\\nat this scale.\\n21\\nA.2\\nToken vs. Sentence Embeddings for Routing\\nWe present the mean and median results for our routing strategies.\\nSpecifically, we assessed\\nperformance by either passing tokens directly to the router or by passing sentence embeddings. Our\\nfindings indicate that, particularly for the T5-XL(3B) model, token routing consistently yields better\\nperformance in terms of both mean and median values. The Anli dataset is excluded from our\\nembedding dataset.\\nMoV – Token vs. Sentence Embedding\\nModel\\nMetric\\nCB\\nRTE\\nWSC\\nWIC\\nCopa\\nWNG\\nHS\\nAverage\\nMoV-10 (Token) - 770M\\nmedian\\n42.86\\n74.19\\n62.5\\n52.64\\n52.64\\n77.0\\n26.34\\n55.12\\nmean\\n42.38\\n74.51\\n59.23\\n52.2\\n52.2\\n74.82\\n26.72\\n54.37\\nMoV-10 (Embedding) - 770M\\nmedian\\n48.21\\n67.15\\n62.98\\n51.8\\n50.99\\n67.0\\n26.38\\n53.5\\nmean\\n51.67\\n67.29\\n58.37\\n51.79\\n50.99\\n65.8\\n26.57\\n53.21\\nMoV-10 (Token) - 3B\\nmedian\\n75.0\\n78.8\\n62.5\\n52.19\\n55.96\\n85.77\\n30.24\\n62.94\\nmean\\n63.45\\n79.49\\n60.19\\n53.04\\n56.27\\n86.41\\n29.11\\n61.14\\nMoV-10 (Embedding) - 3B\\nmedian\\n57.14\\n67.15\\n61.06\\n55.33\\n52.49\\n82.5\\n29.08\\n57.82\\nmean\\n51.07\\n68.81\\n58.65\\n55.28\\n52.57\\n80.53\\n28.51\\n56.49\\nMoV-10 (Token) - 11B\\nmedian\\n76.79\\n78.52\\n53.85\\n51.88\\n63.77\\n94.23\\n33.5\\n64.65\\nmean\\n70.12\\n78.88\\n54.23\\n53.26\\n63.57\\n93.64\\n33.59\\n63.9\\nMoV-10 (Embedding) - 11B\\nmedian\\n75.0\\n78.7\\n57.69\\n54.0\\n57.85\\n92.0\\n33.08\\n64.05\\nmean\\n66.19\\n79.1\\n58.37\\n54.83\\n58.78\\n91.17\\n32.7\\n63.02\\nTable 5: The above results demonstrate the effectiveness of token routing in comparison to imposing\\na strong inductive bias, such as sentence embedding across various model parameters.\\n22\\n', 'source_name': 'Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning', 'source_url': 'https://arxiv.org/abs/2309.05444'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MegaBlocks_NOTES.pdf #43\n",
      "{'content': 'MegaBlocks: Efficient Sparse Training with Mixture-of-Experts \\nMain Idea(s): MegaBlocks aims to improve the challenges of load imbalance and token dropping \\nin MoE architecture using block sparse matrices. The idea is to present a router that dynamically \\nhandles the token allocation to experts. While in a regular MoE architecture each expert is \\nassigned to a single GPU in a fixed allocation system (each expert gets the same amount of \\ncompute), having to drop tokens in the case of overflow to a specific expert/GPU, while at the \\nsame time padding tokens to compensate for idle computational resources in experts which were \\nnot assigned enough tokens in a batch, MegaBlocks makes this allocation dynamically from the \\nstart, so the computational resources assigned to an expert is variable, being adjusted on a per-\\nbatch basis based on the tokens assigned to the expert on that specific batch. \\nOBS: Tutel, a previous work, used a similar strategy, by implementing a dynamic CF (capacity \\nfactor) for each expert, but this leads to computational inefficiencies. \\nMegaBlocks is possible by making use of block-sparse matrix multiplication as opposed to \\nbatched matrix multiplication. This approach maps efficiently to hardware accelerators and \\nallows for variable expert size and allocation. \\nMegaBlocks leads to training speedups, which is logical since it makes optimum use of \\ncomputational resources at each update. \\n \\nMy takeaways: \\n- \\nMegaBlocks is an approach for maximizing computing efficiency when training MoE \\nmodels. It dynamically adjusts how much compute to be given to each expert at every \\nbatch, preventing token dropping and idle resources. Although this is interesting, per the \\nexperiments of ST-MoE, this seems to only be useful at pre-training, as load balancing \\ndoes not seem to affect fine-tuning much. \\n \\n', 'source_name': 'MegaBlocks: Efficient Sparse Training with Mixture-of-Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/MegaBlocks_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "PE_SparsityCrafting.pdf #44\n",
      "{'content': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for\\nInstruction Tuning on General Tasks\\nHaoyuan Wu 1 Haisheng Zheng 1 Zhuolun He 2 Bei Yu 2\\nAbstract\\nLarge language models (LLMs) have demon-\\nstrated considerable proficiency in general natural\\nlanguage processing (NLP) tasks. Instruction tun-\\ning, a successful paradigm, enhances the ability\\nof LLMs to follow natural language instructions\\nand exhibit robust generalization across a wide\\nrange of tasks. However, these models often en-\\ncounter performance limitations across multiple\\ntasks due to constrained model capacity. Expand-\\ning this capacity during the instruction tuning\\nphase poses significant challenges. To address\\nthis issue, we introduce parameter-efficient spar-\\nsity crafting (PESC), which crafts the dense model\\ninto the sparse model using the mixture-of-experts\\n(MoE) architecture. PESC integrates adapters into\\nthe MoE layers of sparse models, differentiating\\nexperts without altering the individual weights\\nwithin these layers. This method significantly\\nreduces computational costs and GPU memory re-\\nquirements, facilitating model capacity expansion\\nthrough a minimal increase in parameters when\\nguaranteeing the quality of the approximation in\\nfunction space compared to original sparse up-\\ncycling. Our empirical evaluation demonstrates\\nthe effectiveness of the PESC method. Using\\nPESC during instruction tuning, our best sparse\\nmodels outperform all other open-source sparse\\nmodels and exhibit superior general capabilities\\ncompared to GPT-3.5.\\n1. Introduction\\nRecent advancements in natural language processing (NLP)\\nhave been significantly propelled by the advent of large lan-\\nguage models (LLMs) such as GPT (Brown et al., 2020;\\nOpenAI, 2023), Claude (Anthropic, 2023), Gemini (Anil\\net al., 2023), Llama (Touvron et al., 2023a;b), Mistral\\n(Mistral-AI, 2023; Jiang et al., 2024), etc. The increasing\\n1Shanghai Artificial Intelligent Laboratory, China 2The Chinese\\nUniversity of Hong Kong, China. Correspondence to: Bei Yu\\n<byu@cse.cuhk.edu.hk>.\\nMBPP\\nNaturalQuestions\\nAverage\\nMMLU\\nMATH\\nGSM8K\\nHellaSwag\\nHumanEval\\nCamelidae-8x34B-pro\\nMixtral-8x7B-instruct\\nYi-34B-chat\\nLLAMA2-70B-chat\\nFigure 1. Camelidae-8×34B-pro achieves the SOTA performance\\nacross a wide variety of tasks, ranging from general language to\\nspecific domains including code and math, superior to existing\\nopen-source sparse models and other notable dense models.\\nscale of these models has established LLMs as the experts\\nfor NLP tasks due to their exceptional ability to identify\\ncomplex linguistic patterns (Wei et al., 2022).\\nA prominent method for training LLMs is instruction tun-\\ning (Wei et al., 2021). This approach utilizes large-scale,\\nwell-formatted instruction data, enabling LLMs to refine\\ntheir pre-trained representations to comply with human in-\\nstructions (Taori et al., 2023; Xu et al., 2024; Dettmers et al.,\\n2023; Mukherjee et al., 2023). Such instruction-tuned LLMs\\nexhibit remarkable generalization capabilities in NLP tasks\\n(Longpre et al., 2023). This generalization requires training\\non a broad range of instruction-following tasks from multi-\\nple domains such as math, code, biology, etc (Chung et al.,\\n2022; Sanh et al., 2021). However, the inherent complexity\\nof these tasks can hinder model fine-tuning (Zhang & Yang,\\n2021). Specifically, models of certain sizes may struggle to\\noptimize losses from conflicting tasks, resulting in subpar\\nperformance for general tasks.\\nThe scaling law suggests that increasing a model’s scale\\nis crucial for enhanced performance (Chung et al., 2022).\\nExpanding the model’s capacity can improve instruction\\ntuning effectiveness for general tasks (Kaplan et al., 2020).\\nNonetheless, most LLMs are pre-trained dense models de-\\n1\\narXiv:2401.02731v3  [cs.AI]  12 Feb 2024\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nsigned based on transformer architecture, which limits scal-\\nability during instruction tuning. (Komatsuzaki et al., 2023)\\npresented a method for upcycling dense models into sparse\\nactivated mixture-of-experts (MoE) models, which boast\\ngreater capacity (Shazeer et al., 2017; Lepikhin et al., 2020;\\nDu et al., 2022; Fedus et al., 2022; Zhou et al., 2022; Ra-\\njbhandari et al., 2022; Puigcerver et al., 2023). Notably,\\n(Shen et al., 2023) suggested that MoE models respond\\nmore effectively to instruction tuning than their dense coun-\\nterparts. Thus, converting dense models into MoE models\\nduring instruction tuning has the potential to achieve great\\nperformance on general tasks. This conversion involves\\ninitializing each expert in the MoE models as a copy of\\nthe feedforward neural network (FFN) layers (Chen et al.,\\n2015; Rae et al., 2021). Given the parameter scale of cur-\\nrent LLMs, training such giant models requires updating the\\nweights of experts in the MoE layer, which is constrained\\nby GPU memory resources and computational costs.\\nTo mitigate these resource and cost challenges, we introduce\\nparameter-efficient sparsity crafting (PESC), an approach\\nthat effectively expands model capacity while synergizing\\nwith PEFT techniques (Houlsby et al., 2019; Dettmers et al.,\\n2023). PESC involves inserting adapters (Houlsby et al.,\\n2019) into the MoE layers of sparse models, allowing dif-\\nferentiation between experts without altering each expert’s\\nweights in the MoE layers when guaranteeing the quality of\\nthe approximation in function space compared to original\\nsparse upcycling (Komatsuzaki et al., 2023). Considering\\nthat the more sophisticated construction can improve the\\napproximation (Ding et al., 2022), we apply the QLoRA\\n(Dettmers et al., 2023) technique, a prevalent PEFT method,\\nto update other weights in the sparse models. As shown in\\nFigure 1, our Camelidae-8×34B-pro, instruction fine-tuned\\nutilizing PESC, achieved state-of-the-art (SOTA) perfor-\\nmance among various open-source sparse models and dense\\nmodels. Our contributions are described as follows:\\n• We propose an approach, parameter-efficient sparsity\\ncrafting (PESC), for the extension of the model capac-\\nity efficiently.\\n• We implement the PESC method for instruction tuning\\nacross general tasks, achieving significant performance\\nimprovements on various benchmarks.\\n• We develop sparse models, Camelidae, using the PESC\\nmethod, achieving SOTA performance across all open-\\nsource sparse models and demonstrating superior gen-\\neral capabilities compared to GPT-3.5.\\n2. Related Work\\n2.1. Mixture-of-Experts\\nModels employing the MoE structure (Shazeer et al., 2017)\\ndemonstrate the ability to significantly scale up model sizes,\\naugmenting parameters while only incurring sub-linear in-\\ncreases in computational costs. This capability has driven\\nconsiderable progress in the development of large-scale lan-\\nguage models, improving their efficiency and effectiveness.\\nThe MoE architecture has been extensively explored in the\\nfield of NLP (Lepikhin et al., 2020; Du et al., 2022; Fe-\\ndus et al., 2022), particularly with its integration into the\\ntransformer block. In this architecture, the FFN layer is\\nreplaced by an MoE layer, directing each input token to a\\nselect group of expert networks for processing. The final\\ntoken representation is an amalgamation of outputs from\\nthese chosen experts. Despite an increase in parameters,\\nthe sparse activation of experts ensures computational effi-\\nciency while enhancing model capabilities. Our approach\\nadopts the routing strategy from (Lepikhin et al., 2020; Du\\net al., 2022), with selective parameter activation to achieve\\ncomputational and energy efficiency.\\n2.2. Reuse of Trained Weights\\nRecent studies have focused on improving training effi-\\nciency by leveraging pre-existing model weights for a warm\\nstart, thus minimizing training expenses (Chen et al., 2015;\\nRae et al., 2021; Yang et al., 2021; Lin et al., 2021; Lan\\net al., 2019). Sparse Upcycling (Komatsuzaki et al., 2023)\\nintroduces a methodology to initialize sparse MoE models\\nusing weights from a pre-trained dense model. This ap-\\nproach significantly reduces the computational resources\\nneeded compared to the training of the original dense model.\\nSparse Upcycling involves the direct transfer of layer nor-\\nmalization, attention, and embedding parameters from the\\ndense model to the new sparse model. Moreover, it replaces\\nsome Multilayer Perceptron (MLP) layers with MoE layers,\\ninitializing the experts in these layers with weights from\\nthe dense model’s MLP. This process effectively transfers\\nvaluable learned representations from the dense model’s\\npre-training phase into the sparse model. In our research,\\nwe adopt this method, reusing weights from a pre-trained\\ndense model for our PESC method.\\n2.3. Parameter-Efficient Fine-Tuning\\nTraditionally, full fine-tuning has been the norm for adapt-\\ning pre-trained models, including LLMs. However, due to\\nthe immense size of LLMs, this approach demands sub-\\nstantial computational resources. To mitigate this, numer-\\nous parameter-efficient fine-tuning (PEFT) methods have\\nemerged (Houlsby et al., 2019; Hu et al., 2021; Li & Liang,\\n2021; Liu et al., 2022). PEFT focuses on training a lim-\\nited subset of parameters, either from the existing model\\nor newly added ones. Adapter-based methods (Houlsby\\net al., 2019; Hu et al., 2021; Liu et al., 2022) integrate small,\\nlearnable modules called adapters into pre-trained models,\\nfine-tuning only these newly inserted parameters. Among\\nthese, QLoRA (Dettmers et al., 2023) has gained popular-\\n2\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nity for its efficiency in fine-tuning LLMs, yielding results\\ncomparable to full fine-tuning.\\nOur PESC method builds on the adapter-based model frame-\\nwork, tuning multiple adapters inserted after the copied FFN\\nlayers instead of fine-tuning all the copied FFN layers in\\ncorresponding experts. Concurrently, we employ QLoRA to\\nupdate other weight metrics of LLMs. Other works also ex-\\nplore the combination of MoE with LoRA (Gou et al., 2023;\\nWu et al., 2024). For instance, MoCLE (Gou et al., 2023)\\nproposed a PEFT framework suitable for vision-language\\ninstruction tuning on multiple tasks to mitigate task conflicts\\nand enjoy the benefits of huge data training simultaneously.\\nHowever, neither approach concentrates on the generaliza-\\ntion capabilities of instruction-tuned models, the primary\\nfocus of our study.\\n3. Method\\n3.1. Preliminaries\\nAdapters. (Houlsby et al., 2019) proposed the integration\\nof adapters into pre-trained transformer-based models to\\nenhance parameter efficiency. This approach involves tuning\\nonly the parameters added by the adapters. An adapter\\nconsists of two matrices, W down ∈Rd1×d2 and W up ∈\\nRd2×d1, coupled with a non-linear function σ(·). Here, d1\\nand d2 denote the feature dimensions in the pre-trained\\nmodels and the adapter’s hidden dimension, respectively,\\nwith d2 < d1 typically. Given a feature U ∈RN×d1 in\\nthe pre-trained model, the output of the Adapter module is\\nexpressed as:\\nU ′ = σ(UW down)W up + U.\\n(1)\\nMixture-of-Experts. As depicted in Figure 2, an MoE layer\\ncomprises n experts, {Ei}n\\ni=1, and a router R. The output\\ny for an input x in the MoE layer is computed as:\\ny =\\nn\\nX\\ni=1\\nR(x)iEi(x),\\n(2)\\nwhere R(x)i represents the output of the gating network for\\nthe i-th expert, and Ei(x) is the output of the i-th expert.\\nSparsity Crafting. Building on the concept of sparsity\\nupcycling (Komatsuzaki et al., 2023), sparsity crafting lever-\\nages the weights of dense models. As depicted in Figure 2,\\nsparsity crafting involves a transformative process: substi-\\ntuting the FFN layer F within each block of the dense trans-\\nformer model with an MoE layer. This replacement gives\\nrise to an innovatively sparse transformer block. During\\nthe initialization phase of sparsity crafting, each expert Ei\\nwithin the MoE layer is initialized with the FFN layer F. To\\nensure structural coherence, other components, such as the\\nAttention Layer\\nFFN Layer\\nNorm Layer\\nNorm Layer\\nDense Transformer Block\\nAttention Layer\\nNorm Layer\\nNorm Layer\\nExpert 1\\nExpert 2\\nExpert n\\n…\\nTop-2 Gate Router\\nWeighted Sum\\n<latexit sha1_base64=\"gGu0nUlD9P54T/72OijRWqQ93Y=\">ACA3icbVDLSsNAFJ34rPUVdaebYBFclUSKuiy6cVnBPqANYTKZtEMnmTBzI5YQcOvuHGhiFt/wp1/46TNQlsPDHM4517uvcdPOFN\\ng29/G0vLK6tp6ZaO6ubW9s2vu7XeUSCWhbSK4kD0fK8pZTNvAgNeIimOfE67/vi68Lv3VCom4juYJNSN8DBmISMYtOSZhwNf8EBNIv1l3dzLBkAfIEuTPfMml23p7AWiVOSGirR8syvQSBIGtEYCMdK9R07ATfDEhjhNK8OUkUTMZ4SPuaxjiys2mN+TWiVYCKxRSvxisqfq7I8ORKtbUlRGkZr3CvE/r59CeOlmLE5SoDGZDQpTboGwikCsgElKgE80wUQyvatFRlhiAjq2qg7BmT95kXTO6s5vXHbqDWvyjgq\\n6Agdo1PkoAvURDeohdqIoEf0jF7Rm/FkvBjvxsesdMkoew7QHxifP9HLmO8=</latexit>W up\\n<latexit sha1_base64=\"IK3f1X4Mq/XQlc3DfhT3daoaeZE=\">ACBXicbVC7TsMwFHXKq5RXgBGiAqJqUpQBYwVLIxFog+prSrHcVurjh3ZN0AVZWHhV1gYQIiVf2Djb3DaDNByJMtH59yre+/\\nxI840uO63VhaXldK6XNja3tnfs3b2mlrEitEkl6rtY05E7QBDhtR4ri0Oe05Y+vMr91R5VmUtzCJK9EA8FGzCwUh9+7DrSx7oSWi+pJX2ky7QB0gCeS/StG+X3Yo7hbNIvJyUY563/7qBpLEIRVAONa647kR9BKsgBFO01I31jTCZIyHtGOowCHVvWR6ReocGyVwBlKZJ8CZqr87EhzqbFTGWIY6XkvE/zOjEMLnoJE1EMVJDZoEHMHZBOFokTMEUJ8IkhmChmdnXICtMwARXMiF48ycvkuZp\\nxTurVG+q5dplHkcRHaAjdI8dI5q6BrVUQMR9Iie0St6s56sF+vd+piVFqy8Zx/9gfX5A3KDmdY=</latexit>W down\\nNon-linearity\\nFFN Layer\\nParameter Eﬃcient\\nSparsity Crafting      \\nSparse Transformer Block\\nCopy Weights\\nParameter-Eﬃcient Expert\\nMoE Layer\\nFigure 2. Overview of the parameter-efficient sparsity crafting.\\nnormalization and attention layers, are replicated directly\\nfrom the dense transformer block.\\nFor clarity, let us define Fi(θi) as the objective function\\nfor the i-th expert in the MoE layer, where θi represents\\nthe parameters for Ei. θi is initialized from θo, which are\\nthe parameters of the FFN layer F from the original dense\\nmodel. The essence of the sparsity crafting training regimen\\nlies in the optimization of Fi(θi). The goal is to derive θ+\\ni ,\\nthe optimized parameters for each expert. This is formally\\nexpressed as:\\nθ+\\ni = arg min\\nθi Fi(θi).\\n(3)\\nAfter the instruction tuning process utilizing the sparsity\\ncrafting technique, the optimized parameter sets {θ+\\ni }n\\ni=1\\nare obtained for experts {Ei}n\\ni=1 in the MoE layer.\\n3.2. Parameter-Efficient Sparsity Crafting\\nAs shown in Equation (3), traditional sparsity crafting neces-\\nsitates optimizing the parameters {θi}n\\ni=1 for each expert Ei\\nin the MoE layer, leading to significant resource consump-\\ntion, including training time and memory costs due to the\\nextensive parameters of FFN layers in LLMs. Consequently,\\nas illustrated in Figure 2, we introduce parameter-efficient\\nsparsity crafting (PESC), an approach that addresses the\\nhigh training time and memory costs associated with spar-\\nsity crafting in LLMs. Specifically, PESC, leveraging the\\nparameter-efficient fine-tuning (PEFT) paradigm, focuses\\non tuning a smaller subset of parameters to achieve effi-\\nciency.\\nThe core of PESC lies in its objective function, ˜\\nFi(θi, ωi),\\nwhere ωi represents the select parameters for tuning. No-\\ntably, the parameters of ωi is significantly less than θi, as\\nindicated by |ωi| ≪|θi|, where | · | indicates the number of\\nparameters involved. Each expert Ei begins the process with\\nthe initial state (θo, ωo), where ωo is initialized to zero to fa-\\n3\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\ncilitate identity mapping, resulting in ˜\\nFi(θo, ωo) = Fi(θo).\\nThe training procedure for PESC is thus the optimization of\\n˜\\nFi(θo, ωi), leading to a solution ω+\\ni defined as:\\nω+\\ni = arg min\\nωi\\n˜\\nFi(θo, ωi).\\n(4)\\nConsidering that |ωi| ≪|θi|, we have\\nn\\nX\\ni=1\\n|ω+\\ni | + |θo| = n × |ωo| + |θo| ≪n × |θo| =\\nn\\nX\\ni=1\\n|θ+\\ni |.\\n(5)\\nConsequently, this solution set {ω+\\ni }n\\ni=1 is more efficient\\nthan the original sparsity crafting parameters {θ+\\ni }n\\ni=1 for\\nthe set {Ei}n\\ni=1.\\nTo ensure the effectiveness of PESC compared to traditional\\nsparsity crafting, it is vital to maintain a small approximation\\nerror, as defined by:\\n| ˜\\nFi(θ+\\ni , ωo) −˜\\nFi(θo, ω+\\ni )| < ξ,\\n(6)\\nwhere ξ is the approximation error. This can be achieved by\\ndesigning an approximate function ˜\\nFi(θo, ω+\\ni ) that closely\\nmatches ˜\\nFi(θ+\\ni , ωo) (Houlsby et al., 2019; Ding et al., 2022).\\nConsidering that the trajectory of θi optimization approxi-\\nmately follows a manifold, which can be projected into a\\nlower-dimensional space such as adapter in Equation (1).\\nThe approximation error is contingent on the representa-\\ntional capacity of the inserted adapters. Given the universal\\napproximation property of MLP layers with general activa-\\ntion functions, the Adapter module is a universal approxima-\\ntor (Funahashi, 1989; Leshno et al., 1993; Kidger & Lyons,\\n2020). As a result, utilizing the adapters as ωi can effectively\\nensure the quality of the approximation of ˜\\nFi(θ+\\ni , ωo).\\n3.3. Model Design\\nParameter-Efficient Experts. According to the analysis in\\nSection 3.2, adapters can guarantee a good lower bound ξ\\nin Equation (6). Consequently, we can introduce parameter-\\nefficient MoE layers by integrating adapters, thereby achiev-\\ning sparsity in a more parameter-efficient manner.\\nIn the training of sparse transformer blocks, gradients are\\nback-propagated to each expert, necessitating parameter up-\\ndates. For a collection of n experts, original sparsity crafting\\ndemands a computational cost n times that of a single FFN\\nlayer. As depicted in Figure 3, our PESC utilizes adapters\\nto circumvent redundant updates of the expert weights θi.\\nSpecifically, we update the ωi of n inserted adapters to dif-\\nferentiate between experts without altering each expert’s\\noriginal weights θo replicated from the original FFN layer.\\nThus, for a given input x, Equation (2) can be reformulated\\nas:\\ny =\\nn\\nX\\ni=0\\nR(x)iAi(E(x)),\\n(7)\\n<latexit sha1_base64=\"gGu0nUlD9P54T/72OijRWqQ93Y=\">ACA3icbVDLSsNAFJ34rPUVdaebYBFclUSKuiy6cVnBPqANYTKZtEMnmTBzI5YQcOv\\nuHGhiFt/wp1/46TNQlsPDHM4517uvcdPOFNg29/G0vLK6tp6ZaO6ubW9s2vu7XeUSCWhbSK4kD0fK8pZTNvAgNeIimOfE67/vi68Lv3VCom4juYJNSN8DBmISMYtOSZhwNf8EBNIv1l3dzLBkAfIEuTPfMml23p7AWiVOSGirR8syvQSBIGtEYCMdK9R07ATfDEhjhNK8OUkUTMZ4SPuaxjiys2mN+TWiVYCKxRSvx\\nisqfq7I8ORKtbUlRGkZr3CvE/r59CeOlmLE5SoDGZDQpTboGwikCsgElKgE80wUQyvatFRlhiAjq2qg7BmT95kXTO6s5vXHbqDWvyjgq6Agdo1PkoAvURDeohdqIoEf0jF7Rm/FkvBjvxsesdMkoew7QHxifP9HLmO8=</latexit>W up\\n<latexit sha1_base64=\"IK3f1X4Mq/XQlc3DfhT3daoaeZE=\">ACBXicbVC7TsMwFHXKq5RXgBGiAqJqUpQBYwVLIxFog+prSrHcVurjh3ZN0AVZ\\nWHhV1gYQIiVf2Djb3DaDNByJMtH59yre+/xI840uO63VhaXldK6XNja3tnfs3b2mlrEitEkl6rtY05E7QBDhtR4ri0Oe05Y+vMr91R5VmUtzCJK9EA8FGzCwUh9+7DrSx7oSWi+pJX2ky7QB0gCeS/StG+X3Yo7hbNIvJyUY563/7qBpLEIRVAONa647kR9BKsgBFO01I31jTCZIyHtGOowCHVvWR6R\\neocGyVwBlKZJ8CZqr87EhzqbFTGWIY6XkvE/zOjEMLnoJE1EMVJDZoEHMHZBOFokTMEUJ8IkhmChmdnXICtMwARXMiF48ycvkuZpxTurVG+q5dplHkcRHaAjdI8dI5q6BrVUQMR9Iie0St6s56sF+vd+piVFqy8Zx/9gfX5A3KDmdY=</latexit>W down\\nNon-linearity\\nFFN Layer\\nAdapter 2\\nFFN Layer\\nAdapter 1\\nFFN Layer\\nAdapter n\\n…\\nTop-2 Gate Router\\nWeighted Sum\\nShare Weights\\nFigure 3. Detailed design of the MoE layer utilizing parameter-\\nefficient experts. All the FFN layers share the same weights.\\nwhere Ai(x) construct the parameter-efficient expert:\\nAi(x) = σ(xW idown)W iup + x.\\n(8)\\nConsidering that the more sophisticated construction can\\nimprove the approximation, we can also update the shared\\nweights θo of {Ei}n\\ni=1. As illustrated in Equation (7), this\\napproach allows for efficient scaling of the model capacity\\nby introducing a minimal number of parameters across n\\ninserted adapters.\\nTop-2 Gate Router. Within the sparse transformer block,\\nthe MoE layer encompasses a specified number of experts.\\nA router, employing a softmax activation function, mod-\\nels a probability distribution over these experts, reflecting\\neach expert’s capability to process incoming tokens. The\\nrouter’s weights, denoted as W r, which are integrated into\\nthe sparse transformer block, are initially randomly initial-\\nized. As depicted in Figure 3, we utilize the top-2 gate\\nrouter within the sparse transformer block (Lepikhin et al.,\\n2020; Du et al., 2022). This router activates the most suit-\\nable two experts out of n experts {Ei}n\\ni=1 for each token x\\nin an input sequence. After receiving the input token x, the\\nrouter produces router logits R(x) = W r · x. Before being\\nnormalized via a softmax distribution over the available n\\nexperts, we perform the KeepTop2 function. The KeepTop2\\nfunction is applied to retain only the top two values of the\\nrouter logits, assigning −∞to the rest, effectively zeroing\\nthem post-softmax normalization. Thus, given a token x,\\nthe router’s output logit is represented as:\\nR(x) = Softmax(KeepTop2(W r · x)).\\n(9)\\nThe gate value of each expert Ei for the input token x is\\nR(x)i. Despite an increase in parameters, the experts of the\\nMoE layer are activated sparsely, implying that only a lim-\\nited subset of experts is used per input token. This approach\\nenhances the capacity of the model while maintaining com-\\nputational efficiency. During inference, the top-2 gate router\\nselects the best two experts for each token dynamically. In\\nan MoE layer with n experts, this enables up to n2 different\\n4\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\ncombinations of experts, as opposed to a single combina-\\ntion in the traditional transformer architecture, providing\\nenhanced computational adaptability.\\nExperts Loading Balance. The top-2 gate router, through\\nits gating mechanism, tends to disproportionately favor a\\nfew experts, leading to an imbalance where these experts\\nare more frequently trained and consequently chosen by\\nthe router. To counter this imbalance and promote uniform\\nexpert utilization, an auxiliary loss as suggested by (Fedus\\net al., 2022) is integrated during training for each sparse\\ntransformer block. With n experts and a batch B containing\\nT tokens, this auxiliary loss is calculated as the scaled dot-\\nproduct of vectors f and p,\\nloss = α · n ·\\nn\\nX\\ni=1\\nfi · pi,\\n(10)\\nwhere fi denotes the fraction of tokens dispatched to expert i\\nand pi represents the fraction of router probability allocated\\nto expert i. α is a multiplicative coefficient for the auxiliary\\nlosses. We utilize an α = 10−2 which was sufficiently\\nlarge to ensure load balancing while small enough to not\\noverwhelm the primary cross-entropy objective. As the ideal\\nscenario entails uniform routing across the n experts, both\\nvectors should ideally have values of 1\\nn. The auxiliary loss\\nof Equation (10) fosters this uniform distribution, achieving\\nits minimum under such conditions.\\n4. Experiments\\n4.1. Settings\\nTraining Data. To demonstrate the learning ability of the\\nsparse model with MoE layers, we simultaneously trained\\nthe model on a diverse set of skills, encompassing cod-\\ning, mathematical, and other general abilities from various\\nsubjects. This training involved integrating three distinct\\ndatasets from varied domains during the instruction tun-\\ning phase: SlimORCA (Lian et al., 2023; Mukherjee et al.,\\n2023; Longpre et al., 2023), Magicoder (Wei et al., 2023),\\nand MetaMathQA (Yu et al., 2023) datasets. After filtration\\nand sampling, we can get two instruction datasets includ-\\ning IDAE-500K and IDAE-720K finally. The details of the\\ninstruction datasets are illustrated in Table 1.\\nEvaluation Benchmarks. Our evaluation compares the per-\\nformance of both dense and sparse models on established\\nacademic benchmarks. The dense models include Llama2\\n(Touvron et al., 2023b), Vicuna (Zheng et al., 2023), Yi\\n(01-AI, 2023), SUSChat (SUSTech-IDEA, 2023), GPT3.5\\n(Brown et al., 2020), and our Camel models, while the\\nsparse models encompass Mixtral (Jiang et al., 2024) and\\nour Camelidae models. Evaluations were conducted using\\nOpenCompass (OpenCompass, 2023), LM-Eval-Harness\\n(Gao et al., 2023), and our internal evaluation libraries, sum-\\nTable 1. The proportion of SlimORCA (Lian et al., 2023; Mukher-\\njee et al., 2023; Longpre et al., 2023), Magicoder (Wei et al., 2023),\\nand MetaMathQA (Yu et al., 2023) datasets in IDAE-500K and\\nIDAE-720K datasets.\\nSlimOrca\\nMagicoder\\nMetaMathQA\\nIDAE-500K\\n300K\\n100K\\n100K\\nIDAE-720K\\n360K\\n180K\\n180K\\nmarizing performances across a spectrum of well-known\\nbenchmarks. These benchmarks are illustrated as follows:\\n• Code: Evaluation includes pass@1 scores for Hu-\\nmanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n2021).\\n• Math: Accuracy scores for GSM8K (Cobbe et al.,\\n2021) (5-shot) and MATH (Hendrycks et al., 2021)\\n(4-shot) benchmarks.\\n• Commonsense Reasoning (CR): Accuracy scores for\\nPIQA (Bisk et al., 2020), HellaSwag (Zellers et al.,\\n2019), WinoGrande (Sakaguchi et al., 2021), ARC-\\neasy, and ARC-challenge (Clark et al., 2018).\\n• Word Knowledge (WK): Assessment of 0-shot perfor-\\nmance on NaturalQuestions (Kwiatkowski et al., 2019)\\nand TriviaQA (Joshi et al., 2017) utilizing the exact\\nmatch (EM) metric.\\n• Aggregated Benchmarks: Overall results for MMLU\\n(Hendrycks et al., 2020) (5-shot) utilizing accuracy\\nscores metrics.\\nNotably, for more detailed experiment results, please refer\\nto Appendix A.2.\\nImplementation Details. We employed QLoRA (Dettmers\\net al., 2023) techniques for effective fine-tuning of both\\nthe Camel and Camelidae models derived from Llama2-\\n7B (Touvron et al., 2023b), Llama2-13B (Touvron et al.,\\n2023b), and Yi-34B (01-AI, 2023). Notably, both Camel and\\nCamelidae models were fine-tuned using identical datasets,\\nIDAE-500K, to ensure fair comparisons between dense and\\nsparse models. Notably, to further enhance the capabilities\\nof the sparse models, we also utilize IDAE-720K for the\\ninstruction-tuning of the Camelidae-pro model. This process\\nentailed using a constant learning rate schedule with a warm-\\nup ratio of 0.03, and the paged AdamW (Dettmers et al.,\\n2023; Loshchilov & Hutter, 2017) optimizer with a learning\\nrate of 2 × 10−4, no weight decay, a batch size of 128, and\\na sequence length of 2048 tokens. The models underwent\\ninstruction tuning for one epoch on 16 A100 GPUs, each\\nequipped with 80G memory. Please refer to Appendix A.1\\nfor more details.\\n4.2. Comparison with Chat LLMs\\nWe present the performance of various chat LLMs on a set\\nof standardized benchmarks. The chat models evaluated are\\n5\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nTable 2. Performance of Camelidae-8×34B-pro on academic benchmarks. We present a detailed comparison of the Camelidae-8×34B-pro\\nmodel with the SOTA open-source sparse chat model and various dense chat models. We present performance metrics for each task,\\nhighlighting the effectiveness of the models under zero-shot or few-shot evaluation. We bold the highest scores among all models.\\nSparse Chat Models\\nDense Chat Models\\nCamelidae-8×34B-pro Mixtral-8×7B-instruct Yi-34B-chat Llama2-70B-chat GPT-3.5\\nMMLU (Acc.)\\n(Hendrycks et al., 2020)\\n75.7%\\n(5-shot)\\n68.7%\\n(5-shot)\\n74.8%\\n(5-shot)\\n63.8%\\n(5-shot)\\n70.0%\\n(5-shot)\\nGSM8K (Acc.)\\n(Cobbe et al., 2021)\\n79.4%\\n(5-shot)\\n71.7%\\n(5-shot)\\n67.6%\\n(5-shot)\\n59.3%\\n(5-shot)\\n57.1%\\n(5-shot)\\nMATH (Acc.)\\n(Hendrycks et al., 2021)\\n24.0%\\n(4-shot)\\n22.1%\\n(4-shot)\\n17.3%\\n(4-shot)\\n10.4%\\n(4-shot)\\n34.1%\\n(4-shot)\\nHumanEval (Pass@1)\\n(Chen et al., 2021)\\n48.8%\\n(0-shot)\\n25.6%\\n(0-shot)\\n20.1%\\n(0-shot)\\n32.3%\\n(0-shot)\\n48.1%\\n(0-shot)\\nMBPP (Pass@1)\\n((Austin et al., 2021)\\n43.2%\\n(4-shot)\\n40.6%\\n(4-shot)\\n41.0%\\n(4-shot)\\n35.6%\\n(4-shot)\\n-\\nHellaSwag (Acc.)\\n(Zellers et al., 2019)\\n85.2%\\n(10-shot)\\n86.5%\\n(10-shot)\\n83.9%\\n(10-shot)\\n84.8%\\n(10-shot)\\n85.5%\\n(10-shot)\\nNaturalQuestions (EM)\\n(Kwiatkowski et al., 2019)\\n31.2%\\n(0-shot)\\n22.5%\\n(0-shot)\\n23.7%\\n(0-shot)\\n30.6%\\n(0-shot)\\n-\\nCamelidae-8×34B-pro, Mixtral-8×7B-instruct (Jiang et al.,\\n2024), Yi-34B-chat (01-AI, 2023), Llama2-70B-chat (Tou-\\nvron et al., 2023b), and GPT-3.5 (Brown et al., 2020). The\\nbenchmarks cover a range of domains, including multiple-\\nchoice questions across 57 subjects (MMLU), grade-school\\nmath (GSM8K), math problems across various difficulty\\nlevels (MATH), Python coding tasks (HumanEval), Python\\ncode generation (MBPP), commonsense reasoning (Hel-\\nlaSwag), and world knowledge question answering (Natu-\\nralQuestions).\\nAs shown in Table 2, Camelidae-8×34B-pro demonstrates\\nits strengths which lie in its wide range of knowledge, mathe-\\nmatical and coding proficiency, efficiency as a sparse model,\\ncompetitive performance against dense models, and solid\\ncommonsense reasoning capabilities.\\nKnowledge and Reasoning Abilities. Camelidae-8×34B-\\npro demonstrates impressive performance across a diverse\\nrange of academic benchmarks.\\nIts ability to handle\\nmultiple-choice questions in 57 subjects with a high success\\nrate of 75.7% indicates a wide-ranging grasp of both pro-\\nfessional and academic knowledge. Meanwhile, Camelidae-\\n8×34B-pro scores 31.2% in NaturalQuestions, demonstrat-\\ning a comprehensive world knowledge base. Although\\nCamelidae-8×34B-pro was slightly weaker than some mod-\\nels (Mixtral-8×7B-instruct and GPT-3.5) in the HellaSwag\\nbenchmark, it still demonstrates a strong ability in com-\\nmonsense reasoning around everyday events, with an 85.2%\\nsuccess rate.\\nMathematical Proficiency. In the GSM8K benchmark,\\nwhich focuses on grade-school mathematics, Camelidae-\\n8×34B-pro outperforms other models with a 79.4% suc-\\ncess rate. Although Camelidae-8×34B-pro performs worse\\nthan GPT-3.5 in the MATH benchmark, it still demonstrates\\nstrong performance with a 24.0% success rate compared to\\nthe other models. The experiment results on math bench-\\nmarks suggest strong capabilities of Camelidae-8×34B-pro\\nin understanding and solving basic mathematical problems.\\nCoding Skills. Camelidae-8×34B-pro excels in Python cod-\\ning tasks (HumanEval) and Python code generation (MBPP),\\nwith success rates of 48.8% and 43.2%, respectively. No-\\ntably, Camelidae-8×34B-pro achieves comparable perfor-\\nmance to GPT-3.5 in the HumanEval benchmark. This indi-\\ncates a notable proficiency in understanding and generating\\ncodes, which is critical in the field of computers.\\nAs a summary, Camelidae-8×34B-pro demonstrates a high\\nlevel of proficiency across a wide range of tasks, outperform-\\ning the Mixtral-8×7B-instruct in most domains and showing\\ncompetitive performance in commonsense reasoning. Its\\nstrengths lie in its broad knowledge base, mathematical and\\ncoding skills, and efficiency as a sparse model, making it\\na strong contender in various academic and computational\\napplications. Meanwhile, Camelidae-8×34B-pro achieves a\\ncomparable performance to GPT-3.5, which is a significant\\nachievement considering its reputation and capabilities.\\n4.3. Comparison between Dense and Sparse Models\\nThis subsection evaluates the efficacy of our novel training\\nmethodology through a comparative analysis of Camelidae\\nmodels, encompassing both dense and sparse configurations\\nacross various parameter sizes (6B-10B, 10B-20B, and 30B-\\n6\\nTable 3. Overall performance on all the evaluation benchmarks of dense models (Camel) and sparse models (Camelidae) across different\\nmodel sizes instruction fine-tuned on IDAE datasets (Table 1). We bold the highest scores separately for different model sizes.\\nCamel-7B Camelidae-8×7B Camel-13B Camelidae-8×13B Camel-34B Camelidae-8×34B Camelidae-8×34B-pro\\n# Total Params\\n7B\\n8B\\n13B\\n15B\\n34B\\n38B\\n38B\\n# Activated Params\\n7B\\n7B\\n13B\\n14B\\n34B\\n35B\\n35B\\n# Training Instructions\\n500K\\n500K\\n500K\\n500K\\n500K\\n500K\\n720K\\nMMLU (Acc.)\\n47.7\\n48.3\\n54.4\\n54.4\\n75.3\\n75.6\\n75.7\\nHumanEval (Pass@1)\\n17.7\\n18.3\\n28.7\\n30.6\\n42.1\\n43.9\\n48.8\\nMBPP (Pass@1)\\n21.0\\n23.4\\n30.3\\n30.4\\n40.6\\n41.4\\n43.2\\nGSM8K (Acc.)\\n40.7\\n44.0\\n50.2\\n52.6\\n76.1\\n78.3\\n79.4\\nMATH (Acc.)\\n4.8\\n5.8\\n8.4\\n9.8\\n18.2\\n22.6\\n24.0\\nPIQA (Acc.)\\n79.7\\n79.9\\n80.9\\n80.9\\n82.3\\n82.7\\n83.6\\nHellaSwag (Acc.)\\n76.8\\n76.8\\n79.8\\n80.1\\n82.6\\n83.2\\n82.5\\nWinogrande (Acc.)\\n71.3\\n72.1\\n74.6\\n74.7\\n80.0\\n80.9\\n80.1\\nARC-easy (Acc.)\\n75.0\\n75.0\\n77.7\\n78.8\\n86.1\\n86.2\\n86.6\\nARC-challenge (Acc.)\\n47.9\\n49.6\\n54.3\\n54.2\\n63.6\\n65.2\\n63.3\\nNaturalQuestions (EM)\\n17.6\\n17.8\\n24.7\\n26.8\\n31.6\\n32.2\\n31.2\\nTriviaQA (EM)\\n51.0\\n51.0\\n57.5\\n59.4\\n63.3\\n63.4\\n62.5\\nTable 4. Overall performance on grouped benchmarks of various\\nchat LLMs (Llama2-chat (Touvron et al., 2023b), Vicuna (Zheng\\net al., 2023), Yi-chat (01-AI, 2023), SUSChat (SUSTech-IDEA,\\n2023)) across different model sizes. We bold the highest scores\\nseparately for different model sizes.\\nModel\\nParams\\nAvg.\\nCode\\nMath\\nCR\\nWK\\nMMLU\\nLlama2-7B-chat\\n7B\\n35.4\\n14.9\\n15.1\\n66.7\\n33.0\\n47.3\\nVicuna-7B\\n7B\\n34.0\\n9.6\\n13.5\\n67.6\\n29.2\\n50.1\\nCamelidae-8×7B\\n8B\\n39.9\\n20.9\\n24.9\\n70.7\\n34.4\\n48.3\\nLlama2-13B-chat\\n13B\\n41.8\\n23.1\\n21.2\\n70.9\\n40.0\\n53.8\\nVicuna-13B\\n13B\\n39.9\\n10.7\\n21.0\\n70.8\\n41.1\\n55.8\\nCamelidae-8×13B\\n15B\\n46.5\\n30.5\\n30.7\\n73.8\\n43.1\\n54.4\\nYi-34B-chat\\n34B\\n51.8\\n30.4\\n42.5\\n73.3\\n38.0\\n74.8\\nSUSChat-34B\\n34B\\n53.3\\n25.9\\n47.2\\n78.8\\n38.3\\n76.4\\nMixtral-8×7B-instruct\\n47B\\n53.6\\n33.1\\n46.9\\n79.3\\n40.1\\n68.7\\nCamelidae-8×34B\\n38B\\n59.3\\n42.7\\n50.5\\n79.7\\n47.8\\n75.6\\nCamelidae-8×34B-pro\\n38B\\n59.9\\n46.0\\n51.7\\n79.2\\n46.9\\n75.7\\n50B), as delineated in Table 3 and Table 4.\\nIn the 6B-10B parameter range, the Camelidae-8x7B model\\ndemonstrates a significant advantage over counterparts such\\nas Llama2-7B-chat and Vicuna-7B. This superiority is par-\\nticularly evident in tasks requiring a deeper level of un-\\nderstanding, including code and mathematical benchmarks.\\nThe disparity was further amplified in the 10B-20B param-\\neter bracket, where the Camelidae-8x13B model exhibits\\nconsiderable enhancements in complex tasks like coding\\nand mathematics, highlighting the efficacy of our training\\napproach in augmenting model capabilities. In the 30B-50B\\nparameter range, both the Camelidae-8x34B and Camelidae-\\n8x34B-pro models outperform, achieving the highest scores\\nin challenging tasks and exceeding the capabilities of the\\nleading sparse model, Mixtral-8x7B-instruct.\\nTo ensure equitable comparisons, both Camel and Cameli-\\ndae models were fine-tuned using the same dataset, IDAE-\\n500K. As indicated in Table 4, the Camelidae models, as\\nTable 5. Overall performance on grouped benchmarks of base mod-\\nels (Llama2 (Touvron et al., 2023b)) and chat models (Camel,\\nCamelidae) across different model sizes. We bold the highest\\nscores separately for different model sizes.\\nModel\\nType\\nAvg.\\nCode\\nMath\\nCR\\nWK\\nMMLU\\nLlama2-7B\\nBase\\n34.9\\n13.8\\n10.0\\n69.0\\n36.0\\n45.7\\nCamel-7B\\nChat\\n38.8\\n19.4\\n22.8\\n70.1\\n34.3\\n47.7\\nCamelidae-8×7B\\nChat\\n39.9\\n20.9\\n24.9\\n70.7\\n34.4\\n48.3\\nLlama2-13B\\nBase\\n41.8\\n22.9\\n17.3\\n71.6\\n42.1\\n55.1\\nCamel-13B\\nChat\\n45.5\\n29.5\\n29.3\\n73.5\\n41.1\\n54.4\\nCamelidae-8×13B\\nChat\\n46.5\\n30.5\\n30.7\\n73.8\\n43.1\\n54.4\\nYi-34B\\nBase\\n55.1\\n32.2\\n41.9\\n78.2\\n47.8\\n75.5\\nCamel-34B\\nChat\\n58.1\\n41.4\\n47.2\\n78.9\\n47.5\\n75.3\\nCamelidae-8×34B\\nChat\\n59.3\\n42.7\\n50.5\\n79.7\\n47.8\\n75.6\\nsparse models, consistently display superior performance\\nover the dense Camel models of comparable sizes. More-\\nover, the effectiveness of our method was sustained even\\nwith the augmentation of training data volume.\\n4.4. Comparison between Base and Chat Models\\nAs depicted in Table 5, the Camel and Camelidae models\\ndemonstrate notable enhancements in average performance\\ncompared to their respective base models. Specifically, these\\nmodels show improved capabilities in the Code, Math, and\\nCommonsense Reasoning benchmarks across various model\\nsizes. Notably, the Camelidae model exhibits more pro-\\nnounced improvements in these areas. For instance, in the\\nCode and Math benchmarks, the Camelidae model consis-\\ntently outperforms the Camel model, showcasing superior\\nproficiency in technical and logical tasks. The Camelidae\\nmodel similarly displays greater understanding and reason-\\ning skills in the Commonsense Reasoning benchmark.\\nHowever, the Camel models reveal certain shortcomings in\\nthe Word Knowledge and MMLU benchmarks, especially\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nSlimOrca\\nMagicoder\\nMetaMathQA\\n0\\n10\\n20\\n30\\n40\\n50\\nProportion (%)\\n(a) Top2 Choice\\nSlimOrca\\nMagicoder\\nMetaMathQA\\n0\\n20\\n40\\n60\\n80\\n100\\nExpert0\\nExpert1\\nExpert2\\nExpert3\\nExpert4\\nExpert5\\nExpert6\\nExpert7\\n(b) First Choice\\nSlimOrca\\nMagicoder\\nMetaMathQA\\n0\\n10\\n20\\n30\\n40\\n50\\n(c) Second Choice\\nFigure 4. Proportion of tokens assigned to each expert on different dataset subsets.\\nTable 6. Evaluation on different numbers of experts in the MoE\\nlayers. We bold the highest scores for each grouped benchmark.\\nModel\\nExperts\\nAvg.\\nCode\\nMath\\nCR\\nWK\\nMMLU\\nCamelidae-4×7B\\n4\\n39.6\\n20.7\\n24.3\\n70.2\\n33.3\\n49.3\\nCamelidae-8×7B\\n8\\n39.9\\n20.9\\n24.9\\n70.7\\n34.4\\n48.3\\nCamelidae-16×7B\\n16\\n40.5\\n21.6\\n25.8\\n70.7\\n35.0\\n49.4\\nwhen contrasted with base models. For example, the per-\\nformance of the Camel models at the 7B, 13B and 34B\\nsizes in Word Knowledge is inferior to that of the Llama2\\nbase models, indicating a potential issue of knowledge for-\\ngetting during the instruction tuning phase. Conversely,\\nthe Camelidae models, developed using our PESC method,\\nexhibit substantial improvements in addressing these chal-\\nlenges. In the Word Knowledge and MMLU benchmarks,\\nthe Camelidae models not only surpass the performance of\\nthe corresponding Camel models but also, in some instances\\nsuch as the 34B model size, exceed that of the base mod-\\nels. This advancement suggests that our PESC effectively\\nmitigates the knowledge forgetting issue observed in the\\ninstruction tuning process.\\n4.5. Analysis of Different Numbers of Experts\\nThe results from the study, as shown in Table 6, clearly\\ndemonstrate that increasing the number of experts in the\\nMoE layers significantly enhances the model’s performance.\\nThis trend is evident in the progressive improvement in\\nscores across various academic benchmarks as the number\\nof experts increases from 4 to 16 in the Camelidae models.\\nNotably, the Camelidae-16×7B model exhibits exceptional\\nperformance on all the benchmarks, thereby highlighting\\nthe effectiveness of this methodology. Moreover, the pos-\\nitive correlation between the number of experts and the\\nmodel’s performance indicates the untapped potential of our\\napproach, suggesting that a further increase in the number\\nof experts might yield even more substantial advancements\\nin model performance.\\n4.6. Routing Analysis\\nOur study rigorously examined the expert selection process\\nby the router, with a keen focus on ascertaining whether spe-\\ncific experts demonstrate specialization in distinct domains\\nsuch as coding and mathematics.\\nThis inquiry involved a thorough analysis of the distribution\\npatterns of selected experts across various dataset subsets.\\nThese included SlimORCA (Lian et al., 2023; Mukherjee\\net al., 2023; Longpre et al., 2023), Magicoder (Wei et al.,\\n2023), and MetaMathQA (Yu et al., 2023). The outcomes\\nof this analysis are depicted in Figure 4, with particular\\nemphasis on the 15th layers of the Camelidae-8×7B model.\\nOur findings highlight discernible variations in the distri-\\nbution of experts among the three datasets. For instance,\\nExpert 1 exhibits a notably higher activation within the\\nMagicoder dataset compared to the other datasets, whereas\\nExpert 6 demonstrates a significant activation rate in the\\nMetaMathQA dataset relative to other experts. These ob-\\nservations suggest that the router operates with a structured\\nsyntactic approach. Importantly, despite the variation in\\nexpert selection across different datasets, certain experts\\n(specifically Experts 1, 2, 5, and 6) consistently exhibit\\nelevated activation rates.\\n5. Conclusion\\nIn this paper, we introduce Parameter-Efficient Sparsity\\nCrafting (PESC) which upcycles dense models into sparse\\nmodels utilizing the MoE architecture. PESC incorporates\\nadapters (Houlsby et al., 2019) within the MoE layers of\\nsparse models, enabling the differentiation of experts with-\\nout modifying the individual weights of each expert, and\\nguarantees the quality of the approximation compared to\\ntraditional sparsity upcycling (Komatsuzaki et al., 2023) in\\nfunction space (Section 3.2). This technique significantly re-\\nduces computational costs and GPU memory requirements.\\nIt facilitates the expansion of model capacity with a minimal\\nparameter increase due to the integration of adapters. We\\napply the PESC method to instruction tuning across various\\ngeneral tasks, resulting in notable performance enhance-\\nments on various benchmarks (Section 4). Additionally, we\\nhave developed sparse models, Camelidae, using the PESC\\napproach. Camelidae-8×34B-pro achieves SOTA perfor-\\nmance across all open-source sparse models and demon-\\nstrates superior general capabilities compared to GPT-3.5.\\n8\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nReferences\\n01-AI. Yi. https://github.com/01-ai/Yi, 2023.\\nAnil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Sori-\\ncut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gem-\\nini: a family of highly capable multimodal models. arXiv\\npreprint arXiv:2312.11805, 2023.\\nAnthropic.\\nClaude2.\\nhttps://www.anthropic.\\ncom/index/claude-2, 2023.\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\\nProgram synthesis with large language models. arXiv\\npreprint arXiv:2108.07732, 2021.\\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\\nabout physical commonsense in natural language. In Pro-\\nceedings of the AAAI conference on artificial intelligence,\\n2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\n2020.\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\\nG., et al. Evaluating large language models trained on\\ncode. arXiv preprint arXiv:2107.03374, 2021.\\nChen, T., Goodfellow, I., and Shlens, J. Net2net: Accel-\\nerating learning via knowledge transfer. arXiv preprint\\narXiv:1511.05641, 2015.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416, 2022.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\narXiv preprint arXiv:2110.14168, 2021.\\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\\nL. Qlora: Efficient finetuning of quantized llms. 2023.\\nDing, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y.,\\nHu, S., Chen, Y., Chan, C.-M., Chen, W., et al. Delta\\ntuning: A comprehensive study of parameter efficient\\nmethods for pre-trained language models. arXiv preprint\\narXiv:2203.06904, 2022.\\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu,\\nY., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam:\\nEfficient scaling of language models with mixture-of-\\nexperts. In International Conference on Machine Learn-\\ning, 2022.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\\ners: Scaling to trillion parameter models with simple\\nand efficient sparsity. The Journal of Machine Learning\\nResearch, 2022.\\nFunahashi, K.-I. On the approximate realization of continu-\\nous mappings by neural networks. Neural networks, 2(3):\\n183–192, 1989.\\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\\nA., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li,\\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\\nA. A framework for few-shot language model evaluation,\\n12 2023. URL https://zenodo.org/records/\\n10256836.\\nGou, Y., Liu, Z., Chen, K., Hong, L., Xu, H., Li, A., Yeung,\\nD.-Y., Kwok, J. T., and Zhang, Y. Mixture of cluster-\\nconditional lora experts for vision-language instruction\\ntuning. arXiv preprint arXiv:2312.12379, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\\nM., Song, D., and Steinhardt, J.\\nMeasuring mas-\\nsive multitask language understanding. arXiv preprint\\narXiv:2009.03300, 2020.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874, 2021.\\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\\nGelly, S. Parameter-efficient transfer learning for nlp. In\\nInternational Conference on Machine Learning, 2019.\\nHu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,\\nL., Chen, W., et al. Lora: Low-rank adaptation of large\\nlanguage models. In International Conference on Learn-\\ning Representations, 2021.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,\\nE. B., Bressand, F., et al. Mixtral of experts. arXiv\\npreprint arXiv:2401.04088, 2024.\\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.\\nTriviaqa: A large scale distantly supervised challenge\\ndataset for reading comprehension.\\narXiv preprint\\narXiv:1705.03551, 2017.\\n9\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint arXiv:2001.08361, 2020.\\nKidger, P. and Lyons, T. Universal approximation with deep\\nnarrow networks. In Conference on learning theory, pp.\\n2306–2327. PMLR, 2020.\\nKomatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C. R.,\\nMustafa, B., Ainslie, J., Tay, Y., Dehghani, M., and\\nHoulsby, N.\\nSparse upcycling: Training mixture-of-\\nexperts from dense checkpoints. In International Confer-\\nence on Learning Representations, 2023.\\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,\\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,\\nJ., Lee, K., et al. Natural questions: a benchmark for ques-\\ntion answering research. Transactions of the Association\\nfor Computational Linguistics, 2019.\\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\\nand Soricut, R. Albert: A lite bert for self-supervised\\nlearning of language representations.\\narXiv preprint\\narXiv:1909.11942, 2019.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\\ngiant models with conditional computation and automatic\\nsharding. arXiv preprint arXiv:2006.16668, 2020.\\nLeshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Mul-\\ntilayer feedforward networks with a nonpolynomial ac-\\ntivation function can approximate any function. Neural\\nnetworks, 6(6):861–867, 1993.\\nLi, X. L. and Liang, P. Prefix-tuning: Optimizing con-\\ntinuous prompts for generation. In The Association for\\nComputational Linguistics, 2021.\\nLian, W., Wang, G., Goodson, B., Pentland, E., Cook, A.,\\nVong, C., and ”Teknium”. Slimorca: An open dataset of\\ngpt-4 augmented flan reasoning traces, with verification,\\n2023.\\nURL https://https://huggingface.\\nco/Open-Orca/SlimOrca.\\nLin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang,\\nA., Zhang, J., Li, Y., Lin, W., et al. M6-10t: A sharing-\\ndelinking paradigm for efficient multi-trillion parameter\\npretraining. arXiv preprint arXiv:2110.03888, 2021.\\nLiu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,\\nM., and Raffel, C. A. Few-shot parameter-efficient fine-\\ntuning is better and cheaper than in-context learning.\\nIn Advances in Neural Information Processing Systems,\\n2022.\\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W.,\\nTay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The\\nflan collection: Designing data and methods for effec-\\ntive instruction tuning. arXiv preprint arXiv:2301.13688,\\n2023.\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization. arXiv preprint arXiv:1711.05101, 2017.\\nMistral-AI. Mistral. https://mistral.ai/news/\\nannouncing-mistral-7b//, 2023.\\nMukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi,\\nH., and Awadallah, A. Orca: Progressive learning from\\ncomplex explanation traces of gpt-4. 2023.\\nOpenAI.\\nGPT-4 Technical Report.\\narXiv preprint\\narXiv:2303.08774, 2023.\\nOpenCompass. Opencompass: A universal evaluation plat-\\nform for foundation models. https://github.com/\\nopen-compass/opencompass, 2023.\\nPuigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N.\\nFrom sparse to soft mixtures of experts. arXiv preprint\\narXiv:2308.00951, 2023.\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann,\\nJ., Song, F., Aslanides, J., Henderson, S., Ring, R.,\\nYoung, S., et al. Scaling language models: Methods,\\nanalysis & insights from training gopher. arXiv preprint\\narXiv:2112.11446, 2021.\\nRajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi,\\nR. Y., Awan, A. A., Rasley, J., and He, Y. Deepspeed-\\nmoe: Advancing mixture-of-experts inference and train-\\ning to power next-generation ai scale. In International\\nConference on Machine Learning, 2022.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\\nWinogrande: An adversarial winograd schema challenge\\nat scale. Communications of the ACM, 2021.\\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\\nA., et al. Multitask prompted training enables zero-shot\\ntask generalization. arXiv preprint arXiv:2110.08207,\\n2021.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\\nQ., Hinton, G., and Dean, J. Outrageously large neural\\nnetworks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538, 2017.\\nShen, S., Hou, L., Zhou, Y., Du, N., Longpre, S., Wei,\\nJ., Chung, H. W., Zoph, B., Fedus, W., Chen, X., et al.\\nMixture-of-experts meets instruction tuning: A winning\\ncombination for large language models. arXiv preprint\\narXiv:2305.14705, 2023.\\n10\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nSUSTech-IDEA.\\nSuschat.\\nhttps://github.com/\\nSUSTech-IDEA/SUS-Chat, 2023.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\\nStanford\\nalpaca:\\nAn\\ninstruction-following\\nllama\\nmodel.\\nhttps://github.com/tatsu-lab/\\nstanford_alpaca, 2023.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`\\nere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation\\nlanguage models. 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. 2023b.\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\\nguage models are zero-shot learners.\\narXiv preprint\\narXiv:2109.01652, 2021.\\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\\nP., Dean, J., and Fedus, W. Emergent abilities of large\\nlanguage models. Journal of Machine Learning Research,\\n2022.\\nWei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. Magi-\\ncoder: Source code is all you need.\\narXiv preprint\\narXiv:2312.02120, 2023.\\nWu, X., Huang, S., and Wei, F. MoLE: Mixture of loRA\\nexperts. In International Conference on Learning Repre-\\nsentations, 2024.\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\\nC., and Jiang, D. Wizardlm: Empowering large language\\nmodels to follow complex instructions. In International\\nConference on Learning Representations, 2024.\\nYang, S., Hou, L., Song, X., Liu, Q., and Zhou, D. Speed-\\ning up deep model training by sharing weights and then\\nunsharing. arXiv preprint arXiv:2110.03848, 2021.\\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,\\nJ. T., Li, Z., Weller, A., and Liu, W. Metamath: Boot-\\nstrap your own mathematical questions for large language\\nmodels. arXiv preprint arXiv:2309.12284, 2023.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\\nY. Hellaswag: Can a machine really finish your sentence?\\narXiv preprint arXiv:1905.07830, 2019.\\nZhang, Y. and Yang, Q. A survey on multi-task learning.\\nIEEE Transactions on Knowledge and Data Engineering,\\n34(12):5586–5609, 2021.\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,\\nDai, A. M., Le, Q. V., Laudon, J., et al. Mixture-of-\\nexperts with expert choice routing. In Advances in Neural\\nInformation Processing Systems, 2022.\\n11\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nA. Appendix\\nA.1. Implementation Details\\nWe show the hyperparameters that we use for instruction tuning in Table 7.\\nTable 7. Hyperparameters of instruction tuning.\\nlr\\nepoch\\nLoRA r\\nLoRA α\\nAdapter Dim\\n2 × 10−4\\n1\\n64\\n16\\n512\\nA.2. Detailed Evaluation Results on Grouped Benchmarks.\\nWe show the detailed evaluation results of each grouped academic benchmark as follows:\\n• In Table 8, we report evaluation details of the MMLU benchmarks.\\n• In Table 9, we report results on the GSM8k and MATH benchmarks.\\n• In Table 10, we compare the results on the Human-Eval and MBPP benchmarks.\\n• In Table 11, we show results on several commonsense reasoning benchmarks.\\n• In Table 12, We evaluate the performance on the NaturalQuestions and TriviaQA benchmarks.\\nTable 8. Five-shot performance on the MMLU benchmark.\\nHumanities\\nSTEM\\nSocial Sciences\\nOther\\nAverage\\nLLaMA2-7B\\n43.2\\n36.9\\n51.7\\n52.6\\n45.7\\nLLaMA2-7B-chat\\n43.4\\n38.7\\n54.7\\n54.6\\n47.3\\nVicuna-7B\\n46.0\\n40.4\\n58.2\\n58.1\\n50.1\\nCamel-7B\\n43.9\\n38.5\\n55.9\\n54.6\\n47.7\\nCamelidae-8×7B\\n44.7\\n38.1\\n56.9\\n55.9\\n48.3\\nLLaMA2-13B\\n52.3\\n44.1\\n63.7\\n62.0\\n55.1\\nLLaMA2-13B-chat\\n50.3\\n43.9\\n62.6\\n60.3\\n53.8\\nVicuna-13B\\n52.1\\n44.6\\n65.3\\n63.5\\n55.8\\nCamel-13B\\n52.0\\n42.2\\n63.0\\n61.7\\n54.4\\nCamelidae-8×13B\\n52.1\\n43.3\\n62.6\\n61.1\\n54.4\\nYi-34B\\n71.3\\n67.3\\n85.4\\n80.2\\n75.5\\nYi-34B-chat\\n70.5\\n66.3\\n84.7\\n79.9\\n74.8\\nSUSChat-34B\\n72.2\\n69.6\\n85.5\\n80.5\\n76.4\\nCamel-34B\\n72.5\\n67.3\\n84.0\\n79.3\\n75.3\\nCamelidae-8×34B\\n72.8\\n66.7\\n83.8\\n80.4\\n75.6\\nCamelidae-8×34B-pro\\n73.8\\n66.0\\n83.8\\n80.3\\n75.7\\n12\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nTable 9. Comparison on math benchmarks.\\nGSM8K\\nMATH\\nAverage\\nLLaMA2-7B\\n16.7\\n3.3\\n10.0\\nLLaMA2-7B-chat\\n26.3\\n3.9\\n15.1\\nVicuna-7B\\n23.7\\n3.2\\n13.5\\nCamel-7B\\n40.7\\n4.8\\n22.8\\nCamelidae-8×7B\\n44.0\\n5.8\\n24.9\\nLLaMA2-13B\\n29.6\\n5.0\\n17.3\\nLLaMA2-13B-chat\\n37.1\\n5.2\\n21.2\\nVicuna-13B\\n37.5\\n4.4\\n21.0\\nCamel-13B\\n50.2\\n8.4\\n29.3\\nCamelidae-8×13B\\n52.6\\n9.8\\n30.7\\nYi-34B\\n67.9\\n15.9\\n41.9\\nYi-34B-chat\\n67.6\\n17.3\\n42.5\\nSUSChat-34B\\n72.3\\n22.0\\n47.2\\nCamel-34B\\n76.1\\n18.2\\n47.2\\nCamelidae-8×34B\\n78.3\\n22.6\\n50.5\\nCamelidae-8×34B-pro\\n79.4\\n24.0\\n51.7\\nTable 10. Comparison on code benchmarks.\\nHumanEval\\nMBPP\\nAverage\\nLLaMA2-7B\\n12.8\\n14.8\\n13.8\\nLLaMA2-7B-chat\\n12.2\\n17.6\\n14.9\\nVicuna-7B\\n7.3\\n11.8\\n9.6\\nCamel-7B\\n17.7\\n21.0\\n19.4\\nCamelidae-8×7B\\n18.3\\n23.4\\n20.9\\nLLaMA2-13B\\n18.9\\n26.8\\n22.9\\nLLaMA2-13B-chat\\n18.9\\n27.2\\n23.1\\nVicuna-13B\\n18.3\\n3.0\\n10.7\\nCamel-13B\\n28.7\\n30.3\\n29.5\\nCamelidae-8×13B\\n30.6\\n30.4\\n30.5\\nYi-34B\\n26.2\\n38.2\\n32.2\\nYi-34B-chat\\n20.1\\n41.0\\n30.4\\nSUSChat-34B\\n11.6\\n40.2\\n25.9\\nCamel-34B\\n42.1\\n40.6\\n41.4\\nCamelidae-8×34B\\n43.9\\n41.4\\n42.7\\nCamelidae-8×34B-pro\\n48.8\\n43.2\\n46.0\\nTable 11. Zero-shot performance on the various commonsense reasoning tasks.\\nPIQA\\nHellaSwag\\nWinoGrande\\nARC-e\\nARC-c\\nAverage\\nLLaMA2-7B\\n78.9\\n75.9\\n69.5\\n74.7\\n46.2\\n69.0\\nLLaMA2-7B\\n77.0\\n75.5\\n66.4\\n69.7\\n44.7\\n66.7\\nCamel-7B\\n78.0\\n73.7\\n69.3\\n71.3\\n45.8\\n67.6\\nCamel-7B\\n79.7\\n76.8\\n71.3\\n75.0\\n47.9\\n70.1\\nCamelidae-8×7B\\n79.9\\n76.8\\n72.1\\n75.0\\n49.6\\n70.7\\nLLaMA2-13B\\n80.7\\n80.8\\n71.9\\n77.4\\n48.9\\n71.6\\nLLaMA2-13B-chat\\n79.1\\n79.7\\n71.3\\n73.8\\n50.3\\n70.9\\nVicuna-13B\\n78.9\\n77.4\\n71.9\\n74.8\\n50.9\\n70.8\\nCamel-13B\\n80.9\\n79.8\\n74.6\\n77.7\\n54.3\\n73.5\\nCamelidae-8×13B\\n80.9\\n80.1\\n74.7\\n78.8\\n54.2\\n73.8\\nYi-34B\\n82.9\\n83.7\\n78.9\\n84.1\\n61.6\\n78.2\\nYi-34B-chat\\n79.9\\n80.7\\n77.1\\n74.3\\n54.6\\n73.3\\nSUSChat-34B\\n82.0\\n83.0\\n81.0\\n84.8\\n63.0\\n78.8\\nCamel-34B\\n82.3\\n82.6\\n80.0\\n86.1\\n63.6\\n78.9\\nCamelidae-8×34B\\n82.7\\n83.2\\n80.9\\n86.2\\n65.2\\n79.7\\nCamelidae-8×34B-pro\\n83.6\\n82.5\\n80.1\\n86.6\\n63.3\\n79.2\\nTable 12. Comparison on the exact match performance of world knowledge tasks (zero-shot).\\nNaturalQuestions\\nTriviaQA\\nAverage\\nLLaMA2-7B\\n19.1\\n52.8\\n36.0\\nLLaMA2-7B-chat\\n19.6\\n46.4\\n33.0\\nVicuna-7B\\n15.6\\n42.8\\n29.2\\nCamel-7B\\n17.6\\n51.0\\n34.3\\nCamelidae-8×7B\\n17.8\\n51.0\\n34.4\\nLLaMA2-13B\\n24.8\\n59.4\\n42.1\\nLLaMA2-13B-chat\\n25.0\\n55.0\\n40.0\\nVicuna-13B\\n25.8\\n56.3\\n41.1\\nCamel-13B\\n24.7\\n57.5\\n41.1\\nCamelidae-8×13B\\n26.8\\n59.4\\n43.1\\nYi-34B\\n33.5\\n62.1\\n47.8\\nYi-34B-chat\\n23.7\\n52.3\\n38.0\\nSUSChat-34B\\n20.4\\n56.1\\n38.3\\nCamel-34B\\n31.6\\n63.3\\n47.5\\nCamelidae-8×34B\\n32.2\\n63.4\\n47.8\\nCamelidae-8×34B-pro\\n31.2\\n62.5\\n46.9\\n13\\n', 'source_name': 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks', 'source_url': 'https://arxiv.org/abs/2401.02731'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FastInferenceMoE_NOTES.pdf #45\n",
      "{'content': 'Fast-Inference of Mixture-of-Experts Language Models with Offloading \\nMain Idea: this paper investigates a strategy to run large MoE in consumer hardware with limited \\naccelerator memory. It succeeds in running Mixtral 8x7B in a free-tier Google Colab instance. \\n \\nMethod for MoE Generative Inference \\n1. Encoding the input prompt. \\na. Done in parallel (layer-by-layer). \\n2. Generate tokens conditioned on the input prompt. \\na. Done sequentially (token-by-token and layer-by-layer). \\nIn other words, step 1 is easy to optimize since we can simply pass all tokens in parallel layer-by-\\nlayer. During token generation this is not possible since we need to pass one token at a time, \\nmaking the offloading challenging to optimize for. \\n \\nImprovements from this approach \\n- \\nCaching experts \\no To exploit the fact that previous work shows that activated experts tend to be \\nactive for more than one token at a time (common for them to stay active for 2-4 \\ntokens in a row), the experts activated from the previous token can simply be \\nstored in a GPU cache. \\n- \\nPrefetching \\no With dense models, offloading is simple due to the fixed order of layers to load. \\nThis is not true in MoE, so future layers cannot be pre-loaded since they are usually \\nselected based on the previous layer’s output. To help with this, a speculative \\nloading technique is developed based on the heuristic that the previous layer’s \\nhidden state can be a good proxy for the next hidden state (since these hidden \\nstates are only updated and not recomputed from scratch). This allows us to \\npredict the next layer’s experts before knowing its hidden state (in case of wrong \\nguesses, the gains are lost since we must load the experts while no computations \\nare being done). \\n- \\nIn terms of quantization, HQQ (data-free) is used for convenience, however, other \\ntechniques such as GPTQ could also work. (QMoE was experimented with on Mixtral \\n8x7B, but loss in quality was too significant due to the 1-bit quantization). \\no Found that ideally experts can be quantized to 3 or even 2 bits and that attention \\nlayers should be kept at a larger bit width (16 or minimum 4 bits). \\n- \\nFor expert offloading, a cache of 2 experts per layer is used with 12 GB GPUs and of 4 for \\n16 GB GPUs. Additionally, 1 or 2 experts per layer are loaded speculatively as soon as the \\nprevious layer’s hidden states are available. \\n \\nResults (on Mixtral 8x7B) \\n- \\nOn the free Google Colab tier, inference speed is of around 2 tokens per second. \\n- \\n In terms of cache hits, the accuracy to guess the next expert goes from around 0.2 with \\ncache size of 1 to around 0.6-0.7 for cache size of 4. \\n- \\nFor speculative loading the results are even better and show that active experts can be \\nestimated even when 10 layers ahead (that is, using the hidden state of the 10th hidden \\nlayer behind it). \\n \\nMy takeaways: \\n- \\nAble to use this method to experiment with Mixtral in Google Colab. \\n', 'source_name': 'Fast-Inference of Mixture-of-Experts Language Models with Offloading', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FastInferenceMoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "PE_MoE_for_LMs.pdf #46\n",
      "{'content': 'Parameter-Efﬁcient Mixture-of-Experts Architecture\\nfor Pre-trained Language Models\\nZe-Feng Gao1,4,5∗, Peiyu Liu1,4∗, Wayne Xin Zhao1,4† , Zhong-Yi Lu2 and Ji-Rong Wen1,3,4\\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China\\n2Department of Physics, Renmin University of China\\n3 School of Information, Renmin University of China\\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\\n5Beijing Academy of Artiﬁcial Intelligence, Beijing, 100084, China\\n{zfgao,liupeiyustu,zlu,jrwen}@ruc.edu.cn, batmanﬂy@gmail.com\\nAbstract\\nRecently, Mixture-of-Experts (short as MoE)\\narchitecture has achieved remarkable success\\nin increasing the model capacity of large-\\nscale language models.\\nHowever, MoE re-\\nquires incorporating signiﬁcantly more param-\\neters than the base model being extended. In\\nthis paper, we propose building a parameter-\\nefﬁcient MoE architecture by sharing informa-\\ntion among experts. We adopt matrix product\\noperator (MPO, a tensor decomposition from\\nquantum many-body physics) to reconstruct\\nthe parameter matrix in the expert layer and in-\\ncrease model capacity for pre-trained language\\nmodels by sharing parameters of the cen-\\ntral tensor (containing the core information)\\namong different experts while enabling the\\nspeciﬁcity through the auxiliary tensors (com-\\nplementing the central tensor) of different ex-\\nperts.\\nTo address the unbalanced optimiza-\\ntion issue, we further design the gradient mask\\nstrategy for the MPO-based MoE architecture.\\nExtensive experiments based on T5 and GPT-2\\nshow improved performance and efﬁciency of\\nthe pre-trained language model (27.2x reduc-\\ntion in total parameters for the superior model\\nperformance, compared with the Switch Trans-\\nformers).\\nOur code is publicly available at\\nhttps://github.com/RUCAIBox/MPOE.\\n1\\nIntroduction\\nLarge-scale pre-trained language models (PLMs),\\nsuch as BERT (Devlin et al., 2018) and T5 (Raffel\\net al., 2020), have become the de facto standard\\nin natural language processing (NLP). By involv-\\ning a huge number of parameters pre-trained on\\nthe general-purpose corpus, PLMs can achieve ex-\\ncellent performance in many NLP tasks. In order\\nto increase the model capacity, a promising direc-\\ntion is to explore the scaling properties with the\\nmixture-of-experts (MoE) paradigm (Jacobs et al.,\\n∗Authors contributed equally.\\n†Corresponding author.\\n1991; Shazeer et al., 2017) for developing more\\npowerful PLMs. By incorporating multiple expert\\nnetworks, MoE schedules the learning of data sam-\\nples through a routing component that is usually\\nimplemented by some gating function, which in-\\ncreases model capacity without a proportional in-\\ncrease in computation costs. Despite the effective-\\nness, it has been shown that the MoE architecture\\nis parameter inefﬁcient (Zuo et al., 2021), consid-\\nering the yielded improvement w.r.t. the involved\\ncosts. Most of the existing studies (Yang et al.,\\n2021; Roller et al., 2021; Lewis et al., 2021) at-\\ntribute this issue to the unbalanced load of experts,\\nfocusing on improving the routing strategies.\\nHowever, an important question about the MoE\\narchitecture has been neglected in previous studies:\\nwhether the increased parameters from the experts\\nare all necessary to increase the model capacity. As\\ndifferent experts from an MoE network are often\\ntrained with correlated data samples (e.g., sample\\ncorrelation from training data), it is likely to lead to\\nparameter redundancy across experts. Indeed, ex-\\npert redundancy has been identiﬁed in existing stud-\\nies, where Fedus et al. (2021) distills sparse MoE\\nmodels into dense models and Kim et al. (2021)\\nprunes experts to compress MoE models. This ﬁnd-\\ning motivates us to develop a parameter-efﬁcient\\nMoE architecture by reducing its parameter redun-\\ndancy. Intuitively, a straightforward approach is\\nto share a certain proportion of parameters among\\nexperts. However, it is difﬁcult to identify and op-\\ntimize the key parameters that encode the shared\\ninformation across experts, since expert networks\\ntypically consist of dense matrices.\\nTo address this issue, we propose a novel param-\\neter sharing approach inspired by the matrix prod-\\nuct operators (MPO) decomposition from quantum\\nmany-body physics (Gao et al., 2020), which de-\\ncomposes a matrix into a sequential product of\\nlocal tensors (either central or auxiliary tensors).\\nUnlike other matrix decomposition methods, MPO\\narXiv:2203.01104v4  [cs.CL]  10 Oct 2022\\ncan effectively reorganize and aggregate important\\ninformation of the original matrix into the central\\ntensor. The auxiliary tensors, on the other hand,\\nserve to complement the central tensor for recov-\\nering the original matrix (Liu et al., 2021). In the\\nsetting of MoE, considering the small parameter\\nvariations among experts, we speculate that the\\ncentral tensors of different experts (with MPO de-\\ncomposition for each expert) are likely to be very\\nsimilar. If the central tensors could be shared for\\nall expert networks, we would signiﬁcantly reduce\\nthe parameters of the MoE architecture.\\nTo this end,\\nwe propose a novel MPO-\\nbased\\nparameter-efﬁcient\\nMoE\\narchitecture,\\ncalled MPOE. Based on classic MoE architec-\\nture (Shazeer et al., 2017), our approach introduces\\na major extension allowing experts to share a\\nglobal central tensor while keeping expert-speciﬁc\\nauxiliary tensors. In our setting, the parameter\\nmatrix of in a single expert is formed by the\\nproduct of the globally shared central tensor and\\nthe corresponding auxiliary tensors.\\nSince the\\ncentral tensor contains most of the parameters\\nfrom an MPO decomposition, our MPOE approach\\ncan signiﬁcantly reduce the parameters of the\\noriginal MoE architecture. Another major merit of\\nMPO is that auxiliary tensors are closely entangled\\nwith the central tensor (Pirvu et al., 2010), and\\nit is theoretically guaranteed that any change\\nfrom auxiliary tensors can be propagated to the\\ncentral tensor.\\nThat is to say, though a large\\nproportion of parameters are shared, local auxiliary\\ntensors still enable the experts to capture speciﬁc\\nvariations or differences according to routing\\ndata samples.\\nHowever, directly optimizing\\nthe MPOE architecture is likely to lead to an\\nunbalanced optimization issue, where the central\\ntensors are updated more frequently than auxiliary\\ntensors during ﬁne-tuning. Therefore, we further\\npropose a gradient mask strategy that masks the\\ncentral tensor gradient to effectively alleviate the\\nunbalanced optimization issue.\\nTo the best of our knowledge, this is the ﬁrst\\nattempt to reduce the parameter redundancy of the\\nMoE architecture with structural matrix decompo-\\nsition. We conduct extensive experiments to evalu-\\nate the effectiveness of the MPOE architecture on\\ntwo representatives PLMs, T5 and GPT. Experi-\\nments have demonstrated the effectiveness of our\\napproach in increasing model capacity (27.2x fewer\\nparameters for the superior model performance,\\ncompared with several competitive MoE-enhanced\\nPLMs.\\n2\\nPreliminary\\n2.1\\nMixture-of-Experts (MoE)\\nWe ﬁrst describe the mixture-of-experts architec-\\nture (MoE) (Shazeer et al., 2017), which has been\\nused to enhance the model capacity of Transformer\\nbased models. Let G(x) and Ei(x) denote the out-\\nput vectors of the gating network and the output\\nof the i-th expert network for a given input x, re-\\nspectively. The output of MoE architecture y can\\nbe formally computed as:\\ny =\\nn\\nX\\ni=1\\nG(x) · Ei(x).\\n(1)\\nThe softmax function is widely adopted as the\\ngating function G(x). The sparsely-gated MoE\\narchitecture, which uses a noisy top-k gating mech-\\nanism to reduce the computational cost, has been\\nproposed in Shazeer et al. (2017). It adds tunable\\nGaussian noise with H(·), and then keeps only the\\ntop-k values with KeepTopK(·) and sets the rest\\n−∞. This keeps only the top k experts to be evalu-\\nated with:\\nG(x) = softmax(KeepTopK(H(x), k)).\\n(2)\\nFurthermore, Switch Transformer designs a\\nswitch routing strategy to simplify this gating func-\\ntion by routing to a single expert (Fedus et al.,\\n2021).\\n2.2\\nTensor and Matrix Product Operators\\nWe refer to one-dimensional arrays as vectors (de-\\nnoted by bold lowercase letters, e.g., v), two-\\ndimensional arrays as matrices (denoted by bold\\nuppercase letters, e.g., W), and arrays of higher di-\\nmensions as tensors (denoted by calligraphic bold\\nuppercase letters, e.g., T ).\\nMPO decomposition (Oseledets, 2011) (a.k.a.\\ntensor-train decomposition) has been a widely used\\nmatrix decomposition technique from quantum\\nmany-body physics, which decomposes a matrix (2-\\norder tensor) into m local tensors (Pirvu et al.,\\n2010). Given a matrix WI×J ∈RI×J, the MPO\\ndecomposition is given in the following format:\\nMPO (W) =\\nm\\nY\\nk=1\\nT(k)[dk−1, ik, jk, dk],\\n(3)\\n𝑾𝑰×𝑱\\n 𝑨𝟏\\n 𝑨𝟐\\n 𝑨𝟑\\n 𝑨𝟒\\n 𝑪\\nMPO\\n%(\\n%)\\n%*\\n%+\\nFigure 1: MPO decomposition for matrix WI×J with\\nﬁve local tensors. Auxiliary tensors ({Ai}4\\ni=1) and cen-\\ntral tensor (C) are marked in orange and blue, respec-\\ntively.\\nwhere I = Qn\\nk=1 ik and J = Qn\\nk=1 jk, T(k) is a\\n4-order tensor with size dk−1 × ik × jk × dk. The\\ndk is dimension of bond linking T(k) and T(k+1).\\nAccording to Gao et al. (2020), the original ma-\\ntrix W can be exactly reconstructed by tensor con-\\ntraction of MPO(W) without truncation of the con-\\nnection bond {dk}m\\nk=1. Figure 1 presents the illus-\\ntration of the MPO decomposition procedure for a\\nmatrix (m = 5). More detailed analysis on differ-\\nent factorization ways (i.e., m = 3, 5, 7, 9) will be\\ngiven in Section 4.5. After MPO decomposition,\\nthe central tensor (the tensor right in the middle)\\nwith most of the parameters can encode the core\\ninformation of the original matrix, while the aux-\\niliary tensors (the rest of these tensors) with only\\na small proportion of parameters play the role of\\ncomplementing the central tensor.\\n3\\nApproach\\nTo reduce the information redundancy across dif-\\nferent experts, we design an MPO-based MoE ar-\\nchitecture for increasing the model capacity in a\\nparameter-efﬁcient way. We ﬁrstly describe the\\nMPO-based MoE architecture and then introduce\\nan improved optimization algorithm for learning\\nthe parameters in this architecture.\\n3.1\\nMPO-based Mixture-of-Experts\\nPrevious MoE architecture (Jacobs et al., 1991;\\nShazeer et al., 2017) usually treats different ex-\\nperts as individual components, requiring a com-\\npete copy of network parameters for each expert.\\nAlthough it has been found (Fedus et al., 2021; Kim\\net al., 2021) that there exists redundant information\\namong different experts in the MoE architecture,\\nit is not easy to identify the shareable parameters\\nfrom the highly coupling network.\\nConsidering this issue, our solution is inspired\\nby an important merit of MPO decomposition: it\\ncan reorganize and aggregate the core information\\nin central tensors (Gao et al., 2020) as aforemen-\\ntioned. Based on this property, the core idea of\\nour approach is to share the central tensors for all\\nthe expert layers and enable speciﬁcity via expert-\\nspeciﬁc auxiliary tensors.\\nParameter-Efﬁcient MoE Architecture.\\nThe\\nTransformer network consists of two major neu-\\nral components, namely FFN and multi-head at-\\ntention. Following previous work on MoE-based\\nPLMs (Shazeer et al., 2017; Fedus et al., 2021),\\nwe consider FFN layers as experts to be extended,\\nwhile our approach is generally applicable to vari-\\nous matrix-based model components. A straightfor-\\nward method to reducing information redundancy\\nis to share a proportion of parameters across ex-\\nperts. However, in Transformer-based networks,\\nthe experts (i.e., FFN here) are mainly composed\\nof large dense matrices, which are difﬁcult for shar-\\ning partial parameters from these matrices. As our\\nsolution, we consider parameter sharing through\\nthe MPO decomposition, so that the derived central\\ntensors can be ﬂexibly shared across matrices.\\nLightweight MoE Design. Speciﬁcally, we sim-\\nplify the discussion by assuming that an expert\\ncorresponds to one parameter matrix at each layer,\\nand it is similar for the multi-matrix cases. We\\nconsider a MoE architecture of n experts each\\nwith L layers, so that there are L × n matrices\\nin total, denoted by {W(l,i)}L,n\\nl=1,i=1. As discussed\\nin Section 2.2, a matrix can be decomposed into\\nm tensors, consisting of one central tensor and\\nm −1 auxiliary tensors. In this work, we con-\\nsider ﬁve decomposed tensors, i.e., m = 5. At\\nthe l-th layer, the decomposition results can be\\ndenoted by {C(l,i), A(l,i)\\n1\\n, A(l,i)\\n2\\n, A(l,i)\\n3\\n, A(l,i)\\n4\\n}n\\ni=1,\\nwhere C(l,i) and A(l,i)\\n·\\nare the central and auxil-\\niary tensors of the i-th parameter matrix, respec-\\ntively, at the l-th layer. To develop the MPO-based\\nMoE architecture, the core idea is to share the cen-\\ntral tensors as global parameters and keep expert-\\nspeciﬁc auxiliary tensors as local parameters, i.e.,\\nC(l,1) = C(l,2) · · · = C(l,n) (∀l = 1 · · · L), and we\\ndenote the global central tensor at the l-th layer by\\nC(l). In this way, we can only keep L central cen-\\nsors for a L-layer MoE architecture. For MPO, the\\ndecomposition process is transparent to external\\nmodules, so that we can reuse the previous rout-\\ning mechanism (Section 2.1) by distributing data\\nsamples to different experts. A slight difference is\\nthat we only need to consider the routing to local\\ntensors for each matrix since the global tensor is\\nshared across experts. We call such an MPO-based\\nMoE architecture as MPOE.\\n(a) MPO-based mixture-of-experts architecture.\\n(b) Gradient mask strategy.\\nFigure 2: Illustration the proposed MPOE architecture and gradient mask strategy. We decompose the weight\\nmatrix of each expert in the MoE architecture into ﬁve local tensors using MPO, containing four auxiliary tensors\\nand one central tensor, which are marked in orange and blue, respectively. In our approach, the central tensor of the\\nn experts is shared in the MPOE architecture. During optimization, each backward propagation process updates\\na set of auxiliary tensors while updating the central tensor with a probability of pb (the mask probability of the\\ncentral tensor), which can effectively avoid the unbalanced optimization of the central tensor.\\nDiscussion. Since the central tensor contains most\\nof the information from original parameter matri-\\nces (Gao et al., 2020), a key question is whether\\nthe current architecture enables sufﬁcient ﬂexibil-\\nity and speciﬁcity for each expert.\\nTo answer\\nthis question, we refer to an important property\\nof MPO decomposition from quantum many-body\\nphysics (Pirvu et al., 2010): it is guaranteed in\\nprinciple, that any change on one tensor will be\\npropagated to the entire local tensor set. In other\\nwords, only tuning the auxiliary tensors (keeping\\nthe central tensor ﬁxed) can lead to the same effect\\nas tuning the whole matrix. Since the parameters of\\nthe central tensor are shared, our approach can sig-\\nniﬁcantly reduce the number of actual parameters\\ngiven the MoE architecture with the same number\\nof experts. Assuming the original model consisting\\nof n experts with T parameters each, we have a\\ntotal number of n · T parameters. Speciﬁcally, let\\nγ denote the parameter ratio of the auxiliary tensor\\nto the central tensor for expert networks. Given the\\ntotal number T for an expert network, the central\\nand auxiliary tensors correspond to the parameters\\nnumbers of\\nγ\\nγ+1T and\\n1\\nγ+1T, respectively. Since\\nour MPOE approach shares the central tensor, the\\nﬁnal number of parameters will be\\nγ\\nγ+1T +\\nn\\nγ+1T.\\nThus, our MPOE approaches corresponds to a ratio\\nof\\nn+γ\\nn(γ+1) of the original parameter scale. In our\\nexperiments, the ratio γ is about 12, and\\nn+γ\\nn(γ+1)\\napproximately equals to 0.19 when n = 8. Such a\\nratio will be further decreased when we have more\\nexperts. It can be seen that our MPOE approach is\\nable to effectively reduce the parameter scale.\\n3.2\\nAlleviate Unbalanced Optimization\\nAs the experts share the central tensor in the MPOE\\napproach, the corresponding parameters of the cen-\\ntral tensor will be updated more frequently than\\nthose in the auxiliary tensors during ﬁne-tuning. It\\ntends to lead to the unbalanced optimization issue\\nas reported by Xu et al. (2021), due to deviation\\nfrom the pre-trained weights. As a result, it is\\ncrucial to develop a more stable optimization tech-\\nnique that is suited to the MPOE architecture.\\nInspired by the solution of gradient dropout strat-\\negy (Tseng et al., 2020; Xu et al., 2021), we pro-\\npose to mask the gradients for the central tensor\\nto improve model optimization for the MPO-based\\nMoE architecture. At each iteration, we take a\\ncertain probability pb to discard the update in the\\ncentral tensor. This can effectively alleviate the\\nunbalanced optimization which is caused by the\\nfrequent updates of the central tensor. Speciﬁcally,\\nwe generate a binary mask b drawn from Bernoulli\\ndistribution with a mask probability pb, which can\\nbe calculated by b ∼Bernoulli(pb). We denote\\nthe ∆C as the update of the central tensor at each\\niteration:\\n∆C = η∂L(C)\\n∂C\\n⊙(1 −b).\\n(4)\\nThe larger pb is, the less frequently the central ten-\\nsor is updated. In particular, when pb is equal to 1,\\nit means that the parameters of the central tensor\\nare frozen for each input of the data. The compu-\\ntational cost of central tensor update can be also\\nreduced with this trick.\\nNote that the gradient mask trick is only applied\\nto central tensors. For auxiliary tensors, we per-\\nform the standard gradient update for learning the\\nparameters. Compared with two alternative ways to\\nimplement the gradient mask technique, i.e., mask\\npre-activation or post-activation in FFN layers, we\\nﬁnd that such a sampling-based masking strategy\\ncan effectively improve the model performance in\\nour experiments.\\n3.3\\nThe Overall Algorithm Procedure\\nOur approach can be generally applied to various\\nMoE-based models for increasing the model ca-\\npacity. In this work, we adopt the MoE-extended\\nPLMs (Radford et al., 2019) for study.\\nAlgorithm 1 presents a complete procedure for\\nthe proposed update procedure, which can be\\nbrieﬂy summarized as follows. First, we obtain the\\nPLM and perform MPO decomposition for each\\nweight matrix of the FFN layers in the Transformer.\\nFor each weight matrix, we decompose it into one\\ncentral tensor C and a list of auxiliary tensors A.\\nIn the original MoE architecture, we will have n\\nsets of such decomposed parameters. Next, the key\\npoint lies in that we share the central tensor C in\\nthe decomposition process but keep expert-speciﬁc\\nauxiliary tensors. In this way, each expert is com-\\nposed of a set of auxiliary tensors and a shared\\ncentral tensor. To recover the original FFN matrix\\nin some speciﬁc expert, we can simply multiply the\\nshared central tensor by expert-speciﬁc auxiliary\\ntensors. Then, we apply the gradient mask strat-\\negy to update the parameters in these experts, i.e.,\\nmasking the gradient of the central tensor.\\nSince the parameters of the central tensor are\\ntwo orders of magnitude larger than the parameters\\nof the auxiliary tensors (Liu et al., 2021), the cost\\nof MoE-based networks will be largely reduced by\\nsharing the central tensor.\\n3.4\\nDiscussion\\nFor the parameter inefﬁciency issue of MoE-based\\nnetworks, existing studies mainly focus on alleviat-\\ning the unbalanced load of experts, which have pro-\\nposed different routing methods to balance the rout-\\ning probabilities of different experts, such as BASE-\\nLayer (Lewis et al., 2021), HASHLayer (Roller\\net al., 2021), GShard (Lepikhin et al., 2021) and\\nSwitch Transformers (Fedus et al., 2021). As a\\ncomparison, we aim to reduce information redun-\\ndancy by sharing common parameters among ex-\\nperts. Actually, the MPOE approach can be further\\nAlgorithm 1 The proposed updating procedure.\\nRequire: {{Aj}4\\nj=1, C}: Initialize experts\\nRequire: α: learning rate\\nRequire: pb: mask probability\\nRequire: time step t ←0 (Initialize timestep)\\n1: while not converged do\\n2:\\nt ←t + 1\\n3:\\ngt\\nC ←∂L(Ct)\\n∂(Ct) ,\\ngt\\nA ←∂L(At)\\n∂(At)\\n(Get gradients at timestep t)\\n4:\\nb ←GenerateMask(pb)\\n(Compute gradient mask)\\n5:\\nCt ←Ct−1 −α · gt\\nC ⊙(1 −b)\\n(Update central tensors)\\n6:\\nAt ←At−1 −α · gt\\nA\\n(Update the routed auxiliary tensors)\\n7: end while\\n8: return {{At\\nj}4\\nj=1, Ct} (Resulting parameters)\\nenhanced with existing improved routing methods.\\nSpeciﬁcally, Deepspeed-MoE proposed to use\\npyramid residual MoE architecture to reduce the\\nparameters of the MoE architecture (Rajbhandari\\net al., 2022), while our work takes a different per-\\nspective to improve the original MoE architecture\\nby sharing parameters among different experts.\\n4\\nExperiments\\nIn this section, we ﬁrst set up the experiments and\\nthen report the results and analysis. Then, we con-\\nduct a detailed analysis under different experimen-\\ntal settings. Here, we use T5 (Raffel et al., 2020)\\nand GPT-2 (Radford et al., 2019) models as the\\nbase model in our experiments.\\n4.1\\nExperimental Setup\\nDatasets.\\nTo evaluate the effectiveness of the pro-\\nposed MPOE as an efﬁcient strategy to improve\\nthe model capacity of PLMs, we follow the set-\\nting of T5 and GPT-2 to perform experiments on\\nNatural Language Understanding (NLU) and Nat-\\nural Language Generation (NLG) tasks. Speciﬁ-\\ncally, we evaluate the NLG tasks in GLUE bench-\\nmark (Wang et al., 2018), the language modeling\\ntask with WikiText-2 (Merity et al., 2017), the text\\ngeneration task with IMDB (Maas et al., 2011) and\\nEMNLP2017 WMT News (Guo et al., 2018). Fur-\\nthermore, we follow the setup of Raffel et al. (2020)\\non the GLUE benchmark for a direct comparison\\nwith the T5 model.\\nGLUE benchmark covers multiple datasets\\nNLU with T5\\nExperiments\\nMNLI\\nQNLI\\nSST-2\\nRTE\\nQQP\\nCoLA\\nMRPC\\nSTS-B\\nAvg.\\n#To (M)\\nT5-Large\\n89.23\\n94.03\\n96.20\\n83.94\\n91.54\\n55.10\\n90.15\\n91.90\\n86.51\\n737\\n+MoEﬁcation♦\\n87.50\\n93.20\\n95.40\\n86.40\\n90.20\\n55.50\\n87.50\\n90.60\\n85.79\\n737\\n+MoEﬁcation++ ♦\\n88.70\\n93.60\\n96.20\\n87.50\\n91.30\\n59.40\\n89.30\\n91.00\\n87.13\\n737\\n+Switch♣\\n/\\n/\\n/\\n/\\n/\\n/\\n/\\n/\\n88.50\\n26000\\n+MPOE\\n87.16\\n94.12\\n96.80\\n88.60\\n90.63\\n67.63\\n93.65\\n91.97\\n88.82\\n956\\nT5-Base\\n87.78\\n93.82\\n94.72\\n71.74\\n91.11\\n53.49\\n89.16\\n91.16\\n84.12\\n223\\n+Switch♣\\n/\\n/\\n/\\n/\\n/\\n/\\n/\\n/\\n86.70\\n3800\\n+Switch♠\\n87.73\\n93.85\\n94.87\\n77.53\\n91.59\\n59.90\\n91.64\\n91.16\\n86.03\\n1015\\n+MoE⋆\\n86.98\\n92.82\\n94.60\\n69.56\\n90.02\\n64.56\\n87.68\\n90.89\\n84.64\\n1015\\n+MPOE\\n87.60\\n93.30\\n94.81\\n77.13\\n90.81\\n65.53\\n93.14\\n91.17\\n86.69\\n294\\n+MPOE++\\n87.78\\n93.93\\n94.83\\n77.42\\n91.61\\n65.90\\n91.14\\n91.65\\n86.78\\n365\\nNLG with GPT-2\\nExperiments\\nWikiText-2\\nEMNLP News\\nIMDB\\n#To (M)\\nPPL (↓)\\nBLEU-2\\nBLEU-4\\nBLEU-2\\nSelf-BLEU-2\\nBLEU-2\\nSelf-BLEU-2\\nGPT-2\\n21.27\\n28.69\\n9.46\\n62.61\\n74.67\\n73.12\\n83.85\\n124\\n+MoE⋆\\n21.86\\n28.27\\n9.14\\n65.27\\n79.79\\n74.46\\n90.01\\n578\\n+Switch♠\\n21.25\\n28.71\\n9.44\\n64.62\\n81.11\\n75.35\\n91.82\\n578\\n+MPOE\\n20.72\\n28.78\\n9.51\\n66.99\\n83.10\\n76.30\\n92.72\\n157\\n+MPOE++\\n20.73\\n28.82\\n9.57\\n68.49\\n83.11\\n76.82\\n93.08\\n171\\nTable 1: Performance comparison of different models on NLU and NLG tasks (in percent). “#To (M)” denote the\\nnumber (in millions) of total parameters. We set the number of experts n = 8 in these models, MPOE. Furthermore,\\nwe use n = 16 for a more powerful version of our approach, denoted by MPOE++. We report the average test\\nperformance of three runs, and the best results are highlighted in bold. ♦: Experimental results by Zhang et al.\\n(2021b) ♣: Experimental results by Fedus et al. (2021) ♠: Our re-implementation by Fedus et al. (2021). ⋆:Apply\\nmethod by Shazeer et al. (2017).\\n(MRPC, QQP, SST-2, MNLI, RTE, QNLI, CoLA)1.\\nThe original test sets are not publicly available, and\\nfollowing Zhang et al. (2021a), for datasets fewer\\nthan 10K samples (RTE, MRPC, STS-B, CoLA),\\nwe divide the original validation set in half, using\\none half for validation and the others for the test.\\nEvaluation\\nMetrics.\\nWe\\nuse\\nperplex-\\nity (PPL) (Brown et al., 1992) to measure\\nhow well the probability model predicts a sample\\ncompared with the ground-truth.\\nTo evaluate\\nthe ratios of the overlapping n-grams between\\ngenerated and real samples, we use BLEU-n\\nscore (Papineni et al., 2002). We also take into\\naccount the Self-BLEU-n score (Zhu et al.,\\n2018) to evaluate the diversity of generated\\nsamples especially. For metrics used in the GLUE\\nbenchmark, we follow Mahabadi et al. (2021) and\\nuse Matthew’s correlation for CoLA, Pearson for\\nSTS-B, and accuracy for the other tasks.\\nComparison methods.\\nWe adopt the T5 and\\nGPT-2 as the base architectures for both MoE and\\nMPOE. Following Shazeer et al. (2017), we ex-\\n1Following Raffel et al. (2020), as a common practice, due\\nto the adversarial nature of WNLI with respect to the training\\nset, we do not experiment with WNLI\\ntend the FFN components with the MoE archi-\\ntecture containing n experts in each Transformer\\nblock of the T5 and GPT-2 model. We refer to this\\nmethod as “+MoE”. The Switch Transformers (Fe-\\ndus et al., 2021) use a simpliﬁed strategy that routes\\nto only a single expert instead of top-2 routing in\\nMoE. We refer to this method as “+Switch”. To\\nensure a fair comparison, we maintain the same\\nnumber (n = 8) of experts for baselines and\\nMPOE. We also implement an enhanced version of\\nMPOE with n = 16 experts, which is referred to as\\n“+MPOE++”. Based on the released gpt2 model2,\\nt5-base model3 and t5-large model4 provided by\\nHuggingface, we ﬁrst initialize the experts, then\\nﬁne-tune the models on the downstream tasks. For\\nthe T5 model, we follow the setting in Mahabadi\\net al. (2021) and ﬁne-tune all parameters of the\\nmodel on all tasks. For different downstream tasks,\\nwe run a hyperparameter sweep and select the best\\nconﬁguration according to the accuracy results on\\nthe validation set. The hyperparameters that we\\ntune include the epochs, batch size and learning\\nrates.\\n2https://huggingface.co/gpt2\\n3https://huggingface.co/t5-base\\n4https://huggingface.co/t5-large\\n4.2\\nMains Results\\nIn our main experiments, we adopt T5 (Raffel\\net al., 2020), GPT-2 (Radford et al., 2019), Switch\\nTransformers (Fedus et al., 2021) and MoEﬁca-\\ntion (Zhang et al., 2021b) as baselines, and report\\nthe comparison results of both NLU and NLG tasks\\nin Table 1.\\nOverall, compared to these MoE variants,\\nour proposed MPOE approach achieves perfor-\\nmance improvement while being more parameter-\\nefﬁcient. For the NLU task, our proposed approach\\n(“+MPOE”) outperforms the best baseline method,\\ni.e., “+Switch” (88.82 vs. 88.50 for T5-Large) with\\nup to 27.2x reduction in total parameters in the\\nGLUE benchmark. By zooming into low-resource\\ndatasets such as CoLA and MRPC, our approach\\nyields more signiﬁcant improvements. This sug-\\ngests that sharing parameters across experts rein-\\nforces the positive transfer effects5 of informa-\\ntion from other datasets toward the learning of\\nlow-resource datasets. For the NLG task, GPT-\\n2+MPOE achieves gains in BLEU-2 score (1.72\\nfor GPT-2+MoE and 2.37 for GPT-2+Switch) with\\n3.7x reduction in total parameters on the EMNLP\\nNews dataset. This indicates that GPT-2 also bene-\\nﬁts from sharing central tensors.\\nMoreover, T5+MPOE++ and GPT-2+MPOE++\\nperform better when we add more auxiliary tensors\\nas additional experts. This demonstrates the neces-\\nsity of improving model capacity (Shazeer et al.,\\n2017), as more parameters of experts tend to result\\nin an improved model capacity.\\n4.3\\nEvaluation on Multi-task Learning\\nTo demonstrate the efﬁciency of MPOE in multi-\\ntask learning, we adopt the T5-Base model for anal-\\nysis to be comparable with Hyperformer (Mahabadi\\net al., 2021). We conduct experiments on the multi-\\ntask GLUE benchmark. The detailed metrics can\\nbe found in Section 4.1. Note that compared to Hy-\\nperformer, MPOE approach does not incorporate\\nadditional neural network components, thus it is\\nmore ﬂexible to be used with the PLMs.\\nTable 2 shows the results on GLUE benchmark\\nfor T5-base (Raffel et al., 2020), Hyperformer (Ma-\\nhabadi et al., 2021) and MPOE. As we can see, the\\nperformance of the MPOE approach is consistently\\nbetter than the Hypernetwork in all cases, while\\n5Here, the positive transfer effects can be referred to Ma-\\nhabadi et al. (2021), which means that the transferred knowl-\\nedge can lead to improved performance for unseen in-domain\\ntasks.\\nDatasets\\nT5-Base\\nHyper♣\\n+MPOE\\nMNLI (acc)\\n87.73\\n85.74\\n87.83\\nQNLI (acc)\\n93.51\\n93.02\\n93.89\\nSST-2 (acc)\\n92.50\\n94.03\\n94.73\\nRTE (acc)\\n75.41\\n75.36\\n75.51\\nQQP (acc)\\n91.12\\n90.28\\n91.17\\nCoLA (mcc)\\n54.93\\n63.73\\n65.85\\nMRPC (acc)\\n89.21\\n89.66\\n90.10\\nSTS-B (pearson)\\n90.75\\n90.00\\n90.92\\nAvg.\\n84.39\\n85.23\\n86.25\\n#To (M)\\n223\\n343\\n258\\nTable 2: Performance of multi-task learning on GLUE\\nbenchmark obtained by ﬁne-tuning T5-Base (in per-\\ncent). ♣: Experimental results from Hyperformer (Ma-\\nhabadi et al., 2021).\\nVariants\\nWikiText-2\\n#To (M)\\nPPL (↓)\\nB2\\nB4\\n+MoE⋆\\n21.86\\n28.27\\n9.14\\n578\\nw/o PS\\n21.28\\n28.67\\n9.44\\n153\\nw/o GM\\n21.17\\n28.71\\n9.47\\n157\\n+MPOE\\n20.72\\n28.78\\n9.51\\n157\\nTable 3: Ablation study on the WikiText-2 dataset\\nabout the NLG tasks (in percent). “B2” and “B4” are\\nshort for BLEU-2 and BLEU-4, respectively. ⋆: The\\nmethod from Shazeer et al. (2017)\\nthe MPOE is more parameter-efﬁcient (258M vs.\\n343M in total parameters). It further demonstrates\\nthe potential beneﬁts of the MPOE approach in a\\nmulti-task learning setting, where the central tensor\\nlearns common information across tasks and the\\nauxiliary tensor learns task-speciﬁc information.\\n4.4\\nAblation Results\\nOur approach has incorporated two novel improve-\\nments: (1) MoE architecture with parameters shar-\\ning (PS) among experts based on MPO decom-\\nposition and (2) gradient mask (GM) to alleviate\\nunbalanced optimization.\\nTo verify the effectiveness of each component,\\nwe conduct the ablation study on the WikiText-2\\ndataset to analyze the contribution of each part. We\\nadopt PPL, BLEU-2 and BLEU-4 as the evalua-\\ntion metrics, and consider removing the parameters\\nsharing and gradient mask strategy respectively.\\nThe ablation results of our MPOE approach are\\nshown in Table 3. We can see that removing any\\ncomponent would lead to a decrease in the model\\nperformance. It shows the effectiveness of all these\\ncomponents in our approach. Besides, parameter\\nsharing seems more important than the gradient\\nVariants\\nPPL (↓)\\nWikiText-2\\nB2\\nB4\\n#To (M)\\nGPT-2\\n21.27\\n28.69\\n9.46\\n124.4\\nMPOE (m=3)\\n24.01\\n27.86\\n8.93\\n130.3\\nMPOE (m=5)\\n20.72\\n28.77\\n9.48\\n157.4\\nMPOE (m=7)\\n20.73\\n28.76\\n9.47\\n198.7\\nMPOE (m=9)\\n20.78\\n28.45\\n9.38\\n214.6\\nTable 4: Evaluation with different factorization manner\\non the WikiText-2 dataset about the NLG tasks (in per-\\ncent). “B2” and “B4” are short for BLEU-2 and BLEU-\\n4, respectively.\\nmask strategy, which yields a larger performance\\ndrop after being removed.\\n4.5\\nDetailed Analysis\\nMPO decomposition has different factorization\\nmanners. However, the MPOE approach requires\\na deﬁned MPO decomposition form to be given\\nbefore it can be used. Therefore, different factor-\\nization manners may affect the efﬁciency of the\\nMPOE approach. To vertify this, we perform a\\ndetailed analysis on different factorization man-\\nners of MPO decomposition. We present three\\nvariants of MPOE with different lengths of local\\ntensors produced by MPO decomposition empiri-\\ncally. Tabel 4 shows the evaluation results on the\\nWikiText-2 dataset about NLG tasks. As we can\\nsee, the variants of m > 3 are all superior to the\\nGPT-2 model. Additionally, we can observe that\\nmore local tensors performs similarly but leads to\\nhigher memory cost. Thus we ﬁnally choose to\\nset m = 5 for MPOE architecture considering the\\ntrade-off between the cost and quality.\\n5\\nRelated Work\\nWe will review the related works in four aspects.\\nPLMs with MoE.\\nIt has been reported that mod-\\nels with more parameters are usually considered to\\nhave a larger model capacity (Fedus et al., 2021;\\nZuo et al., 2021). In order to increase the model ca-\\npacity, a promising direction is to explore the scal-\\ning properties with MoE architecture which was\\nintroduced by Jacobs et al. (1991). Thus, Shazeer\\net al. (2017) ﬁrst applied the MoE architecture to\\nlarge-scale language models. Then, Switch Trans-\\nformers (Fedus et al., 2021), GShard (Lepikhin\\net al., 2021), BASELayer (Lewis et al., 2021) and\\nHashLayer (Roller et al., 2021) studied how to\\nbuild large-scale Transformer-based model with\\nMoE as well as improving routing strategy, which\\ncan better utilize the model capacity.\\nIn addi-\\ntion, Zhang et al. (2021b) proposed a strategy for\\nsparse activation of MoE architecture. He et al.\\n(2021) suggested a distributed training system for\\nfast training of MoE. Zoph et al. (2022) proposed a\\nsparse expert model with more stable training. Yu\\net al. (2022) proposed a sparse expert model based\\non all-MLP architecture. In contrast, our approach\\naims to reduce information redundancy by sharing\\nparameters among experts.\\nMatrix\\nProduct\\nOperators\\nDecomposition.\\nMatrix product operators (MPO) (Pirvu et al.,\\n2010) decomposition was proposed in quantum\\nmany-body physics, a.k.a. tensor-train (TT) de-\\ncomposition (Oseledets, 2011). A major category\\nof MPO studies relies on model compression (Gao\\net al., 2020). They focus on compressing weight\\nmatrix and convolutional layers (Novikov et al.,\\n2015; Garipov et al., 2016; Sun et al., 2020).\\nFurthermore, the MPO decomposition was used to\\ncompress the PLMs as well as enable lightweight\\nﬁne-tuning in downstream tasks (Liu et al., 2021).\\nIn this work, we utilize such a decomposition\\nmechanism for parameter sharing to construct a\\nparameter-efﬁcient MoE architecture.\\nImproved\\nVariants\\nof\\nMoE.\\nDespite\\nthe\\nachieved performance performance, MoE architec-\\nture has been hindered by the model complexity\\nand high memory costs (Shazeer et al., 2017;\\nFedus et al., 2021). This problem can be alleviated\\nby using distillation (Fedus et al., 2021) and expert\\npruning (Kim et al., 2021). Then,\\nKudugunta\\net al. (2021) and Zuo et al. (2021) indicated\\nthat sub-networks can be employed when using\\nthe model. Indeed, our approach can be further\\nenhanced by these existing methods for improving\\ninference time.\\nMulti-task Learning.\\nThe exploitation of MoE\\narchitectures for multi-task learning is a very\\npromising direction in recent years (Ma et al.,\\n2018). Houlsby et al. (2019) suggested training\\nadapters for each task separately while keeping the\\nmodel ﬁxed. Further research suggested that model\\nparameters could be shared across tasks, and task-\\nspeciﬁc adapter parameters were introduced (Stick-\\nland and Murray, 2019). Based on this idea, Ma-\\nhabadi et al. (2021) and\\nPilault et al. (2020)\\nproposed that parameter-efﬁcient multi-task ﬁne-\\ntuning for transformer-based models via shared\\nhypernetworks. Our approach differs from these\\nworks in that the MPOE approach allows us to re-\\nduce model size while keeping the same number\\nof experts, and meanwhile achieve performance\\nimprovement for multi-task learning.\\n6\\nConclusion\\nIn this paper, we proposed a parameter-efﬁcient\\nMoE architecture for increasing model capacity\\nbased on the MPO decomposition. First, we shared\\nthe central tensors among different experts based\\non MPO decomposition, which largely reduced the\\nmodel parameters of MoE architecture. Then, we\\ndesigned the gradient mask strategy to alleviate\\nthe unbalanced optimization issues and ensured\\nthat different tensors capture different types of in-\\nformation efﬁciently. Extensive experiments have\\nshown that our approach outperforms several com-\\npetitive PLM scaling strategies, especially in terms\\nof improving the parameter efﬁciency of the MoE\\narchitecture.\\nIn the future, we will enhance the proposed\\nMPOE approach with recently proposed routing\\nmethods, such as BASELayer (Lewis et al., 2021),\\nHASHLayer (Roller et al., 2021) and GShard (Lep-\\nikhin et al., 2021). We will also consider exploring\\nadditional decomposition methods for developing\\nparameter-efﬁcient MoE architecture.\\nAcknowledgments\\nThis work was partially supported by Beijing Natu-\\nral Science Foundation under Grant No. 4222027,\\nNational Natural Science Foundation of China un-\\nder Grants No. 62206299 and 11934020, Beijing\\nOutstanding Young Scientist Program under Grant\\nNo. BJJWZYJH012019100020098 and Beijing\\nAcademy of Artiﬁcial Intelligence (BAAI). Xin\\nZhao is the corresponding author.\\nReferences\\nPeter F. Brown, Stephen Della Pietra, Vincent J. Della\\nPietra, Jennifer C. Lai, and Robert L. Mercer. 1992.\\nAn estimate of an upper bound for the entropy of\\nenglish. Comput. Linguistics, 18(1):31–40.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. Bert: Pre-training of deep\\nbidirectional transformers for language understand-\\ning. arXiv preprint arXiv:1810.04805.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\\nSwitch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity.\\narXiv\\npreprint arXiv:2101.03961.\\nZe-Feng Gao, Song Cheng, Rong-Qiang He, ZY Xie,\\nHui-Hai Zhao,\\nZhong-Yi Lu,\\nand Tao Xiang.\\n2020.\\nCompressing deep neural networks by ma-\\ntrix product operators. Physical Review Research,\\n2(2):023300.\\nTimur\\nGaripov,\\nDmitry\\nPodoprikhin,\\nAlexander\\nNovikov, and Dmitry Vetrov. 2016.\\nUltimate ten-\\nsorization: compressing convolutional and fc layers\\nalike. arXiv preprint arXiv:1611.03214.\\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\\nYu, and Jun Wang. 2018. Long text generation via\\nadversarial training with leaked information. In Pro-\\nceedings of the Thirty-Second AAAI Conference on\\nArtiﬁcial Intelligence, (AAAI-18), the 30th innova-\\ntive Applications of Artiﬁcial Intelligence (IAAI-18),\\nand the 8th AAAI Symposium on Educational Ad-\\nvances in Artiﬁcial Intelligence (EAAI-18), New Or-\\nleans, Louisiana, USA, February 2-7, 2018, pages\\n5141–5148. AAAI Press.\\nJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Ji-\\ndong Zhai, and Jie Tang. 2021.\\nFastmoe: A fast\\nmixture-of-expert training system.\\narXiv preprint\\narXiv:2103.13262.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin de Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly.\\n2019. Parameter-efﬁcient transfer learning for NLP.\\nIn Proceedings of the 36th International Confer-\\nence on Machine Learning, ICML 2019, 9-15 June\\n2019, Long Beach, California, USA, volume 97 of\\nProceedings of Machine Learning Research, pages\\n2790–2799. PMLR.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\\nlocal experts. Neural computation, 3(1):79–87.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre\\nMuzio, Andres Felipe Cruz Salinas, Liyang Lu,\\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and\\nHany Hassan Awadalla. 2021.\\nScalable and efﬁ-\\ncient moe training for multitask multilingual models.\\narXiv preprint arXiv:2109.10465.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna,\\nMaxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-\\nong, and Orhan Firat. 2021.\\nBeyond distillation:\\nTask-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\\nGshard: Scaling giant models with conditional com-\\nputation and automatic sharding.\\nIn 9th Inter-\\nnational Conference on Learning Representations,\\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\\nGoyal, and Luke Zettlemoyer. 2021.\\nBase layers:\\nSimplifying training of large, sparse models. arXiv\\npreprint arXiv:2103.16716.\\nPeiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan\\nXie, Zhong-Yi Lu, and Ji-Rong Wen. 2021.\\nEn-\\nabling lightweight ﬁne-tuning for pre-trained lan-\\nguage model compression based on matrix product\\noperators. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Nat-\\nural Language Processing, ACL/IJCNLP 2021, (Vol-\\nume 1: Long Papers), Virtual Event, August 1-6,\\n2021, pages 5388–5398. Association for Computa-\\ntional Linguistics.\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan\\nHong, and Ed H. Chi. 2018.\\nModeling task re-\\nlationships in multi-task learning with multi-gate\\nmixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge\\nDiscovery & Data Mining, KDD 2018, London, UK,\\nAugust 19-23, 2018, pages 1930–1939. ACM.\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\\nDan Huang, Andrew Y. Ng, and Christopher Potts.\\n2011. Learning word vectors for sentiment analy-\\nsis. In Proceedings of the 49th Annual Meeting of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 142–150, Port-\\nland, Oregon, USA. Association for Computational\\nLinguistics.\\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\\nDehghani, and James Henderson. 2021. Parameter-\\nefﬁcient multi-task ﬁne-tuning for transformers via\\nshared hypernetworks.\\nIn Proceedings of the\\n59th Annual Meeting of the Association for Com-\\nputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing,\\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\\ntual Event, August 1-6, 2021, pages 565–576. Asso-\\nciation for Computational Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury, and\\nRichard Socher. 2017. Pointer sentinel mixture mod-\\nels.\\nIn 5th International Conference on Learning\\nRepresentations, ICLR 2017, Toulon, France, April\\n24-26, 2017, Conference Track Proceedings. Open-\\nReview.net.\\nAlexander Novikov, Dmitry Podoprikhin, Anton Os-\\nokin, and Dmitry Vetrov. 2015. Tensorizing neural\\nnetworks. arXiv preprint arXiv:1509.06569.\\nIvan V Oseledets. 2011. Tensor-train decomposition.\\nSIAM Journal on Scientiﬁc Computing, 33(5):2295–\\n2317.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic eval-\\nuation of machine translation. In Proceedings of the\\n40th Annual Meeting of the Association for Compu-\\ntational Linguistics, July 6-12, 2002, Philadelphia,\\nPA, USA, pages 311–318. ACL.\\nJonathan Pilault, Amine Elhattami, and Christopher\\nPal. 2020. Conditionally adaptive multi-task learn-\\ning:\\nImproving transfer learning in nlp using\\nfewer parameters & less data.\\narXiv preprint\\narXiv:2009.09139.\\nBogdan Pirvu, Valentin Murg, J Ignacio Cirac, and\\nFrank Verstraete. 2010. Matrix product operator rep-\\nresentations. New Journal of Physics, 12(2):025012.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, Ilya Sutskever, et al. 2019.\\nLan-\\nguage models are unsupervised multitask learners.\\nOpenAI blog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Min-\\njia Zhang, Reza Yazdani Aminabadi, Ammar Ah-\\nmad Awan, Jeff Rasley, and Yuxiong He. 2022.\\nDeepspeed-moe: Advancing mixture-of-experts in-\\nference and training to power next-generation ai\\nscale. arXiv preprint arXiv:2201.05596.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\\nand Jason Weston. 2021.\\nHash layers for large\\nsparse models. arXiv preprint arXiv:2106.04426.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\\nDean. 2017.\\nOutrageously large neural networks:\\nThe sparsely-gated mixture-of-experts layer. arXiv\\npreprint arXiv:1701.06538.\\nAsa Cooper Stickland and Iain Murray. 2019. BERT\\nand pals:\\nProjected attention layers for efﬁcient\\nadaptation in multi-task learning.\\nIn Proceedings\\nof the 36th International Conference on Machine\\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\\nCalifornia, USA, volume 97 of Proceedings of Ma-\\nchine Learning Research, pages 5986–5995. PMLR.\\nXingwei Sun, Ze-Feng Gao, Zhong-Yi Lu, Junfeng Li,\\nand Yonghong Yan. 2020.\\nA model compression\\nmethod with matrix product operators for speech\\nenhancement.\\nIEEE/ACM Transactions on Audio,\\nSpeech, and Language Processing, 28:2837–2847.\\nHung-Yu Tseng, Yi-Wen Chen, Yi-Hsuan Tsai, Sifei\\nLiu, Yen-Yu Lin, and Ming-Hsuan Yang. 2020. Reg-\\nularizing meta-learning via gradient dropout. In Pro-\\nceedings of the Asian Conference on Computer Vi-\\nsion.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. 2018.\\nGlue: A multi-task benchmark and analysis platform\\nfor natural language understanding. EMNLP 2018,\\npage 353.\\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\\nBaobao Chang, Songfang Huang, and Fei Huang.\\n2021. Raise a child in large language model: To-\\nwards effective and generalizable ﬁne-tuning.\\nIn\\nProceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2021, Virtual Event / Punta Cana, Dominican Re-\\npublic, 7-11 November, 2021, pages 9514–9528. As-\\nsociation for Computational Linguistics.\\nAn Yang, Junyang Lin, Rui Men, Chang Zhou,\\nLe Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\\nmang Wang, Yong Li, Di Zhang, Wei Lin, Lin\\nQu, Jingren Zhou, and Hongxia Yang. 2021.\\nEx-\\nploring sparse expert models and beyond.\\nCoRR,\\nabs/2105.15082.\\nPing Yu, Mikel Artetxe, Myle Ott, Sam Shleifer,\\nHongyu Gong, Ves Stoyanov, and Xian Li. 2022.\\nEfﬁcient language modeling with sparse all-mlp.\\narXiv preprint arXiv:2203.06850.\\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.\\nWeinberger, and Yoav Artzi. 2021a. Revisiting few-\\nsample BERT ﬁne-tuning. In 9th International Con-\\nference on Learning Representations, ICLR 2021,\\nVirtual Event, Austria, May 3-7, 2021. OpenRe-\\nview.net.\\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng\\nLi, Maosong Sun, and Jie Zhou. 2021b.\\nMoeﬁ-\\ncation:\\nConditional computation of transformer\\nmodels for efﬁcient inference.\\narXiv preprint\\narXiv:2110.01786.\\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\\ngen: A benchmarking platform for text generation\\nmodels. In The 41st International ACM SIGIR Con-\\nference on Research & Development in Information\\nRetrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-\\n12, 2018, pages 1097–1100. ACM.\\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du,\\nYanping Huang, Jeff Dean, Noam Shazeer, and\\nWilliam Fedus. 2022. Designing effective sparse ex-\\npert models. arXiv preprint arXiv:2202.08906.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin\\nKim, Hany Hassan, Ruofei Zhang, Tuo Zhao,\\nand Jianfeng Gao. 2021.\\nTaming sparsely acti-\\nvated transformer with stochastic experts.\\nCoRR,\\nabs/2110.04260.\\n', 'source_name': 'Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models', 'source_url': 'https://arxiv.org/abs/2203.01104'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "PE_MoE_for_LMs_NOTES.pdf #47\n",
      "{'content': 'Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language \\nModels \\nMain Idea: proposes an architecture to make more efficient use of parameters in MoE models by \\nsharing information among experts. Mainly uses matrix product operator (MPO), a tensor \\ndecomposition approach from quantum physics to reconstruct the expert layer, then shares \\nparameters from the central tensor (core information) between experts while maintaining \\nspecificity through auxiliary tensors (complementary to the central tensor). The intuition behind \\nthis approach is to solve MoE’s issue of expert redundancy (different experts learning common \\nknowledge, leading to parameter-inefficiency). \\n \\nApproach – MPOE \\n- \\nCore idea is to share the central tensors from the expert layers and enable specificity via \\nexpert-specific auxiliary tensors based on the matric decomposition strategy. \\no The final MoE layer would consist of a shared central tensor (looks the same for \\neach expert) and small auxiliary tensors (unique to each expert). \\n- \\nThe central tensor acts like a global parameter – is the same for each expert in a layer. \\no Less total parameters are then needed in total since each expert layer will contain \\na globally shared tensor for all experts (the central tensor) while retaining expert \\nspecificity through auxiliary tensors specific to each expert. \\no Idea is to capture the shared knowledge between experts in the central tensor, \\nand the specialized expert knowledge in the auxiliary tensors. \\n- \\nIn theory, MPOE leads to suboptimal optimization since central tensors are always \\nupdated. To stabilize the optimization process, a gradient mask strategy is used: \\no The central tensor is not always updated (determined randomly). \\no Equivalent to a gradient dropout, employed in the central tensor of each MoE \\nlayer. \\n- \\nMPOE is employed on already pre-trained language models (for the matrix decomposition \\nto make sense, the models need to already have been pre-trained, having knowledge to \\ndecompose). \\n \\nExperiments \\n- \\nGPT-2 (decoder-only) and T5 (encoder-decoder) are used as base models for MPOE. \\n- \\n8 experts per MoE layer are generally used. \\n- \\nAdding MPOE to fine-tune pre-trained LMs in downstream tasks leads to better \\nperformance than Switch with a 27.2x parameter reduction. \\no MPOE is especially better at low-resource tasks, indicating that MPOE’s \\nparameter-sharing leads to positive task transfers. \\no The caveat is that MPOE needs an already pre-trained LLM. \\n- \\nAdding more experts (and thus having more auxiliary tensors) leads to improved MPOE \\nperformance. \\n- \\nMPOE can also potentially work well in a multi-task setting (with task-level routing). \\n \\nMy takeaways: \\n- \\nDeepSeekMoE is a recent model that was also trained with the idea of improving \\nparameter efficiency by sharing weights of experts to capture common knowledge. \\n- \\nAlso is like sparse upcycling and parameter-efficient sparsity crafting in the sense that it \\ntakes a pre-trained LLM and modifies its architecture to have the advantages of MoE. \\n- \\nThis approach is compatible with distillation techniques to further improve inference \\ntime. \\n \\n', 'source_name': 'Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_MoE_for_LMs_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FFFs_NOTES.pdf #48\n",
      "{'content': 'Fast Feedforward Networks + Exponentially Faster Language Modeling \\nMain Idea: the goal of this work is to introduce a new MoE architecture to improve inference \\ntime (up to 6x faster than other MoEs). They also claim that FFF (Fast FeedForward) has better \\ntraining properties due to noiseless conditional execution (no randomness in the gating function). \\n- \\nTraditional MoEs scale down inference time but remain linear in the width of the \\nfeedforward layer (increase in expert parameters). These models also rely on noisy gating \\nfor load balancing, which complicates training. \\n- \\nFFF, on the other hand, uses a binary tree-like structure to improve on these challenges. \\n \\nMethod \\n- \\nFFF uses nodes to aid the routing mechanism and leaves for the experts. \\n- \\nThe input representation goes through a first node (which is a common MLP layer). The \\nnode’s output is then passed to a sigmoid to give a probability p. This probability p is used \\nto route the input representation into the next node (left or right branch, as in a binary \\ntree). This process is repeated until a leaf node (expert) is reached) \\no The number of nodes the input goes through (in case of hard routing) corresponds \\nto the depth d of the network. \\n- \\nMoE chooses an expert width e (size of expert) and trains n separate expert blocks by the \\npartially randomized output of a gating network of width g = [w/e]. The target is then \\npredicted based on the mixture of the k best scoring experts. \\no MoE cost of inference is k*e neurons plus the gating overhead g (g tends to be \\nsmall). \\no FFFs of depth d learn a tree partition R1, … , R2^d of the input space determined \\nby their nodes, and 2^d small leaf feedforward networks (experts) of width l. \\no FFFs uses a soft routing approach to training, meaning that backpropagation is \\ndone by considering the soft routing probabilities p, so training a FFF is more costly \\nthan even a feedforward network. However, a hard routing approach at inference \\n(routing only happens through the most relevant nodes) ensures an inference gain \\nover MoE. \\n▪ This soft to hard routing transition is referred to as hardening. \\n- \\nThe FFF routing is more efficient than regular MoE routing. \\no In MoE, a gating network for each expert is needed to calculate the suitability of \\nthe specific expert to the input. \\no In FFF, the input is passed through d nodes. Since each node halves the number of \\nexperts (leaves) to be considered in future routings for the same input, and \\nbecause the left/right decision is simpler and thus requires less parameters than \\na normal MoE gating function, FFF provides a logarithmic routing improvement \\nover MoE in terms of computational overhead at inference. This is especially \\nsignificant when scaling the number of experts. \\n- \\nThe strategy of soft routing during training comes with the idea that as the leaves \\nspecialize, the nodes will be more confident in the routing, leading to probabilities closer \\nto 1 (to the correct path). This process is referred to as hardening. If hardening does not \\noccur at the expected rate, the hard routing required for inference might not work as \\nwell. In those cases, a hardening loss is used. \\n- \\nLocalized overfitting can occur with a high number of leaves, with each leaf being \\nresponsible for a very small part of the input space. To diminish this, one can add random \\nchild transpositions (flip the p scores given by a node to its child nodes randomly), which \\nensures the gradients are more diversely distributed, and exposing different nodes and \\nleaves to areas of the input space they otherwise wouldn’t see. \\no Hardening can also lead to a shrinking batch problem, mitigated by using larger \\nbatch sizes, gradient accumulation and smaller learning rates. \\n \\nFFFs Applied to NLP \\nA variant of BERT, deemed UltraFastBERT, is developed, where the feedforward layers are \\nreplaced with FFFs. \\nFFFs provide a forward pass speedup over regular FFs of O(log^2n) compared to O(n), a \\nlogarithmic improvement (where n is neurons). This improvement comes from FFF’s balanced \\nbinary tree structure, which only executes one branch of the tree conditionally on the input. \\nUltraFastBERT has 4095 neurons (leaves + nodes) and is compared to a 3072-neuron BERT. \\nUltraFastBERT only uses 1/341 of its neurons for inference while BERT uses all its 3072 neurons. \\n- \\nThis leads to a 78x speedup (not a 341x speedup, as would be expected) due to hardware \\noptimization for matrix multiplication favoring FFs. \\nResults \\n- \\nUltraFastBERT performs on-par with BERT on fine-tuning in downstream tasks, with a 78x \\ninference speedup. \\n- \\nUltraFastBERT shows that only a fraction of parameters of feedforward networks needs \\nto be applied at inference. \\n- \\nThe concept of FFFs can technically be applied to decoder-only models as well. \\n \\nMy takeaways: \\n- \\nThe efficiency gains are a result of instead of passing the input to a routing mechanism \\nwhich considers all experts, having the router only decide between two experts (sending \\nthe input to a specific side of the binary tree. \\no While traditional routing expects the router to choose the specific part of the input \\nspace of each expert, this binary tree approach has the router dividing the input \\nspace in half at every decision, eventually leading to the desired input space. \\n- \\nFFF routing seems to be theoretically less expensive, but not allow parallelization (each \\nnode decision needs to be performed sequentially), so gains might not be as significant \\nas expected. \\n', 'source_name': 'Fast Feedforward Networks', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FFFs/FFFs_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FFF.pdf #49\n",
      "{'content': 'Fast Feedforward Networks\\nPeter Belcak and Roger Wattenhofer\\nETH Z¨\\nurich\\n{belcak,wattenhofer}@ethz.ch\\nAbstract\\nWe break the linear link between the layer size and its infer-\\nence cost by introducing the fast feedforward1 (FFF) archi-\\ntecture, a log-time alternative to feedforward networks.\\nWe demonstrate that FFFs are up to 220x faster than feedfor-\\nward networks, up to 6x faster than mixture-of-experts net-\\nworks, and exhibit better training properties than mixtures of\\nexperts thanks to noiseless conditional execution.\\nPushing FFFs to the limit, we show that they can use as little\\nas 1% of layer neurons for inference in vision transformers\\nwhile preserving 94.2% of predictive performance.\\nIntroduction\\nThe feedforward layer is a parameter-heavy building block\\nof transformer models (Vaswani et al. 2017). Growing to\\ntens of thousands of hidden neurons in recent years, the cost\\nof feedforward layer inference is now in the sights of those\\nseeking to make large models faster.\\nIt has been recognized that in very large networks, only\\na small portion of the feedforward hidden neurons plays a\\nrole in determining the output for any single input, and that\\nit is possible to design networks that are modular in order to\\nutilize this fact (Bengio et al. 2015).\\nThe most recent work on the modularization of feedfor-\\nward layers aims at architectural designs that implicitly en-\\ncourage sparsity (Shazeer et al. 2017; Lepikhin et al. 2020;\\nFedus, Zoph, and Shazeer 2022). They share the common\\napproach of subdividing the feedforward layer into separate\\nblocks of neurons – “experts” – and training a gating layer\\nto determine the mixture of experts to be used in the forward\\npass. Inference acceleration is then achieved by using only\\nthe best-scoring k blocks, or a variant thereof. This approach\\nscales down the inference time by a constant but remains lin-\\near in the width of the feedforward layer. Moreover, it relies\\non noisy gating to allow for load balancing among the ex-\\nperts, complicating training and encouraging duplicity.\\nOutline.\\nWe introduce the Fast Feedforward (FFF) archi-\\ntecture – a peer of the feedforward (FF) architecture that\\naccesses blocks of its neurons in logarithmic time. FFF di-\\nvides the input space into disjoint regions by means of a dif-\\nferentiable binary tree and performs simultaneous learning\\n1https://github.com/pbelcak/fastfeedforward\\nFigure 1: A fast feedforward network set in comparison to its\\npeers. Bottom. Illustrations of the resulting regionalization\\nof the input space and varying boundary hardness.\\nof the region boundaries and the neural blocks assigned to\\nthese regions. This is achieved by tree-conditional execution\\nof neurons: a small subset of node neurons is set apart to\\nchoose what mixtures of leaf neuron blocks are to be com-\\nputed to produce the final output (Figure 1). As the training\\nprogresses, the region boundaries harden, and the mixtures\\ntend toward selecting only one log-time-accessible leaf.\\narXiv:2308.14711v2  [cs.LG]  18 Sep 2023\\nFormally, let f : DI →DO be the learning target.\\nThe naive approach is to train a feedforward layer (FF) F\\nof width w to approximate f on DI, i.e. F ≈DI f.\\nThe mixture-of-experts (MoE) approach (Shazeer et al.\\n2017) is to choose an expert width e that does not hin-\\nder performance, and then train separate expert blocks\\nE1, . . . , E⌈w/e⌉of neurons mixed by the partially random-\\nized output of a gating network of width g = ⌈w/e⌉. The\\nlearning target f is then approximated on DI under the mix-\\nture of k best-scoring experts, i.e. m1Eb1 +...+mkEbk ≈DI\\nf. For the corresponding FF network of width w, a MoE net-\\nwork with g = ⌈w/e⌉experts uses ke neurons for inference\\nat the mixture overhead of g.\\nFast feedforward networks (FFFs) are designed to lever-\\nage the fact that different regions of the input space activate\\ndifferent sets of neurons in wide networks. FFFs of depth d\\njointly learn a tree partition R1, . . . , R2d of the input space\\ndetermined by their nodes, and 2d small leaf feedforward\\nnetworks L1, . . . , L2d of width ℓ, which are trained so that\\neach Li approximates the partial target function f|Ri on Ri,\\ni.e. Li ≈Ri f. Crucially, an FFF with depth d and leaf width\\nℓcan use all 2dℓhidden neurons to learn f, but requires only\\none leaf of width ℓto compute its output for any ι ∈DI, and\\ndoes so at the lookup overhead of only d neurons.\\nTo draw a comparison, the leaves Li are to FFFs what\\nexperts Ej are to MoEs, but the FFF tree network is re-\\ngionalizing the input space rather than noisily voting on ex-\\npert prowess. For the corresponding feedforward network\\nof width w and a choice of leaf size ℓ, one can choose\\nd = log2⌈w/ℓ⌉and access the leaf in O(d) = O(log w)\\ninstead of O(g) = O(w) time. Coincidentally, the FFF ap-\\nproach also happens to represent a differentiable relaxation\\nof the classical notion of k-d trees (Bentley 1975).\\nThe process enabling the fragmentation of a large feedfor-\\nward layer to a number of smaller leaf layers while preserv-\\ning its predictive performance is hardening. In training, the\\nnodes of FFF recursively perform a soft choice ⟨1 −p, p⟩\\nover the outputs of their two children (a “mixture of child\\nexperts” if we must), and based on the final loss incurred,\\nthe optimizer updates both the weights of the children and\\nthe weights of the parent that computed the choice. During\\ninference, a single hard decision (i.e. “proceed to left/right\\nchild”) is made depending on the rounding result [p] at each\\nnode. Hardening is the process of nodes learning boundaries\\nsuch that the soft choices they make for individual inputs go\\nfrom indecisive mixtures (e.g. ⟨49, 51⟩) toward more deci-\\nsive ones (e.g. ⟨3, 97⟩). We observe that in settings where\\nrepresentation power is plentiful (i.e. wide leaves and deep\\nFFFs), the process often takes place on its own. In settings\\nwhere more representational power may be warranted (e.g.\\nwide vision transformers with FFF leaf bottlenecks), this\\nprocess either takes place but stalls prematurely, or takes\\nplace at a very low rate, and we choose to encourage it with\\nthe addition of a hardening loss.\\nThanks to hardening, the performance of the soft training\\nsetting carries over to inference. As a byproduct, the learned\\nregions can also be used as a partition of the input space for\\ninterpretability, surgical model editing, catastrophic forget-\\nting mitigation, reduction of replay data budget, etc..\\nUser manual.\\nSuppose that you have an existing architec-\\nture featuring a FF layer of width w and want to replace it\\nwith a FFF layer.\\nCase 1: “I want faster inference”. Choose ℓ<< w such\\nthat ℓfits your target inference budget, and then experiment\\nwith different depths d ≥log2(w/ℓ) to achieve the desired\\nperformance. Note that ℓstill needs to be large enough to be\\nable to learn the partial function on its region, and that the\\nfinal training width 2dℓmight end up being larger than w.\\nCase 2: “I want a partition of the input space”. Choose\\nd such that 2d meets your expectation on the number of\\nregions. Then experiment with ℓ≥w2−d for best perfor-\\nmance. Note that ℓagain needs to be large enough to be able\\nto learn the partial function on its region, and that for large d\\nyou might have to actively counter the effects of overfrage-\\nmentation.\\nContributions.\\n1. We introduce the fast feedforward (FFF) architecture, a\\npeer to the feedforward (FF) architecture that uses only a\\nlog-time-accessible fraction of its neurons at any point.\\n2. We investigate the effect of leaf size and depth on the\\npredictive performance of FFFs as models in their own\\nright and show that in sufficiently large settings, FFFs\\ngive performance comparable to FFs of the same train-\\ning width while carrying out the inference significantly\\nfaster. We further show that FFFs deliver better memo-\\nrization and generalization performance than the FFs of\\nthe same inference size.\\n3. We compare the FFF architecture against the mixture-of-\\nexperts (MoE) approach in terms of their predictive per-\\nformance and inference speed as the number of blocks\\nincreases, and support the claimed advantages of the de-\\nsign experimentally.\\n4. We demonstrate that FFFs can be feasibly used in place\\nof FFs as parts of larger, more complex architectures such\\nas transformers.\\nThe text culminates into a comparison with related work.\\nAlgorithm\\nDenote the network input, output dimension dimI, dimO.\\nDenote ⟨a, b, c⟩-feedforward network a feedforward net-\\nwork with a inputs, b neurons, and c outputs.\\nNotice that we override the terminology designed for\\nmulti-layer networks and talk of only one set of neurons that\\nhas both input and output weights. For example, we would\\nrefer to the BERT-base feedforward layer with input dimen-\\nsion 768, 3072 hidden neurons, and 768 output neurons as\\nto the feedforward layer with 3072 neurons, each with 768\\ninputs and 768 outputs. This greatly simplifies our presenta-\\ntion.\\nDefinition.\\nA fast feedforward network of depth d ≥0,\\nnode size n, and leaf size ℓis a pair ⟨N, L⟩.\\nN\\n:=\\n{N0,0, . . . , Nd−1,2d−1−1} is the set of node\\n⟨dimI, n, 1⟩-feedforward\\nnetworks\\nwith\\nan\\naddi-\\ntional sigmoidal activation on the output. These nodes\\nAlgorithm 1: FFF forward pass.\\nInput: Input sample ι ∈DI, the root node N0,0\\nOutput: Output ∈DO of the FFF for ι\\nFunction FO R WARDT\\n\\x00ι, Nm,n\\n\\x01\\n:\\nif Nm,n ∈L then\\nreturn Nm,n (ι)\\nelse\\ncm,n ←Nm,n (ι)\\nreturn cm,n FORWARDT\\n\\x00ι, Nm+1,2n+1\\n\\x01\\n+ (1 −cm,n) FORWARDT\\n\\x00ι, Nm+1,2n\\n\\x01\\nend\\nFunction FO R WARDI\\n\\x00ι, Nm,n\\n\\x01\\n:\\nif Nm,n ∈L then\\nreturn Nm,n (ι)\\nelse\\ncm,n ←Nm,n (ι)\\nif cm,n ≥1\\n2 then\\nreturn FORWARDI\\n\\x00ι, Nm+1,2n+1\\n\\x01\\nelse\\nreturn FORWARDI\\n\\x00ι, Nm+1,2n\\n\\x01\\nend\\nend\\nform a balanced differentiable binary tree such that\\nNm+1,2n, Nm+1,2n+1 are the children of Nm,n.\\nL\\n:=\\n{Nd,0, . . . , Nd,2d−1}\\nis\\nthe\\nset\\nof\\nleaf\\n⟨dimI, ℓ, dimO⟩-feedforward networks. All weights are\\ntrainable by default and the forward pass is governed by the\\nfully deterministic Algorithm 1.\\nThe nodes of FFF are arranged in a differentiable dimI-d\\ntree that makes a soft choice over the leaves in the form of\\na stochastic vector c. In training, FFF performs a mixture of\\nexperts over all leaves in L, with the choice weights of the\\nmixture c computed by ascending through the tree from the\\nroot node N0,0 (cf. FORWARDT ). During inference, the de-\\ncision at each node is taken to be the closer of {0, 1}, and\\nthe forward pass algorithm proceeds from the root, always\\nchoosing only one branch depending on the local node deci-\\nsion (cf. FORWARDI).\\nRegions of responsibility and their boundaries.\\nThe tree\\ncomponent of the FFF yields a partition of the input space.\\nEach leaf is responsible for exactly one region of this parti-\\ntion, even though during training, its prediction on its own\\nregion of responsibility is mixed with the predictions of\\nother leaves. The boundaries between the individual regions\\nare determined by the node networks.\\nIn the case when n = 1 and there is no activation on the\\nnode network but the head sigmoid, the boundary is the ac-\\ntivation plane of the hidden neuron. The norm of the plane’s\\nnormal vector (=weights of the neuron) affects how quickly\\nthe sigmoid goes from 0 to 1 around the boundary (cf. Fig-\\nure 1 bottom-left). This determines how clearly the bound-\\nary is defined.\\nHardening.\\nFor\\nthe\\npredictive\\nperformance\\nof\\nFORWARDT\\nto carry over to FORWARDI, one must\\nnot lose predictive information when rounding the choice\\nscores. This loss of information is minimal when the\\nboundary decisions have been properly hardened (cf. Intro-\\nduction). As hinted at above, hardening at a node generally\\ndoes not have to involve adjustment to the boundary as\\na manifold in space – progressive uniform rescaling of\\nboundary coefficients (i.e. squashing of the final sigmoid\\ntoward the step function to make the boundary more clearly\\ndefined) suffices.\\nInterpreting the node choice scores as Bernoulli probabili-\\nties, hardening can be tracked by monitoring the batch mean\\nof entropies of the choices scores at each node. In our exper-\\nimentation, we found that rounding choice pairs ⟨1 −p, p⟩\\nwith entropies below 0.10 tends to lead to only very modest\\ndeviations from the FORWARDT performance. For situations\\nwhere the hardening of node decisions does not occur to a\\nsufficient extent on its own, hardening can be encouraged by\\nthe addition of hardening loss.\\nLet Lpred be the loss due to the outputs of FFF. Then one\\ncan take the total loss to be Ltotal := Lpred + hLharden with\\nLharden :=\\nX\\nι∈B\\nX\\nN∈N\\nH (N(ι)) ,\\nwhere B ⊆DI is a batch of samples, H(p) the entropy of a\\nBernoulli random variable, and h the training hyperparame-\\nter controlling the effect of the hardening loss.\\nOverfragmentation.\\nIf pushed to the extreme, allowing\\nfast feedforward networks to learn too many hard bound-\\naries leads to overfragmentation – a phenomenon in which\\nthe network divides the input space into exponentially many\\ndisjoint regions and learns to approximate parts of the learn-\\ning target in a way that is too specific for each region. Over-\\nfragmentation has two direct consequences: localized over-\\nfitting and the “shrinking batch problem”.\\nLocalized overfitting denotes a tail process that occurs\\nonce the model has sufficiently hardened, in which the re-\\ngion boundaries are no longer flexible and certain leaves\\nlearn to overfit the training data on their regions of respon-\\nsibility. This is because they stop receiving meaningfully\\nlarge gradient updates from the neighboring regions, but\\nmay be responsible for handling test samples that are not\\nwell understood by the training data for their region. Local-\\nized overfitting manifests itself just like classical overfitting\\n– the validation performance ceases to improve or deterio-\\nrates while learning on the training set continues. It can be\\nmitigated by randomized child transpositions – the soft de-\\ncisions ⟨1 −p, p⟩at each node can be randomly transposed\\nwith some low probability into ⟨p, 1 −p⟩. This is to expose\\ntheir children to the training data of neighboring regions in\\norder to aid generalization performance and does to some\\nextent already happen for soft boundaries, but it becomes\\nrare as the boundaries harden.\\nThe FFF variant of the shrinking batch problem is also a\\nresult of the leaf region boundaries hardening, and it arises in\\nsituations when the batch size becomes too small for the par-\\ntition of the input space learned by the FFF tree. If the par-\\ntition of the input space is too finely grained and the bound-\\naries hardened, each leaf ends up receiving meaningful gra-\\ndient updates from only a small fraction of the training sam-\\nples, resulting in inaccurate gradient descent progress. Batch\\nshrinking leads to poor learning performance (e.g. low train-\\ning set accuracy, early stalling, chaotic development) but can\\nbe mitigated – naively with larger batch sizes, gradient accu-\\nmulation, and smaller learning rates; in full generality with\\nlocalized optimization.\\nWe consider the complexity of our algorithms in terms of the\\nparameters d, n, ℓ. Note that we found n = 1 to suffice in all\\nour experiments, but we keep n in for the sake of generality.\\nTraining\\ncomplexity.\\nThe\\ntraining\\nforward\\npass\\nFORWARDT ascends through d levels of the tree, passing\\nthrough node neurons to compute the final choice vector c in\\nO((2d −1)n) time. Then, the leaf outputs are computed and\\nmixed by c in O(2dℓ) time. This means O(2d(ℓ+ n) −n)\\ntime for the forward pass, and a (d + 1)-step backward pass\\nback to the decision on the root.\\nFrom the implementation standpoint, we express the as-\\ncent through the tree as a single loop making d identical\\nbatched computations, and then perform the final leaf for-\\nward pass and expert mixture.\\nInference complexity.\\nFORWARDI ascends through the\\nFFF tree in d steps, always executing exactly one node net-\\nwork. Then, it performs inference on one leaf, leading to\\nO(dn + ℓ) time.\\nIn terms of the implementation, the ascent from the\\nroot through the tree is executed as a batched computa-\\ntion of an indexed set of weights and biases (multiply-and-\\naccumulate), comparison of the logit to 0, and advancing of\\nthe index depending on the result of the comparison.\\nIn our experience of using ahead-of-time compilation for\\nCUDA, the selective indexing of weights for node decisions\\nmanifested itself in the native code as a simple offset in the\\ndata load for batched matrix multiplication, having only a\\nsmall constant implementation overhead on the hardware\\nlevel when compared to feedforward layers.\\nSize and width.\\nFast feedfoward networks consist of neu-\\nrons of two types: node and leaf neurons. For clarity and to\\nmake direct comparisons to the corresponding feedforward\\nnetworks, we distinguish between variants of network size\\nand width.\\nAn FFF with d, n, ℓas in the definition has training size of\\n(2d−1)n+2dℓ– these are all the neurons of the network, and\\nthey are all affected by optimization. It further has inference\\nsize of dn + ℓ, as these are the neurons engaged to produce\\ninference output by FORWARDI.\\nHowever, only the neurons of leaves produce output, with\\nthe node neurons being involved solely in the computation\\nof the mixture of the outputs of individual leaves. Therefore,\\nwe say that the FFF has training width of 2dℓand inference\\nwidth ℓ. Note that the FFF with all weights of node networks\\nset to 0 is equivalent to a vanilla feedforward network with\\n2dℓneurons (up to a uniform rescaling of the output weights,\\nwhich is learnable). We refer to the difference between the\\ntraining/inference size and width as overhead.\\nExperiments\\nWe conduct a number of experiments to (1) explore the ef-\\nfects of assigning neurons with learnable regions of influ-\\nence, (2) compare the predictive performance and speed of\\nFFFs to that of MoEs, and (3) assess the feasibility of FFFs\\nas parts of deeper architectures. The task of each experiment\\nis image classification, and we evaluate the classification ac-\\ncuracy of the softmax of output logits in the usual way. For\\nFFFs, we measure the accuracy of making “hard” decisions\\nat every node (i.e. we use FORWARDI).\\nFor each dataset considered, we use the designated train-\\ning and test sets as provided. We further split the full training\\nset 9 : 1 into training and validation subsets.\\nTo compare the qualities of individual models, we mea-\\nsure four quantities.\\nMemorization accuracy (MA). Interpreting (fast) feed-\\nforward networks as model memories (Bau et al. 2020), we\\nmeasure their ability to learn the training set by computing\\nthe accuracy of overfitted networks on the training set. That\\nis, we train networks until their accuracy in training stops\\nimproving, and then run a test on the training data. A result-\\ning MA of 100% means that the network has successfully\\nmemorized all the predictions for the training data.\\nGeneralization accuracy (GA). Treating (fast) feedfor-\\nward networks as predictive models in their own right, we\\nmeasure their ability to correctly predict the classes of pre-\\nviously unseen samples in the test set. For this, we train net-\\nworks until their validation accuracy stops improving, and\\nuse the best model in terms of the validation accuracy for\\nevaluation.\\nInference time and speedup. Our own implementa-\\ntion of the FFF algorithms is provided through pip\\ninstall fastfeedforward and on GitHub1. We\\ncompile our implementation of FORWARDI for NVIDIA\\nA100 GPUs using PyTorch 2.0.1 model compilation in the\\nreduce-overhead mode. We then run each model 104\\ntimes with batch size 2048 on a single NVIDIA A100 GPU.\\nWhere relevant for comparison, we report the mean infer-\\nence time t• per single forward pass under repeated trials, to-\\ngether with its standard deviation. We further report speedup\\n– the fraction tF F /tF F F , where tF F F is the mean inference\\ntime for the given FFF model and tF F is the mean infer-\\nence time for the vanilla feedforward network of the same\\ntraining width. Simply put, speedup says how much faster\\nthe FFF was than the FF with the same number of neurons\\navailable for making predictions in training, measured using\\nthis choice of software and hardware. The means and devia-\\ntions for speedups are in the appendix.\\nExplorative evaluation\\nTo examine the nature of fast feedforward networks as an al-\\nternative to feedforward networks, we measure the effect of\\ntheir parameters on their predictive performance and speed\\nin the context.\\nEvaluation with training counterparts\\nSubject. We investigate the relationship between the config-\\nuration of leaf size, depth, and training width and the memo-\\nModel\\nUSPS\\n16\\n32\\n64\\n128\\nMA\\nGA\\nspeedup\\nMA\\nGA\\nspeedup\\nMA\\nGA\\nspeedup\\nMA\\nGA\\nspeedup\\nvanilla FF\\n100.0\\n93.1\\n1.00x\\n100.0\\n93.7\\n1.00x\\n100.0\\n94.1\\n1.00x\\n100.0\\n94.2\\n1.00x\\nFFF\\nℓ= 8\\n99.3\\n92.2\\n1.07x\\n99.2\\n91.8\\n1.16x\\n99.2\\n92.3\\n1.53x\\n99.5\\n92.1\\n2.56x\\nℓ= 4\\n94.1\\n87.6\\n0.98x\\n97.2\\n89.5\\n1.08x\\n97.6\\n90.6\\n1.56x\\n97.1\\n90.3\\n2.40x\\nℓ= 2\\n92.0\\n85.5\\n0.90x\\n93.4\\n86.4\\n1.07x\\n90.6\\n84.4\\n1.39x\\n94.3\\n88.1\\n2.22x\\nℓ= 1\\n83.4\\n77.0\\n0.85x\\n77.3\\n74.2\\n0.99x\\n79.2\\n77.1\\n1.34x\\n81.4\\n77.8\\n2.12x\\nModel\\nMNIST\\nvanilla FF\\n98.0\\n95.2\\n1.00x\\n100.0\\n96.6\\n1.00x\\n100.0\\n97.7\\n1.00x\\n100.0\\n98.1\\n1.00x\\nFFF\\nℓ= 8\\n94.6\\n93.1\\n1.13x\\n96.5\\n93.9\\n1.50x\\n97.7\\n94.2\\n2.20x\\n99.3\\n94.9\\n3.39x\\nℓ= 4\\n91.6\\n90.8\\n1.33x\\n96.2\\n93.1\\n1.35x\\n96.7\\n93.3\\n2.33x\\n97.6\\n93.6\\n3.29x\\nℓ= 2\\n92.1\\n90.3\\n1.19x\\n94.0\\n91.4\\n1.48x\\n95.2\\n92.1\\n2.33x\\n96.2\\n92.4\\n3.47x\\nℓ= 1\\n91.7\\n89.9\\n1.04x\\n94.4\\n92.0\\n1.26x\\n94.5\\n91.4\\n1.91x\\n94.1\\n92.0\\n3.93x\\nModel\\nFashionMNIST\\nvanilla FF\\n91.0\\n86.4\\n1.00x\\n94.8\\n87.8\\n1.00x\\n98.5\\n89.0\\n1.00x\\n99.3\\n89.6\\n1.00x\\nFFF\\nℓ= 8\\n86.7\\n84.2\\n1.34x\\n87.8\\n85.2\\n1.44x\\n88.8\\n85.2\\n2.02x\\n90.5\\n86.1\\n3.78x\\nℓ= 4\\n86.4\\n83.3\\n1.27x\\n86.6\\n84.5\\n1.32x\\n89.1\\n85.1\\n2.02x\\n89.0\\n85.4\\n3.41x\\nℓ= 2\\n84.5\\n83.0\\n1.24x\\n85.4\\n82.9\\n1.34x\\n87.2\\n84.1\\n2.01x\\n87.3\\n84.3\\n3.28x\\nℓ= 1\\n79.7\\n78.4\\n1.04x\\n79.4\\n77.8\\n1.29x\\n79.9\\n79.5\\n1.90x\\n78.7\\n77.7\\n2.92x\\nTable 1: The results of the explorative experimentation on FFFs. Reading top-to-bottom shows the effect of decreasing the leaf\\nsize and correspondingly increasing the depth. Left-to-right: The effect of increasing the training width and model depth while\\nkeeping the leaf size constant. Diagonally bottom-left-to-top-right: The effect of keeping the depth constant while increasing\\nthe leaf size and training width. Emphasis and emphasis mark the best speedups per ℓ, dataset and dataset, respectively.\\nrization and generalization performance of fast feedforward\\nnetworks. For each FFF, we also consider the performance\\nof a FF of the same training width. We make the compar-\\nison with them having the same training width rather than\\ntraining size since only leaf neurons are directly involved in\\ncomputing the classification prediction for the inputs given.\\nMethod.\\nWe train fast feedforward networks for train-\\ning widths w = 16, 32, 64, 128, leaf sizes ℓ= 1, 2, 4,\\ndatasets USPS (Hull 1994), MNIST (LeCun, Cortes, and\\nBurges 2010), and FashionMNIST (Xiao, Rasul, and Voll-\\ngraf 2017). For each w, ℓconfiguration we compute the\\ndepth as log2(w/ℓ). The set of widths has been chosen on\\npurpose: notice that any fast feedforward network with w, ℓ\\nas above has inference width smaller than 16 – the narrow-\\nest of our configurations. For each width, we further train a\\nvanilla feedforward network as a baseline.\\nWe feed the networks flattened images. For ease of com-\\nparison, we use batch size 256 and pure SGD optimization\\nwith learning rate of 0.2 irrespective of the size or the depth\\nof the networks, but we note that deeper FFFs have bene-\\nfited from larger batch sizes and smaller learning rates. We\\nengage the hardening loss with scale parameter h = 3.0. We\\nexecute 10 runs for each configuration, and since this is an\\nevaluation of architectural limits, we report the performance\\nof the best model. Means and deviations are in the appendix.\\nDiscussion.\\nOur results are listed in Table 1. A visualiza-\\ntion of the hardening process can be found in Figure 5 of\\nthe appendix. The general observations are that each of: in-\\ncreasing width, increasing leaf size, and increasing leaf size\\nwhile keeping the depth constant; universally help memo-\\nrization and generalization performance. We make several\\nspecific observations in relation to our contributions.\\nFFFs perform comparably to FFs. For sufficiently large\\nwidths and depths on USPS and MNIST, fast feedforward\\nnetworks are only slightly (2-3%) worse than vanilla feed-\\nforward networks. Coincidentally, these are also the config-\\nurations in which FFFs deliver the best inference speed im-\\nprovements over classical feedforward layers as measured\\non our hardware.\\nNotice the performances of the FFFs with w = 128, ℓ= 8\\nacross datasets relative to FFs with w = 16. The perfor-\\nmance is remarkably close and even exceeds that of FFs, all\\nthat while the inference size of these FFFs (12) remains be-\\nlow that of the FFs (16).\\nOn FashionMNIST we observe the same trends but note\\nthat an FFF beyond our testing range (w = 512, ℓ= 8)\\nwas eventually necessary to bring FFFs close (MA=97.1,\\nGA=88.1) to the performance of FFs.\\nSpeedup increases with width. The wider and deeper\\nnetworks become while keeping the leaf size constant, the\\nmore significant the inference speed improvement delivered\\nby the fast feedforward architecture.\\nFigure 2: A visualization of the comparison of memorization and generalization performance of fast feedforward (d=2,6) and\\nfeedforward (d=0) networks. Horizontally: the inference size in neurons. Vertically: accuracy.\\nConstant width leads to a speed-performance trade-\\noff. If the training width is kept fixed, there is clearly a\\ntrade-off between increasing depth (and therefore increasing\\nspeed) and performance (cf. Table 1 top-to-bottom).\\nIncreasing the depth of the network while keeping the\\ntraining width constant results in a gradual decrease in per-\\nformance due to the FFF having smaller leaves, hence our\\nnote in the “User manual” (cf. Introduction). Later we will\\nsee that in deeper architectures the trade-off in constant-\\nwidth networks between inference speed and performance\\ndecrease due to small leaf size appears to be lessened in\\nmulti-layer architectures.\\nLarge-depth small-leaf networks exhibit overfragmen-\\ntation. We observe (USPS, FashionMNIST) that decreasing\\nthe leaf size while increasing depth (keeping the width con-\\nstant) leads to quickly worsening memorization and general-\\nization performance. The difference is particularly stark with\\ngreater depths (ℓ=1,2, w=64, 128). This is well explained by\\noverfragmentation, since especially for ℓ= 1, w = 128,\\neach leaf receives only 2 samples per batch on average. The\\nresults on USPS for ℓ=1, w=16,32 are a model example of\\nthis: we see that FFF with ℓ=1,d=4 delivers MA of 83.4,\\nbut that its deeper cousin ℓ=1,d=5 – which has more of the\\nsame-sized leaves at its disposal – yields MA of only 77.3.\\nTL;DR.\\nFFFs give predictive performance comparable to\\nFFs of the same training width, are faster as the training\\nwidth increases, and if pushed to the limit exhibit speed-\\nperformance trade-offs and overfragmentation.\\nEvaluation with inference counterparts\\nSubject.\\nSimilarly to the experimentation above with the\\nmain consideration for training width, we now evaluate fast\\nfeedforward networks of varying depths and leaf sizes with\\nrespect to their inference size. We make the inference size\\nthe point of comparison with feedforward networks since,\\nunlike the inference width, it is directly proportional to the\\ncomputational cost of inference.\\nMethod.\\nWe train fast feedforward networks for ℓ=\\n2, 4, 6, 8, 16, 32, d = 2, 6, datasets SVHN (Netzer et al.\\n2011), CIFAR10, CIFAR100 (Krizhevsky, Hinton et al.\\n2009), and for each ℓ, d configuration we compute the in-\\nference size as ℓ+ d. For each inference size, we further\\ntrain a vanilla feedforward network as a baseline.\\nWe train the networks with all parameters as above except\\nh, where we do not engage the hardening loss (h = 0) as\\nwe found that the hardening tended to occur on its own. We\\nexecute 10 runs for each configuration and report the best\\nperformances.\\nDiscussion.\\nOur results are sparse because of all the dif-\\nferent possible sums of leaf size and depth and are shown\\nin Figure 2. Aligned with intuition, we observe that bigger\\ndepth and larger leaf sizes lead to better memorization and\\ngeneralization performance.\\nFurther, FFFs outperform FFs of the same inference\\nsize. FFFs of varying depths and sizes consistently outper-\\nform the FFs with widths equal to the FFF inference sizes,\\nboth in terms of MA and GA. In terms of MA, the difference\\nis stark and grows with the depth and leaf size. In terms of\\nGA, FFFs initially gain an edge over FFs, but later the per-\\nformances of all models unite toward plateauing out at the\\nlimit of the naive, single-layer feedforward-only approach.\\nTL;DR.\\nFFFs deliver performance more readily than FFs\\nof the same inference width.\\nWidth\\nModel\\nfeedforward\\nmixture-of-experts (e=16, k=2)\\nfast feedforward (ℓ=32)\\nMA\\nETT\\nGA\\nETT\\nMA\\nETT\\nGA\\nETT\\nMA\\nETT\\nGA\\nETT\\nw = 64\\n87.2\\n307\\n49.3\\n55\\n57.8\\n5354\\n29.4\\n4880\\n85.8\\n302\\n45.9\\n22\\nw = 128\\n95.5\\n200\\n51.5\\n46\\n62.0\\n6074\\n33.6\\n938\\n90.1\\n305\\n45.5\\n22\\nw = 256\\n99.9\\n105\\n52.0\\n48\\n62.4\\n2001\\n33.9\\n372\\n91.2\\n244\\n44.4\\n17\\nw = 512\\n99.9\\n85\\n52.4\\n31\\n65.4\\n3834\\n34.5\\n315\\n96.2\\n175\\n43.7\\n10\\nw = 1024\\n99.9\\n82\\n53.0\\n21\\n65.3\\n1575\\n35.2\\n327\\n96.0\\n180\\n41.3\\n9\\nTable 2: The results of the comparison of feedforward, mixture-of-experts, and fast feedforward networks, for various training\\nwidths. The inference width is fixed to 32 for mixture-of-experts and fast feedforward networks. The ETT columns to the right\\nof metric columns list the “epochs to train”, i.e. the number of training epochs that have elapsed until the score to the left was\\nobserved.\\nComparative evaluation\\nThe direct contender architecture to fast feedforward, com-\\ning along with its own set of design parameters, is the\\nmixture-of-expert layer, which we take in its original form\\n(Shazeer et al. 2017).\\nPredictive performance comparison\\nSubject.\\nWe compare FFFs against MoEs and FFs of vary-\\ning training widths in terms of their predictive performance.\\nWe keep the leaf and expert width constant and focus on the\\nability of the architectures to deliver good memorization and\\ngeneralization properties as well as on the training compute\\nnecessary to reach those properties.\\nMethod.\\nWe experiment on the unaugmented CIFAR10\\ndataset. We train feedforward, mixture-of-experts, and fast\\nfeedforward networks of increasing size so that they always\\nagree in the training width. To keep the inference width the\\nsame, we set the leaf width to 32 and expert width to 16\\nwith always engaging k = 2 experts. Note that while single-\\nexpert networks can be used for inference, they are not able\\nto propagate gradients to the gating network (cf. Shazeer\\net al. (2017)). We take widths w = 26, 27, . . . , 210, which\\ncorrespond to 16- to 64-expert MoE networks and FFFs of\\ndepths 1 to 5. To encourage importance equality and load\\nbalancing in MoEs, we set wimportance = wload = 0.1 in\\nline with previous work. To encourage FFF hardening, we\\nuse h = 3.0. We train all models width batch size 4096 for\\n7000 epochs at most, with early stopping after 350 epochs\\nwhere no improvement in the respective validation accura-\\ncies is seen. All models have been trained with the Adam op-\\ntimizer, learning rate of 0.001, with the learning rate halving\\non 250-epoch training accuracy plateaus.\\nDiscussion.\\nThe results are listed in Table 2. On the outset,\\nwe observe that the MA and GA benefit from larger training\\nwidth across all models except GA on FFFs, where we see\\nthe unmitigated localized overfitting negatively affecting the\\nperformance.\\nFFFs are the fastest to deliver MA and GA. We see that\\nthe FFFs are the fastest (in terms of ETT) to deliver both\\nMA and GA, but, consistently with our previous explorative\\nevaluation, deliver slightly lower MA than FFs of the same\\ntraining width, and suffer from localized overfitting with the\\nincreasing depth.\\nFFFs outperform MoEs of the same training width.\\nWe observe that FFFs consistently deliver better MA and\\nGA scores than the MoE networks of the same training\\nwidth. We further see that they do so at ETTs smaller by an\\norder of magnitude. We attribute this difference mainly to\\nthe learnably controlled noise introduced to the expert mix-\\nture computation to aid load balancing and generalization.\\nWithout the noise, however, MoE networks would overfit to\\nlearn only with a handful of experts. We also experimented\\nwith varying values of wimportance and wload, but we found\\nthose to be broadly detrimental to the load balancing effort.\\nOur final values of batch importance and load were consis-\\ntent with those arrived at in Shazeer et al. (2017).\\nTL;DR.\\nFFFs deliver representational power more readily\\nthan the MoEs of equal training widths.\\nInference speed comparison\\nSubject.\\nSince the operations involved in the computation\\nof the expert/leaf network output are the same, the difference\\nin inference speed between mixture-of-experts and fast feed-\\nforward networks comes solely from the functioning of the\\ngating/lookup mechanism. We therefore keep the expert/leaf\\nwidth constant and measure the time needed to execute in-\\nference forward passes of feedforward, mixture-of-experts,\\nand fast feedforward networks across repeated trials for in-\\ncreasing numbers of experts/leaves (i.e. wider and wider net-\\nworks).\\nTo add realism, we simulate the conditions of a BERT-\\nbase (Devlin et al. 2018) feedforward layer, setting the input\\nand output widths of all neurons to 768 each.\\nMethod.\\nWe consider FF models of width 32 × 21 to\\n32 × 25, where we highlight the 32 neuron blocks for di-\\nrect comparison with the other models. We further evaluate\\nMoE models with expert width e = 32 and 21 to 215 ex-\\nperts, and FFF models with leaf width e = 32 and depths\\n1 to 15. To eliminate the effect of the mixture computation\\non our measurements, we keep e = ℓand set k = 1, even\\nthough this MoE parameter configuration is not trainable (cf.\\nFigure 3: A visualization of the performance measure-\\nment results. The horizontal axis denotes the number of\\nblocks/experts/leaves and is scaled logarithmically, the point\\nvalues are the mean inference times per single forward pass,\\nand the error bars show the standard deviation.\\nabove). When measuring, each model performs inference on\\nBERT-base inputs with batch size 256 exactly 20000 times.\\nDiscussion.\\nThe inference speed measurement results are\\nvisualized in Figures 3–4. We observe that both MoEs and\\nFFFs offer significant acceleration to the inference speed\\nwhen compared to the FF baseline. However, Figure 4,\\nshows the clear tendency of MoE model inference time to\\ngrow exponentially with the exponent of the expert count, in\\nstark contrast with the linear relationship between the two\\nexhibited by the FFF models. This is fully aligned with our\\ntheoretical analysis of the inference time complexity of the\\ntwo architectures.\\nTL;DR.\\nWe have experimentally confirmed the exponen-\\ntial difference between the inference time complexity of the\\nMoE’s and FFF’s internal mechanism.\\nFast feedforward layers as building blocks\\nSubject.\\nWe demonstrate that fast feedforward networks\\ncan be used as layers in place of standard feedforward layers\\nin the transformer architecture, thus giving it a significant\\nperformance boost. Previously, we noted that leaf sizes that\\nare too small for a given problem may lead to the occurrence\\nof overfragmentation. Here we push our experimental setup\\nto the limit in terms of leaf size and investigate the effect of\\noverfragmentation in a deep transformer.\\nMethod.\\nWe experiment on the CIFAR10 dataset of\\n32x32-pixel 3-channel images, with random horizontal, ver-\\ntical flipping, and random linear augmentations (translate,\\nrotate, scale). As models, we use 4-layer vision transform-\\ners with patch size 4, hidden dimension 128, input dropout\\n0.1, and no layer dropout.\\nWe consider vision transformers with their feedforward\\nlayers replaced by fast feedforward layers of training width\\nFigure 4: A close-up on the visualization of the performance\\nmeasurement results. The axes and values are as in Figure 3.\\n128, and a baseline vision transformer with feedforward lay-\\ners of width w = 128. The fast feedforward layers have leaf\\nsize ℓ= 1, 2, 4, 8, 16, 32 and depths log2(w/ℓ). We try three\\nlevels of hardening: h = 5, 10, ∞, where ∞denotes that the\\nFFF tree has been effectively frozen from the beginning (i.e.\\nthe boundaries are not trainable). We use Adam optimizer\\nwith the initial learning rate of 4e−4 and learning rate halv-\\ning on 50-epoch validation accuracy plateaus. For each ℓ,d-\\nconfiguration, we report the generalization performance of\\nthe best model and the measured speedup at the feedforward\\nlayers (not the whole transformer).\\nDiscussion.\\nThe results of our experimentation can be\\nseen in Table 3. A visualization of the hardening process\\nacross the layers of the transformer can be found in Figure 6\\nof the appendix. In line with our assessment of the algorithm\\ncomplexity, the measured speedup at the feedforward layers\\nincreases with decreasing leaf size. Further:\\nSingle-neuron FFFs suffice. We observe that even fast\\nfeedforward layers with inference width 1 are sufficient for\\nthe vision transformer to deliver reasonable performance,\\nwith relative decrease in performance of only 5.8%.\\nThe effects of overfragmentation are suppressed. We\\nobserve that the generalization performance of GA suffers\\nonly relatively mildly due to the increase in depth and de-\\ncrease in leaf size, which is in stark contrast with the results\\nof Table 1. We attribute this to the depth of the transformer\\nand take it as an encouraging sign of the feasibility of FFFs\\nas replacements for FFs.\\nTL;DR.\\nFast feedforward layers can deliver inference ac-\\nceleration in vision transformers over feedforward layers of\\nthe same training width at the cost of a small performance\\ndecrease.\\nRelated work\\nOur work overlaps with the research efforts in two areas of\\ninference acceleration.\\nModel\\nProperty\\ndepth\\ntraining width\\ntraining size\\ninference width\\ninference size\\nspeedup\\nGA\\nFF\\nw = 128\\n–\\n128\\n128 (100%)\\n128 (100%)\\n128 (100%)\\n1.00x\\n84.7\\nfast FF\\nℓ= 32\\n2\\n128\\n131 (102%)\\n32 (25%)\\n34 (27%)\\n2.44x\\n83.6\\nℓ= 16\\n3\\n128\\n135 (105%)\\n16 (13%)\\n19 (15%)\\n2.80x\\n83.2\\nℓ= 8\\n4\\n128\\n143 (112%)\\n8\\n(6%)\\n12\\n(9%)\\n3.29x\\n82.8\\nℓ= 4\\n5\\n128\\n159 (124%)\\n4\\n(3%)\\n9\\n(7%)\\n3.39x\\n81.6\\nℓ= 2\\n6\\n128\\n191 (149%)\\n2\\n(1%)\\n8\\n(6%)\\n3.47x\\n80.1\\nℓ= 1\\n7\\n128\\n255 (199%)\\n1\\n(1%)\\n8\\n(6%)\\n3.93x\\n79.8\\nTable 3: The results of the testing of vision transformers leveraging feedforward and fast feedforward layers. All sizes are given\\nin neurons. Bracketed percentages describe quantities relative to their counterparts in the vanilla feedforward layers. GA is the\\ngeneralization accuracy of the fully trained vision transformer and “speedup” gives the performance improvement over vanilla\\nfeedforward layers in our testing setup.\\nConditional execution.\\nAlthough nowadays largely inac-\\ntive due to the tendency to move away from custom archi-\\ntectures, modified designs for MLP and CNN models were\\nproposed to allow for their partial execution where possible.\\nA number of methods were proposed (Davis and Arel\\n2013; Bengio, L´\\neonard, and Courville 2013; Bengio et al.\\n2015; Almahairi et al. 2016) to learn either policy distribu-\\ntions or additional controlling neural components to decide\\nwhich blocks of layer neurons to execute during forward\\npass.\\nIn comparison, fast feedforward networks completely\\nconceal the learning of leaf regions from the user (save from\\nthe hyperparameter h if used) and come in an inference-\\nready form once trained, requiring no adjustment when in-\\ncluded as a part of transformers.\\nIn a notable generalization of this line of work to deep ar-\\nchitectures, Ioannou et al. (2016) proposed an approach peer\\nto deep convolutional neural networks that learns to route\\nthe input through a sequence of chosen intermediate layers.\\nWhile our method quickly routes the signal to a single-leaf\\nfeedforward neural network, it draws no comparison to deep\\nnetworks.\\nModular “mixture-of-experts” models.\\nVery large mod-\\nels practically demand modularity. The most straightforward\\nway to modularize large transformer models in order to re-\\nduce their inference cost is to subdivide their feedforward\\nlayers into n blocks of neurons, and then train a controlling\\nclassifier to choose which block to involve in forward pass.\\nThis is usually done by training a wide softmax-activated\\nlinear layer to produce a stochastic vector of mixture scores\\nto be applied to the outputs per block in order to produce the\\nfinal output. Several variants of this method have been pro-\\nposed and tested across a variety of large models (Shazeer\\net al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer\\n2022).\\nWe thoroughly compare fast feedforward to mixture-of-\\nexpert networks in earlier sections. To briefly summarise,\\nthe mixture-of-experts approach reduces the layer inference\\nwidth by a factor of n/k, where k is the number of best-\\nscoring blocks to engage in inference, but requires O(n)\\ntime to select the k blocks, and often relies on controlled\\nrandomization to avoid the formation of a strong preference\\nfor only a handful experts. This holds true even when mul-\\ntiple layers of expert mixers are introduced. In direct com-\\nparison, a fast feedforward network of depth d = log2 n\\nreduces the inference by a factor of n and requires only\\nO(d) = O(log n) time to decide on which leaf to use. Ad-\\nmittedly, to compensate for the effect of having only one\\nleaf to make the decision, the leaves of the fast feedforward\\nlayer might have to be slightly wider than the blocks of the\\ncorresponding mixture-of-experts layer.\\nRegionalization.\\nAn additional advantage of FFF over all\\nof the related work is that there is a direct correspondence\\nbetween parts of the network used in inference and alge-\\nbraically identifiable regions of the input space. This can be\\nleveraged to mitigate catastrophic forgetting when editing\\nmodels and to significantly reduce replay data budgets by\\napplying the learned partition of the input space to partition\\nthe training data.\\nReferences\\nAlmahairi, A.; Ballas, N.; Cooijmans, T.; Zheng, Y.;\\nLarochelle, H.; and Courville, A. 2016. Dynamic capacity\\nnetworks. In International Conference on Machine Learn-\\ning, 2549–2558. PMLR.\\nBau, D.; Liu, S.; Wang, T.; Zhu, J.-Y.; and Torralba, A.\\n2020.\\nRewriting a deep generative model.\\nIn Computer\\nVision–ECCV 2020: 16th European Conference, Glasgow,\\nUK, August 23–28, 2020, Proceedings, Part I 16, 351–369.\\nSpringer.\\nBengio, E.; Bacon, P.-L.; Pineau, J.; and Precup, D. 2015.\\nConditional computation in neural networks for faster mod-\\nels. arXiv preprint arXiv:1511.06297.\\nBengio, Y.; L´\\neonard, N.; and Courville, A. 2013. Estimat-\\ning or propagating gradients through stochastic neurons for\\nconditional computation. arXiv preprint arXiv:1308.3432.\\nBentley, J. L. 1975. Multidimensional binary search trees\\nused for associative searching. Communications of the ACM,\\n18(9): 509–517.\\nDavis, A.; and Arel, I. 2013.\\nLow-rank approximations\\nfor conditional feedforward computation in deep neural net-\\nworks. arXiv preprint arXiv:1312.4461.\\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\\nBert: Pre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805.\\nFedus, W.; Zoph, B.; and Shazeer, N. 2022. Switch trans-\\nformers: Scaling to trillion parameter models with simple\\nand efficient sparsity. The Journal of Machine Learning Re-\\nsearch, 23(1): 5232–5270.\\nHull, J. J. 1994. A database for handwritten text recogni-\\ntion research. IEEE Transactions on pattern analysis and\\nmachine intelligence, 16(5): 550–554.\\nIoannou, Y.; Robertson, D.; Zikic, D.; Kontschieder, P.;\\nShotton, J.; Brown, M.; and Criminisi, A. 2016. Decision\\nforests, convolutional networks and the models in-between.\\narXiv preprint arXiv:1603.01250.\\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\\nlayers of features from tiny images.\\nLeCun, Y.; Cortes, C.; and Burges, C. 2010.\\nMNIST\\nhandwritten digit database. ATT Labs [Online]. Available:\\nhttp://yann.lecun.com/exdb/mnist, 2.\\nLepikhin, D.; Lee, H.; Xu, Y.; Chen, D.; Firat, O.; Huang,\\nY.; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:\\nScaling giant models with conditional computation and au-\\ntomatic sharding. arXiv preprint arXiv:2006.16668.\\nNetzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and\\nNg, A. Y. 2011. Reading digits in natural images with unsu-\\npervised feature learning.\\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\\nHinton, G.; and Dean, J. 2017.\\nOutrageously large neu-\\nral networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\\ntention is all you need. Advances in neural information pro-\\ncessing systems, 30.\\nXiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-mnist:\\na novel image dataset for benchmarking machine learning\\nalgorithms. arXiv preprint arXiv:1708.07747.\\nExtended results of Table 1\\nTable 4 lists the means and standard deviations of our explo-\\nrative evaluation of fast feedforward networks in comparison\\nwith feedforward networks.\\nOn top of the observations made in the main text, we\\nsee that the variance of MA and GA when performing re-\\npeated runs increases with the decreasing leaf size and is\\nmost clearly seen for small training widths.\\nVisualization of the hardening process\\nTo provide some insight and intuition into the hardening of\\nthe node decision boundaries, we track the batch mean en-\\ntropies across the nodes of fast feedforward layers and report\\non their evolution as training progresses visually.\\nFigure 5 shows the evolution of the batched mean entropy\\nfor three models of varying depths whose training was ter-\\nminated by early stopping on validation FORWARDI accu-\\nracy plateau. We observe that by keeping the dataset (and\\ntherefore the complexity of the input space) and leaf size\\nconstant, the batch mean accuracy of deep layers converges\\nfaster for deeper networks. We attribute this simply to the\\nfact that the deeper the fast feedforward network in this ex-\\nperiment, the more leaves were available to selectively cater\\nto sub-spaces of the input space, allowing the networks to\\nmake cleaner separations between the regions learned.\\nFigure 6 reports the evolution of batched mean entropies\\nper layer for a 4-layer visual transformer model with fast\\nfeedforward networks in place of the vanilla feedforward\\nnetworks. We notice that early into the training, entropies of\\nthe layers closer to the input (i.e. the earlier/lower layers) are\\nfaster to converge. However, we see that the entropy of the\\nsecond layer then ceases to meaningfully decrease, and that\\nthe entropies of the first layer actually begin to climb. This\\ncan be explained by hardened decision boundaries showing\\nsigns of bottleneck behavior in higher layers of the trans-\\nformer, thus necessitating that the lower layers contribute\\nwith learned representations of higher quality.\\nInterpreting the stalling or climbing of the entropy as a\\nsign of the need for more representational power, we note\\nacross multiple experiments that the ideal shape for a vision\\ntransformer, even on a relatively simple dataset such as CI-\\nFAR10, appears to be more toward the shape of a pyramid\\nthan a uniform-width line of layers.\\nRelationship to multi-gating MoE networks\\nMore recent literature on mixture-of-experts networks (Lep-\\nikhin et al. 2020; Fedus, Zoph, and Shazeer 2022) suggests\\nthe use of a hierarchical, two-layer gating structure. Save\\nfrom the inherent randomness of the gating network selec-\\ntion, this can be seen as a small step towards FFFs. However,\\nFFFs attend to clearly specified regions of space rather than\\nbeing noisily voted in as the most appropriate experts for a\\ngiven input (which we have shown gives them much better\\ntraining and inference properties), and further, even under\\nsuch an approach, the inference time reduction of mixture-\\nof-expert networks remains constant in the width of the tar-\\nget network.\\nFigure 5: The evolution of batched mean decision entropy in\\na fast feedforward layer with ℓ= 8, d = 2, 3, 4, h = 3.0,\\ntrained on the MNIST dataset.\\nFigure 6: The evolution of batched mean decision en-\\ntropies across a vision 4-layer transformer of dimension 128\\nequipped with fast feedforward layers of ℓ= 32, d = 2, h =\\n0.10, trained on the CIFAR10 dataset.\\nCode\\nWe provide our implementations as a Python package built\\non top of the PyTorch framework.\\nJust use pip install fastfeedforward. Docu-\\nmentation is provided with the code.\\nModel\\nUSPS\\n16\\n32\\n64\\n128\\nMA\\nGA\\ntime\\nMA\\nGA\\ntime\\nMA\\nGA\\ntime\\nMA\\nGA\\ntime\\nvanilla FF\\n100.0 ± 0.0\\n93.1 ± 0.4\\n0.13 ± 0.02ms\\n100.0 ± 0.0\\n93.7 ± 0.4\\n0.16 ± 0.01ms\\n100.0 ± 0.0\\n94.1 ± 0.3\\n0.24 ± 0.02ms\\n100.0 ± 0.0\\n94.2 ± 0.3\\n0.41 ± 0.03ms\\nfast FF\\nℓ= 8\\n99.3 ± 0.5\\n92.2 ± 0.4\\n0.12 ± 0.03ms\\n99.2 ± 0.5\\n91.8 ± 0.6\\n0.14 ± 0.03ms\\n99.2 ± 0.4\\n92.3 ± 0.7\\n0.16 ± 0.03ms\\n99.5 ± 0.8\\n92.1 ± 0.4\\n0.16 ± 0.02ms\\nℓ= 4\\n94.1 ± 0.5\\n87.6 ± 0.7\\n0.13 ± 0.03ms\\n97.2 ± 3.4\\n89.5 ± 2.0\\n0.15 ± 0.03ms\\n97.6 ± 2.3\\n90.6 ± 1.4\\n0.16 ± 0.02ms\\n97.1 ± 1.3\\n90.3 ± 1.2\\n0.17 ± 0.02ms\\nℓ= 2\\n92.0 ± 9.2\\n85.5 ± 8.0\\n0.14 ± 0.03ms\\n93.4 ± 10.0\\n86.4 ± 8.7\\n0.15 ± 0.02ms\\n90.6 ± 7.7\\n84.4 ± 6.0\\n0.18 ± 0.03ms\\n94.3 ± 8.6\\n88.1 ± 7.1\\n0.18 ± 0.03ms\\nℓ= 1\\n83.4 ± 12.1\\n77.0 ± 11.1\\n0.15 ± 0.02ms\\n77.3 ± 5.8\\n74.2 ± 5.0\\n0.16 ± 0.01ms\\n79.2 ± 8.1\\n77.1 ± 7.1\\n0.18 ± 0.02ms\\n81.4 ± 9.2\\n77.8 ± 8.3\\n0.19 ± 0.02ms\\nModel\\nMNIST\\nvanilla FF\\n98.0 ± 0.9\\n95.2 ± 0.5\\n0.34 ± 0.11ms\\n100.0 ± 0.0\\n96.6 ± 0.2\\n0.42 ± 0.06ms\\n100.0 ± 0.0\\n97.7 ± 0.2\\n0.69 ± 0.10ms\\n100.0 ± 0.0\\n98.1 ± 0.1\\n1.13 ± 0.06ms\\nfast FF\\nℓ= 8\\n94.6 ± 19.5\\n93.1 ± 16.6\\n0.30 ± 0.11ms\\n96.5 ± 2.3\\n93.9 ± 1.2\\n0.28 ± 0.05ms\\n97.7 ± 4.3\\n94.2 ± 2.4\\n0.31 ± 0.06ms\\n99.3 ± 1.0\\n94.9 ± 0.6\\n0.33 ± 0.08ms\\nℓ= 4\\n91.6 ± 29.3\\n90.8 ± 27.2\\n0.26 ± 0.07ms\\n96.2 ± 24.3\\n93.1 ± 23.9\\n0.31 ± 0.09ms\\n96.7 ± 1.0\\n93.3 ± 0.6\\n0.30 ± 0.06ms\\n97.6 ± 0.6\\n93.6 ± 0.5\\n0.34 ± 0.08ms\\nℓ= 2\\n92.1 ± 7.3\\n90.3 ± 5.6\\n0.28 ± 0.08ms\\n94.0 ± 1.4\\n91.4 ± 1.0\\n0.28 ± 0.05ms\\n95.2 ± 1.8\\n92.1 ± 1.2\\n0.30 ± 0.07ms\\n96.2 ± 1.4\\n92.4 ± 0.6\\n0.32 ± 0.06ms\\nℓ= 1\\n91.7 ± 7.4\\n89.9 ± 6.4\\n0.33 ± 0.11ms\\n94.4 ± 3.5\\n92.0 ± 3.1\\n0.33 ± 0.07ms\\n94.5 ± 1.8\\n91.4 ± 1.1\\n0.36 ± 0.09ms\\n94.1 ± 0.9\\n92.0 ± 0.7\\n0.29 ± 0.03ms\\nModel\\nFashionMNIST\\nvanilla FF\\n91.0 ± 0.7\\n86.4 ± 0.4\\n0.34 ± 0.10ms\\n94.8 ± 0.9\\n87.8 ± 0.2\\n0.42 ± 0.07ms\\n98.5 ± 0.8\\n89.0 ± 0.4\\n0.64 ± 0.07ms\\n99.3 ± 0.4\\n89.6 ± 0.2\\n1.13 ± 0.05ms\\nfast FF\\nℓ= 8\\n86.7 ± 12.1\\n84.2 ± 10.9\\n0.26 ± 0.07ms\\n87.8 ± 17.6\\n85.2 ± 16.1\\n0.29 ± 0.10ms\\n88.8 ± 5.6\\n85.2 ± 3.8\\n0.32 ± 0.05ms\\n90.5 ± 1.7\\n86.1 ± 1.0\\n0.30 ± 0.06ms\\nℓ= 4\\n84.5 ± 25.0\\n83.0 ± 24.5\\n0.27 ± 0.11ms\\n86.6 ± 8.6\\n84.5 ± 6.4\\n0.32 ± 0.08ms\\n89.1 ± 3.5\\n85.1 ± 2.3\\n0.32 ± 0.07ms\\n89.0 ± 0.7\\n85.4 ± 0.7\\n0.33 ± 0.07ms\\nℓ= 2\\n83.6 ± 21.0\\n82.5 ± 11.0\\n0.28 ± 0.10ms\\n85.4 ± 8.4\\n82.9 ± 6.5\\n0.32 ± 0.09ms\\n87.2 ± 7.1\\n84.1 ± 5.9\\n0.32 ± 0.06ms\\n85.3 ± 5.2\\n81.5 ± 3.7\\n0.35 ± 0.08ms\\nℓ= 1\\n86.4 ± 9.0\\n83.3 ± 8.0\\n0.33 ± 0.09ms\\n79.4 ± 6.2\\n77.8 ± 5.5\\n0.33 ± 0.07ms\\n79.9 ± 3.5\\n79.5 ± 3.7\\n0.34 ± 0.08ms\\n78.7 ± 4.6\\n77.7 ± 3.8\\n0.39 ± 0.08ms\\nTable 4: The detailed results of the explorative experimentation on FFFs. Reading top-to-bottom shows the effect of decreasing the leaf size and correspondingly\\nincreasing the depth. Left-to-right: The effect of increasing the training width and model depth while keeping the leaf size constant. Diagonally bottom-left-to-top-\\nright: The effect of keeping the depth constant while increasing the leaf size and training width.\\n', 'source_name': 'Fast Feedforward Networks', 'source_url': 'https://arxiv.org/abs/2308.14711'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FFF_to_language.pdf #50\n",
      "{'content': 'Exponentially Faster Language Modeling\\nPeter Belcak and Roger Wattenhofer\\nETH Z¨\\nurich\\n{belcak,wattenhofer}@ethz.ch\\nAbstract\\nLanguage models only really need to use an ex-\\nponential fraction of their neurons for individual\\ninferences.\\nAs proof, we present UltraFastBERT, a BERT\\nvariant that uses 0.3% of its neurons during infer-\\nence while performing on par with similar BERT\\nmodels. UltraFastBERT selectively engages just\\n12 out of 4095 neurons for each layer inference.\\nThis is achieved by replacing feedforward net-\\nworks with fast feedforward networks (FFFs).\\nWhile no truly efficient implementation currently\\nexists to unlock the full acceleration potential of\\nconditional neural execution, we provide high-\\nlevel CPU code achieving 78x speedup over the\\noptimized baseline feedforward implementation,\\nand a PyTorch implementation delivering 40x\\nspeedup over the equivalent batched feedforward\\ninference.\\nWe publish our training code, benchmarking\\nsetup, and model weights.1\\n1. Introduction\\nFeedforward layers hold the majority of the parameters of\\nlarge language models (Brown et al., 2020; Anil et al., 2023).\\nHowever, not all of their neurons need to be engaged in the\\ncomputation of the feedforward layer output at inference\\ntime for every input.\\nFor a generally accessible proof, we present UltraFastBERT,\\na variant of the BERT architecture (Devlin et al., 2018) that\\nreplaces feedforward layers with fast feedforward networks.\\nIn terms of downstream performance, UltraFastBERT per-\\nforms on par with other BERT-like models that are similar\\nin size and undergo similar training procedures. The inter-\\nmediate layers of UltraFastBERT are, however, exponen-\\ntially faster by design: given a feedforward (FF) and a fast\\nfeedforward (FFF) network, each with n neurons, the time\\ncomplexity of a forward pass through the FFF is O (log2 n)\\n1https://github.com/pbelcak/UltraFastBERT\\ninstead of O (n) as for FF. This is a consequence of the fact\\nthat FFFs organize their neurons into a balanced binary tree,\\nand execute only one branch of the tree conditionally on the\\ninput.\\nPerforming inference on an FFF amounts to performing con-\\nditional matrix multiplication (CMM), in which the rows of\\nthe input dot with the columns of neural weights one at a\\ntime, and the weight column to proceed with is chosen de-\\npending on the output of the previous dot-product operation.\\nIn this manner, all neurons are used only by some inputs\\nand no input needs more than just a handful of neurons to\\nbe handled by the network. This is in contrast with dense\\nmatrix multiplication (DMM), which lies at the heart of the\\ntraditional feedforward networks, and which computes the\\ndot products of all rows with all columns.\\nNo native, efficient implementation of conditional matrix\\nmultiplication exists, and no popular deep learning frame-\\nwork offers any interface that could be used to implement\\nit besides a high-level simulation. We therefore provide\\na set of CPU implementations based on pointer-batched\\nmatrix multiplication routines of the BLAS library. In a\\nlater section, we give a comparison between CPU and GPU\\nimplementations at various levels of optimization and note\\nthat while there already is clear evidence of significant ac-\\nceleration, there is potential for more.\\nThe role of attention.\\nA large body of literature already\\naddresses the topic of speeding up the execution of the\\nattention mechanism. We note that for a BERT-base-sized\\nmodel with the usual pre-training context size of 128 (Devlin\\net al., 2018), the per-token inference cost of its attention to\\nall other tokens amounts to only a little more than the cost\\nof 128-neuron feedforward network inference. We therefore\\nleave the attention layers untouched and focus solely on the\\nintermediate layers hosting the feedforward networks.\\nPoints of comparison.\\nBERT-base feedforward networks\\nconsist of 3072 neurons. This is not close to any power\\nof two, and so in the design of UltraFastBERT, we round\\nthis number to 4095 – the number of nodes in a balanced\\nbinary tree of maximum depth 11. In this frame of reference,\\nUltraFastBERT uses only 1/256 (0.04%) of the 3072 BERT-\\n1\\narXiv:2311.10770v2  [cs.CL]  21 Nov 2023\\nExponentially Faster Language Modeling\\nbase neurons for inference. Nevertheless, UltraFastBERT\\niself consists of 4095 neurons, and so uses 1/341 (0.03%)\\nof its neurons for inference.\\nWhen reporting model performance on downstream tasks in\\nSection 3.3, we give both a 3072-neuron and a 4095-neuron\\nbaseline for completeness.\\nWhy only 78x and not 341x speedup?\\nDense matrix mul-\\ntiplication is the most optimized mathematical operation in\\nthe history of computing. A tremendous effort has been\\nput into designing memories, chips, instruction sets, and\\nsoftware routines that execute it as fast as possible. Many of\\nthese advancements have been – be it for their complexity\\nor for competitive advantage – kept confidential and ex-\\nposed to the end user only through powerful but restrictive\\nprogramming interfaces.\\nTherefore, despite having no need for new hardware, we are\\nstill forced to rely on combining high-level linear-algebraic\\nroutines to implement CMM, hence the reduction in the\\nspeedup. We elaborate on this in Section 3.\\nReproducibility.\\nWe share the weights of our best model.\\nWhile we do not provide an efficient PyTorch or TensorFlow\\nimplementation of CMM, the fact that only 12 neurons are\\nused in the inference of UltraFastBERT can be verified\\nsimply by masking out the output of all but the chosen\\nneurons, and we give the code for this.\\nTakeaways.\\n• We present UltraFastBERT, a BERT-like model that\\nhas 4095 neurons but selectively uses only 12 (0.03%)\\nfor inference.\\n• We finetune UltraFastBERT for standard downstream\\ntasks and find that it performs on par with its BERT\\npeers.\\n• We provide a naive implementation of the conditional\\nmatrix multiplication that underlies fast feedforward\\nnetwork inference.\\nWe find that it leads to a 78x\\nspeedup over the natively optimized dense matrix mul-\\ntiplication.\\n• Through UltraFastBERT and the already considerable\\nspeedups by simple FFF implementations, we demon-\\nstrate the considerable potential of conditional neural\\nexecution in language modelling.\\n2. Model\\n2.1. Architecture\\nOur architectural starting point is the crammedBERT archi-\\ntecture (Geiping & Goldstein, 2023), which we implement\\nto the letter in all but the nature of intermediate layers.\\nThere, the feedforward networks contained in the interme-\\ndiate layers of the crammedBERT transformer encoder are\\nreplaced with fast feedforward networks (Belcak & Watten-\\nhofer, 2023).\\nWe make the following simplifying changes to the original\\nfast feedforward networks:\\n1. Remove all differences between leaf and non-leaf nodes.\\nIn particular, we use the same (GeLU) activation func-\\ntion across all nodes, equip all nodes with output\\nweights, and remove all output biases.\\n2. Fix the leaf size to 1.\\n3. Allow multiple FFF trees in parallel. We allow for\\nmultiple FFF trees to jointly compute the intermediate\\nlayer outputs. This is achieved by summing the outputs\\nof the individual trees and presenting the sum as the\\nintermediate layer output.\\nWe denote a model with K trees of depth D + 1 by append-\\ning a suffix to the model name, i.e. UltraFastBERT-KxD.\\nNote that for consistency with our inference code, we con-\\nsider a tree with no edges to have depth 0 – hence the tree\\nwith maximum depth D has depth D + 1. A BERT-base-\\nsized model with the traditional feedforward layer of width\\n3072 is then just a special case of UltraFastBERT, namely\\nUltraFastBERT-3072x0.\\nWhile we share only our fastest model, we train a full\\nrange of increasingly deeper and narrower models, start-\\ning from UltraFastBERT-3072x0 and proceeding with\\nUltraFastBERT-1536x1, UltraFastBERT-512x2, etc..\\n2.2. Training\\nWe follow the final training procedure of crammedBERT\\n(Geiping & Goldstein, 2023), namely disabling dropout in\\npretraining and making use of the 1-cycle triangular learning\\nrate schedule. By default, we train every model for 1 day on\\na single A6000 GPU, except for the final UltraFastBERT-\\n1x11-long model, which we train 2 times longer using the\\nsame regime for slightly better downstream performance.\\n2.3. Downstream Performance\\n2.3.1. SETUP\\nWe finetune all UltraFastBERT models for the RTE, MRPC,\\nSST, STS-B, MNLI, QQP, QNLI, and CoLA tasks of the\\nGLUE benchmark (Wang et al., 2018) and report evaluation\\nscores as in Geiping & Goldstein (2023) for consistency. In\\nshort, this approach amounts to finetuning for 5 epochs with\\nlearning rate 4 × 10−5 across all tasks.\\n2\\nExponentially Faster Language Modeling\\nModel\\nNT\\nNI/NT\\nRTE\\nMRPC\\nSTSB\\nSST-2\\nMNLI\\nQNLI\\nQQP\\nAvg\\nCoLA\\nAvg\\nBaselines\\ncrammedBERT-3072\\n4095\\n100.0%\\n58.8\\n87.6\\n85.2\\n91.9\\n82.8\\n90.4\\n89.0\\n83.6\\n45.0\\n79.3\\ncrammedBERT-4095\\n3072\\n100.0%\\n57.6\\n89.1\\n85.9\\n91.9\\n81.3\\n90.9\\n87.6\\n83.2\\n47.9\\n79.3\\nUltraFastBERTs\\nUltraFastBERT-3072x0\\n3072\\n100.0%\\n56.7\\n88.9\\n86.3\\n92.3\\n82.9\\n92.3\\n88.0\\n83.8\\n48.4\\n79.9\\nUltraFastBERT-1536x1\\n4608\\n66.6%\\n55.2\\n89.4\\n85.0\\n91.9\\n82.2\\n90.1\\n89.0\\n83.1\\n47.5\\n79.2\\nUltraFastBERT-512x2\\n3584\\n42.9%\\n59.2\\n87.7\\n86.0\\n89.9\\n81.9\\n90.3\\n89.3\\n83.3\\n46.2\\n79.2\\nUltraFastBERT-256x3\\n3840\\n26.7%\\n54.2\\n87.4\\n85.9\\n91.6\\n81.6\\n90.0\\n89.1\\n82.7\\n48.0\\n78.8\\nUltraFastBERT-128x4\\n3968\\n16.1%\\n58.4\\n87.5\\n87.2\\n92.3\\n81.2\\n89.9\\n90.0\\n83.5\\n45.9\\n79.3\\nUltraFastBERT-64x5\\n4032\\n9.5%\\n55.7\\n89.0\\n87.2\\n91.4\\n81.6\\n90.2\\n89.4\\n83.3\\n46.1\\n79.1\\nUltraFastBERT-32x6\\n4064\\n5.5%\\n57.6\\n88.2\\n86.1\\n91.2\\n81.0\\n89.2\\n88.3\\n82.8\\n40.6\\n78.1\\nUltraFastBERT-16x7\\n4080\\n3.1%\\n55.5\\n89.0\\n86.7\\n88.9\\n80.1\\n89.4\\n86.9\\n82.1\\n41.5\\n77.6\\nUltraFastBERT-8x8\\n4088\\n1.8%\\n56.2\\n88.4\\n85.4\\n88.7\\n80.6\\n89.3\\n86.4\\n81.9\\n32.7\\n76.5\\nUltraFastBERT-4x9\\n4092\\n1.0%\\n53.8\\n85.9\\n85.7\\n89.6\\n81.9\\n89.3\\n88.0\\n82.0\\n31.8\\n76.4\\nUltraFastBERT-2x10\\n4094\\n0.5%\\n59.9\\n88.8\\n85.3\\n87.4\\n79.9\\n89.2\\n86.1\\n82.0\\n35.4\\n76.9\\nUltraFastBERT-1x11\\n4095\\n0.3%\\n57.8\\n88.1\\n86.1\\n89.7\\n80.2\\n89.3\\n87.1\\n82.3\\n37.1\\n77.3\\nFinal Model\\nUltraFastBERT-1x11-long\\n4095\\n0.3%\\n60.7\\n87.5\\n86.4\\n89.9\\n81.3\\n89.7\\n87.6\\n83.0\\n35.1\\n77.7\\nExternal Baselines\\nOpenAI GPT\\n3072\\n100%\\n56.0\\n82.3\\n80.0\\n91.3\\n81.4\\n87.4\\n70.3\\n78.8\\n45.4\\n75.1\\nDistilBERT\\n3072\\n100%\\n59.9\\n87.5\\n86.9\\n91.3\\n82.2\\n89.2\\n71.3\\n81.2\\n52.1\\n77.6\\nBERT-base\\n3072\\n100%\\n66.4\\n88.9\\n85.8\\n93.5\\n83.4\\n90.5\\n71.2\\n83.0\\n51.3\\n79.6\\nTable 1. The results of various language models on the GLUE-dev test sets. NT denotes the number of neurons available for training,\\nNI/NT the proportion of neurons that are used for a single inference. “Avg” denotes the average score of all the task results to the left of\\nthe column. Emphasis marks the best crammed 1-day UltraFastBERT performance for the given column. OpenAI GPT, DistilBERT, and\\nBERT-base refer to models reported in Radford et al. (2018); Sanh et al. (2019); Devlin et al. (2018).\\nWe find that UltraFastBERT models finetuned in this man-\\nner for CoLA end up being undertrained if only 5 train-\\ning epochs are used. Therefore, we extend the number of\\nCoLA finetuning epochs to 15. This leads to little to no\\nimprovement for the baseline crammedBERT models but\\nhas a significant impact on the CoLA performance of Ultra-\\nFastBERTs.\\n2.3.2. RESULTS\\nThe results of our finetuning are listed in Table 1.\\nWe see that UltraFastBERT variants trained for 1 day on a\\nsingle A6000 GPU all retain at least 96.0% of the GLUE\\ndownstream predictive performance of the original BERT-\\nbase model (Devlin et al., 2018). We also observe that\\nthe performance decreases with the increasing depth of the\\nFFFs. Note, however, that the majority of the performance\\ndecrease due to the increasing depth is caused by only a\\nsingle task – CoLA. This behaviour has previously been ob-\\nserved in the literature and is in line with other work trying\\nto compress BERT behaviour into smaller models (Sun et al.,\\n2019; Turc et al., 2019; Mukherjee et al., 2021). If we disre-\\ngard CoLA, at least 98.6% of the predictive performance is\\npreserved by all UltraFastBERT model.\\nFurthermore, we see that save from CoLA, our best model –\\nUltraFastBERT-1x11-long – performs on par with the orig-\\ninal BERT-base model while using only 0.3% of its own\\nneurons, which amounts to a mere 0.4% of BERT-base neu-\\nrons. We make the weights of this model public.\\n3. Inference\\nIf the purpose of the above part was to report the finding\\nthat only very few neurons are needed per inference, it is\\nthe goal of this section to adopt the engineering perspec-\\ntive and outline how this can be taken advantage of on the\\nimplementation front.\\nFast feedforward networks as a part of large language mod-\\nels have a huge acceleration potential. To indicate the sort\\nof speedup ballpark one could hope for, take GPT-3 (Brown\\net al., 2020), the first large language model widely lauded\\nfor the plausibility of its outputs. The feedforward networks\\nof each transformer layer of GPT-3 consist of 49152 neu-\\nrons. If trainable, this network could be replaced with a fast\\nfeedforward network of maximum depth 15, which would\\n3\\nExponentially Faster Language Modeling\\nCPU Implementation\\nGPU Implementation\\nModel\\nLimit\\nLevel 1\\nLevel 2\\nLevel 3\\nNative fused\\nPytorch BMM\\nNaive CUDA\\nBERT-base-4095\\n1.00x\\n1.00x\\n1.00x\\n1.00x\\n1.00x\\n1.00x\\n1.00x\\nBERT-base-3072\\n1.33x\\n1.55x\\n1.74x\\n1.39x\\n1.33x\\n1.61x\\n1.82x\\nUltraFastBERT-1x11\\n341.25x\\n130.7\\n255.1\\n-\\n-\\n39.45x\\n117.83x\\nTable 2. The results of the inference acceleration evaluation. Emphasis highlights the best “fair comparison” performance.\\nAlgorithm 1: FFF inference forward pass.\\nInput: B × H input matrix I,\\n(2D −1) × H weight matrix W in,\\n(2D −1) × H weight matrix W out\\nIntermediate :B × D logit matrix L,\\nB × D node index matrix N\\nOutput: B × H matrix O\\nFunction CMM(I, W in):\\nfor d ∈{1, . . . , D −1} do\\nL⋆,d ←I\\n\\x10\\nW in\\n[N⋆,d−1],⋆\\n\\x11T\\nN⋆,d ←2N⋆,d−1 + 1 + (L⋆,d > 0)\\nend\\nreturn L, N\\nFunction FFFI(I, W in, W out):\\nL, N ←CMM(I, W in)\\nL ←AC T IVATION(L)\\nfor d ∈{0, . . . , D −1} do\\nO⋆,d ←L⋆,d · W out\\nN⋆,d,⋆\\nend\\nreturn O\\ncontain 65536 neurons but use only 16 for inference. This\\namounts to about 0.03% of GPT-3’s neurons.\\nAt the center of this promise sits the operation of conditional\\nmatrix multiplication, with its pseudocode given below, and\\nwith our future efforts focused on its efficient implementa-\\ntion.\\n3.1. Algorithm\\nBelcak & Wattenhofer (2023) gives recursive pseudocode\\nfor FFF inference. We list the pseudocode for CMM and\\nthe consecutive inference for FFFs, with modifications as\\nper Section 2.1. In Algorithm 1, B denotes the batch size,\\nH the layer input width (transformer hidden dimension),\\n2D −1 is the number of neurons, and M⋆,k, Ml,⋆denote\\nthe k-th column and l-th row of M, respectively. The result\\nof the >-comparison in CMM is assumed to be an integer\\n∈{0, 1}.\\n3.2. Compatibility\\nOne may ask whether the conditionality introduced by the\\nuse of CMM does not make FFFs incompatible with the\\nprocesses and hardware already in place for dense matrix\\nmultiplication and deep learning more broadly. In short, the\\nanswer is “No, it does not, save for some increased caching\\ncomplexity.”\\nSingle-threaded CPU DMM as a part of feedforward infer-\\nence relies on sequential execution of multiplication and\\naccumulation (MAC) instructions. As such, CPUs, espe-\\ncially edge CPUs, stand to benefit the most easily from the\\nreplacement of DMM with CMM as seen in UltraFastBERT,\\nsimply because fewer executions of the per-element MAC\\ninstructions are needed to compute layer output. In spite of\\nthe apparent use of conditionality, which is commonly asso-\\nciated with branching in CPU code, the “neural branching”\\nseen in CMM manifests itself only as an addition of a mem-\\nory offset to the relevant pointers. Hence, instruction branch\\nprediction is never engaged to facilitate CMM conditionality.\\nIn order to make full use of weight caching to speed up the\\naccess to weights, the CPU might need to be hinted to load\\nonly relevant columns of the weight matrix and only one at\\na time. Since CMM continues to perform row-column dot\\nproducts, vector single-instruction-multiple-data (SIMD)\\nparallel processing remains a viable option for speeding up\\ndevice-specific inference implementations.\\nThe implicitly multi-threaded GPU DMM computation\\nmakes extensive use of the single-instruction-multiple-\\nthreads (SIMT) approach behind modern GPUs by exe-\\ncuting the same MAC instructions in each thread, just on\\ndifferent patches of the matrices. As above, note that this\\nreadily carries over to CMM since the conditionality rep-\\nresented by proceeding to different columns of the weight\\nmatrices affects only the offset to the memory used, and\\nnot which, if, or how many times the MAC instructions are\\nexecuted. Nevertheless, efficient DMM implementations\\ndistribute the matrix multiplication workload (the pairs of\\nmatrix patches to be multiplied) in a manner that maximizes\\nthe use of distributed cache so that the accesses to the global\\ndevice memory, being significantly slower than accessing\\ncache, are limited. To achieve its full potential with respect\\nto the DMM baseline, any efficient implementation of CMM\\n4\\nExponentially Faster Language Modeling\\nhas to explicitly manage its caching in a way that is optimal\\nfor tree traversal, and not patched dense matrix multiplica-\\ntion. This can be done by always pre-loading the weights of\\nthe relevant sub-trees or by using DMM patching strategies\\nbut discarding intermediate results from the results of patch\\nmargins where not needed. Either way, it remains to be\\na challenge to make these optimizations without intimate\\n(and often confidential) knowledge of the implementation’s\\ntarget device.\\n3.3. Inference Performance\\nWe compare the speed of several available FF/FFF inference\\nimplementations.\\nImplementations.\\nFor CPU inference, we use the Math\\nKernel Library available as a part of the Intel oneAPI.\\n• Level 1 implementation is the implementation con-\\nstructed using only BLAS Level 1 routines and BLAS-\\nlike Level 1 extensions, namely the vector-vector dot\\nproduct and scalar-vector product.\\n• Level 2 implementation uses batched BLAS Level 2\\nroutines and BLAS-like Level 1 extensions, namely\\nthe batched matrix-vector multiplication and batched\\nscalar-vector product.\\n• Level 3 implementation uses the (non-batched) BLAS\\nLevel 3 matrix-matrix multiplication. This is the fastest\\nCPU implementation for FF, but no such implemen-\\ntation can be provided at this time for FFF due to the\\nvector-level sparsity of CMM not being supported by\\nthe library.\\nFor the GPU implementations, we use either PyTorch ker-\\nnels or custom CUDA kernels.\\n• Native fused implementation uses the native fused\\nfeedforward layer kernel. Note that this is the fastest\\nGPU implementation for FF layers but again, no such\\nkernel currently exists for FFFs due to the nature of\\nCMM.\\n• BMM implementation uses the batched matrix multi-\\nplication and activation kernels for both FFs and FFFs.\\nIn the case of FFFs, we extensively use vector copying\\nat each step of tree descent to simulate conditionality.\\n• Naive CUDA implementation is our custom CUDA\\nkernel code for both FFs and FFFs, performing\\nfused DMM/CMM and activation on the level of vec-\\ntor/matrix elements, executed as a PyTorch extension.\\nMethodology.\\nFor CPU inference, we perform 250 for-\\nward passes per entry on Intel(R) Core(TM) i7-6700HQ\\nCPUs under Intel MKL v2023.2.0, using 64-bit variants\\nof all routines. We report the mean time taken by single\\ninference, noting that the value of the standard deviation\\nalways lay well under 2% of the mean. For GPU inference,\\nwe perform 1000 forward passes per entry on NVIDIA RTX\\nA6000 GPUs under CUDA v11.7 and PyTorch 2.0.1. We\\nmeasure the GPU time and report the mean time taken, with\\nthe standard deviation again well under 2% of the mean in\\nall cases. We take batch size B = 128 × 128 (equivalent to\\nthe BERT pretraining context token batch size) and hidden\\ndimension H = 768.\\nResults.\\nTable 2 lists the performance comparison of feed-\\nforward and fast feedforward layers as they appear in BERT-\\nbase and UltraFastBERT-1x11. Each column of the ta-\\nble lists the relative inference FFF-over-FF implementa-\\ntion speedups when using the same linear-algebraic routine\\nprimitives.\\nThe two entries missing Table 2 are for the currently un-\\navailable BLAS Level 3 and Native fused implementations\\nof FFFs.\\nFurther comparisons.\\nAll of the speedups reported in\\nTable 2 give “fair comparisons”, meaning that in each case,\\nboth the FF and FFF implementation used exactly the same\\nprimitive linear-algebraic operations. One may also be in-\\nterested in knowing how the best implementations of FFF\\ncurrently fair against the best implementations of FF, even\\nthough the ones for FF use primitives unavailable for FFF.\\nOn CPU, the Level 1 and Level 2 implementations of FFF\\nperform inference 48x and 78x faster than the fastest (Level\\n3) implementation of FF, respectively. On GPU, the PyTorch\\nBMM implementation of FFF delivers a 3.15x speedup over\\nthe fastest (Native fused) implementation of FF.\\n3.4. Future outlook\\nThe broad strokes for starting efficient implementation of\\nFFF inference have already been painted as a part of the\\nPyTorch library. Hybrid vector-level sparse tensors, if fully\\nsupported for singular and batched matrix multiplication,\\nwould suffice to implement CMM and FFF inference as in\\nAlgorithm 1.\\nA further native implementation of CMM as a part of device-\\nspecific Intel MKL/NVIDIA cuBLAS code would stand a\\nreal chance of fully delivering on the promise of 341-fold\\nspeedup.\\n5\\nExponentially Faster Language Modeling\\n4. Conclusion\\nWe present UltraFastBERT, a modified version of the\\n(crammed)BERT architecture that uses fast feedforward in-\\nstead of feedforward networks in its intermediate layers. Ul-\\ntraFastBERT serves as proof that large language models only\\nreally need to engage an exponential fraction of their param-\\neters to perform individual inferences. UltraFastBERT-1x11,\\nour deepest model with the highest promise of acceleration,\\nuses only 0.3% of its neurons during inference and already\\nachieves a 78x CPU speedup over the inference time of\\nthe corresponding feedforward layer. With a theoretical\\nspeedup promise of 341x at the scale of BERT-base models,\\nwe hope that our work will inspire an effort to implement\\nprimitives for conditional neural execution as a part of de-\\nvice programming interfaces.\\nReferences\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\\nZ., et al.\\nPalm 2 technical report.\\narXiv preprint\\narXiv:2305.10403, 2023.\\nBelcak, P. and Wattenhofer, R. Fast feedforward networks.\\narXiv preprint arXiv:2308.14711, 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nGeiping, J. and Goldstein, T. Cramming: Training a lan-\\nguage model on a single gpu in one day. In International\\nConference on Machine Learning, pp. 11117–11143.\\nPMLR, 2023.\\nMukherjee, S., Awadallah, A. H., and Gao, J. Xtremedistil-\\ntransformers: Task transfer for task-agnostic distillation.\\narXiv preprint arXiv:2106.04563, 2021.\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\\net al. Improving language understanding by generative\\npre-training. 2018.\\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\\na distilled version of bert: smaller, faster, cheaper and\\nlighter. arXiv preprint arXiv:1910.01108, 2019.\\nSun, S., Cheng, Y., Gan, Z., and Liu, J. Patient knowledge\\ndistillation for bert model compression. arXiv preprint\\narXiv:1908.09355, 2019.\\nTurc, I., Chang, M.-W., Lee, K., and Toutanova, K.\\nWell-read students learn better:\\nOn the importance\\nof pre-training compact models.\\narXiv preprint\\narXiv:1908.08962, 2019.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and anal-\\nysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461, 2018.\\n6\\n', 'source_name': 'Exponentially Faster Language Modeling', 'source_url': 'https://arxiv.org/abs/2311.10770'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DSelect_k_NOTES.pdf #51\n",
      "{'content': 'DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to \\nMulti-Task Learning \\nMain Idea: sparse gates commonly used for MoE, like top-k, are not smooth (continuously \\ndifferentiable), which can lead to performance issues in gradient-based methods. Dselect-k \\npresents a fully differentiable sparse gate for MoE. \\nDifferentiability – a function that is differentiable is a function which has a defined derivative at \\nevery point. This is a requirement for gradient-based methods. \\nContinuous Differentiability – a function is continuously differentiable if it is fully differentiable \\nAND the corresponding derivative is continuous. Continuous meaning smooth, with no abrupt \\nchanges and bumps. Continuous differentiability is not a requirement but optimizes performance \\nof gradient-based methods. \\n- \\nTop-k routing is not continuously differentiable due to the router’s hard selection of \\nexperts. This hard routing leads to it being possible for small changes in the input score \\nto have large changes in the expert weights, which is not ideal. \\n- \\nDselect-k achieves continuous differentiability through smoothing techniques. \\n \\nMy takeaways: \\n- \\nAlthough Dselect-k in theory should perform better than top-k, this technique has not \\nbeen applied much in practice. This can be attributed to the increased computational \\ncomplexity it brings, as well as to the simplicity and proven practical use of top-k. \\no Some other recently proposed continuous differentiability methods for MoE \\nrouting, check soft MoE (optimized for vision) and Mixture-of-Tokens (optimized \\nfor text generation). \\n \\n', 'source_name': 'DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DSelect_k_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Hash_Layers_NOTES.pdf #52\n",
      "{'content': 'Hash Layers for Large Sparse Models \\nMain Idea: this paper experiments with using a hash router in an MoE architecture and compares \\nit to other methods like top-k (Switch) and BASE.  \\n \\nHashing \\n- \\nHashing refers to using a function that converts the input into a fixed size output that is \\nunique for each input.  \\n- \\nHashing does not need to be learned and there is no need for a load balancing loss. \\n- \\nIn a setting where the goal is to scale a model as much as possible (MoE), it is not realistic \\nto try to optimize every hyperparameter and modeling decision (extensive tuning at this \\nscale is too expensive). Hashing does not require this. \\n- \\nHashing is also a fixed mapping function, meaning it does not suffer from the issue from \\nrouting fluctuation/inconsistency during training (this was explored in the StableMoE \\npaper). \\n- \\nSome hashing functions used: \\no Clustered hashes – hash training inputs based on k-means clustering. \\no Dispersed hashes – assume the opposite of clustered hashes, that similar inputs \\nneed a more fine-grained distinction and should be assigned to different experts \\n(closer inputs should be routed to different experts). \\no Random hashing. \\no Balanced assignment hashing. \\no Oracle future hash – obtains a hash to route token t based on token t+1 (the next \\ntoken). \\n- \\nThis paper also experiments with what they call MultiHashLayer, which consists of using \\ndifferent hashing strategies in the same network, as to not rely on a single hashing \\nstrategy. \\n \\nModels used in Experiments \\n- \\nBaseline is a 222M parameter dense Transformer. \\n- \\nWider dense Transformer of 755M parameters. \\n- \\nDeeper dense Transformer of 755M parameters. \\n- \\nTo compare to BASE, a 4.5B total parameter architecture with balanced assignment \\nhashing is used. \\n \\nResults \\n- \\nWhen using a single MoE layer in a Transformer architecture (all other FFs remain the \\nsame), balanced Hash slightly outperforms Switch (using 64 experts in a model of 751M \\ntotal parameters). \\n- \\nDeep dense models of the same size as MoE (in terms of total parameters) outperform \\nMoE, showing good dense models make better use of each parameter. At the same \\ninference speed (active parameters), MoE performs better. \\n- \\nIncreasing the number of experts (from 64 to 128) leads to a better increase in Hash over \\nSwitch. \\no This indicates that the more experts there are in a layer, the less important it is to \\nlearn to route. \\n- \\nAs with BASE layers, adding a single Hash layer to a Transformer is shown to work better \\nat later layers of the network. \\n- \\nIncreasing the number of sparse layers (in a setting where dense FFs and MoE layers are \\nalternated) (5 sparse layers with 16 experts each) leads to better Switch performance \\nover Hash. \\no Increasing the number of experts per layer might change this. \\n- \\nFine-tuning trends are consistent with pre-training trends. \\no The only part that can be frozen without hurting performance are the sparse \\nlayers. \\n \\nAnalysis/Results of Hashing Strategies \\n- \\nRandom and balanced hashing have similar performance (but balanced hashing has \\ntraining advantages over distributed training schemes). \\n- \\nRandom hashing outperforms clustered hashes. \\no Proves the hypothesis that if tokens are like each other, a more fine-grained \\ndistinction is needed, and the tokens need to be routed differently. \\n- \\nDispersed hashing (opposite of clustered hashing) performs slightly better than random \\nhashing. \\n- \\nLearned routing (like BASE or Switch) generally provide clustered expert modules, which \\ncould be a disadvantage based on the results obtained during this research. \\n- \\nBigram and previous token hashing perform worse than just relying on the current token. \\no This indicates that using the previous token to help with routing is harmful. \\n- \\nIncreasing the dictionary size used for tokenization (thus increasing the number of \\npossible hashes) leads to a decrease in performance against Switch. \\no This indicates that Hash might be better suited for scenarios where the dictionary \\nsize is small (so there are less possible hashes), while Switch is better suited to \\nlarge dictionary size scenarios. \\n- \\nOracle future token hashing essentially solves the task. \\no This is expected since the hashing is performed on the target token (the answer). \\n- \\nIncreasing the diversity of hashing strategies (MultiHashLayer) seems to help. \\n- \\nA learned routing based on the current token (and not on the hidden state, as Switch \\nrouting works) leads to small improvements. \\no This is a mix between the hashing strategy and Switch. \\n- \\nWhen comparing Hash vs BASE, Hash outperforms BASE in every training step. BASE also \\nshows instabilities at late stages of training, while Hash’s performance consistently \\nimproves (due to fixed routing). \\n \\nConclusion \\nHash shows that there are lots of room for improvement in learned routing strategies. Hash \\nshould be used as a baseline for improving learned strategies in future work. \\n \\nMy takeaways: \\n- \\nWhy is it that random routing outperforms clustered hashes and dispersed hashing \\nperforms even better? Shouldn’t clustered hashing make more sense since we want \\nexperts to specialize in specific clusters of the input space? These results seem to indicate \\nthe opposite. \\n \\n', 'source_name': 'Hash Layers for Large Sparse Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Hash_Layers_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Soft_MoE_NOTES.pdf #53\n",
      "{'content': 'From Sparse to Soft Mixtures of Experts \\nMain Idea: Soft MoE presents a strategy to combat the issue of a traditional MoE of not having \\nthe property of continuous differentiability. By making a discrete choice (hard routing) to obtain \\nsparsity, traditional MoE introduces training instabilities, as small changes in the input may lead \\nto large changes in the model’s output, since this small change may end up changing the expert(s) \\nchosen. The soft MoE architecture is compatible with certain tasks such as image classification in \\nVision or machine translation in Language, but it is not compatible with Natural Language \\nGeneration (NLG). An equivalent approach that could be compatible with language generation \\nwas proposed as “Mixture-of-Tokens” (MoT) (in a different paper), but this MoT architecture \\nseemed to also bring significant challenges that remain unsolved. The continuous differentiability \\nproperty of Soft MoE is, therefore, only able to be applied to a limited set of tasks. \\n \\nTraditional MoE Routing \\nIn traditional MoE, each input is routed to its corresponding expert in a hard manner (SoftMax is \\nconverted to 1 or 0) and the available slots are then occupied by a single token at a time (each \\nslot gets 1 token). This means the experts will be updated solely based on that token. \\nOBS: slot refers to each inference run supported by the expert until it reaches its maximum \\ncapacity. \\n \\nSoft MoE Algorithm \\n- \\nEach slot pi of each expert has learnable parameters. \\n- \\nThe input tokens X are passed through each slot and a SoftMax is applied at the column \\nlevel. \\no This means that the input slots to be passed to each expert are simply a weighted \\nlinear combination of all the input tokens with the respective slot’s learnable \\nparameters. \\n- \\nWe then obtain the output slots by passing each input slot to a corresponding expert. \\n- \\nThe output slots are then merged through some combine weights, which are the inputs \\npassed through the slot’s learnable parameters but now softmaxed at the row level (per \\ntoken). \\n- \\nAs explained in the paper’s figure: \\n \\no Soft MoE first computes scores or logits for every pair of input token and slot, \\nbased on some learnable per-slot parameters. \\no These logits are then normalized per slot (columns)  \\n▪ So now we have, for each slot, a weight to give to each input token, which \\nsum up to one per slot. \\no And every slot computes a linear combination of all the input tokens based on \\nthese weights. \\n▪ The \\ntokens’ \\nweights/embeddings \\nare \\nadjusted \\nbased \\non \\nthe \\nweights/importance assigned to them per slot. \\no Each expert (an MLP) then processes its slots. \\n▪ Now we have the experts’ outputs. \\no Finally, the same original logits are then normalized per token (by row) and used \\nto combine all the slot outputs, for every token. \\n▪ To get the final output for each token, we then obtain the softmaxed \\nweights now normalized per token (instead of the slot’s weights sum up to \\n1, each token’s weights sum up to 1) and combine the expert’s outputs \\nwith those weights accordingly. \\nIntuition for softmaxes \\n- \\nBy slot (column) \\no Leads to scores being given for each token by the slot, used to measure the \\nimportance which should be given to each token for a specific slot (how much \\nshould the slot consider each token). \\n- \\nBy token (row) \\no Leads to scores being given for each slot (by the token), used to measure the \\nimportance which should be given to each slot for a specific token (to help \\ndetermine the final output for each token) (how much should the token consider \\neach slot). \\nProperties of Soft MoEs \\n- \\nUsually to get past the token-expert assignment problem, MoE architectures resort to \\nhard assignment methods such as top-k token-choice or expert-choice. These measures \\nare discrete in nature, and thus non-differentiable. Soft MoE, on the other hand, is fully \\ndifferentiable and continuous. \\n- \\nSoft MoE does not suffer from token dropping or expert imbalance. \\n- \\nSoft MoEs adjust better to hardware accelerators than “hard” MoE methods, mainly due \\nto avoiding top-k/sorting routing operations (these are not well suited for hardware \\naccelerators). Therefore, Soft MoEs are fast. \\n- \\nSoft MoEs are neither sparse (since every token is a weighted average for all input tokens) \\nnor dense (since every expert only processes a subset of the slots, and not all input \\ntokens). \\n- \\nTraditional MoE models are not so predictable at the sequence-level since inputting a \\nsingle sequence may force the router to use every expert to balance the load and thus \\nminimize the loss. This can lead to too generalist experts. Traditional MoEs are more \\npredictable at the batch-level (more tokens) since a small number of tokens can fight for \\nthe same expert at the sequence level, but this risk is smaller at the batch level. Since in \\nsoft MoEs all tokens are grouped together and every expert handles tokens from every \\ninput, this risk is not present – leading to more deterministic/predictable and faster \\ninference. \\nOBS: the number of slots in a soft MoE is a hyper-parameter (must be equal or greater than the \\nnumber of experts). \\nLimitation in NLG: \\n- \\nSoft MoE was only experimented with in an image classification scenario. Translating this \\nmethod to an NLG setting is not so straightforward. \\no This is because soft MoE uses all input tokens to compute all output tokens at \\nonce. In NLG, each input token is generated at a time/separately (one-by-one) and \\nis used as a part of the context to predict the next token. It is possible to use causal \\nmasking techniques to only take one token at a time, but this can lead to a bias in \\ntraining (correlation between token position and a slot). \\no The sequential nature of token generation thus complicates the application of the \\nSoft MoE architecture to language generation tasks. \\n- \\nMore research is needed to translate Soft MoE into an NLG setting. \\nMemory Consumption \\nSoft MoE works best when each expert is assigned to one slot only. Therefore, many experts need \\nto be trained and stored, which comes with big costs in terms of memory. \\nExperiments (Image Classification only) \\n- \\nSoft MoE is compared to other MoE methods – token-choice and expert-choice – and a \\ndense setting and outperforms all of them in all hyperparameter scenarios. \\n- \\nWith cheaper training and inference costs, Soft MoE outperforms Vision Transformers at \\na large scale for a given compute budget in both pre-training and fine-tuning. \\n- \\nSoft MoE scales the number of experts well (more experts = better). Additionally, scaling \\nthe number of experts in Soft MoE doesn’t really change training time, while this can have \\na tremendous negative effect in training time with token-choice and expert-choice. \\n \\nMy takeaways: \\n- \\nSoft MoE seems to instead of routing each token individually, to route all tokens to each \\nexpert. This means that the expert will choose how much importance to give to each input \\ntoken. The weighted average of the experts is then summed up based on the weights \\ngiven to each token (normalized per token) to get the final output for each token. \\n \\n', 'source_name': 'From Sparse to Soft Mixture of Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Soft_MoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Expert_Choice_NOTES.pdf #54\n",
      "{'content': 'Mixture-of-Experts with Expert Choice Routing \\nMain Idea: the main goal of this paper is to tackle the limitations in the MoE architecture caused \\nby imperfect load balancing, which leads to under-training some experts and over-training \\nothers, as well as dropping tokens, through a novel approach called Expert Choice (EC). \\n \\nHow Traditional MoE (Token-Choice) Works \\n- \\nPass inputs into a gating mechanism which selects the most relevant k expert(s), in a \\nprocess that relies on each individual token selecting the most relevant expert (token-\\nchoice). This leads to training inefficiency as the tokens are unevenly distributed.  \\no To help with this, an auxiliary loss is commonly used and added to the loss \\nfunction, but this still leads to some imbalance.  \\n- \\nThe issue of expert capacity is also prevalent, since for efficient computation, usually each \\nexpert has a fixed block size to work with in terms of token assignment. A capacity factor \\ncan be increased to minimize dropped tokens, but this leads to more memory inefficiency. \\n- \\nIn token-choice, each input is also assigned a fixed compute, regardless of its complexity \\nand/or task. \\nExpert Choice (EC) \\nAt a high-level, EC routing has the expert models picking the most relevant input tokens instead \\nof the other way around. \\n \\n𝐾= 𝑒𝑥𝑝𝑒𝑟𝑡 𝑐𝑎𝑝𝑎𝑐𝑖𝑡𝑦=\\n𝑛∗𝑐\\n𝑒, where n is the number of tokens in a batch, c is the capacity factor \\nhyperparameter and e is the number of experts. (c can also be thought of as the average number \\nof experts assigned to each token). Expert capacity is the maximum number of tokens that can \\nbe assigned to each expert at a batch-level. \\nS =  token –  expert affinity =  SoftMax(x, Wg), where x is the input token representation and \\n𝑊\\n𝑔 is expert’s g embedding. \\nG = matrix with weights given to each expert. \\nI = index matrix where I[I, j] specifies the j-th selected token of the i-th expert. \\n𝐺, 𝐼= 𝑇𝑜𝑝𝐾(𝑆𝑡, 𝐾) \\n𝑃= 𝑂𝑛𝑒𝐻𝑜𝑡(𝐼) \\nThe gating input to each expert is then determined by 𝑋𝑖𝑛= 𝑃∗𝑋 \\nW1, W2 = parameters of the experts. \\nXe[i]  =  output of expert I =  GeLU(Xin[i] ∗ W\\n1[i]) ∗ W2[i]t \\nOBS: EC routing has no constraints for the number of experts assigned to each token.  \\nOBS2: The capacity factor in EC is the equivalent to top-k in token-choice -> it is the average \\nexperts assigned to each token. \\n \\nResults \\n- \\nEC with a capacity factor of 2 should be computationally equivalent to top-2 routing. EC-\\nCF2 has training convergence 2x faster than GShard top-2 routing. \\n- \\nScaling the number of experts during pre-training, given the same expert size, leads to \\nbetter results, as expected (more total parameters = more specialized model = better \\nquality). \\n- \\nEC-CF2 performs better than Switch top-1 and GShard top-2 in all settings, but given a \\nfixed expert size of 100M, increasing the number of experts seems to lead to worse fine-\\ntuning results (opposite to pre-training results). \\n- \\nCapping the number of experts to be assigned to each token leads to worse fine-tuning \\nresults. This shows that allowing variable number of experts per token is indeed helpful. \\n- \\nEC learns to allocate a variable number of experts per token. \\n \\nMy takeaways: \\n- \\nUnderstanding the routing mechanism as an unsupervised clustering method \\no At the early stages of training a model with MoE layers, the routing mechanism \\n(assuming it is a token-choice method and that it is learned) is random, that is, it \\ndoes not have information regarding of the area it will specialize in on the \\nembedding input space. Without load balancing, the risk is of a specific expert \\nbeing disproportionately chosen at these early stages, and thus taking up a large \\narea of the input space for itself.  \\n▪ In other words, as an expert is picked by the routing mechanism on inputs \\nof a specific cluster that it performs well on in relation to other experts, it \\nwill gain abilities that can be generalized to other clusters that other \\nexperts still do not have, due to the lack of tokens being assigned to them. \\nThis will create a feedback loop that results in a single expert taking up \\nmore and more input space, due, again, to the generalization abilities that \\nit picks up along the way, which will end up averaging a single dense model, \\nsince the tendency is for this over-generalized expert to take up the entire \\ninput space area for itself. \\no The addition of an auxiliary load balancing loss is added to prevent this. To \\nvisualize this, we can think of an expert trying to grow its input space area but \\nquickly reverting to a smaller area because of penalization effects.  \\n▪ Although this is helpful, there is still the risk of non-perfect clusters being \\nassigned to each expert, especially at a batch level, which leads to other \\nissues like token dropping. \\no In Expert Choice, this auxiliary load balancing loss is not needed, as the experts \\nthemselves will pick the tokens that are more relevant to them at a batch level \\n(and not the other way around). If an expert has already reached full capacity, the \\n2nd expert that wanted that token the most will be chosen, etc. \\n▪ I can imagine this leading to other problems. For example, some batches \\nwill contain tons of tokens that are part of the cluster of a specific expert, \\nbut the expert won’t be able to choose them because it has reached full \\ncapacity. In a token-choice scenario, this might lead to token dropping, \\nwhich has the negative consequence of certain tokens not being used for \\ninference (loss of information). In EC, this is not felt, but a new \\nconsequence may arise: tokens from the cluster of an expert will be given \\nto another expert. Due to this impacting the next update, it can lead to \\nnearby experts fighting for the input space of other nearby experts. \\nAlthough this can be suboptimal at a batch level, training for many batches \\nmight neglect this effect (?). \\no Thought: Training an MoE model using token-choice and the strategy of \\nMegaBlocks seems to be the ideal way to train a MoE model. This would get rid of \\nthe token dropping of token-choice, and not suffer from the negative \\nconsequence created by EC. The only assumption we’d have to make is that the \\nload balancing loss and random noise penalties are a reliable way to find optimal \\ntoken-expert assignments, given that token dropping is not an issue. \\no Future work idea – visualize the gating mechanism process and how it routes \\ntraining inputs based on the clusters of each input embedding. Perhaps this can \\nbe done by using the checkpoints of the OpenMoE model (12 checkpoints \\navailable at HF I believe)? \\n- \\nBy not enforcing a constraint on the number of experts that can choose each token, EC \\ncreates a way for experts to determine how much compute will be used for each input. \\nThe idea is that the experts will learn complex and trivial inputs, maybe the intuition for \\nthis can be that complex inputs are in more complex/gray areas of the input space. With \\ncomplex inputs, more experts will choose the token, leading to more computation being \\nassigned to it. With trivial inputs that do not affect the output, no expert will choose the \\ntoken, leading to no compute being applied to the token (token is dropped). \\no The difference between this token-dropping and token-choice’s token dropping is \\nthat this token dropping is learned, and not forced by lack of expert capacity. \\no This is shown to be helpful to fine-tuning performance. \\n- \\nThe result of increasing the number of experts helping in pre-training but hurting fine-\\ntuning performance matches the findings of previous papers already discussed here. \\n \\n', 'source_name': 'Mixture-of-Experts with Expert Choice Routing', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Expert_Choice_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Expert_Choice.pdf #55\n",
      "{'content': \"Mixture-of-Experts with Expert Choice Routing\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng\\nChen, Quoc Le, and James Laudon\\nGoogle, Mountain View, CA, USA\\n{yanqiz, taole, hanxiaol, dunan, huangyp, vzhao, adai, zhifengc, qvl,\\njlaudon}@google.com\\nAbstract\\nSparsely-activated Mixture-of-experts (MoE) models allow the number of param-\\neters to greatly increase while keeping the amount of computation for a given\\ntoken or a given sample unchanged. However, a poor expert routing strategy can\\ncause certain experts to be under-trained, leading to an expert being under or\\nover-specialized. Prior work allocates a ﬁxed number of experts to each token\\nusing a top-k function regardless of the relative importance of different tokens. To\\naddress this, we propose a heterogeneous mixture-of-experts employing an expert\\nchoice method. Instead of letting tokens select the top-k experts, we have experts\\nselecting the top-k tokens. As a result, each token can be routed to a variable\\nnumber of experts and each expert can have a ﬁxed bucket size. We systematically\\nstudy pre-training speedups using the same computational resources of the Switch\\nTransformer top-1 and GShard top-2 gating of prior work and ﬁnd that our method\\nimproves training convergence time by more than 2×. For the same computational\\ncost, our method demonstrates higher performance in ﬁne-tuning 11 selected tasks\\nin the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our\\nmethod outperforms the T5 dense model in 7 out of the 11 tasks.\\n1\\nIntroduction\\nScaling up model capacity, dataset size, and training time has demonstrated huge success in enhancing\\nthe performance of computer vision architectures [4, 11, 13, 14] as well as neural language models [2,\\n20, 26, 27]. The ﬁnal model quality has been found to have a power-law relationship with the amount\\nof data, model size, and compute time [16, 20]. However, training efﬁciency, which is deﬁned as\\nthe total amount of computation used to achieve superior model quality than the state of the art\\nsystem [21], should receive greater attention as we increase our efforts towards green AI [29].\\nSparsely gated mixture-of-experts [31] (MoE) provides an effective way to scale model capacity\\ngiven a ﬁxed computational cost, and has recently played an important role in increasing the training\\nefﬁciency of large-scale language models [10, 21]. MoE operate by adopting a number of experts,\\neach as a sub-network, and by activating only one or a few experts for each input token. A gating\\nnetwork must be chosen and optimized in order to route each token to the most suited expert(s). For\\nexample, recent work has implemented sparse routing via k-means clustering [12], linear assignment\\nto maximize token-expert afﬁnities [22], or hashing [8, 28]. Many of the prior work use a routing\\nstrategy concerning the token choice, where each token selects the best one or two experts.\\nWe argue that the independent token choice of prior work often leads to an imbalanced load of experts,\\nwhich causes training inefﬁciency and sub-optimal training of the model. In order to mitigate this\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2202.09368v2  [cs.LG]  14 Oct 2022\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nFFN 1\\nFFN 2\\nFFN 3\\nFFN 4\\nRouter\\nRouter\\nToken 1\\nToken 2\\nWe\\nLike\\np = 0.8\\np = 0.65\\nRouter\\nRouter\\nToken 1\\nToken 2\\nWe            Like            To               Play          Soccer        In              The            Field\\nWe\\nLike\\nTo\\nPlay\\nToken 3\\nToken 4\\nToken 5\\nToken 6\\nToken 7\\nToken 8\\nWe\\nLike\\nSoccer\\nField\\nFFN 1\\nFFN 2\\nTop-k\\nTop-k\\nFigure 1: High-level Comparison Between Conventional MoE and expert choice MoE.\\nissue, previous sparsely gated networks introduce additional auxiliary losses as regularization to\\nprevent too many tokens being routed to a single expert, but the effectiveness is still limited. Recent\\napproaches [8, 22, 28] explore alternative strategies for routing, but they focus on pre-training only\\nand do not demonstrate performance gain on downstream tasks. Moreover, none of the previous\\nmethods consider allocating a variable number of experts to each token based on importance, which\\ncan be beneﬁcial.\\nWe propose a very simple yet effective routing method we are calling expert choice. Unlike conven-\\ntional MoE where tokens select one or two top-scoring experts, our method lets each expert pick the\\ntop-k tokens. Our method guarantees perfect load balancing, allows a variable number of experts\\nfor each token, and achieves substantial gains in training efﬁciency and downstream performance as\\ndemonstrated in our experiments. Our major contributions include:\\n• We identify common pitfalls in conventional MoE such as load imbalance as described\\nin Section 3.1. We then propose a heterogeneous, expert choice method to provide a ﬂuid\\nallocation of model parameters based on a learnt token-to-expert importance. This method\\nintrinsically guarantees load balance without imposing an auxiliary loss.\\n• We show our method provides over 2× faster training convergence in a 8B/64E (8 billion\\nactivated parameters, 64 experts) model, compared to the top-1 and top-2 gating counterparts\\nin Switch Transformer [10] and GShard [21].\\n• We show our method demonstrates strong scaling when increasing the number of experts\\nfrom 16 to 128, evaluated in training perplexity.\\n• We show our method demonstrates strong performance on downstream tasks selected from\\nGLUE and SuperGLUE at all the evaluated scales. More speciﬁcally, our 8B/64E model\\noutperforms a T5 11B dense model in 7 out of 11 tasks evaluated.\\n2\\nRelated Work\\nScaling: Various approaches have been proposed to scale up neural network capacity to improve\\nperformance. Recent works have successfully scaled models to billions of parameters via various\\nforms of model parallelism [2, 21, 26, 27, 33]. Model parallelism [30] splits weights and tensors\\nacross multiple cores while pipeline parallelism [18, 24] splits different layers across devices with\\nmicro-batches pipelined to the different layers. To enable continued scaling of neural networks,\\nimproving model training and serving efﬁciency has become a critical research area.\\nConditional Computation: Computation decisions can be made dynamically based on the input [23,\\n25]. Conditional computation has been proposed as a way to increase the capacity of a deep\\nneural network without increasing the amount of computation, by activating certain parameters and\\ncomputation on demand, on a per-example or per-token basis [3]. Conditional convolution layers [1]\\nwith task-speciﬁc gating has been used to combat catastrophic forgetting when a sequence of learning\\nproblems are optimized. The gating decisions may be binary or sparse and continuous, stochastic or\\ndeterministic.\\n2\\nMixture of Experts: Sparsely-gated MoE [31] is the ﬁrst model to demonstrate massive improve-\\nments in model capacity, training time, or model quality with gating. Switch Transformer [10]\\nsimpliﬁes the gating by selecting only the top expert per token using a softmax over the hidden state\\nand demonstrates better scaling than previous work. All the prior work requires an auxiliary loss to\\nexplicitly encourage balancing. This loss term has to be carefully weighted to not overwhelm the\\nprimary loss. However, auxiliary loss does not guarantee balancing and a hard capacity factor has to\\nbe imposed. As a result, many tokens can still be unprocessed by the MoE layer. Hard MoE [12] with\\na single decoding layer can be efﬁciently trained to good effect on large scale hashtag prediction tasks.\\nBase Layers [22] formulate a linear assignment that maximizes token-expert afﬁnities while ensuring\\neach expert receives an equal number of tokens. Hash layers [8, 28] devise hashing techniques on\\ninput tokens. However, the evaluations are limited to pre-training perplexity. THOR [? ] randomly\\nactivates experts during training and inference and is trained with a consistency regularization loss.\\nTHOR has demonstrated strong performance on translation tasks. Different from these prior works,\\nour method is a learnt method that enables heterogeneous MoE and effectively improves downstream\\nﬁne-tuning performance.\\n3\\nMethod\\nWe ﬁrst identify a few pitfalls in the routing method of conventional mixture-of-experts (MoE) models\\nand then present our method using expert choice to tackle these problems.\\n3.1\\nPitfalls of Token-Choice Routing\\nMoE can be computationally advantageous compared to a dense model, a routing strategy must be\\nused to assign each token to the most-suited experts. Conventional MoE models employ token-choice\\nrouting which independently selects the top-k experts for each token [10, 21, 31]. We argue that this\\nstrategy has a few pitfalls that lead to sub-optimal training.\\nLoad Imbalance: Token-choice routing often lead to poor load balancing across experts. That is,\\nsome experts may be trained with most tokens, leaving the remaining experts under-utilized. Experts\\ncan be under specialized because a lot of model capacity in the under-utilized experts are wasted.\\nOn the other side, some tokens will not be processed, since over-utilized experts can only take a\\nmaximum number of tokens at each step in order to avoid running out of memory. Load imbalance can\\nalso hurt step latency, thus inference time, as the step latency can be determined by the most loaded\\nexpert. Previous methods add an auxiliary loss on load balancing to mitigate the issue. However, this\\nauxiliary loss does not guarantee a balanced load, especially during the important early stages of\\ntraining. Indeed, we empirically observe that the over-capacity ratio can reach 20%–40% for\\nsome experts in token choice routing, indicating that a signiﬁcant portion of the tokens routed to\\nthese experts will be dropped.\\nUnder Specialization: Each MoE layer uses a gating network to learn token-to-expert afﬁnity.\\nIdeally, the learnt gating network should produce the afﬁnity such that similar or relevant tokens are\\nrouted to the same expert. A sub-optimal strategy can produce redundant experts and/or experts that\\nare not sufﬁciently specialized. Under specialization may result by imposing an large auxiliary loss\\nwhich favors more load balanced but less effective routing. Finding the right balance on the auxiliary\\nloss to promote both load balancing and specialization is challenging for token-choice routing.\\nSame Compute for Every Token: Finally, in a token-choice strategy each token receives exactly\\nk experts and therefore occupies the same amount of compute. We hypothesize that this is not\\nnecessary nor desired. Instead, a MoE model should ﬂexibly allocate its compute resource based\\non the complexity of the input. Motivated by the aforementioned observations, we next describe a\\nsimple yet effective method which produces load balanced assignments based on expert choice.\\n3.2\\nHeterogeneous MoE via Expert Choice\\nDifferent from conventional routing, an expert choice method independently selects top-k tokens\\nfor each expert, where k is a ﬁxed expert capacity (i.e. the number of tokens each expert can take).\\nDespite its simplicity, expert choice achieves perfect load balancing by design. It also enables more\\nﬂexible allocation of model compute since tokens can be received by a variable number of experts.\\n3\\nIn our experiments, we set k as\\nk = n × c\\ne\\n(1)\\nwhere n is the total number of tokens in the input batch (such as batch size × sequence length), c is\\nthe capacity factor, and e is the number of experts. The capacity factor c denotes on average how\\nmany experts are utilized by a token. Given input token representations X ∈Rn×d where d is the\\nmodel hidden dimension, our method produces a token-to-expert assignment denoted by three output\\nmatrices I, G and P. The matrix I is an index matrix where I[i, j] speciﬁes j-th selected token of\\nthe i-th expert. The gating matrix G ∈Re×k denotes the weight of expert for the selected token,\\nand P ∈Re×k×n refers to an one-hot version of I that will be used to gather tokens for each expert.\\nThese matrices are computed using a gating function,\\nS = Softmax(X · Wg),\\nS ∈Rn×e\\nG, I = TopK(S⊤, k), P = Onehot(I)\\n(2)\\nwhere S denotes the token-to-expert afﬁnity scores, Wg ∈Rd×e denotes the expert embeddings, and\\nTopK() selects the k largest entries for each row of S⊤.\\nSimilar to Switch Transformer [10] and GShard [21], we apply mixture of experts and the gating\\nfunction in the dense feed-forward (FFN) layer, as it is the most computationally expensive part in\\na Transformer-based network. The input to the gated FFN, denoted by Xin ∈Re×k×d, is produced\\nusing the permutation matrix P. Here Xin[i] ∈Rk×d denotes the input of the i-th expert. Similarly,\\nlet W1 and W2 denote the parameters of gated FFN in which W1[i] and W2[i] ∈Rd×d′ denote the\\nparameter matrices of the i-th expert. We compute the output of each expert Xe[i] as follows,\\nXin = P · X\\n∀i : Xe[i] = GeLU(Xin[i] · W1[i]) · W2[i]⊤\\n(3)\\nWe omit the bias terms here for brevity. The ﬁnally output of the gated FFN layer Xout ∈Rn×d can\\nbe obtained given Xe, the permutation and gating matrices P and G,\\nXout[l, d] =\\nX\\ni,j\\nP[i, j, l] G[i, j] Xe[i, j, d]\\n(4)\\nBoth Xe and Xout can be efﬁciently computed using Einstein summation (einsum) operations.\\n3.3\\nExpert Choice with Additional Constraint\\nWe also consider regularizing our expert choice routing by limiting the maximum number of experts\\nfor each token. We are interested in whether adding this constraint improves pre-training and ﬁne-\\ntuning results. More importantly, it helps analyzing to what degree using a variable number of experts\\nper token affects the model performance.\\nLet A ∈Re×n be a positive matrix where A[i, j] represents whether the i-th expert selects j-th token.\\nWe solve the following entropy-regularized linear programming problem\\nmax\\nA\\n\\nS⊤, A\\n\\x0b\\n+ λH(A)\\ns.t.\\n∀i :\\nX\\nj′\\nA[i, j′] = k; ∀j :\\nX\\ni′\\nA[i′, j] ≤b; ∀i, j : 0 ≤A[i, j] ≤1\\nwhere < S⊤, A > denotes the inner product, H(A) is the sum of element-wise entropy1, and b > 0\\nis an integer that upper bounds the selection for each token. Adding a small entropy term gives a\\nnear-integer solution while enabling a fast iterative solver we can run on TPUs. Speciﬁcally, the\\nsolution space is the intersection of three convex sets each satisfying one of the linear constraints.\\nWe use Dykstra’s algorithm [9] that alternatively projects the intermediate solution onto one of the\\nconvex sets.2 After A is computed, the routing indices I is selected using TopK(A, k) instead.\\n1H(A) = P\\nij −A[i, j] log A[i, j]\\n2We use λ = 0.001 and a maximum of 100 iterations.\\n4\\nModel\\nType\\nnparams\\nnact-params\\nL\\nM\\nH\\nnheads\\ndhead\\nE\\n0.1B\\nDense\\n130M\\n130M\\n-\\n0.1B/16E\\nMoE\\n548M\\n145M\\n16\\n0.1B/32E\\nMoE\\n1.0B\\n145M\\n12\\n768\\n3,072\\n12\\n64\\n32\\n0.1B/64E\\nMoE\\n1.9B\\n145M\\n64\\n0.1B/128E\\nMoE\\n3.7B\\n145M\\n128\\n8B\\nDense\\n8.7B\\n8.7B\\n32\\n4,096\\n16,384\\n32\\n128\\n-\\n8B/64E\\nMoE\\n143B\\n9.8B\\n64\\nTable 1: Sizes and architectures of both MoE and dense models that were trained in our experiments.\\nModels are grouped by the number of activated parameters per token. All trained models share the\\nsame learning hyperparameters described in Section 4.1.\\n3.4\\nModel Architecture\\nAt the high level, we adopt the idea of sparsely activated Mixture-of-Experts (MoE) [31]. We use\\na Transformer architecture and replace the feed-forward component of every other Transformer\\nlayer with a MoE layer, following recent practice [10, 21]. Interleaving regular Transformer layers\\nand MoE layers empirically improves model performance and training efﬁciency, probably because\\nforcing some shared components in between MoE layers can mitigate the negative effects of skipping\\ntokens. Several additional modiﬁcations adopted in recent work have been applied in our experiments.\\nFor example, we replace the standard positional embedding with per-layer relative positional bias [5].\\nIn the non-MoE feed-forward sub-layers (only every other layers are MoE layers), we replace the\\nﬁrst linear projection and the activation function with the Gated Linear Unit [6], which computes\\nthe component-wise product of two linear transformation of the input, followed by a Gaussian Error\\nLinear Unit [15] activation function.\\nAs described earlier, each MoE layer consists of a group of independent feed-forward networks as\\ndenoted as “experts”. The gating function in Eq. (2) uses a softmax activation function to model a\\nprobability distribution over these experts. This distribution denotes the preference over experts of\\neach incoming token, which is computed similarly in a conventional gating network [10, 21, 31].\\nDuring training, each MoE layer’s learnable gating network described in Eq. (2) is trained to use\\nthe input to activate the best subset of experts using a top-k function along the token dimension. An\\n“shufﬂe” stage and an “unshufﬂe” stage are inserted to the MoE layer, where the ﬁrst stage gathers the\\ntokens to their designated experts while the second stage permutes the tokens back to their original\\norder in the input batch. This step is formulated in Eq. (3) and Eq. (4).\\nSimilar to conventional MoE method, there are more parameters in the MoE layer. However, the\\nactivated model size per token can be comparable to a dense layer because during training or inference,\\nonly a limited subset of experts is activated for any given token. For instance, Switch Transformer [10]\\nhas only one activated expert while GShard [21] uses two experts per token. In our method, the\\nnumber of activated experts can vary for each token but the overall computation is kept the same as\\nthe baseline architectures by ﬁxing the capacity factor c in Eq. (1). Unless otherwise speciﬁed, we set\\nc = 2 such that our method can be directly compared to the top-2 token-choice gating in GShard.\\nWe train several variants of our architecture at the 100M scale (i.e. 100M expert size) by increasing\\nthe number of experts to understand the scaling effects of our method. We also train a 8B scale\\nMoE model. The large MoE model is partitioned with a 2D sharding algorithm as presented in\\nGSPMD [36], which fully exploits the 2D topology of the TPU cluster [19]. Across different\\nscales and setups, our method outperforms related work and demonstrates strong downstream task\\nperformance on selected tasks in GLUE and SuperGLUE.\\n4\\nExperiments\\n4.1\\nSetup\\nTable 1 summarizes the hyperparameter settings of different MoE models. As a reference point,\\nwe also include the respective dense model conﬁgurations with comparable numbers of activated\\nparameters per-token during inference. To study of the effect of scaling the number of experts, we\\n5\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n10K Steps\\n2.6\\n2.7\\n2.8\\n2.9\\n3.0\\n3.1\\nEval Perplexity\\nEC_128E\\nTOP2_128E\\nEC_64E\\nTOP2_64E\\nEC_32E\\nTOP2_32E\\nEC_16E\\nTOP2_16E\\n(a)\\n(b)\\nFigure 2: (a) Training convergence is more than 2x faster using our method compared to GShard\\ntop-2 gating. (b) Training perplexity scales strongly with the number of experts while keeping the\\nexpert size ﬁxed. EC consistently outperforms GShard top-2 gating.\\nstudied varying the number of experts but ﬁxing the per expert size to 100M parameters. For example,\\n0.1B/64E represents the architecture of an approximately 100M parameter dense model with every\\nother layer replaced by a 64-expert MoE layer. The MoE model degenerates into a dense transformer\\narchitecture when each MoE layer only has one expert. While nparams is the total number of trainable\\nparameters, nact−params represents the number of activated parameters per token. L is the total\\nnumber of Transformer layers, M is the model dimension, H is the hidden dimension after the\\nprojection in each transformer layer, nheads is the number of attention heads, and dhead is the hidden\\ndimension of each attention head.\\nDataset: We use the high-quality dataset from GLaM [? ] of 1.6 trillion tokens that are representative\\nof a wide range of natural language use cases. An in-house classiﬁer is trained to classify between\\na collection of curated text and other webpages and estimate the content quality of a webpage. A\\nhigh-quality ﬁltered subset of webpages are combined with books, Wikipedia pages, conversations,\\nforums, and news to create the ﬁnal dataset. The data and mixture weights can be found in Table 3 in\\nthe GLaM paper.\\nModel Training: Our model training follows the setups of GLaM [? ] where a maximum sequence\\nlength of 1024 tokens is adopted. We use an Adafactor optimizer [32] with ﬁrst-moment decay\\nβ1 = 0 and second-moment decay β2 = 0.99. We keep the learning rate constant for the ﬁrst 10K\\ntraining steps, and then decay it with an inverse square root schedule. Unlike most related works, we\\ndo not impose any auxiliary loss for load balance, such as described in Switch Transformer [10] and\\nGShard [21]. We use the SentencePiece subword tokenizer with a vocabulary of size of 256K. The\\nlargest model (8B/64E) is trained on 512 TPU V4 chips. We use a dropout rate of 0 during training\\nas the number of tokens in the training data corpus is much greater than the total number of tokens\\nduring training.\\nModel Evaluation: We mainly focus on evaluating the ﬁnetuning performance on the 11 selected\\ntasks from GLUE and SuperGLUE benchmarks [34, 35].\\n4.2\\nTraining Efﬁciency\\nWe ﬁrst study training efﬁciency and convergence. We use expert choice with a capacity factor of 2\\n(EC-CF2) to match the activated model size and computational cost on a per token basis in GShard\\ntop-2 gating and run both for a ﬁxed number of steps. The results are shown in Fig. 2 (a). Comparing\\nto GShard top-2 gating, which showed stronger performance in both perplexity in the evaluation\\ndataset and ﬁne-tuning on downstream tasks compared to Switch Transformer top-1 gating, EC-CF2\\nconverges more than 2x faster during training. More speciﬁcally, EC-CF2 reaches the same perplexity\\nas GShard top-2 in less than half the steps, and with each GShard top-2 step being 20% slower than\\nour method. As explained in Section 3.1, the slower step time in top-2 gating is due to load imbalance\\n6\\n100M/128E\\n100M/64E\\nName\\nMetric\\nSplit\\nST Top-1\\nGS Top-2\\nEC-CF2\\nST Top-1\\nGS Top-2\\nEC-CF2\\nBoolQ\\nacc\\ndev\\n77.4\\n76.5\\n76.9\\n73.2\\n77.5\\n79.7\\nCB\\nacc\\ndev\\n87.5\\n80.9\\n89.1\\n85.9\\n84.4\\n89.1\\nCoLA\\nacc\\ndev\\n78.9\\n84.0\\n86.7\\n64.1\\n85.2\\n88.3\\nMNLI\\nacc\\ndev\\n82.3\\n83.6\\n84.9\\n80.8\\n85.2\\n86.7\\nMRPC\\nacc\\ndev\\n82.6\\n81.0\\n83.1\\n81.3\\n81.3\\n84.4\\nQNLI\\nacc\\ndev\\n89.5\\n88.6\\n89.0\\n89.4\\n89.7\\n91.3\\nQQP\\nacc\\ndev\\n90.6\\n90.3\\n90.4\\n88.9\\n90.5\\n91.0\\nRTE\\nacc\\ndev\\n77.0\\n78.9\\n78.5\\n74.1\\n79.3\\n81.6\\nSST2\\nacc\\ndev\\n92.0\\n94.5\\n94.6\\n91.8\\n95.1\\n95.1\\nWiC\\nacc\\ndev\\n67.8\\n65.5\\n68.1\\n64.4\\n67.8\\n65.6\\nWNLI\\nacc\\ndev\\n65.6\\n70.3\\n67.2\\n68.8\\n68.8\\n71.7\\nAvg\\n-\\n-\\n81.0\\n81.3\\n82.6\\n78.4\\n82.2\\n84.0\\n100M/32E\\n8B/64E\\nName\\nMetric\\nSplit\\nST Top-1\\nGS Top-2\\nEC-CF2\\nST Top-1\\nGS Top-2\\nEC-CF2\\nBoolQ\\nacc\\ndev\\n74.5\\n79.0\\n79.3\\n89.1\\n89.5\\n89.2\\nCB\\nacc\\ndev\\n80.6\\n81.3\\n92.2\\n93.8\\n96.7\\n100\\nCoLA\\nacc\\ndev\\n87.5\\n92.2\\n93.8\\n88.3\\n87.5\\n89.1\\nMNLI\\nacc\\ndev\\n83.1\\n87.8\\n88.0\\n90.7\\n91.4\\n91.1\\nMRPC\\nacc\\ndev\\n82.3\\n85.2\\n84.4\\n89.3\\n91.7\\n90.6\\nQNLI\\nacc\\ndev\\n91.6\\n91.9\\n92.5\\n94.5\\n94.9\\n95.0\\nQQP\\nacc\\ndev\\n90.1\\n91.5\\n92.0\\n92.1\\n92.5\\n93.8\\nRTE\\nacc\\ndev\\n75.0\\n79.1\\n78.1\\n91.0\\n92.2\\n95.2\\nSST2\\nacc\\ndev\\n93.3\\n94.4\\n95.4\\n97.1\\n98.0\\n97.7\\nWiC\\nacc\\ndev\\n62.5\\n65.9\\n69.8\\n74.5\\n76.4\\n83.8\\nWNLI\\nacc\\ndev\\n65.6\\n64.1\\n68.8\\n78.1\\n82.8\\n92.8\\nAvg\\n-\\n-\\n80.6\\n83.5\\n85.0\\n88.9\\n90.3\\n92.6\\nTable 2: Expert choice with capacity factor of 2 (EC-CF2) outperforms Top-1 gating in Switch\\nTransformer (ST) and top-2 gating in GShard (GS) on GLUE and SuperGLUE tasks. Note that with\\nan expert size of 100M parameters, 100M/32E works best for our method and Ghard Top-2 while\\n100M/128E works better for Switch Transformer Top-1. Our method consistently outperforms the\\nothers across all the scales.\\nwhere some experts can receive a lot more tokens than the desired capacity. As a result, the step\\nlatency will be bottlenecked by the most loaded expert.\\n4.3\\nScaling the Number of Experts\\nAs presented in Table 1, increasing the number of experts effectively increases model capacity without\\nincreasing activated model size. We scale the number of experts while ﬁxing the expert size to 100M\\nparameters for both expert choice (EC) and GShard (Top-2) methods and ﬁnd both methods work\\nwell in terms of perplexity on the evaluation dataset during pre-training. As demonstrated in Fig. 2\\n(b), having more experts consistently improves training perplexity.\\n4.4\\nFine-tuning on GLUE and SuperGLUE\\nTo validate whether improved perplexity directly translates to better performance in downstream tasks,\\nwe perform ﬁne-tuning on 11 selected tasks from GLUE and SuperGLUE. We compare three MoE\\nmethods including Switch Transformer top-1 gating (ST Top-1), GShard top-2 gating (GS Top-2)\\nand our method (EC-CF2) that matches the activation memory size and computational cost of GS\\nTop-2. Indicated by the results in Table 2, our EC-CF2 method consistently outperforms the related\\nmethods and yields more than 2% average accuracy increase in a large 8B/64E setting. Table 3 further\\ncompares our 8B/64E model against its dense counterpart. Again, our method achieves stronger\\nﬁne-tuning results, increasing the average score by 3.4 point.\\nInterestingly, we observe the 100M/32E model setting works the best for both GS Top-2 and EC-CF2,\\neven though the effective model capacity is smaller than that of 100M/64E and 100M/128E. This\\nresult indicates that a good training perplexity does not always translate to better performance of\\ndownstream tasks.\\n7\\nModel\\nBoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI Avg\\nDense 8B\\n88.2\\n100\\n86.4\\n91.3\\n86.7\\n94.7\\n91.2\\n92.2\\n97.2\\n75.6\\n78.1\\n89.2\\nEC-CF2 8B/64E 89.2\\n100\\n89.1\\n91.1\\n90.6\\n95.0\\n93.8\\n95.2\\n97.7\\n83.8\\n92.8\\n92.6\\nTable 3: Comparison between Dense 8B and Expert Choice (EC-CF2) 8B/64E models: Our method\\nsigniﬁcantly outperforms the dense model in downstream tasks.\\nTable 4: (a) Limiting the number of experts\\nper token in expert choice method affects\\ndownstream accuracy. (b) Comparing to Hash\\nLayer.\\nMethod\\nMax # of Experts\\nAvg acc.\\nEC-CAP2\\n2\\n83.2 ± 0.4\\nEC-CAP3\\n3\\n84.0 ± 0.4\\nEC-CF2\\n-\\n84.0 ± 0.2\\nHash Layer\\n-\\n81.3 ± 0.1\\nFigure 3: Distribution of the number of experts\\nrouted to per token in a 100M/64E model.\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0\\n1\\n2\\n3\\n4\\n>4\\nFraction of Tokens\\nNumber of Experts\\n4.5\\nHeterogeneity Matters\\nCapped Expert Choice: We regularized expert choice by limiting the maximum number of experts\\nfor each token, using the method described in Section 3.3. Table 4 reports the average accuracy on\\nthe 11 selected datasets. EC-CAP2 is the variant of our expert choice method by limiting the number\\nof experts of each token to 2. This decreases the ﬁne-tuning accuracy by 0.8 points on average. In\\naddition, EC-CAP3 allows a maximum of 3 experts per token and achieves on par results compared\\nto the vanilla expert choice method. This ablation study conﬁrms that allowing variable number of\\nexperts per token is indeed helpful.\\nVariable Experts per Token: We compute statistics on token-to-expert routing, particularly on the\\nratio of tokens that have been routed to a certain number of experts. According to Fig. 3, a majority\\nof tokens have been routed to one or two experts while 23% have been routed to three or four experts\\nand only about 3% tokens have been routed to more than 4 experts. This plot veriﬁes our hypothesis\\nthat our method learns to allocate a variable number experts to tokens, which can be beneﬁcial for\\nimportant tokens.\\n4.6\\nComparison with Hash Layer\\nIn this section, we compare our method with Hash Layers [28]. We use\\nmod x to map a token ID\\nto an expert ID. This ensures load balance and generates specialized experts. The ﬁne-tuning results\\nare presented in the last row in Table 4. Hashing based routing performs worse than expert choice in\\nterms of average scores and variance. This indicates that load balancing alone does not generate\\nall the beneﬁts.\\n4.7\\nAblation\\nCapacity Factor: We study the capacity factor in our expert choice method and compare the training\\nperplexity with the baseline top-1 gating method used in Switch Transformer. As described in Eq. (1),\\nthe capacity factor determines how many experts on average each token can be routed to, thus the\\nbucket size k of each expert. In all our previous experiments, we use a capacity factor of 2, which\\nmatches the computational footprint of the top-2 gating used in GShard method. To match the\\ncomputation cost on a per-token basis fairly with top-1 gating used in Switch Transformer, we reduce\\nthe capacity factor to 1 and plot the training perplexity in Fig. 4 (a). Not surprisingly, using a smaller\\ncapacity factor yields higher perplexity, but our method still signiﬁcantly outperforms top-1 gating.\\nWe further push the capacity factor down to 0.5, and observe that it still outperforms the top-1 gating.\\nComparison with Dense Models on Pre-training: We compare our method with dense models\\non pre-training. As shown in Fig. 4 (b), our method consistently outperforms the dense method in\\n8\\n0\\n100 200 300 400 500 600 700\\n10K Steps\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\nEval Perplexity\\nEC_CF2\\nEC_CF1\\nEC_CF0.5\\nTOP1\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n10K Steps\\n2.0\\n2.2\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\nEval Perplexity\\nEC_10064E\\nDENSE_100M\\nEC_8B64E\\nDENSE_8B\\n(a)\\n(b)\\nFigure 4: (a) Varying the capacity factor in our expert choice method: Decreasing the capacity factor\\nfrom two to one degrades the perplexity but still outperforms the top-1 gating. (b) Training perplexity\\ncomparison with dense models.\\nperplexity and convergence time. For a small expert size of 100M parameters, the beneﬁt of sparse\\ngating is even more signiﬁcant. Orthogonal to results presented in Fig. 2 (b), where scaling the\\nnumber of experts improves model performance, Fig. 4 (b) shows that increasing expert capacity\\nalso signiﬁcantly increases model performance.\\n5\\nConclusion\\nWe propose a new routing method for sparsely activated mixture-of-experts (MoE) models. This\\nmethod addresses load imbalance and under-utilization of experts in conventional MoE methods,\\nand enables selecting different numbers of experts for each token. Our model demonstrates more\\nthan 2x training efﬁciency improvements when compared to the state-of-the-art GShard and Switch\\nTransformer models, and also achieves strong gains when ﬁnetuning on 11 datasets in the GLUE and\\nSuperGLUE benchmark.\\n6\\nLimitations\\nThe expert choice method might not immediately apply to auto-regressive text generation as our\\ncurrent implementation takes in the past and future tokens to perform the top-k selection. One\\npossible solution is to collect a large batch of input sequences, dispatch tokens of the same sequence\\ninto separate groups, and perform expert choice routing for each group. Another scenario where the\\nexpert choice method does not immediately apply is when the batch size becomes very small during\\nserving or inference. A global top-k can be selected instead and we can cap the number of times each\\nexpert or token gets selected. We leave these possible improvements for future work.\\nAnother long-standing issue with MoE has been the large memory footprint. Even though computa-\\ntional cost can be reduced using sparsely gated networks, the total number of parameters increases\\nlinearly or sub-linearly with the number of experts. Increasing the number of experts requires reser-\\nvation of a large number of hardware devices. Therefore, dynamic (used) power is saved while static\\n(reserved) power is not. Power saving techniques such as the ability to put hardware devices into low\\npower states while not in use [17] can help with reducing the reserved power requirements.\\nReferences\\n[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and\\nBabak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual\\nlearning. In CVPR, pages 3930–3939. Computer Vision Foundation / IEEE, 2020.\\n9\\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. In Advances in Neural Information Processing Systems.\\n[3] Kyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation\\nratio for conditional computation in deep learning, 2014.\\n[4] Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. CoAtNet: Marrying convolution and\\nattention for all data sizes. In Advances in Neural Information Processing Systems, 2021.\\n[5] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, July\\n2019. Association for Computational Linguistics.\\n[6] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\\ngated convolutional networks. In Proceedings of the 34th International Conference on Machine\\nLearning - Volume 70, ICML’17, page 933–941. JMLR.org, 2017.\\n[7] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten\\nBosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin\\nRobinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui\\nWu, Zhifeng Chen, and Claire Cui. Glam: Efﬁcient scaling of language models with mixture-\\nof-experts, 2021.\\n[8] Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan.\\nTricks for training sparse translation models, 2021.\\n[9] Richard L Dykstra. An iterative procedure for obtaining i-projections onto the intersection of\\nconvex sets. The annals of Probability, pages 975–984, 1985.\\n[10] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efﬁcient sparsity, 2021.\\n[11] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. NAS-FPN: learning scalable feature pyramid\\narchitecture for object detection. In CVPR, pages 7036–7045. Computer Vision Foundation /\\nIEEE, 2019.\\n[12] Sam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale\\nweakly supervised vision, 2017.\\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\\npages 770–778, 2016.\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\\nnetworks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision –\\nECCV 2016, pages 630–645, Cham, 2016. Springer International Publishing.\\n[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2016.\\n[16] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is\\npredictable, empirically, 2017.\\n[17] Ping Huang, Zuocheng Xing, Tianran Wang, Qiang Wei, Hongyan Wang, and Guitao Fu. A\\nbrief survey on power gating design. In 2010 10th IEEE International Conference on Solid-State\\nand Integrated Circuit Technology, pages 788–790, 2010.\\n10\\n[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen,\\nHyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efﬁcient\\ntraining of giant neural networks using pipeline parallelism. In Hanna M. Wallach, Hugo\\nLarochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett,\\neditors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural\\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\\nCanada, pages 103–112, 2019.\\n[19] Norman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon,\\nCliff Young, and David A. Patterson. A domain-speciﬁc supercomputer for training deep neural\\nnetworks. Commun. ACM, 63(7):67–78, 2020.\\n[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels, 2020.\\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with condi-\\ntional computation and automatic sharding. In International Conference on Learning Represen-\\ntations, 2021.\\n[22] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base\\nlayers: Simplifying training of large, sparse models.\\nIn Marina Meila and Tong Zhang,\\neditors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of\\nProceedings of Machine Learning Research, pages 6265–6274. PMLR, 18–24 Jul 2021.\\n[23] Min Lin, Jie Fu, and Yoshua Bengio. Conditional computation for continual learning, 2019.\\n[24] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur,\\nGregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline\\nparallelism for dnn training. New York, NY, USA, 2019. Association for Computing Machinery.\\n[25] Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Cédric Renggli, André Susano Pinto,\\nSylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models.\\nIn ICLR. OpenReview.net, 2021.\\n[26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\\n[28] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large\\nsparse models, 2021.\\n[29] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai, 2019.\\n[30] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\\nBlake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers. In Proceedings of\\nthe 32nd International Conference on Neural Information Processing Systems, NIPS’18, page\\n10435–10444, Red Hook, NY, USA, 2018. Curran Associates Inc.\\n[31] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.\\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\\nexperts layer. In ICLR (Poster). OpenReview.net, 2017.\\n[32] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\\ncost. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,\\npages 4596–4604. PMLR, 10–15 Jul 2018.\\n11\\n[33] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\\nparallelism, 2020.\\n[34] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose\\nlanguage understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\\nE. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems. Curran\\nAssociates, Inc.\\n[35] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, Brussels, Belgium, November 2018. Association for Computational\\nLinguistics.\\n[36] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul\\nJoshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam\\nShazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and\\nscalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021.\\n7\\nChecklist\\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contribu-\\ntions and scope? Yes\\n(b) Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes\\n(c) Did you discuss any potential negative societal impacts of your work? N/A. Not any.\\n(d) Did you describe the limitations of your work? Yes\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental\\nresults? Yes. We include details in the experiment setup to help reproduce the main results.\\n(b) Did you specify all the training details? Yes\\n(c) Did you report error bars? Yes\\n(d) Did you include the amount of compute and the type of resources used (e.g., type of GPUs,\\ninternal cluster, or cloud provider)? Yes\\n(a) If your work uses existing assets, did you cite the creators? Yes\\n(b) Did you mention the license of the assets? No. The used dataset is not released yet.\\n(c) Did you include any new assets either in the supplemental material or as a URL? No. The dataset\\nis not released yet.\\n(d) Did you discuss whether and how consent was obtained from people whose data you’re using/cu-\\nrating? No. Not using persons’ data.\\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable in-\\nformation or offensive content? Yes. The dataset does not contain any personally identiﬁable\\ninformation or offensive content.\\n12\\nA\\nComparison on Fine-tuning with a Dense Model\\nOur 8B MoE model achieves stronger pre-training perplexity than its dense counterpart. However,\\na better perplexity does not always directly translate to downstream performance as demonstrated\\nin Section 4.4. To this end, we compare ﬁne-tuning performance of the 8B dense model and MoE\\nmodel in Table 1. As shown in the table, our MoE model using expert choice routing consistently\\noutperforms the dense model across the 11 tasks in GLUE and SuperGLUE.\\nModel\\nBoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI Avg\\nDense 8B\\n88.2\\n100\\n86.4\\n91.3\\n86.7\\n94.7\\n91.2\\n92.2\\n97.2\\n75.6\\n78.1\\n89.2\\nEC-CF2 8B/64E 89.2\\n100\\n89.1\\n91.1\\n90.6\\n95.0\\n93.8\\n95.2\\n97.7\\n83.8\\n92.8\\n92.6\\nTable 1: Comparison between Dense 8B and Expert Choice (EC-CF2) 8B/64E models: Our method\\nsigniﬁcantly outperforms the dense model in downstream tasks.\\nB\\nCapacity Factor\\nWe evaluate the downstream task ﬁne-tuning performance by varying the capacity factors. Note that\\na capacity factor of n indicates on average how many experts each token can be received. EC-CF2 is\\nour baseline expert choice, which matches GShard top-2 gating computational footprint. EC-CF1,\\nhowever, matches Switch Transformer top-1 gating computational footprint. EC-CF0.5 further\\nveriﬁes that an aggressively lowered capacity factor can provide strong enough performance, that\\nalmost matches the top-2 gating baseline.\\nModel\\nBoolQ\\nCB\\nCoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI\\nAvg\\nTop-2\\n78.1\\n87.0\\n88.3\\n85.0\\n82.6\\n90.1\\n90.7\\n81.6\\n94.7\\n68.2\\n67.2\\n83.0±0.3\\nEC-CAP2\\n78.2\\n88.0\\n88.5\\n85.7\\n83.0\\n90.8\\n91.1\\n80.0\\n95.4\\n70.4\\n64.1\\n83.2±0.4\\nEC-CAP3\\n78.5\\n91.7\\n89.3\\n86.3\\n83.5\\n90.9\\n91.1\\n81.8\\n94.9\\n70.0\\n65.6\\n84.0±0.4\\nEC-CF2\\n79.1\\n89.6\\n89.3\\n86.8\\n84.3\\n91.3\\n91.2\\n81.1\\n95.2\\n68.1\\n68.0\\n84.0±0.2\\nEC-CF1\\n77.4\\n90.6\\n88.0\\n85.5\\n83.6\\n90.3\\n91.2\\n79.8\\n95.3\\n66.5\\n64.9\\n83.0±0.2\\nEC-CF0.5\\n77.4\\n89.6\\n86.3\\n85.2\\n82.7\\n91.7\\n91.0\\n79.6\\n94.9\\n67.3\\n63.5\\n83.0 ±0.05\\nHash Layers 76.1\\n85.2\\n86.7\\n83.4\\n82.5\\n90.0\\n90.3\\n75.7\\n94.0\\n67.4\\n63.3\\n81.3±1.0\\nTable 2: Comparison between different routing methods in ﬁne-tuning of 100M/64E models. We\\nperform 3 independent ﬁne-tuning runs for each method and report the average results. This gives\\nmore accurate difference between the variants of expert choice method, since they achieve close\\nﬁne-tuning results. We do not report averaged results in other experiments.\\nC\\nCapped Expert Choice\\nAs described in Section 4.5, the maximum number of experts each token is assigned can be capped\\nby an entropy-regularized linear programming. Figure 1 compares the validation perplexity when\\ntraining the 100M/64E models using the base expert choice method (EC-BASE), expert choice capped\\nby two experts per token (EC-CAP2), expert choice capped by three experts per token (EC-CAP3),\\nand GShard top-2 gating.\\nAs shown in the ﬁgure, restricting the number of experts to 2 degrades the perplexity compared to\\nthe base expert choice method. This suggests that a more ﬂexible allocation of experts (e.g. more\\nthan 2 experts for a token) can enhance model expressiveness. On the other hand, our EC-CAP2\\nand EC-CAP3 methods still outperform the top-2 gating method by a clear margin. We believe this\\nconﬁrms the effectiveness of a load balanced training, provided by our method. Finally, EC-CAP3\\nobtains comparable perplexity to EC-BASE. As indicated by Figure 3, only a little fraction of tokens\\nuse more than 3 experts therefore we see little or no difference between EC-BASE and EC-CAP3\\nvariants. We present the ﬁne-tuning results of these methods in Table 2.\\n1\\narXiv:2202.09368v2  [cs.LG]  14 Oct 2022\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n10K Steps\\n2.65\\n2.70\\n2.75\\n2.80\\n2.85\\nEval Perplexity\\nComparison with Capped Expert Choice\\nEC_BASE\\nEC_CAP2\\nEC_CAP3\\nGS_TOP2\\nFigure 1: Validation perplexity during pre-training using various expert choice methods and top-2\\ngating.\\nD\\nComparison with Hash Layer\\nIn this section, we compare our method with Hash Layers [? ]. We use\\nmod x to map a token\\nID to an expert ID. This in some way ensures load balance and generates specialized experts. The\\nﬁne-tuning results are presented in the last row in Table 2. Hashing based routing performs much\\nworse than expert choice in terms of average scores and variance.\\nE\\nFine-tuning Details\\nWe did a hyperparameter search for both baseline models and expert choice method. For ﬁne-tuning\\nof the 8B dense model, we use a constant learning rate of 0.0001 and a dropout rate of 0.1. We freeze\\nthe attention layer and feed-forward layer while leaving the embedding and layer normalization\\ntrainable. This setting has been found optimal for the 8B dense model. For MoE 8B/64E models\\nincluding GShard top-2 gating and expert choice, we found continuing the learning rate from the\\npre-trained model while using a square root learning rate decay works better. In addition, we do not\\napply parameter freezing for ﬁne-tuning MoE models. For models with 100M expert size, we use a\\nconstant learning rate of 0.0001 and no dropout is used.\\n2\\n\", 'source_name': 'Mixture-of-Experts with Expert Choice Routing', 'source_url': 'https://arxiv.org/abs/2202.09368'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Soft_MoE.pdf #56\n",
      "{'content': 'From Sparse to Soft Mixtures of Experts\\nJoan Puigcerver∗\\nCarlos Riquelme∗\\nBasil Mustafa\\nNeil Houlsby\\nGoogle DeepMind\\nAbstract\\nSparse mixture of expert architectures (MoEs) scale model capacity without large increases in training\\nor inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token\\ndropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose\\nSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the\\nbenefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations\\nof all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of\\nthe (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual\\nrecognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens\\nChoice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5× lower inference cost (5.7×\\nlower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE\\nalso scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40× more parameters than\\nViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better.\\n1\\nIntroduction\\nLarger Transformers improve performance at increased computational cost. Recent studies suggest that model\\nsize and training data must be scaled together to optimally use any given training compute budget (Kaplan\\net al., 2020; Hoffmann et al., 2022; Zhai et al., 2022a). A promising alternative that allows to scale models\\nin size without paying their full computational cost is sparse mixtures of experts (MoEs). Recently, a\\nnumber of successful approaches have proposed ways to sparsely activate token paths across the network\\nin language (Lepikhin et al., 2020; Fedus et al., 2022), vision (Riquelme et al., 2021), and multimodal\\nmodels (Mustafa et al., 2022).\\nAt the core of sparse MoE Transformers lies a discrete optimization problem: deciding which modules should\\nbe applied to each input token. These modules are commonly referred to as experts and are usually MLPs.\\nMany techniques have been devised to find good token-to-expert matches: linear programs (Lewis et al.,\\n2021), reinforcement learning (Bengio et al., 2015), deterministic fixed rules (Roller et al., 2021), optimal\\ntransport (Liu et al., 2022), greedy top-k experts per token (Shazeer et al., 2017), or greedy top-k tokens\\nper expert (Zhou et al., 2022). In many cases, heuristic auxiliary losses are required to balance utilization\\nof experts and minimize unassigned tokens. These challenges can be exacerbated in out-of-distribution\\nscenarios: small inference batch sizes, novel inputs, or in transfer learning.\\nWe introduce a new approach, Soft MoE, that overcomes many of these challenges. Rather than employing a\\nsparse and discrete router that tries to find a good hard assignment between tokens and experts, Soft MoEs\\ninstead perform a soft assignment by mixing tokens. In particular, we compute several weighted averages\\nof all tokens—with weights depending on both tokens and experts—and then we process each weighted\\naverage by its corresponding expert.\\nSoft MoE models avoid most of the challenges mentioned above which are caused by the discrete procedure\\nat the core of sparse MoEs. Popular sparse MoE algorithms learn some router parameters, and the source\\nof gradients is usually two-fold: post-multiplication of expert outputs with the selected routing scores, and\\n∗Equal contribution. The order was decided by a coin toss.\\n1\\narXiv:2308.00951v1  [cs.LG]  2 Aug 2023\\nSparse MoE\\n...\\n...\\n...\\nExpert 1\\nExpert 2\\nExpert n\\nAssign\\nSlots\\nSoft MoE\\n...\\n...\\n...\\nExpert 1\\nExpert 2\\nExpert n\\nWeighted average\\n0.01\\n0.01\\n0.01\\n0.01\\n0.02\\n0.09\\n0.09\\n0.30\\n0.40\\n0.01\\n0.03\\n0.01\\n0.01\\nSlots\\nFigure 1: Main differences between Sparse and Soft MoE layers. While the router in Sparse MoE layers\\n(left) learns to assign individual input tokens to each of the available slots, in Soft MoE layers (right) each slot\\nis the result of a (different) weighted average of all the input tokens. Learning to make discrete assignments\\nintroduces several optimization and implementation issues that Soft MoE sidesteps.\\nauxiliary losses that enforce some desired behaviour and also depend on the routing scores. It has been\\nobserved that these mechanisms are often no better than random fixed routing (Roller et al., 2021). Soft MoE\\nsidesteps this issue as every routing (or mixing) parameter is directly updated based on every single input\\ntoken. Soft routing can provide stability while training a router; (Mustafa et al., 2022) observed that during\\ntraining large fractions of input tokens can simultaneously change discrete routes through the network,\\nleading to training challenges. Further, hard routing can be challenging with many experts, with most works\\ntraining with just a few dozen. We show that Soft MoE scales to thousands of experts, and it is balanced\\nby construction. Finally, there are no batch-effects at inference, where one input can affect routing (due to\\nlimited expert capacity), and hence prediction, for other inputs.\\nSoft MoE L/16 beats ViT H/14 on upstream, fewshot and finetuning while requiring almost half the training\\ntime, and being 2× faster at inference. Moreover, Soft MoE B/16 matches ViT H/14 on fewshot and finetuning\\nand outperforms it on upstream metrics after a comparable amount of training. Remarkably, Soft MoE B/16\\nis 5.7× faster at inference despite having 5.5× the number of parameters of ViT H/14. Section 4 demonstrates\\nSoft MoE’s potential to extend to other tasks: we train a contrastive model text tower against the frozen vision\\ntower, showing that representations learned via soft routing preserve their benefits for image-text alignment.\\n2\\nSoft Mixture of Experts\\n2.1\\nAlgorithm description\\nThe Soft MoE routing algorithm is depicted in Figure 2. We denote the inputs tokens for one sequence by\\nX ∈Rm×d, where m is the number of tokens and d is their dimension. Each MoE layer uses a set of n expert\\nfunctions1 applied on individual tokens, namely {fi : Rd →Rd}1:n. Each expert will process p slots, and each\\nslot has a corresponding d-dimensional vector of parameters. We denote these parameters by Φ ∈Rd×(n·p).\\nIn particular, the input slots ˜\\nX ∈R(n·p)×d are the result of convex combinations of all the m input tokens, X:\\nDij =\\nexp((XΦ)ij)\\nPm\\ni′=1 exp((XΦ)i′j)\\n˜\\nX = D⊤X.\\n(1)\\nNotice that D, which we call the dispatch weights, is simply the result of applying a softmax over the columns\\nof XΦ. Then, as mentioned above, the corresponding expert function is applied on each slot (i.e. on rows of\\n1In practice, all experts apply the same function with different parameters, usually an MLP.\\n2\\nSoft MoE\\nWeighting\\nLogits\\nExpert 1\\nExpert 2\\nExpert E\\nSlot 1\\nSlot 2\\nSlot 3\\nSlot 4\\nSlot S\\nSlot 1\\nSlot 2\\nSlot 3\\nSlot 4\\nSlot S\\nslots S\\ninput tokens N\\nToken 1\\nToken 2\\nToken 3\\nToken N\\nDispatch\\nWeights\\nsoftmax per slot\\n(normalized per column)\\nCombine\\nWeights\\nsoftmax per token\\n(normalized per row)\\nToken 1\\nToken 2\\nToken 3\\nToken N\\nPer-slot\\nLearnable\\nParameters\\n...\\n...\\n...\\n...\\n...\\nslot linear\\ncombination\\noutput linear\\ncombination\\nSoft MoE Layer\\nFigure 2: The Soft MoE routing algorithm. Soft MoE first computes scores or logits for every pair of input\\ntoken and slot, based on some learnable per-slot parameters. These logits are then normalized per slot\\n(columns) and every slot computes a linear combination of all the input tokens based on these weights (in\\ngreen). Each expert (an MLP in this work) then processes its slots (e.g. 2 slots per expert, in this diagram).\\nFinally, the same original logits are normalized per token (i.e. by row) and used to combine all the slot\\noutputs, for every input token (in blue). Dashed boxes represent learnable parameters.\\n˜\\nX) to obtain the output slots:\\n˜\\nYi = f⌊i/p⌋( ˜\\nXi).\\n(2)\\nFinally, the output tokens Y are computed as a convex combination of all (n · p) output slots, ˜\\nY, whose\\nweights are computed similarly as before:\\nCij =\\nexp((XΦ)ij)\\nPn·p\\nj′=1 exp((XΦ)ij′)\\nY = C ˜\\nY.\\n(3)\\nWe refer to C as the combine weights, and it is the result of applying a softmax over the rows of XΦ.\\nFollowing the usual design for Sparse MoEs (Lepikhin et al., 2020; Fedus et al., 2022; Riquelme et al., 2021;\\nZhou et al., 2022), we replace a subset of the Transformer’s MLP blocks with Soft MoE blocks. In particular,\\nwe typically replace the second half of MLP blocks. The total number of slots is a key hyperparameter of\\nSoft MoE layers because the time complexity depends on the number of slots rather than on the number of\\nexperts. For example, one can set the number of slots equal to the input sequence length to match the FLOPs\\nof the equivalent dense Transformer.\\n1 def\\nsoft_moe_layer (X, Phi , experts):\\n2\\n# Compute\\nthe\\ndispatch\\nand\\ncombine\\nweights.\\n3\\nlogits = jnp.einsum(’md ,dnp ->mnp ’, X, Phi)\\n4\\nD = jax.nn.softmax(logits , axis =(0 ,))\\n5\\nC = jax.nn.softmax(logits , axis =(1, 2))\\n6\\n# The\\ninput\\nslots\\nare a weighted\\naverage\\nof all the\\ninput\\ntokens ,\\n7\\n# given by the\\ndispatch\\nweights.\\n8\\nXs = jnp.einsum(’md ,mnp ->npd’, X, D)\\n9\\n# Apply\\nthe\\ncorresponding\\nexpert\\nfunction\\nto each\\ninput\\nslot.\\n10\\nYs = jnp.stack ([\\n11\\nf_i(Xs[i, :, :]) for i, f_i in\\nenumerate(experts)],\\n12\\naxis =0)\\n13\\n# The\\noutput\\ntokens\\nare a weighted\\naverage\\nof all the\\noutput\\nslots ,\\n14\\n# given by the\\ncombine\\nweights.\\n15\\nY = jnp.einsum(’npd ,mnp ->md’, Ys , C)\\n16\\nreturn Y\\nAlgorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at\\nhttps://github.com/google-research/vmoe.\\n3\\n2.2\\nProperties of Soft MoEs and connections with Sparse MoEs\\nFully differentiable\\nAt the core of all Sparse MoE algorithms there is an assignment problem between\\ntokens and experts, which is usually subject to some specific capacity and balance constraints. Different\\nalgorithms relax the problem or approximate the solution in different ways: the Top-k or “Token Choice”\\nrouter (Shazeer et al., 2017; Lepikhin et al., 2020; Riquelme et al., 2021), for instance, selects the top-k-\\nscored experts for each token, while there are slots available in such expert (i.e. the expert has not filled\\nits capacity). The “Expert Choice” router (Zhou et al., 2022) selects the top-capacity-scored tokens for each\\nexpert. Other works suggest more advanced (and often costly) algorithms to compute the assignments, such\\nas approaches based on Linear Programming algorithms (Lewis et al., 2021), Optimal Transport (Liu et al.,\\n2022; Clark et al., 2022) or Reinforcement Learning (Clark et al., 2022). Nevertheless virtually all of these\\napproaches are discrete in nature, and thus non-differentiable. In contrast, all operations in Soft MoE layers\\nare continuous and fully differentiable. Indeed, we can interpret the weighted averages with softmax scores\\nas soft assignments –which motivates our algorithm’s name– rather than the hard assignments that Sparse\\nMoE methods typically use.\\nNo token dropping and expert unbalance\\nThe classical routing mechanisms mentioned above tend to\\nsuffer from issues such as “token dropping” (i.e. some tokens are not assigned to any expert), or “expert\\nunbalance” (i.e. some experts receive far more tokens than others). Unfortunately, performance can be\\nseverely impacted as a consequence. For instance, the popular Top-k or “Token Choice” router (Shazeer et al.,\\n2017) suffers from both, while the “Expert Choice” router (Zhou et al., 2022) only suffers from the former\\n(see Appendix B for some experiments regarding dropping in both cases). Soft MoEs are basically immune\\nto token dropping and expert unbalance since every slot is filled with a weighted average of all tokens. All\\nweights are (in theory) strictly positive thanks to the softmax (see Section 5 for detailed experiments).\\nFast\\nThe total number of slots is the main hyperparameter that determines the cost of a Soft MoE layer.\\nEvery input applies such number of MLPs. The total number of experts is irrelevant in this calculation: few\\nexperts with many slots per expert or many experts with few slots per expert will have matching costs if\\nthe total number of slots is identical. The only constraint we must meet is that the number of slots has to be\\ngreater or equal to the number of experts (as each expert must process at least one slot). The main advantage\\nof Soft MoE is completely avoiding sort or top-k operations which are slow and typically not well suited for\\nhardware accelerators. As a result, Soft MoE is significantly faster than most sparse MoEs (Figure 6). See\\nSection 2.3 for time complexity details.\\nFeatures of both sparse and dense\\nThe sparsity in Sparse MoEs comes from the fact that expert parameters\\nare only applied to a subset of the input tokens. However, Soft MoEs are not technically sparse, since every\\nslot is a weighted average of all the input tokens. Every input token fractionally activates all the model\\nparameters. Likewise, all output tokens are fractionally dependent on all slots (and experts). Finally, notice\\nalso that Soft MoEs are not Dense MoEs, where every expert processes all input tokens, since every expert\\nonly processes a subset of the slots.\\nPer-sequence determinism\\nUnder capacity constraints, all Sparse MoE approaches route tokens in groups of\\na fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different\\nsequences or inputs, these tokens often compete against each other for available spots in expert buffers. As a\\nconsequence, the model is no longer deterministic at the sequence-level, but only at the batch-level, as some\\ninput sequences may affect the final prediction for other inputs. Models using larger groups tend to provide\\nmore freedom to the routing algorithm and usually perform better, while their computational cost is also\\nhigher. On the other hand, when groups contain tokens from a single sequence, the model is forced to use\\nevery expert on every input sequence. This may lead to more generalist experts. Moreover, changing the\\ngroup size between training and inference can be problematic due to the potential distributional shift in\\ntoken-to-expert assignments. We explore these aspects in Section 3.5.\\n4\\nSoft MoE gracefully sidesteps all these challenges. Since it combines all tokens in each input sequence, we\\njust set the group size to be a single sequence. Every expert does handle tokens from every input, maybe\\nsomewhat limiting the amount of high-level specialization. Yet, this also implies that it is per-example\\ndeterministic and fast, while typical instances of Sparse MoEs are not.\\n2.3\\nImplementation\\nTime complexity\\nAssume the per-token cost of a single expert function is O(k). The time complexity of a\\nSoft MoE layer is then O(mnpd + npk). By choosing p = O(m/n) slots per expert, i.e. the number of tokens\\nover the number of experts, the cost reduces to O(m2d + mk). Given that each expert function has its own set\\nof parameters, increasing the number of experts n and scaling p accordingly, allows us to increase the total\\nnumber of parameters without any impact on the time complexity. Moreover, when the cost of applying an\\nexpert is large, the mk term dominates over m2d, and the overall cost of a Soft MoE layer becomes comparable\\nto that of applying a single expert on all the input tokens. Finally, even when m2d is not dominated, this is the\\nsame as the (single-headed) self-attention cost, thus it does not become a bottleneck in Transformer models.\\nNormalization\\nIn Transformers, MoE layers are typically used to replace the feedforward layer in each\\nencoder block. Thus, when using pre-normalization as most modern Transformer architectures (Domhan,\\n2018; Xiong et al., 2020; Riquelme et al., 2021; Fedus et al., 2022), the inputs to the MoE layer are “layer\\nnormalized”. This causes stability issues when scaling the model dimension d, since the softmax approaches\\na one-hot vector as d →∞(see Appendix E). Thus, in Line 3 of algorithm 1 we replace X and Phi with\\nl2_normalize(X, axis=1) and scale * l2_normalize(Phi, axis=0), respectively; where scale is a trainable scalar,\\nand l2_normalize normalizes the corresponding axis to have unit (L2) norm, as Algorithm 2 shows.\\n1 def\\nl2_normalize (x, axis , eps =1e-6):\\n2\\nnorm = jnp.sqrt(jnp.square(x).sum(axis=axis , keepdims=True))\\n3\\nreturn x * jnp.reciprocal(norm + eps)\\nAlgorithm 2: JAX implementation of the L2 normalization used in Soft MoE layers.\\nFor relatively small values of d (e.g. the model dimension used for ViT models up to ViT-H, that use d ≤1280),\\nthe normalization has little impact on the model’s quality. However, with the proposed normalization in the\\nSoft MoE layer, we can eventually make the model dimension bigger and/or increase the learning rate (see\\nAppendix E). Accordingly, we use it in all our experiments.\\nDistributed model\\nWhen the number of experts increases significantly, it is not possible to fit the entire\\nmodel in memory on a single device, especially during training or when using MoEs on top of large model\\nbackbones. In these cases, we employ the standard techniques to distribute the model across many devices,\\nas in (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022) and other works training large MoE\\nmodels. Distributing the model typically adds an overhead in the cost of the model, which is not captured by\\nthe time complexity analysis based on FLOPs that we derived above. In order to account for this difference,\\nin all of our experiments we measure not only the FLOPs, but also the wall-clock time in TPUv3-chip-hours.\\n2.4\\nConnections with other methods\\nMany existing works merge, mix or fuse input tokens to reduce the input sequence length (Jaegle et al., 2021;\\nRyoo et al., 2021; Renggli et al., 2022; Wang et al., 2022), typically using attention-like weighted averages\\nwith fixed keys, to try to alleviate the quadratic cost of self-attention with respect to the sequence length.\\nAlthough our dispatch and combine weights are computed in a similar fashion to these approaches, our goal\\nis not to reduce the sequence length (while it is possible), and we actually recover the original sequence\\nlength after weighting the experts’ outputs with the combine weights, at the end of each Soft MoE layer.\\n5\\nMulti-headed attention also shows some similarities with Soft MoE, beyond the use of softmax in weighted\\naverages: the h different heads can be interpreted as different (linear) experts. The distinction is that, if m is\\nthe sequence length and each input token has dimensionality d, each of the h heads processes m vectors of\\nsize d/h. The m resulting vectors are combined using different weights for each of the m′ output tokens (i.e.\\nthe attention weights), on each head independently, and then the resulting (d/h)-dimensional vectors from\\neach head are concatenated into one of dimension d. Our experts are non-linear and combine vectors of size\\nd, at the input and output of such experts.\\nFinally, there are also connections with other MoE works that use a weighted combination of the experts\\nparameters, rather than doing a sparse routing of the examples (Yang et al., 2019; Tian et al., 2020; Muqeeth\\net al., 2023). These approaches are also fully differentiable, although they can have a much higher cost, since\\n1) they must average the parameters of the experts, which can become a time and/or memory bottleneck\\nwhen experts with many parameters are used; and 2) they cannot take advantage of vectorized operations as\\nbroadly as Soft (and Sparse) MoEs, since every input uses a different weighted combination of the parameters. We\\nrecommend the “computational cost” discussion in (Muqeeth et al., 2023) that addresses these issues.\\n2.5\\nCurrent limitations\\nAuto-regressive decoding\\nOne of the key aspects of Soft MoE consists in smartly merging all tokens in\\nthe input. This makes the use of Soft MoEs in auto-regressive decoders difficult, since causality between\\npast and future tokens has to be preserved during training. Although causal masks used in attention layers\\ncould be used, one must be careful to not introduce any correlation between token and slot indices, since this\\nwould bias which token indices each expert is trained on. The use of Soft MoE in auto-regressive decoders is\\na promising research avenue that we leave for future works.\\nLazy experts & memory consumption\\nWe extensively show in Section 3 that one slot per expert tends to\\nbe the optimal choice. In other words, rather than feeding one expert with two slots (or mixes of tokens),\\nit is more effective from a performance standpoint to use two experts with one slot each. We hypothesize\\nsame-expert slots tend to somewhat align and provide small informational gains, and single experts may lack\\nthe flexibility to accommodate very different slot projections. We show this in Appendix H. Consequently,\\nSoft MoE tends to leverage a large number of experts and –while its cost is still similar to the dense backbone–\\nthe memory requirements of the model can grow large.\\n3\\nImage Classification Experiments\\nWe present three types of experiments on image classification:\\nTraining Pareto frontiers. First, in Section 3.3 we systematically compare dense ViT models at the Small,\\nBase, Large and Huge sizes with their sparse counterparts based on the most common routing techniques\\n(Tokens Choice, Experts Choice) and Soft MoE routing. We study the training FLOPs versus performance\\nand training time versus performance plots to conclude that Soft MoE dominates all other approaches.\\nInference-time optimized models. Second, in Section 3.4, we present longer training runs (“overtraining”).\\nRelative to ViT, Soft MoE brings large improvements in terms of inference speed (small models: S, B) and\\nabsolute performance (large models: L, H).\\nModel ablations. Third, in Section 3.5 we investigate some of the central aspects of Soft MoE routing (such\\nas number of experts, slots per expert, etc), and compare their behavior with other routing algorithms. We\\npresent the optimal configurations for Soft MoE and the source of the improvement benefits.\\n6\\n3.1\\nTraining and evaluation data\\nWe pretrain our models on JFT-4B (Sun et al., 2017), a proprietary dataset whose latest version contains\\nmore than 4B images, covering more than 29k classes. During pretraining, we typically evaluate the models\\non two metrics: upstream validation precision-at-1 on JFT-4B, and ImageNet 10-shot accuracy. The latter is\\ncomputed by freezing the model weights and replacing the head with a new one that is only trained on a\\ndataset containing 10 images per class from ImageNet-1k (Deng et al., 2009). Finally, we provide the accuracy\\non the validation set of ImageNet-1k after finetuning on the training set of ImageNet-1k (1.3 million images).\\n3.2\\nSparse routing algorithms\\nWe compare to the following popular MoE routing algorithms:\\nTokens Choice. Every token selects the top-K experts with the highest routing score for the token (Shazeer et al.,\\n2017). Increasing K typically leads to better performance at the expense of extra computational cost. Batch\\nPriority Routing (BPR) (Riquelme et al., 2021) significantly improves the model performance, especially in\\nthe case of K = 1 (see Appendix, Table 8). Accordingly we use Top-K routing with BPR and K ∈{1, 2}. We\\nalso optimize the number of experts (Appendix, Figure 15).\\nExperts Choice. Alternatively, experts can select the top-C tokens in terms of routing scores (Zhou et al.,\\n2022). In this case, C is the buffer size, and we typically set E · C = c · T where E is the number of experts,\\nT is the total number of tokens in the group, and c is the capacity multiplier. When c = 1, all tokens can\\nbe served via the union of experts. Note that in this type of routing, it is common that some tokens are\\nsimultaneously selected by several experts whereas some other tokens are not selected at all. Figure 14\\nillustrates this phenomenon. We experiment with c = 0.5, c = 1 and c = 2.\\n3.3\\nTraining Pareto-optimal models\\nWe train VIT-S/8, VIT-S/16, VIT-S/32, VIT-B/16, VIT-B/32, VIT-L/16, VIT-L/32 and VIT-H/14 models, and\\ntheir sparse counterparts. We consider three routing algorithms for the sparse models: Token Choice, Expert\\nChoice, and Soft MoE. In each case, we train several model variants (different K, C and number of experts\\nwhere it corresponds). In total, we train 106 models. The models are trained for 300k steps with batch size\\n4096 in all cases, and inverse square root learning rate decay.\\nFigure 3a and Figure 3b show the results for models in each class that lie on their respective training cost\\n/ performance Pareto frontiers. On both metrics, Soft MoE strongly outperforms dense and other sparse\\napproaches for any given FLOPs or time budget. Table 9 in Appendix I list all the models, with their\\nparameters, performance and costs, and are shown in Figure 22. Figures 23 to 25 in Appendix F compare\\nSoft MoE individually to Dense, Token Choice and Expert Choice models respectively.\\n3.4\\nLong training runs\\nIn addition to shorter runs and ablations, we trained a number of models for much longer (a few million\\nsteps) to test the Soft MoE capabilities at larger computational scales. We present two experiments.\\nFirst, in Section 3.4.1, we trained ViT and Soft MoE of different sizes ranging from Small to Huge for 4M\\nsteps. Figure 4 and Table 2 show the results. Second, in Section 3.4.2, we kept training some of the previous\\nSoft MoE models beyond the optimal point suggested by standard dense scaling laws. Sparse models can\\nleverage the extra capacity to steadily improve their performance, leading to very strong Soft MoE models\\nthat are notably cheap at inference.\\n7\\n101\\n102\\nTotal Training TPUv3-days\\n0.44\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\n0.58\\nJFT-4B Precision-at-1\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\nTokens Choice\\nDense\\n(a) JFT-4B Precision-at-1\\n101\\n102\\nTotal Training TPUv3-days\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Accuracy\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\nTokens Choice\\nDense\\n(b) ImageNet 10-shot Accuracy\\nFigure 3: Pareto Models. Soft MoE dominates both ViTs (dense) and popular MoEs (Experts Choice,Tokens\\nChoice) on the training cost / performance Pareto frontier. Each point is a model trained for 300k steps, and\\nlarger marker sizes indicate larger models: S/32, S/16, B/32, B/16, L/16 and H/14. Cost is shown both in\\nterms of FLOPS and realized TPU-v3 training time. MoE runs include different configuration; for clarity,\\nonly models on their respective Pareto frontier are displayed. Figure 22 in Appendix F shows all models.\\n3.4.1\\nComparison with large-scale Vision Transformers\\nWe trained a number of Soft MoEs on JFT, following a comparable setting to that used by Zhai et al. (2022a).\\nWe replace the last half of the blocks in ViT S/16, B/16, L/16, and H/14 with Soft MoE layers with 128 experts,\\nusing one slot per expert. We train models ranging from 1B to 54B parameters. Large Soft MoE models incur\\nin a small wall-clock time overhead compared to their dense counterparts due to the extra data transfers\\nrequired by model parallelism. All variants were trained for 4M steps, except for H/14s which was trained\\nfor 2M steps for cost reasons.\\nFigure 4 shows the JFT-4B precision, ImageNet 10-shot accuracy, and the ImageNet finetuning accuracy for\\nSoft MoE and ViT versus training cost in ExaFLOPS. Table 2 contains all the results, and Figure 19 shows\\nperformance versus core-hours. Soft MoE models widely outperform Vision Transformer models for a given\\ncompute budget. For instance, the Soft MoE S/16 performs better than ViT B/16 on JFT and 10-shot ImageNet,\\nand it also improves finetuning scores on the full ImageNet data, even though its training (and inference) cost\\nis significantly smaller. Similarly, Soft MoE B/16 outperforms ViT L/16 upstream, and only lags 0.5 behind\\nafter finetuning while being 3x faster and requiring almost 4x fewer FLOPs. Finally, the Soft MoE L/16 model\\noutperforms the dense H/14 one while again being around 3x faster to train and serve at inference.\\n3.4.2\\nSoft MoEs optimized for inference\\nEncouraged by the fact that Soft MoEs with smaller backbones can match the quality of larger Vision\\nTransformers, we continue training the small backbones to obtain models of higher quality at very low\\ninference cost. Even after additional (over) training, the overall training time with respect to larger ViT\\nmodels is comparable. For these long runs, we observe that longer cooldowns (period where the learning\\nrate is decreased linearly to zero (Zhai et al., 2022a)) work well for Soft MoE. Therefore, we increase the\\ncooldown from 50k steps (used elsewhere) to up to 500k steps. Figure 5 presents these models.\\nWe now summarize our main results. Soft MoE B/16 trained for 1k TPUv3 days outperforms ViT H/14\\ntrained on a similar time budget (see Table 1, ViT H/14, 1M steps) while being 10× cheaper at inference in\\nFLOPs and 5.7× in wall-clock time, and it almost matches the ViT H/14 model performance even if we double\\nViT-H/14’s training budget (2M steps and 2039.8 train days for ViT H/14 versus 1011.4 days for Soft MoE\\nB/16). Soft MoE L/16 beats all models substantially while being almost 2× faster at inference than ViT H/14.\\n8\\n103\\nTotal Training ExaFLOPs\\n67%\\n73%\\n77%\\n81%\\n84%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTotal Training ExaFLOPs\\n51%\\n54%\\n57%\\n59%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTotal Training ExaFLOPs\\n84%\\n85%\\n87%\\n88%\\n89%\\nImageNet Finetune Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\nSoft MoE\\nDense\\nFigure 4: Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14 models\\ntrained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar training\\ncosts, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision at 1\\n(middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We report\\ntraining wall-clock time in Figure 19.\\nTable 1: Training and finetuning results for Soft MoE and dense models. Finetuning performed on ImageNet\\nat 384 resolution. Steps used for linear cooldown indicated in parentheses, these are included in the total\\ntrain steps count. Results are plotted in Figure 5.\\nModel\\nParams Train steps Train days & exaFLOP Eval Ms/img & GFLOP/img JFT P@1 IN/10shot IN/ft\\nViT S/16\\n33M\\n4M (50k)\\n153.5\\n227.1\\n0.5\\n9.2\\n51.3\\n67.6\\n84.0\\nViT B/16\\n108M\\n4M (50k)\\n410.1\\n864.1\\n1.3\\n35.1\\n56.2\\n76.8\\n86.6\\nViT L/16\\n333M\\n4M (50k)\\n1290.1\\n3025.4\\n4.9\\n122.9\\n59.8\\n81.5\\n88.5\\nViT H/14\\n669M\\n2M (50k)\\n2039.8\\n4120.3\\n8.6\\n334.2\\n59.7\\n83.3\\n88.9\\nSoft MoE S/14 256E\\n1.8B 10M (50k)\\n494.7\\n814.2\\n0.9\\n13.2\\n60.1\\n80.6\\n87.5\\nSoft MoE B/16 128E\\n3.7B 9M (500k)\\n1011.4\\n1769.5\\n1.5\\n32.0\\n62.4\\n82.9\\n88.5\\nSoft MoE L/16 128E\\n13.1B 4M (500k)\\n1355.4\\n2734.1\\n4.8\\n111.1\\n63.0\\n84.3\\n89.2\\n3.5\\nSoft MoE Ablations\\nHere we establish the optimal configurations for Soft MoE models by exploring the following:\\nOptimal number of slots per expert. One or two slots per expert work best. We demonstrate this by fixing the\\ntotal number of slots (which determines the compute cost of the model), and changing the number of experts,\\ni.e. the slots per expert (Figure 6).\\nOptimal number of experts. Roughly the same number of experts as input tokens work best when using one\\nslot per expert. The model is then similarly expensive in terms of FLOPs as its dense equivalent. To show\\nthis, we increase the number of experts and train models for the same amount of time, and find the best\\nperforming model (Figure 8).\\nArchitectural/algorithmic ablations. To disentangle the source of the benefits, we compare Soft MoE to a number\\nof ablated versions: route token i deterministically to expert i, fixed uniform dispatch/combine weights, and\\nothers (TablTable 3).\\nMoE layers placement. An additional ablation regarding where to place MoE layers is presented in Appendix D.\\n9\\n101\\n102\\nEvaluation cost (GFLOP/img)\\n70%\\n75%\\n80%\\n85%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\nS/14\\nB/16\\n10.4×\\n101\\n102\\nEvaluation cost (GFLOP/img)\\n52%\\n55%\\n57%\\n60%\\n62%\\n65%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\n10.4×\\n103\\nTraining cost (exaFLOP)\\n70%\\n75%\\n80%\\n85%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTraining cost (exaFLOP)\\n52%\\n55%\\n57%\\n60%\\n62%\\n65%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\nS/14\\nB/16 L/16\\nViT\\nSoft MoE\\nSoft MoE (long)\\n100\\n101\\nEvaluation time (TPUv3 ms/img)\\n70%\\n75%\\n80%\\n85%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\nS/14\\nB/16\\n5.7×\\n100\\n101\\nEvaluation time (TPUv3 ms/img)\\n52%\\n55%\\n57%\\n60%\\n62%\\n65%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\n5.7×\\n103\\nTraining time (TPUv3 days)\\n70%\\n75%\\n80%\\n85%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTraining time (TPUv3 days)\\n52%\\n55%\\n57%\\n60%\\n62%\\n65%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\nS/14\\nB/16L/16\\nFigure 5: Soft MoE optimized for inference. These plots show the quality on JFT-4B (Precision-at-1)\\nand ImageNet (10-shot Accuracy) achieved by different models with different training and inference cost\\n(measured both in TPUv3 time and FLOPs). Red and light blue curves correspond (respectively) to ViT and\\nSoft MoE S/16, S/14, B/16, L/16 and H/14 trained for 4 million steps (except H/14, that was trained for 2\\nmillion steps), following a recipe similar to (Zhai et al., 2022a). Dark blue curves correspond to Soft MoE\\nS/14, B/16, L/16 trained for additional steps as detailed in Table 1. We observe that the overtrained Soft MoE\\nB/16 is better than the best ViT model (H/14) while using 10× less computation (5.7× time). Soft MoE L/16\\nis the most performant model requiring one third of the inference FLOPs (one half of the time). Detailed\\nresults in Tables 1 and 2.\\n3.5.1\\nNumber of Experts and Slots per Expert\\nWhen applying Soft MoE to a given architecture and input sequence length, one must decide how many\\nexperts and how many slots per expert to use. The total number of slots determines the amount of work\\n(FLOPs) applied in the MoE layer (ignoring the small the routing cost). If the total number of slots is greater\\nthan the number of input tokens, the model will require more FLOPs than dense Transformers: more “tokens”\\nwill be processed in the MoE layer. Conversely, if the number of slots is lower than the original number of\\ntokens, Soft MoE will save some compute relative to dense Transformers.\\nUnless stated otherwise, the following experiments use a ViT-S/16 backbone trained for 300k steps with\\nbatch size 4096. The MoEs have expert layers in their last six of twelve blocks.\\nOptimal number of slots per expert. In this experiment the total amount of compute is fixed, and we\\ncompare different configurations. Specifically, we fix the total number of slots to 4096, and we train models\\nwith different number of experts. MoE algorithms are often unable to scale well to a large number of experts\\n(over 100). The model sizes range from just 38M (with 2 experts) to 9.7B parameters (when using 4096\\nexperts). Figure 6 (and Figure 26) shows the results in terms of pre-training precision (left) and the few-shot\\nevaluation (middle). In the case of Experts Choice and Tokens Choice MoEs, the size of the union of all\\nexpert buffers is also 4096 per input image. We just vary the number of experts keeping the total number of\\ntokens processed across the union of experts constant, as for Soft MoE. For the Sparse MoEs (Experts/Tokens\\nChoice), there is an implementation detail: The “group size” is the subset of the batch that is routed together.\\nAll tokens in a group compete to be selected by each expert. This can range from one image/group to the\\nentire batch/group; the latter is more flexible, but increases computational overhead in routing (sorting the\\n10\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.515\\n0.520\\n0.525\\n0.530\\n0.535\\n0.540\\n0.545\\nJFT-4B Precision-at-1\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.70\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\nImageNet 10-shot Accuracy\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\n(normalized) Train Step Time\\n128\\n64\\n32\\n16\\n8\\n4\\n2\\n1\\nSlots / Expert\\n128\\n64\\n32\\n16\\n8\\n4\\n2\\n1\\nSlots / Expert\\n128\\n64\\n32\\n16\\n8\\n4\\n2\\n1\\nSlots / Expert\\nSoft MoE\\nExperts Choice\\nTokens Choice\\nFigure 6: Performance (left, center), and training step time (right) as a function of number of experts, for\\nmodels with a fixed number of slots (Soft MoE) or expert buffer capacity (Sparse MoEs) on a ViT-S/16\\nbackbone with MoEs in the last two layers. Soft MoE achieves much better scaling with more experts, while\\ncost is roughly constant. However, with Experts and Tokens Choice routers, having too many experts not only\\nhurts performance but also significantly increases the cost (Tokens Choice reaches 3.9x with 4096 experts).\\nitems). In Figure 6, we use group size eight. Figure 20, Appendix, shows other options.\\nFigure 6 shows that Soft MoE scales with increased experts. The best configurations are 2048 and 4096\\nexperts, at one/two slots per experts, respectively. In contrast, Experts Choice and Tokens Choice do not\\nscale well with the number of experts, and performance degrades after 512 experts. In addition, Figure 6,\\nright, shows the step time for each model. Due to sorting leading to increased computational overhead, the\\nSparse MoE’s step time increases substantially with more experts, which is not the case for Soft MoE.\\nOptimal number of experts. From the previous analysis, we set the number of slots per expert to one. The\\nnext question is how many experts to use. Here, the cost of models are not matched: more experts will\\nincrease cost (through more slots). Figure 7 shows that, both for Soft MoE and Experts Choice, more experts\\ndo better (up to 1024).\\nNext, we match the total training time for each model by adjusting the number of training steps (Figure 8).\\nAt this scale (ViT-S), the optimal number of experts for a given training budget is around 128 or 256 experts.\\nThe number of input tokens is 196, this corresponds to the minimum number of experts that does not lead to\\na strong token bottleneck (many fewer than 196 slots) in the MoE layer. For any number of experts, Soft MoE\\noutperforms Experts Choice. Both models have the same capacity, but Experts Choice is significantly more\\nexpensive, especially with large group size.\\nMore slots per expert. Appendix C explores how Soft MoE behaves when increasing the number of slots per\\nexpert. Appendix H looks at the (strong) correlation between the learned slot parameters in this case.\\n3.5.2\\nAlgorithmic Ablations: Identity & Uniform Routing\\nSoft MoE relies on learning how to mix tokens for each expert. To understand the impact of finding useful\\nlinear combinations of input tokens, we ablate this aspect by testing some natural choices:\\nIdentity routing. Tokens are not mixed: the first token goes to first expert, second token goes to second expert,\\netc.\\nUniform Mixing. Every slot mixes all input tokens in the same way: by uniformly averaging them, both for\\ndispatching and combining. In this case, we must independently and randomly initialize every expert as\\n11\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\nJFT-4B Precision-at-1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\nImageNet 10-shot Accuracy\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n100\\n200\\n300\\n400\\n500\\n600\\nTrain step time in milliseconds\\nSoft-MoE\\nExperts Choice (gs=1 img)\\nExperts Choice (gs=32 img)\\nFigure 7: Performance (left, center) and step time (right) for models trained with increased experts and\\none slot (or token) per expert for a fixed number of steps (300k). The performance of all models improves\\nas their capacity increases. However, the cost of Experts Choice grows faster than that of Soft MoE, especially\\nwhen the group size is larger (gs= 32).\\notherwise the additional capacity coming from different experts will not be used (we end up with copies).\\nSoft / Uniform. We learn to mix tokens to create the slots (dispatch weights), but we uniformly average all\\nexpert outputs. This implies every input token is identically updated before the residual connection.\\nUniform / Soft. All slots are filled with the uniform average of the input tokens. We learn to mix the expert\\noutput tokens depending on the input tokens.\\nTable 3 summarizes our results, and Appendix A contains further details. Learning to mix tokens for\\ndispatching and for combining tokens after expert processing seems essential to perform well, and dispatch\\nmixing is slightly more important than the combine mixing. Dense underperform all variants.\\n4\\nContrastive learning experiments\\nWe test whether the learned representations are also significantly better when used for other tasks. In this\\nsection we explore a popular paradigm, image-language contrastive learning. We follow the approach in\\nZhai et al. (2022b) where the image tower is pre-trained on an image classification task, and then frozen\\nwhile training the text encoder on a dataset of image-text pairs.\\nWe re-use the models trained on JFT in the previous section and compare their performance on a number\\nof downstream applications. For contrastive learning we train on WebLI (Chen et al., 2022), a proprietary\\ndataset consisting of 10B images and their ALT texts crawled from the internet. The image encoder is frozen,\\nwhile the text encoder is trained from scratch.\\nTable 4 shows our results. Overall, the gaps we observed on image classification are preserved in this setting.\\nFor instance, Soft MoE-L/16 outperforms ViT-L/16 by more than 1% and 2% on Imagenet and Cifar-100\\nzero-shot, respectively. Retrieval numbers are generally modest.\\n5\\nModel Inspection\\nIn this section, we take a look at various aspects of the routing the model learns.\\n12\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\nJFT-4B Precision-at-1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\n0.775\\nImageNet 10-shot Accuracy\\nSoft MoE\\nExperts Choice (gs=1 img)\\nExperts Choice (gs=8 img)\\nDense\\nFigure 8: Performance of models trained with increasing experts (one slot/token per expert), with matched\\ntraining duration. The total number of steps in each case is computed to match the total training time of 300k\\nsteps for 1024-expert Experts Choice with 32 images per group. For context, the dashed line corresponds to\\nDense ViT-S/16. Here, Soft MoE outperforms Experts Choice at all capacities, and the optimum point is at\\naround 512 experts.\\nTokens contributions to slots. While there is no dropping in Soft MoE, it is still possible that some tokens\\ncontribute little to all slots if their logits are much lower than those of other tokens. We would like to see if\\nsome tokens contribute to slots in a disproportionate manner. Figure 9 (left) shows the distribution across\\ntokens for the total weight each token provides to slots (i.e. summed over all slots). This was computed\\nover a batch with 256 images with 196 tokens each on a Soft MoE S/16 finetuned on ImageNet. We see\\nthere is a heavy tail of tokens that provide a stronger total contribution to slots, and the shape is somewhat\\nsimilar across layers. Around 2-5% of the tokens provide a summed weight above 2. Also, between 15% and\\n20% of the tokens only contribute up to 0.25 in total weight. The last layer is slightly different, where token\\ncontribution is softer tailed. Appendix G further explores this.\\nExperts contributions to outputs. Similarly, we would like to understand how much different slots end up\\ncontributing to the output tokens. We focus on the case of one slot per expert. We can approximate the total\\ncontribution of each expert (equivalently, slot) by averaging their corresponding coefficients in the linear\\ncombinations for all output tokens in a batch. Figure 9 (center) shows such (normalized) importance across\\nexperts for different MoE layers. We see that, depending on the layer, some experts can impact output tokens\\nbetween 3x and 14x more than others.\\nNumber of input tokens per slot. For each slot, Figure 9 (right) shows how many input tokens are required to\\nachieve a certain cumulative weight in its linear combination. The distribution varies significantly across slots.\\nFor a few slots the top 20-25 tokens account for 90% of the slot weight, while for other slots the distribution is\\nmore uniform and many tokens contribute to fill in the slot. In general, we see that slots tend to mix a large\\nnumber of tokens unlike in standard Sparse MoEs.\\nVisual inspection. In order to provide some intuition regarding how slots average input tokens, Figure 10\\ngraphically shows the linear combinations for 8 different slots for the image shown in Figure 1. We shade\\npatches inversely proportionally to their weight in the slots; note that all tokens representations are eventually\\ncombined into a single one (with hidden dimension h) before being passed to the expert (unlike in our plot,\\nwhere they are arranged in the usual way). These plots correspond to a Soft MoE S/16 with 128 experts and\\none slot per expert, and we handpicked 8 out of the 128 slots to highlight how different slots tend to focus on\\ndifferent elements of the image.\\n13\\nTable 2: Training and finetuning results for Soft MoE and dense models. Finetuning results on ImageNet\\nat 384 resolution. We use one slot per expert and did not increase this number during finetuning, thus\\nSoft MoEs become cheaper than ViT, as the number of input tokens grows to 576 (patch size 16x16) and 752\\n(patch size 14x14) but the number slots is fixed to a much smaller number (either 128 or 256).\\nModel\\nParams Train steps Train days & exaFLOP Eval Ms/img & GFLOP/img JFT P@1 IN/10s IN/ft\\nViT S/16\\n33M\\n4M (50k)\\n153.5\\n227.1\\n0.5\\n9.2\\n51.3\\n67.6\\n84.0\\nSoft MoE S/16 128E\\n933M\\n4M (50k)\\n175.1\\n211.9\\n0.7\\n8.6\\n58.1\\n78.8\\n86.8\\nSoft MoE S/16 128E\\n933M\\n10M (50k)\\n437.7\\n529.8\\n0.7\\n8.6\\n59.2\\n79.8\\n87.1\\nSoft MoE S/14 256E\\n1.8B\\n4M (50k)\\n197.9\\n325.7\\n0.9\\n13.2\\n58.9\\n80.0\\n87.2\\nSoft MoE S/14 256E\\n1.8B 10M (500k)\\n494.7\\n814.2\\n0.9\\n13.2\\n60.9\\n80.7\\n87.7\\nViT B/16\\n108M\\n4M (50k)\\n410.1\\n864.1\\n1.3\\n35.1\\n56.2\\n76.8\\n86.6\\nSoft MoE B/16 128E\\n3.7B\\n4M (50k)\\n449.5\\n786.4\\n1.5\\n32.0\\n60.0\\n82.0\\n88.0\\nViT L/16\\n333M\\n4M (50k)\\n1290.1\\n3025.4\\n4.9\\n122.9\\n59.8\\n81.5\\n88.5\\nSoft MoE L/16 128E\\n13.1B\\n1M (50k)\\n338.9\\n683.5\\n4.8\\n111.1\\n60.2\\n82.9\\n88.4\\nSoft MoE L/16 128E\\n13.1B\\n2M (50k)\\n677.7\\n1367.0\\n4.8\\n111.1\\n61.3\\n83.3\\n88.9\\nSoft MoE L/16 128E\\n13.1B\\n4M (50k)\\n1355.4\\n2734.1\\n4.8\\n111.1\\n61.3\\n83.7\\n88.9\\nViT H/14\\n669M\\n1M (50k)\\n1019.9\\n2060.2\\n8.6\\n334.2\\n58.8\\n82.7\\n88.6\\nViT H/14\\n669M\\n2M (50k)\\n2039.8\\n4120.3\\n8.6\\n334.2\\n59.7\\n83.3\\n88.9\\nSoft MoE H/14 128E\\n27.3B\\n1M (50k)\\n1112.7\\n1754.6\\n8.8\\n284.6\\n61.0\\n83.7\\n88.9\\nSoft MoE H/14 128E\\n27.3B\\n2M (50k)\\n2225.4\\n3509.2\\n8.8\\n284.6\\n61.7\\n84.2\\n89.1\\nSoft MoE H/14 256E\\n54.1B\\n1M (50k)\\n1276.9\\n2110.1\\n10.9\\n342.4\\n60.8\\n83.6\\n88.9\\nSoft MoE H/14 256E\\n54.1B\\n2M (50k)\\n2553.7\\n4220.3\\n10.9\\n342.4\\n62.1\\n84.3\\n89.1\\nTable 3: Algorithmic ablation on an S/14 backbone trained for 300k steps (with 256 experts).\\nMethod\\nExperts\\nMixing\\nLearned Dispatch\\nLearned Combine\\nJFT p@1\\nIN/10shot\\nSoft MoE\\n✓\\n✓\\n✓\\n✓\\n54.3%\\n74.8%\\nSoft / Uniform\\n✓\\n✓\\n✓\\n53.6%\\n72.0%\\nUniform / Soft\\n✓\\n✓\\n✓\\n52.6%\\n71.8%\\nUniform\\n✓\\n✓\\n51.8%\\n70.0%\\nIdentity\\n✓\\n51.5%\\n69.1%\\nDense ViT\\n48.3%\\n62.3%\\nTable 4: LIT-style evaluation with a ViT-g text tower trained for 18B input images (∼5 epochs).\\nModel\\nExperts IN/0shot Cifar100/0shot Pet/0shot Coco Img2Text Coco Text2Img\\nViT-S/16\\n–\\n74.2%\\n56.6%\\n94.8%\\n53.6%\\n37.0%\\nSoft MoE-S/16\\n128\\n81.2%\\n67.2%\\n96.6%\\n56.0%\\n39.0%\\nSoft MoE-S/14\\n256\\n82.0%\\n75.1%\\n97.1%\\n56.5%\\n39.4%\\nViT-B/16\\n–\\n79.6%\\n71.0%\\n96.4%\\n58.2%\\n41.5%\\nSoft MoE-B/16\\n128\\n82.5%\\n74.4%\\n97.6%\\n58.3%\\n41.6%\\nViT-L/16\\n–\\n82.7%\\n77.5%\\n97.1%\\n60.7%\\n43.3%\\nSoft MoE-L/16\\n128\\n83.8%\\n79.9%\\n97.3%\\n60.9%\\n43.4%\\nSouped Soft MoE-L/16\\n128\\n84.3%\\n81.3%\\n97.2%\\n61.1%\\n44.5%\\nViT-H/14\\n–\\n83.8%\\n84.7%\\n97.5%\\n62.7%\\n45.2%\\nSoft MoE-H/14\\n256\\n84.6%\\n86.3%\\n97.4%\\n61.0%\\n44.8%\\n14\\n0.25\\n1\\n2\\n3\\n4\\n5\\nper-token sum of its dispatch weights\\n0.05\\n0.10\\n0.15\\n0.20\\n0.30\\n0.40\\n0.50\\n0.60\\n0.70\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\ncumulative percentage of tokens\\nMoE Layer 6\\nMoE Layer 7\\nMoE Layer 8\\nMoE Layer 9\\nMoE Layer 10\\nMoE Layer 11\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n(sorted) expert id\\n1\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n(normalized) average combine weight across tokens\\nMoE Layer 6\\nMoE Layer 7\\nMoE Layer 8\\nMoE Layer 9\\nMoE Layer 10\\nMoE Layer 11\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200\\ntop-k tokens for slot s\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\ncumulative weight for top-k tokens\\nFigure 9: (Left) Distribution of summed dispatch weights per token for different MoE layers. For instance,\\nin layer 11, the dispatch weights for 90-95% of the input tokens summed over all the slots are at most 1. Only\\na tiny fraction of tokens contribute to slots by summing more than 3. (Middle) Distribution of combine\\nweights per slot (or expert, as we use one slot per expert) summed across all input tokens. We normalize the\\nsum by its minimum value across experts. (Right) Each curve corresponds to one slot. Dispatch weights\\nfrom all tokens to each slot add up to 1. Distribution of how many inputs tokens are needed to achieve a\\ncertain fraction of the total weight for the slot.\\n6\\nDiscussion\\nSparse models can face infrastructural challenges which may have slowed down their broad adoption. Since\\nthese models were originally conceived to unlock massive model sizes, they tend to be distributed and most\\nrouting algorithms require additional communication costs: additional activations, gradients, or expert\\nparameters are sent across devices. This is also true for Soft MoEs, where the experts may also be distributed.\\nHowever, modern dense models are now sufficiently large that they are also distributed, thus closing the gap\\nin this axis. In addition, the benefits of sparsity shine at small model scales, both in prior work (Riquelme\\net al., 2021) and with Soft MoE, fitting with the current needs of the industry for faster inference.\\nWe presented Soft MoE, a new sparse Transformer architecture that avoids the discrete token-to-expert\\nassignment problem that is common in sparse mixture of experts models. By merging input tokens into linear\\ncombinations before dispatching them to experts, we are able to train a fast and fully-differentiable model.\\nWe perform extensive image-classification and image-language contrastive learning experiments comparing\\nthe performance of dense models and several sparse methods (Tokens Choice, Experts Choice, Soft MoE).\\nThese experiments suggest Soft MoE is surprisingly effective and strongly outperforms the other approaches\\nwhile often being computationally cheaper. How to deal with causal masking for language decoders is an\\nexciting and impactful research direction for future work.\\nAcknowledgements\\nWe thank Rodolphe Jenatton, who provided extremely valuable feedback on an earlier version of this\\nmanuscript; Ilya Tolstikhin, who suggested the “Identity router” used in Appendix A (or “Liquid router”, as\\nhe dubbed it); and the rest of Google DeepMind folks for providing a supportive research environment, very\\nespecially to our colleagues in Europe.\\n15\\nSlot / Expert 1\\nSlot / Expert 2\\nSlot / Expert 3\\nSlot / Expert 4\\nSlot / Expert 5\\nSlot / Expert 6\\nSlot / Expert 7\\nSlot / Expert 8\\nFigure 10: Linear combinations for 8 slots when using input image in Figure 1. Model is Soft MoE S/16 with\\n128 experts and one slot per expert, and it was finetuned on ImageNet. We show results for the first MoE\\nlayer (seventh block). The selected slots (among 128) are cherry-picked to highlight differences across slots.\\n16\\nReferences\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural\\nnetworks for faster models. arXiv preprint arXiv:1511.06297, 2015.\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: composable transformations\\nof Python+ NumPy programs, 2018.\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\\narXiv preprint arXiv:2209.06794, 2022.\\nAidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan\\nDamoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language\\nmodels. In International Conference on Machine Learning, pages 4057–4086. PMLR, 2022.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\\nTobias Domhan. How much attention do you need? a granular analysis of neural machine translation\\narchitectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), pages 1799–1808, 2018.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models\\nwith simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.\\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.\\nIn Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256.\\nJMLR Workshop and Conference Proceedings, 2010.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-\\nlevel performance on imagenet classification. In Proceedings of the IEEE international conference on computer\\nvision, pages 1026–1034, 2015.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver:\\nGeneral perception with iterative attention. In International conference on machine learning, pages 4651–4664.\\nPMLR, 2021.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\\narXiv:2001.08361, 2020.\\nGünter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural\\nnetworks. Advances in neural information processing systems, 30, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\\nand automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying\\ntraining of large, sparse models. In International Conference on Machine Learning, pages 6265–6274. PMLR,\\n2021.\\nTianlin Liu, Joan Puigcerver, and Mathieu Blondel. Sparsity-constrained optimal transport. arXiv preprint\\narXiv:2209.15466, 2022.\\n17\\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing, 2023.\\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal con-\\ntrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770,\\n2022.\\nCedric Renggli, André Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, and Carlos Riquelme.\\nLearning to merge tokens in vision transformers. arXiv preprint arXiv:2202.12015, 2022.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto,\\nDaniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural\\nInformation Processing Systems, 34:8583–8595, 2021.\\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in\\nNeural Information Processing Systems, 34:17555–17566, 2021.\\nMichael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner:\\nWhat can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\\narXiv:1701.06538, 2017.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness\\nof data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages\\n843–852, 2017.\\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In Computer\\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages\\n282–298. Springer, 2020.\\nYikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang. Multimodal token\\nfusion for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 12186–12195, June 2022.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International\\nConference on Machine Learning, pages 10524–10533. PMLR, 2020.\\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized\\nconvolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019.\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–12113, 2022a.\\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas\\nBeyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 18123–18133, 2022b.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James\\nLaudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing\\nSystems, 35:7103–7114, 2022.\\n18\\nA\\nSoft vs. Uniform vs. Identity dispatch and combine weights\\nIn this section, we compare Soft MoE (i.e. the algorithm that uses the dispatch and combine weights computed\\nby Soft MoE in eq. (1) and eq. (3)) with different “fixed routing” alternatives, where neither the expert\\nselected nor the weight of the convex combinations depend on the content of the tokens.\\nWe consider the following simple modifications of Soft MoE:\\nIdentity. The first token in the sequence is processed by the first expert, the second token by the second\\nexpert, and so on in a round robin fashion. When the sequence length is the same as the number of slots and\\nexperts, this is equivalent to replacing the matrix D in eq. (1) (resp. C in eq. (3)) with an identity matrix.\\nUniform. Every input slot is filled with a uniform average of all input tokens, and every output token is a\\nuniform average of all output slots. This is equivalent to replacing the matrix D from eq. (1) with values 1\\nm\\nin all elements, and a matrix C from eq. (3) with values\\n1\\nnp in all elements. We randomly and independently\\ninitialize every expert.\\nUniform / Soft. Every input slot is filled with a uniform average of all input tokens, but we keep the definition\\nof C from eq. (3).\\nSoft / Uniform. Every output token is a uniform average of all output slots, but we keep the definition of D\\nin eq. (1).\\nFigure 11 and Table 3 shows the results from this experiment, training a S/14 backbone model with MoEs on\\nthe last 6 layers. Since the sequence length is 256, we choose 256 experts and slots (i.e. 1 slot per expert),\\nso that the matrices D and C are squared. As shown in the figure, Soft MoE is far better than all the other\\nalternatives. For context, we also add the dense ViT S/14 to the comparison.\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nJFT-4B Precision-at-1\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\nImageNet 10shot Accuracy\\nIdentity\\nUniform\\nUniform / Soft\\nSoft / Uniform\\nSoft\\nDense\\nFigure 11: Soft MoE compared against different “fixed routing” strategies. Identity processes the i-th token\\nwith the i-th expert; Uniform replaces both the dispatch and combine matrices with uniform averages; Uniform\\n/ Soft replaces the dispatch weights with a uniform average, but the combine weights are computed as in\\nSoft MoE; Soft / Uniform does the opposite replacement; and Soft uses the algorithm we present in Section 2.\\n19\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\nImageNet 10-shot Accuracy\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\nJFT-4B Precision-at-1\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nMaximum Dropping Ratio across Layers\\nExperts Choose C=1\\nT\\nokens Choose K=1 C=1\\nFigure 12: S/14. Performance and amount of token dropping for increasing experts for Experts Choose\\n(C = 1) and Tokens Choose (K = 1 and C = 1).\\nB\\nToken Dropping\\nIn this appendix, we briefly explore token dropping for the Experts Choose and Tokens Choose algorithms.\\nFor Tokens Choose, each token selects K experts. When experts are full, some tokens assigned to that expert\\nwill not be processed. A token is “dropped” when none of its choices go through, and no expert at all\\nprocesses the token. Expert Choose algorithms lead to an uneven amount of processing per token: some\\ninput tokens are selected by many experts, while some others are not selected by any. We usually define\\nthe number of tokens to be processed by each expert in a way that the combined capacity of all experts\\ncorresponds to the number of input tokens (or a multiple C of them). If we use a multiplier C higher than\\none (say, 2x or 3x), the amount of dropping will decrease but we will pay an increased computational cost.\\nThus, we mainly explore the K = 1 and C = 1 setup, where there is no slack in the buffers.\\nIn all cases to follow we see a common trend: fixing everything constant, increasing the number of experts\\nleads to more and more dropping both in Experts Choose and Tokens Choose.\\nFigure 12 compares Experts Choose and Tokens Choose with the same multiplier C = 1. This is the cheapest\\nsetup where every token could be assigned to an expert with balanced routing. We see that in both cases the\\namount of dropping quickly grows with the number of experts. Moreover, even though Experts Choose has\\nhigher levels of dropping (especially for large number of experts), it is still more performant than Tokens\\nChoose. Note there is a fundamental difference: when Tokens Choose drops a token, the model wastes that\\namount of potential compute. On the other hand, for Experts Choose dropping just means some other token\\ngot that spot in the expert buffer, thus the model just transferred compute from one unlucky token to another\\nlucky one.\\nIn this setup, for a small number of experts (16-32) it is common to observe a ∼15% rate of dropping. On the\\nother hand, we also experimented with a large number of experts (100-1000) where each expert selects very\\nfew tokens. In this case, the dropping rate for Experts Choose can grow above 40-50% in some layers: most\\nexperts select the very same tokens. Tokens Choose seems to completely drop up to ∼25% of the tokens.\\nIn Figures 13 and 14 we study how much a little bit of buffer slack (C = 1.125) can help in terms of\\nperformance and dropping to Experts Choose and Tokens Choose, respectively. Both plots are similar: the\\namount of dropping goes down around ∼5% and performance slightly increases when the number of experts\\nis large. Note that the step time also increases in these cases.\\nFinally, Figure 15 shows the effect of Batch Priority Routing (Riquelme et al., 2021) for Tokens Choose. By\\nsmartly selecting which tokens to drop we do not only uniformly reduce the amount of dropping, but we\\nsignificantly bump up performance.\\n20\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\nImageNet 10-shot Accuracy\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\nJFT-4B Precision-at-1\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nMaximum Dropping Ratio across Layers\\nT\\nokens Choose K=1 C=1\\nT\\nokens Choose K=1 C=1.125\\nFigure 13: S/14. Performance and amount of token dropping for increasing experts for Tokens Choose with\\ntight buffers (K = 1 and C = 1) and some amount of buffer slack (K = 1 and C = 1.125).\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\nImageNet 10-shot Accuracy\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\nJFT-4B Precision-at-1\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nMaximum Dropping Ratio across Layers\\nExperts Choose C=1\\nExperts Choose C=1.125\\nFigure 14: S/14. Performance and amount of token dropping for increasing experts for Experts Choose with\\ntight buffers (C = 1) and slightly larger buffers (C = 1.125).\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\n0.675\\n0.700\\n0.725\\n0.750\\nImageNet 10-shot Accuracy\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\nJFT-4B Precision-at-1\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\ntotal number of experts\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nMaximum Dropping Ratio across Layers\\nT\\nokens Choose K=1 C=1\\nT\\nokens Choose K=1 C=1 No BPR\\nFigure 15: S/14. Performance and amount of token dropping for increasing experts with and without BPR\\nfor Tokens Choose.\\n21\\n1\\n2\\n3\\n4\\n8\\n16\\n32\\nslots per expert\\n0.49\\n0.50\\n0.51\\n0.52\\n0.53\\nJFT-4B Precision-at-1\\n1\\n2\\n3\\n4\\n8\\n16\\n32\\nslots per expert\\n0.64\\n0.66\\n0.68\\n0.70\\n0.72\\nImageNet 10-shot Accuracy\\n1\\n2\\n3\\n4\\n8\\n16\\n32\\nslots per expert\\n100\\n125\\n150\\n175\\n200\\n225\\n250\\nTrain step time in milliseconds\\nSoft-MoE\\nExperts Choice (gs = 1 img)\\nExperts Choice (gs = 8 img)\\nFigure 16: Performance (left, center) and step time (right) of models with 32 experts, but increased slots,\\nall trained for the same number of steps (300k). Increasing the number of slots per expert only increases\\nperformance of Soft MoE a small amount, while increasing cost substantially.\\nC\\nSoft MoE Increasing Slots\\nIn this section we explore the following question: for a fixed number of experts, how much does Soft MoE\\nrouting benefit from having additional slots per expert? Figure 16 shows results for Soft MoE S/16 with\\n32 experts. We also show Experts Choice with group sizes of one and eight images. When increasing the\\nnumber of slots, the performance grows only modestly, while cost increases quickly. Experts Choice benefits\\nmuch more from increased slots, catching up at a large group size, but at a very large cost.\\n22\\nD\\nSparse Layers Placement\\nSoft MoE does unlock the effective use of a large number of experts. An important design choice for sparse\\nmodels is the number and location of sparse layers, together with the number of experts per layer. Unfortu-\\nnately, the large number of degrees of freedom in these choices has usually made thorough ablations and\\noptimization unfeasible. In this section, we provide the results of a simple experiment that can help better\\ndesign the configuration of sparse models. We fix a total number of experts (E = 512) with one slot per\\nexpert, thus leading to matched number of parameters (note in this case FLOPs may vary greatly depending\\non the number of sparse layers). Then, for an S/16 backbone architecture, we distribute those experts in\\nvarious ways (all in one layer, half of them in two layers, etc) and compare their performance after 300k\\ntraining steps. Table 5 shows the results. Again, we observe that a number of experts close to the number\\nof input tokens (there are 196 tokens, given the 16x16 patch size for 224x224 images) split over the last few\\nlayers works best. Moreover, note these models are indeed cheaper than those in the comparison with 512 or\\n256 experts per layer. Table 6 offers results for Tokens Choose routing with K = 1 and BPR Riquelme et al.\\n(2021). In this case, all algorithms use a comparable FLOPs count (ignoring slightly increasing routing costs\\nwith more experts). Results are essentially similar, thus suggesting optimal expert placement (including\\nexpert count and location) may not strongly depend on the routing algorithm.\\nTable 5: Expert placing ablation with a Soft MoE S/16 with 12 layers (indexed from 0 to 11).\\nSparse Layers\\nExperts per Layer\\nTotal Experts\\nIN/10shot\\nJFT prec1\\n11\\n512\\n512\\n70.0%\\n51.5%\\n10\\n512\\n512\\n70.1%\\n52.0%\\n10, 11\\n256\\n512\\n71.7%\\n52.2%\\n5, 11\\n256\\n512\\n70.4%\\n52.1%\\n8, 9, 10, 11\\n128\\n512\\n72.8%\\n53.2%\\n2, 5, 8, 11\\n128\\n512\\n71.1%\\n52.5%\\n4:11\\n64\\n512\\n72.1%\\n53.1%\\n1:4, 8:11\\n64\\n512\\n70.5%\\n52.1%\\nTable 6: Expert placing ablation with a V-MoE S/16 Tokens Choose K = 1 with 12 layers (indexed as 0:11).\\nSparse Layers\\nExperts per Layer\\nTotal Experts\\nIN/10shot\\nJFT prec1\\n11\\n512\\n512\\n64.4%\\n50.1%\\n10\\n512\\n512\\n67.2%\\n51.9%\\n10, 11\\n256\\n512\\n68.6%\\n51.3%\\n5, 11\\n256\\n512\\n65.3%\\n50.6%\\n8, 9, 10, 11\\n128\\n512\\n69.1%\\n52.3%\\n2, 5, 8, 11\\n128\\n512\\n67.3%\\n51.1%\\n4:11\\n64\\n512\\n69.9%\\n52.2%\\n1:4, 8:11\\n64\\n512\\n68.0%\\n51.2%\\n23\\nTable 7: Expert placing ablation with a V-MoE S/16 Experts Choose C = 1 with 12 layers (indexed as 0:11).\\nSparse Layers\\nExperts per Layer\\nTotal Experts\\nIN/10shot\\nJFT prec1\\n11\\n512\\n512\\n65.3%\\n50.3%\\n10\\n512\\n512\\n66.5%\\n51.7%\\n10, 11\\n256\\n512\\n68.8%\\n51.8%\\n5, 11\\n256\\n512\\n65.9%\\n51.1%\\n8, 9, 10, 11\\n128\\n512\\n69.4%\\n52.2%\\n2, 5, 8, 11\\n128\\n512\\n68.0%\\n51.7%\\n4:11\\n64\\n512\\n69.0%\\n52.2%\\n1:4, 8:11\\n64\\n512\\n67.4%\\n51.1%\\n24\\nE\\nThe collapse of softmax layers applied after layer normalization\\nE.1\\nTheoretical analysis\\nA softmax layer with parameters Θ ∈Rn×d transforms a vector x ∈Rd into the vector softmax(Θx) ∈Rn,\\nwith elements:\\nsoftmax(Θx)i =\\nexp((Θx)i)\\nPn\\nj=1 exp((Θx)j) =\\nexp(Pd\\nk=1 θikxk)\\nPn\\nj=1 exp(Pd\\nk=1 θjkxk)\\n(4)\\nLayer normalization applies the following operation on x ∈Rd.\\nLN(x)i = αi\\nxi −µ(x)\\nσ(x)\\n+ βi;\\nwhere µ(x) = 1\\nd\\nd\\nX\\ni=1\\nxi and σ(x) =\\nv\\nu\\nu\\nt1\\nd\\nd\\nX\\ni=1\\n(xi −µ(xi))2\\n(5)\\nNotice that LN(x) = LN(x −µ(x)), thus we can rewrite LayerNorm with respect to the centered vector\\n˜\\nx = x −µ(x), and the centered vector scaled to have unit norm ˆ\\nxi =\\n˜\\nxi\\n∥˜\\nx∥:\\nLN(˜\\nx)i = αi\\n˜\\nxi\\nq\\n1\\nd\\nPd\\nj=1 ˜\\nx2\\nj\\n+ βi =\\n√\\ndαi\\n˜\\nxi\\n∥˜\\nx∥+ βi =\\n√\\ndαiˆ\\nxi + βi\\n(6)\\nWhen a softmax layer is applied to the outputs of layer normalization, the outputs of the softmax are given\\nby the equation:\\nsoftmax(ΘLN(x))i =\\nexp(Pd\\nk=1 θik(\\n√\\ndαkˆ\\nxk + βk))\\nPn\\nj=1 exp(Pd\\nk=1 θjk(\\n√\\ndαkˆ\\nxk + βk))\\n(7)\\nBy setting ϑi = Pd\\nk=1 θikαkˆ\\nxk, and δi = Pd\\nk=1 θikβk, the previous equation can be rewritten as:\\nsoftmax(ΘLN(x))i =\\nexp(\\n√\\ndϑi + δi)\\nPn\\nj=1 exp(\\n√\\ndϑj + δj)\\n(8)\\nDefine m = maxi∈[n]\\n√\\ndϑi −δi, M = {i ∈[n] :\\n√\\ndϑi −δi = m}. Then, the following equality holds:\\nsoftmax(ΘLN(x))i =\\nexp(\\n√\\ndϑi + δi −m)\\nPn\\nj=1 exp(\\n√\\ndϑj + δj −m)\\n(9)\\nGiven that limd→∞exp(\\n√\\ndϑi + δi −m) =\\n(\\n1 : i ∈M\\n0 : i /\\n∈M\\nthe output of the softmax tends to:\\nlim\\nd→∞softmax(ΘLN(x))i =\\n(\\n1\\n|M|\\ni ∈M\\n0\\ni /\\n∈M\\n(10)\\nIn particular, when the maximum is only achieved by one of the components (i.e. |M| = 1), the softmax\\ncollapses to a one-hot vector (a vector with all elements equal to 0 except for one).\\nE.2\\nEmpirical analysis\\nThe previous theoretical analysis assumes that the parameters of the softmax layer are constants, or more\\nspecifically that they do not depend on d. One might argue that using modern parameter initialization\\ntechniques, which take into account\\n1\\n√\\nd in the standard deviation of the initialization Glorot and Bengio\\n(2010); He et al. (2015); Klambauer et al. (2017), might fix this issue. We found that they don’t (in particular,\\nwe use the initialization from Glorot and Bengio (2010)).\\n25\\nFigure 17 shows different metric curves during the training of a small SoftMoE model with different model\\ndimensions. The model dimensions are those corresponding to different standard backbones: S (384), B\\n(768), L (1024), H (1280) and G (1664). The rest of the architecture parameters are fixed: 6 layers (3 dense\\nlayers followed by 3 MoE layers with 256 experts), 14x14 patches, and a MLP dimension of 1536. As the\\nmodel dimension d increases, the figure shows that, if the inputs to the softmax in the SoftMoE layers are not\\nnormalized, the average maximum values of the dispatch and combine weights tend to grow (especially the\\nformer). When d is big enough, the ImageNet 10shot accuracy is significantly worse than that achieved by\\nproperly normalizing the inputs.\\nIn the previous experiment, we trained our model with a linear decay schedule and a peak value of 10−3. In\\naddition, we also found that applying the softmax layer directly on the output of layer normalization is also\\nvery sensible to the learning rate’s configuration. Once again, our recipe suggested in Section 2.3 gives equal\\nor better quality, and is generally more stable. Figure 18 shows different metric curves during the training\\nof the same small SoftMoE model as before, with a model dimension of d = 1664, using an inverse square\\nroot learning rate schedule, with a fixed timescale of 105, a linear warmup phase of 105 steps, and a linear\\ncooldown of 5 · 105 steps, varying the peak learning rate value. In this figure, similarly to the results from the\\nprevious experiment, the average maximum values of the dispatch and combine weights grows to values\\napproaching 1.0 (indicating a collapse in the softmax layers to a one-hot vector), when the inputs to the\\nsoftmax in the SoftMoE layers are not normalized, which eventually severely hurts the accuracy of the model.\\nHowever, using the normalization in Section 2.3 gives better accuracy and makes the model less sensible to\\nthe choice of the peak value of the learning rate.\\n26\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nImageNet 10shot Accuracy\\nd = 384\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 768\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1024\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1280\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1664\\nNormalization\\nNo\\nYes\\n(a) ImageNet 10shot accuracy.\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nAverage maximum dispatch weight\\nd = 384\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 768\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1024\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1280\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1664\\nNormalization\\nNo\\nYes\\n(b) Average value of the maximum dispatch weight per slot.\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage maximum combine weight\\nd = 384\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 768\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1024\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1280\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nd = 1664\\nNormalization\\nNo\\nYes\\n(c) Average value of the maximum combine weight per token.\\nFigure 17: Training plots of the ImageNet 10shot accuracy (top), the average value of the maximum dispatch\\nweight per slot (middle) and the average value of the maximum combine weight per token (bottom) for\\ndifferent model dimensions d. Observe that maximum values of the combine and (especially) the dispatch\\nweights grow as the model dimension grows during training, as our theoretical analysis predicted. Although\\nthe ImageNet 10shot accuracy is similar for small model dimensions, applying the softmax layer directly on\\nthe output of layer normalization, without any further re-normalization, hurts the accuracy as the model\\ndimension d grows. By normalizing the inputs to the softmax as suggested in Section 2.3 improves the\\nperformance for large values of d.\\n27\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nImageNet 10shot Accuracy\\nLearning rate = 0.0003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.0006\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.001\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.006\\nNormalization\\nNo\\nYes\\n(a) ImageNet 10shot accuracy.\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nAverage maximum dispatch weight\\nLearning rate = 0.0003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.0006\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.001\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.006\\nNormalization\\nNo\\nYes\\n(b) Average value of the maximum dispatch weight per slot.\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage maximum combine weight\\nLearning rate = 0.0003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.0006\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.001\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.003\\n0\\n1\\n2\\n3\\nTrain steps\\n×105\\nLearning rate = 0.006\\nNormalization\\nNo\\nYes\\n(c) Average value of the maximum combine weight per token.\\nFigure 18: Training plots of the ImageNet 10shot accuracy (top), the average value of the maximum dispatch\\nweight per slot (middle) and the average value of the maximum combine weight per token (bottom) for\\ndifferent peak values of the learning rate, using a model dimension of d = 1664 (i.e. that of a G backbone).\\n28\\nF\\nAdditional Results\\nTable 8: Comparison between Top-K with and without BPR.\\nModel\\nNumber of Experts\\nK\\nBPR\\nJFT prec@1\\nIN/10shot\\nV-MoE S/16\\n32\\n1\\nNo\\n50.1%\\n64.5%\\nV-MoE S/16\\n32\\n1\\nYes\\n51.2%\\n68.9%\\nV-MoE S/16\\n32\\n2\\nNo\\n52.5%\\n71.0%\\nV-MoE S/16\\n32\\n2\\nYes\\n52.8%\\n71.4%\\nV-MoE S/16\\n64\\n1\\nNo\\n50.0%\\n64.4%\\nV-MoE S/16\\n64\\n1\\nYes\\n51.5%\\n69.1%\\nV-MoE S/16\\n64\\n2\\nNo\\n52.9%\\n70.9%\\nV-MoE S/16\\n64\\n2\\nYes\\n52.9%\\n71.4%\\n103\\nTotal Training TPUv3-days\\n67%\\n73%\\n77%\\n81%\\n84%\\nImageNet 10-shot Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTotal Training TPUv3-days\\n51%\\n54%\\n57%\\n59%\\nJFT-4B Precision-at-1\\nS/16\\nB/16\\nL/16\\nH/14\\n103\\nTotal Training TPUv3-days\\n84%\\n85%\\n87%\\n88%\\n89%\\nImageNet Finetune Accuracy\\nS/16\\nB/16\\nL/16\\nH/14\\nSoft MoE\\nDense\\nFigure 19: Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14\\nmodels trained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar\\ntraining costs, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision\\nat 1 (middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We\\nreport training FLOPs in Figure 4.\\n29\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.515\\n0.520\\n0.525\\n0.530\\n0.535\\n0.540\\n0.545\\nJFT-4B Precision-at-1\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.69\\n0.70\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\nImageNet 10-shot Accuracy\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\n2.50\\n(normalized) Train Step Time\\nSoft MoE\\nExperts Choice (gs=1 img)\\nExperts Choice (gs=8 img)\\nExperts Choice (gs=32 img)\\nFigure 20: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing\\nthe total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently\\nbetter results with more experts, whereas cost is kept roughly constant. Adding too many experts to Experts\\nChoice hurt performance and significantly increases the cost. Experts Choice can perform well with many\\nexperts if we increase the group size up to 32 images per group. The normalized train step time is computed\\nwith respect to Soft MoE with 32 experts. Experts Choice with 32 images per group and 4096 experts requires\\nmore than 2.5x its cost.\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.515\\n0.520\\n0.525\\n0.530\\n0.535\\n0.540\\n0.545\\nJFT-4B Precision-at-1\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n0.70\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\nImageNet 10-shot Accuracy\\n32\\n64\\n128 256 512 1024 2048 4096\\nNumber of Experts\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\n2.50\\n3.00\\n3.50\\n4.00\\n(normalized) Train Step Time\\nSoft MoE\\nTokens Choice (gs=8 img)\\nTokens Choice (gs=16 img)\\nFigure 21: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing\\nthe total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently\\nbetter results with more experts, whereas cost is kept roughly constant. Adding too many experts to Tokens\\nChoice hurt performance and significantly increases the cost. Even with a large group size (16 images),\\nTokens Choice struggles to perform well with a few thousand experts. The normalized train step time is\\ncomputed with respect to Soft MoE with 32 experts. Tokens Choice with 8 or 16 images per group and 4096\\nexperts requires almost 4x its cost.\\n30\\n101\\n102\\nTotal Training TPUv3-days\\n0.44\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\n0.58\\nJFT-4B Precision-at-1\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\nTokens Choice\\nDense\\n(a) JFT-4B Precision-at-1\\n101\\n102\\nTotal Training TPUv3-days\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Accuracy\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\nTokens Choice\\nDense\\n(b) ImageNet 10-shot Accuracy\\nFigure 22: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k steps). The size of the\\nmarker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different\\nmethods: Soft MoE (blue), Sparse MoEs with Experts Choice (orange) and Tokens Choice routing (green),\\nand a Dense (red) model. MoE runs include different configurations.\\n101\\n102\\nTotal Training TPUv3-days\\n0.44\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\n0.58\\nJFT-4B Precision-at-1\\n101\\n102\\nTotal Training ExaFLOP\\nSoft MoE\\nDense\\n(a) JFT-4B Precision-at-1\\n101\\n102\\nTotal Training TPUv3-days\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Accuracy\\n101\\n102\\nTotal Training ExaFLOP\\nSoft MoE\\nDense\\n(b) ImageNet 10-shot Accuracy\\nFigure 23: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\\ndifferent methods: Soft MoE (blue) and Dense (red) models. MoE runs include different configurations. We\\nonly show the runs that are not dominated by another model using the same method (S/8 and L/32 were\\nalways dominated).\\n31\\n101\\n102\\nTotal Training TPUv3-days\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\n0.58\\nJFT-4B Precision-at-1\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\n(a) JFT-4B Precision-at-1\\n101\\n102\\nTotal Training TPUv3-days\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Accuracy\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nExperts Choice\\n(b) ImageNet 10-shot Accuracy\\nFigure 24: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\\ndifferent methods: Soft MoE (blue) and Sparse MoEs with Experts Choice (orange) models. MoE runs\\ninclude different configurations. We only show the runs that are not dominated by another model using the\\nsame method (S/8 and L/32 were always dominated).\\n101\\n102\\nTotal Training TPUv3-days\\n0.46\\n0.48\\n0.50\\n0.52\\n0.54\\n0.56\\n0.58\\nJFT-4B Precision-at-1\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nTokens Choice\\n(a) JFT-4B Precision-at-1\\n101\\n102\\nTotal Training TPUv3-days\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nImageNet 10-shot Accuracy\\n101\\n102\\n103\\nTotal Training ExaFLOP\\nSoft MoE\\nTokens Choice\\n(b) ImageNet 10-shot Accuracy\\nFigure 25: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\\ndifferent methods: Soft MoE (blue) and Sparse MoEs with Tokens Choice (green) models. MoE runs include\\ndifferent configurations. We only show the runs that are not dominated by another model using the same\\nmethod (S/8 and L/32 were always dominated).\\n32\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nNumber of Experts\\n0.48\\n0.49\\n0.50\\n0.51\\n0.52\\n0.53\\n0.54\\nJFT-4B Precision-at-1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nNumber of Experts\\n0.62\\n0.64\\n0.66\\n0.68\\n0.70\\n0.72\\n0.74\\nImageNet 10-shot Accuracy\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nNumber of Experts\\n0.90\\n0.95\\n1.00\\n1.05\\n1.10\\n1.15\\n(normalized) Train Step Time\\nSoft MoE\\nFigure 26: JFT Precision-at-1, ImageNet 10-shot Accuracy, and normalized Training Step time when\\nincreasing the total number of experts while keeping the total amount of slots fixed (4096). Soft MoE\\nachieves consistently better results with more experts, whereas cost is kept roughly constant (same FLOPs\\nbut communication costs vary due to higher topologies needed for larger models). The normalized train step\\ntime is computed with respect to Soft MoE with 32 experts. Model sizes range from 38M (2 experts) to 9.7B\\nparameters (4096 experts).\\n33\\nG\\nAdditional analysis\\nG.1\\nCumulative sum of dispatch and combine weights\\nFigure 27 shows the distribution over slots of the cumulative sum (over tokens) of their corresponding\\ndispatch weights. For each slot we compute the cumulative sum of the dispatch weights over tokens sorted in\\ndecreasing order. This indicates how many tokens are necessary to cover a given percentage of the total mass\\nof the weighted average. We compute this cumulative sum for all slots over all the 50 000 ImageNet validation\\nimages, across all layers of the Soft MoE H/16 model after finetuning. In the plot, we represent with a solid\\nline the average (over all slots and images) cumulative sum, and the different colored areas represent the\\ncentral 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter colors) of cumulative sums.\\nThis tells us, for instance, how uniform is the weighted average over tokens used to compute each input\\nslot. In particular, each slot in the last two layers is close to a uniform average of all the tokens (a completely\\nuniform average would be represented by a straight line). This tells us that in these layers, every expert\\nprocesses roughly the same inputs, at least after the model is trained. However, this weighted average is far\\nfrom uniform in the rest of the layers, meaning that there are tokens that contribute far more than others. For\\nexample, in layer 28, a few tens of tokens already cover 80% of the weighted average mass. Finally, given\\nthe width of the colored areas, we can also see that there’s a significant difference on the weighted averages\\ndepending on the slot, across all layers (except maybe the last two). This indicates that the dispatch weights\\nvary across different slots and images.\\nSimilarly, Figure 28 shows the corresponding plots for the cumulative sum of the combine weights. In this\\ncase, for each output token we compute the cumulative sum of the combine weights over slots sorted in\\ndecreasing order. Notice that, although the dispatch weights in the last two layers were almost uniform, the\\ncombine weights are not. This indicates that some slots (and thus, experts) are more important than others\\nin computing the output tokens, and thus their corresponding expert parameters are not redundant. Of\\ncourse, the identity of the “important” slots may vary depending on the input token.\\n34\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative dispatch weights (%)\\nBlock 16\\nBlock 17\\nBlock 18\\nBlock 19\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative dispatch weights (%)\\nBlock 20\\nBlock 21\\nBlock 22\\nBlock 23\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative dispatch weights (%)\\nBlock 24\\nBlock 25\\nBlock 26\\nBlock 27\\n0\\n250\\n500\\n750\\nNumber of tokens\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative dispatch weights (%)\\nBlock 28\\n0\\n250\\n500\\n750\\nNumber of tokens\\nBlock 29\\n0\\n250\\n500\\n750\\nNumber of tokens\\nBlock 30\\n0\\n250\\n500\\n750\\nNumber of tokens\\nBlock 31\\nFigure 27: Distribution of the cumulative sum of dispatch weights. For each input slot, we compute the\\ncumulative sum of its corresponding dispatch weights (sorted by decreasing value). This indicates over\\nhow many input tokens a certain cumulative weight is distributed over. The line in each plot represents the\\naverage computed over all slots and ImageNet validation images of the given block in the SoftMoE H/14\\nmodel. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker\\nto lighter, better seen in color).\\n35\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative combine weights (%)\\nBlock 16\\nBlock 17\\nBlock 18\\nBlock 19\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative combine weights (%)\\nBlock 20\\nBlock 21\\nBlock 22\\nBlock 23\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative combine weights (%)\\nBlock 24\\nBlock 25\\nBlock 26\\nBlock 27\\n0\\n100\\n200\\nNumber of slots\\n0\\n20\\n40\\n60\\n80\\n100\\nCumulative combine weights (%)\\nBlock 28\\n0\\n100\\n200\\nNumber of slots\\nBlock 29\\n0\\n100\\n200\\nNumber of slots\\nBlock 30\\n0\\n100\\n200\\nNumber of slots\\nBlock 31\\nFigure 28: Distribution of the cumulative sum of combine weights. For each output token, we compute\\nthe cumulative sum of its corresponding combine weights (sorted by decreasing value). This indicates over\\nhow many output slots a certain cumulative weight is distributed over. The line in each plot represents the\\naverage computed over all tokens and ImageNet validation images of the given block in the SoftMoE H/14\\nmodel. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker\\nto lighter, better seen in color).\\n36\\nH\\nSlot Correlation\\nIn this section we explore the correlation between the different slot parameters that Soft MoE learns, and its\\nrelationship with the number of slots per expert. Figures 29 to 31 show for each of 6 layers in a Soft MoE\\nS/16 the inner product between each pair of (normalized) slot parameter vectors.\\nWhile Figure 29 shows no clear relationship between slots from different experts (as each expert only has\\none slot), we observe in Figures 30 and 31 how consecutive slots (corresponding to the same expert) are\\nextremely aligned. This confirms our hypothesis that adding more slots to experts does not work very well\\nas these slots end up aligning their value, and computing somewhat similar linear combinations. Therefore,\\nthese projections do not add too much useful information to the different tokens to be processed by the\\nexperts (in the extreme, these slots would be identical).\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 6\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 7\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 8\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 9\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 10\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\n1\\n4\\n8\\n12\\n16\\n20\\n24\\n28\\n32\\nslots\\nSoft MoE Layer 11\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFigure 29: Soft MoE S/16 with 1 slot per expert.\\n37\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 7\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 8\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 9\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 10\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nslots\\nSoft MoE Layer 11\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFigure 30: Soft MoE S/16 with 4 slots per expert.\\n38\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 7\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 8\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 9\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 10\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\n0\\n100\\n200\\n300\\n400\\n500\\nSlots\\nSoft MoE Layer 11\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFigure 31: Soft MoE S/16 with 16 slots per expert.\\n39\\nI\\nPareto Models\\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\\nRef Model Routing\\nExperts Group Size K\\nC JFT P@1 IN/10shot Train exaFLOP Train Days\\n1\\nS/32\\nDense\\n–\\n– –\\n–\\n42.8\\n51.0\\n4.2\\n6.3\\n2\\nS/32\\nExperts Choice\\n32\\n392 –\\n0.5\\n45.5\\n56.4\\n3.7\\n6.9\\n3\\nS/32\\nSoft MoE\\n32\\n49 –\\n–\\n48.2\\n62.3\\n3.8\\n7.0\\n4\\nS/32\\nExperts Choice\\n32\\n49 –\\n0.5\\n44.3\\n54.8\\n3.8\\n7.0\\n5\\nS/32\\nExperts Choice\\n32\\n392 –\\n1.0\\n46.6\\n58.2\\n4.4\\n7.6\\n6\\nS/32\\nExperts Choice\\n64\\n392 –\\n1.0\\n46.7\\n58.4\\n4.4\\n7.8\\n7\\nS/32\\nSoft MoE\\n64\\n49 –\\n–\\n49.4\\n65.1\\n4.6\\n7.8\\n8\\nS/32\\nExperts Choice\\n64\\n49 –\\n1.0\\n45.4\\n56.3\\n4.6\\n7.9\\n9\\nS/32\\nExperts Choice\\n32\\n49 –\\n1.0\\n45.5\\n56.9\\n4.6\\n7.9\\n10\\nS/32\\nTokens Choice\\n32\\n392 1\\n1.0\\n46.3\\n58.7\\n4.4\\n8.0\\n11\\nS/32\\nTokens Choice\\n64\\n392 1\\n1.0\\n47.1\\n59.4\\n4.4\\n8.1\\n12\\nS/32\\nTokens Choice\\n32\\n392 2\\n1.0\\n47.4\\n60.3\\n6.0\\n9.3\\n13\\nS/32\\nTokens Choice\\n64\\n392 2\\n1.0\\n48.0\\n61.9\\n6.0\\n9.8\\n14\\nB/32\\nDense\\n–\\n– –\\n–\\n48.0\\n63.4\\n16.0\\n11.7\\n15\\nB/32\\nSoft MoE\\n32\\n49 –\\n–\\n50.9\\n69.7\\n14.3\\n11.7\\n16\\nS/32\\nTokens Choice\\n64\\n392 2\\n2.0\\n48.9\\n63.9\\n9.0\\n12.0\\n17\\nS/32\\nTokens Choice\\n32\\n392 2\\n2.0\\n48.5\\n62.8\\n9.1\\n12.1\\n18\\nS/16\\nSoft MoE\\n16\\n196 –\\n–\\n50.9\\n68.1\\n12.4\\n14.2\\n19\\nS/16\\nSoft MoE\\n32\\n196 –\\n–\\n52.0\\n70.8\\n12.9\\n14.8\\n20\\nS/16\\nDense\\n–\\n– –\\n–\\n47.9\\n60.8\\n17.0\\n15.3\\n21\\nS/16\\nSoft MoE\\n64\\n196 –\\n–\\n52.9\\n72.3\\n13.9\\n15.7\\n22\\nS/16\\nSoft MoE\\n128\\n196 –\\n–\\n53.6\\n73.0\\n15.9\\n17.6\\n23\\nB/32\\nExperts Choice\\n32\\n392 –\\n0.5\\n49.0\\n64.3\\n13.7\\n18.0\\n24\\nB/32\\nExperts Choice\\n32\\n49 –\\n0.5\\n47.8\\n62.4\\n14.3\\n18.2\\n25\\nS/16\\nExperts Choice\\n128\\n196 –\\n0.5\\n50.2\\n66.7\\n15.8\\n18.5\\n26\\nS/16\\nExperts Choice\\n32\\n196 –\\n1.0\\n50.5\\n67.4\\n17.5\\n18.8\\n27\\nS/16\\nExperts Choice\\n128\\n1568 –\\n0.5\\n51.4\\n67.7\\n16.8\\n19.7\\n28\\nB/32\\nExperts Choice\\n32\\n392 –\\n1.0\\n49.9\\n66.0\\n16.5\\n19.7\\n29\\nB/32\\nExperts Choice\\n64\\n392 –\\n1.0\\n49.9\\n65.5\\n16.5\\n19.8\\n30\\nB/32\\nTokens Choice\\n32\\n392 1\\n1.0\\n49.7\\n66.2\\n16.5\\n20.0\\n31\\nB/32\\nTokens Choice\\n64\\n392 1\\n1.0\\n49.8\\n65.6\\n16.5\\n20.2\\n32\\nB/32\\nSoft MoE\\n64\\n49 –\\n–\\n51.8\\n70.7\\n17.8\\n20.3\\n33\\nB/32\\nExperts Choice\\n64\\n49 –\\n1.0\\n48.6\\n64.0\\n17.7\\n20.3\\n34\\nB/32\\nExperts Choice\\n32\\n49 –\\n1.0\\n48.4\\n63.8\\n17.7\\n20.5\\n35\\nS/16\\nExperts Choice\\n32\\n1568 –\\n1.0\\n51.3\\n68.7\\n21.5\\n21.5\\n36\\nS/16\\nSoft MoE\\n256\\n196 –\\n–\\n53.8\\n73.7\\n19.9\\n22.1\\n37\\nS/16\\nTokens Choice\\n32\\n1568 1\\n1.0\\n51.2\\n68.9\\n21.5\\n23.2\\n38\\nS/16\\nExperts Choice\\n256\\n196 –\\n1.0\\n50.7\\n67.7\\n19.8\\n23.3\\n39\\nS/16\\nExperts Choice\\n32\\n196 –\\n2.0\\n51.0\\n68.3\\n23.1\\n23.5\\n40\\nB/32\\nTokens Choice\\n32\\n392 2\\n1.0\\n50.2\\n67.4\\n22.0\\n23.6\\n41\\nB/32\\nTokens Choice\\n64\\n392 2\\n1.0\\n50.8\\n68.0\\n22.1\\n23.8\\n42\\nS/16\\nTokens Choice\\n64\\n1568 1\\n1.0\\n51.5\\n69.1\\n21.3\\n24.9\\n43\\nS/16\\nExperts Choice\\n256\\n1568 –\\n1.0\\n52.3\\n69.7\\n21.7\\n25.5\\n44\\nS/16\\nExperts Choice\\n32\\n1568 –\\n2.0\\n52.4\\n70.3\\n31.0\\n27.8\\n45\\nS/16\\nTokens Choice\\n32\\n1568 2\\n1.0\\n52.1\\n70.3\\n31.0\\n30.0\\n46\\nB/32\\nTokens Choice\\n64\\n392 2\\n2.0\\n51.2\\n70.0\\n33.2\\n30.4\\n40\\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\\nRef Model Routing\\nExperts Group Size K\\nC JFT P@1 IN/10shot Train exaFLOP Train Days\\n47\\nB/32\\nTokens Choice\\n32\\n392 2\\n2.0\\n51.0\\n69.5\\n33.6\\n31.1\\n48\\nS/16\\nTokens Choice\\n64\\n1568 2\\n1.0\\n52.3\\n70.4\\n31.1\\n32.0\\n49\\nS/16\\nTokens Choice\\n32\\n1568 2\\n2.0\\n52.8\\n71.4\\n50.0\\n42.5\\n50\\nS/16\\nTokens Choice\\n64\\n1568 2\\n2.0\\n52.9\\n71.4\\n50.1\\n45.1\\n51\\nB/16\\nDense\\n–\\n– –\\n–\\n52.0\\n71.8\\n64.8\\n45.2\\n52\\nB/16\\nSoft MoE\\n128\\n196 –\\n–\\n55.3\\n77.0\\n59.0\\n46.8\\n53\\nB/16\\nExperts Choice\\n128\\n1568 –\\n0.5\\n53.7\\n73.0\\n59.0\\n48.2\\n54\\nB/16\\nExperts Choice\\n32\\n196 –\\n1.0\\n53.3\\n73.0\\n65.6\\n51.0\\n55\\nB/16\\nExperts Choice\\n128\\n196 –\\n0.5\\n52.5\\n72.2\\n58.8\\n52.6\\n56\\nL/32\\nDense\\n–\\n– –\\n–\\n51.3\\n70.9\\n55.9\\n54.9\\n57\\nL/32\\nExperts Choice\\n32\\n392 –\\n0.5\\n52.3\\n71.2\\n47.4\\n55.2\\n58\\nL/32\\nExperts Choice\\n32\\n49 –\\n0.5\\n51.1\\n70.6\\n49.8\\n55.7\\n59\\nL/32\\nSoft MoE\\n32\\n49 –\\n–\\n53.5\\n75.0\\n49.8\\n56.0\\n60\\nB/16\\nExperts Choice\\n32\\n1568 –\\n1.0\\n54.2\\n74.5\\n73.6\\n56.2\\n61\\nB/16\\nTokens Choice\\n32\\n1568 1\\n1.0\\n53.7\\n74.4\\n73.6\\n57.8\\n62\\nB/16\\nExperts Choice\\n256\\n196 –\\n1.0\\n52.7\\n72.7\\n73.4\\n58.1\\n63\\nB/16\\nSoft MoE\\n256\\n196 –\\n–\\n55.8\\n78.0\\n73.7\\n58.2\\n64\\nB/16\\nTokens Choice\\n64\\n1568 1\\n1.0\\n54.0\\n74.8\\n73.2\\n58.7\\n65\\nL/32\\nExperts Choice\\n64\\n392 –\\n1.0\\n52.7\\n72.1\\n56.9\\n60.4\\n66\\nB/16\\nExperts Choice\\n256\\n1568 –\\n1.0\\n53.9\\n73.5\\n73.8\\n60.5\\n67\\nL/32\\nExperts Choice\\n32\\n392 –\\n1.0\\n52.7\\n71.7\\n56.8\\n60.6\\n68\\nL/32\\nTokens Choice\\n64\\n392 1\\n1.0\\n51.9\\n71.4\\n56.9\\n61.0\\n69\\nL/32\\nTokens Choice\\n32\\n392 1\\n1.0\\n52.3\\n71.7\\n57.1\\n61.6\\n70\\nL/32\\nExperts Choice\\n64\\n49 –\\n1.0\\n51.1\\n70.7\\n61.6\\n62.6\\n71\\nL/32\\nSoft MoE\\n64\\n49 –\\n–\\n54.0\\n75.2\\n61.7\\n62.8\\n72\\nL/32\\nExperts Choice\\n32\\n49 –\\n1.0\\n51.4\\n70.3\\n61.5\\n63.2\\n73\\nB/16\\nExperts Choice\\n32\\n196 –\\n2.0\\n53.1\\n73.9\\n86.8\\n64.2\\n74\\nL/32\\nTokens Choice\\n32\\n392 2\\n1.0\\n51.5\\n70.7\\n76.0\\n72.2\\n75\\nB/16\\nExperts Choice\\n32\\n1568 –\\n2.0\\n54.6\\n75.6\\n102.9\\n72.5\\n76\\nL/32\\nTokens Choice\\n64\\n392 2\\n1.0\\n52.0\\n71.8\\n76.0\\n72.5\\n77\\nB/16\\nTokens Choice\\n32\\n1568 2\\n1.0\\n53.9\\n74.7\\n102.9\\n74.7\\n78\\nB/16\\nSoft MoE\\n512\\n196 –\\n–\\n56.1\\n78.5\\n103.1\\n76.5\\n79\\nB/16\\nTokens Choice\\n64\\n1568 2\\n1.0\\n54.3\\n74.8\\n103.0\\n76.5\\n80\\nS/8\\nDense\\n–\\n– –\\n–\\n49.9\\n66.7\\n82.7\\n77.7\\n81\\nS/8\\nSoft MoE\\n512\\n784 –\\n–\\n56.1\\n78.0\\n85.6\\n88.5\\n82\\nS/8\\nExperts Choice\\n32\\n784 –\\n1.0\\n52.9\\n72.6\\n91.3\\n93.0\\n83\\nL/32\\nTokens Choice\\n64\\n392 2\\n2.0\\n52.9\\n72.9\\n114.3\\n93.2\\n84\\nL/32\\nTokens Choice\\n32\\n392 2\\n2.0\\n52.5\\n72.5\\n115.7\\n95.8\\n85\\nL/16\\nDense\\n–\\n– –\\n–\\n54.8\\n77.8\\n226.9\\n100.9\\n86\\nL/16\\nExperts Choice\\n128\\n196 –\\n0.5\\n54.0\\n76.7\\n204.6\\n104.9\\n87\\nL/16\\nSoft MoE\\n128\\n196 –\\n–\\n57.2\\n80.3\\n205.0\\n106.0\\n88\\nB/16\\nTokens Choice\\n32\\n1568 2\\n2.0\\n54.4\\n75.7\\n161.4\\n108.4\\n89\\nB/16\\nTokens Choice\\n64\\n1568 2\\n2.0\\n54.8\\n76.0\\n161.5\\n110.5\\n90\\nL/16\\nExperts Choice\\n32\\n196 –\\n1.0\\n55.1\\n77.5\\n228.6\\n113.6\\n91\\nL/16\\nTokens Choice\\n32\\n1568 1\\n1.0\\n55.9\\n78.5\\n250.4\\n125.1\\n92\\nL/16\\nTokens Choice\\n64\\n1568 1\\n1.0\\n56.2\\n78.6\\n248.8\\n125.7\\n93\\nS/8\\nExperts Choice\\n32\\n6272 –\\n1.0\\n53.6\\n73.4\\n160.6\\n126.6\\n94\\nS/8\\nExperts Choice\\n512\\n784 –\\n1.0\\n53.4\\n72.4\\n104.1\\n129.0\\n41\\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\\nRef Model Routing\\nExperts Group Size K\\nC JFT P@1 IN/10shot Train exaFLOP Train Days\\n95\\nL/16\\nSoft MoE\\n256\\n196 –\\n–\\n57.4\\n80.2\\n256.0\\n129.6\\n96\\nS/8\\nTokens Choice\\n32\\n6272 1\\n1.0\\n53.8\\n73.7\\n162.5\\n129.8\\n97\\nL/16\\nExperts Choice\\n256\\n196 –\\n1.0\\n54.1\\n76.7\\n255.2\\n130.1\\n98\\nL/16\\nExperts Choice\\n32\\n196 –\\n2.0\\n55.2\\n77.8\\n301.0\\n140.3\\n99\\nS/8\\nExperts Choice\\n512\\n6272 –\\n1.0\\n54.8\\n74.6\\n149.3\\n161.9\\n100 S/8\\nTokens Choice\\n32\\n6272 2\\n1.0\\n54.2\\n74.6\\n243.4\\n166.6\\n101 H/14\\nSoft MoE\\n128\\n256 –\\n–\\n58.0\\n81.6\\n599.2\\n170.5\\n102 H/14\\nDense\\n–\\n– –\\n–\\n56.5\\n80.1\\n680.5\\n196.2\\n103 H/14\\nExperts Choice\\n64\\n2048 – 1.25\\n57.3\\n80.4\\n855.9\\n210.9\\n104 L/16\\nTokens Choice\\n32\\n1568 2\\n2.0\\n53.5\\n74.6\\n534.5\\n218.5\\n105 L/16\\nTokens Choice\\n64\\n1568 2\\n2.0\\n53.3\\n73.3\\n535.1\\n226.9\\n106 H/14\\nTokens Choice\\n64\\n2048 1 1.25\\n56.7\\n79.8\\n857.0\\n230.7\\n107 S/8\\nTokens Choice\\n32\\n6272 2\\n2.0\\n54.1\\n74.8\\n424.4\\n255.4\\n42\\n', 'source_name': 'From Sparse to Soft Mixture of Experts', 'source_url': 'https://arxiv.org/abs/2308.00951'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BASE_layers.pdf #57\n",
      "{'content': 'BASE Layers: Simplifying Training of Large, Sparse Models\\nMike Lewis 1 Shruti Bhosale 1 Tim Dettmers 1 2 Naman Goyal 1 Luke Zettlemoyer 1 2\\nAbstract\\nWe introduce a new balanced assignment of ex-\\nperts (BASE) layer for large language models that\\ngreatly simpliﬁes existing high capacity sparse\\nlayers. Sparse layers can dramatically improve\\nthe efﬁciency of training and inference by routing\\neach token to specialized expert modules that con-\\ntain only a small fraction of the model parameters.\\nHowever, it can be difﬁcult to learn balanced rout-\\ning functions that make full use of the available\\nexperts; existing approaches typically use routing\\nheuristics or auxiliary expert-balancing loss func-\\ntions. In contrast, we formulate token-to-expert\\nallocation as a linear assignment problem, allow-\\ning an optimal assignment in which each expert\\nreceives an equal number of tokens. This opti-\\nmal assignment scheme improves efﬁciency by\\nguaranteeing balanced compute loads, and also\\nsimpliﬁes training by not requiring any new hyper-\\nparameters or auxiliary losses. Code is publicly\\nreleased.1\\n1. Introduction\\nSparse expert models enable sparse computation by spread-\\ning model capacity across a set of experts, while ensuring\\nthat only a small subset of the experts are used for each\\ninput (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus\\net al., 2021). Sparse models can often realize the strong per-\\nformance gains that come with training very large models,\\nwhile also alleviating much of the associated computational,\\nﬁnancial and environmental costs (Strubell et al., 2019).\\nHowever, such models are notoriously difﬁcult to train; the\\nexperts must be carefully balanced so that they can spe-\\ncialize to different parts of the input space. In this paper,\\nwe present a simple, efﬁcient, and performant method for\\nexpert-based sparsity in language models, built around the\\nuse of a linear assignment algorithm to explicitly balance\\n1Facebook AI Research 2University of Washington. Correspon-\\ndence to: Mike Lewis <mikelewis@fb.com>.\\nProceedings of the 38 th International Conference on Machine\\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\\n1https://github.com/pytorch/fairseq/\\nExpert1\\nbark\\nDogs\\npurr\\nCats\\nCats\\nDogs\\npurr\\nbark\\nExpert1\\nExpert2 Expert2\\n+\\nHidden states hi\\nMix in expert output:\\nhi +σ(wai . hi )fai(hi )\\nBalanced assignment of \\ntoken i to expert ai\\n+\\n+\\n+\\nExpert Computation\\nf(hi )\\nWorker 1\\nWorker 2\\nRe-route to original worker\\nbark\\nDogs\\npurr\\nCats\\nFigure 1. Overview of a BASE layer. Each worker contains a\\nseparate expert module. During training, we compute a balanced\\nassignment of tokens such that each worker sends an equal number\\nof tokens to each expert. By softly mixing in the expert module,\\nexperts can learn to specialize for particular types of tokens.\\nthe assignment of tokens to experts during training.\\nThe mostly widely used Sparse Expert models are mixtures\\nof experts (MoE) models (Shazeer et al., 2017; Lepikhin\\net al., 2020) that learn a gating function to route each to-\\nken to a few experts, which creates a challenging, discrete\\nlatent variable learning problem. In practice, carefully tun-\\ning and the introduction of extra loss functions with new\\nhyperparameters is required to avoid imbalanced or degen-\\nerate experts. Recently, the Switch transformer (Fedus et al.,\\n2021) simpliﬁed the framework by routing tokens to only a\\nsingle expert, improving stability and efﬁciency overall but\\nagain using custom auxiliary losses that require tuning, and\\nrequiring capacity factors to prevent too many tokens being\\nassigned to a single expert. We show that it is possible to\\ngo even further. We also assign a single expert per token\\nbut are the ﬁrst to algorithmically balance the assignment\\nwith no extra model modiﬁcations, providing more formal\\nguarantees of balanced compute while simplifying both the\\nimplementation and optimization.\\nWe introduce a simple and effective solution for routing to-\\nkens to experts during training, which we use to estimate a\\nnew Balanced Assignment of Sparse Experts (BASE) layer.\\nTo ensure balanced routing in the BASE layer, we formulate\\na linear assignment problem that maximizes token-expert\\nafﬁnities while ensuring that each expert receives an equal\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nnumber of tokens. This approach ensures that the assign-\\nment will be balanced, and therefore each expert will operate\\nat maximum capacity, while also eliminating load-balancing\\nloss functions and capacity factors from previous work. We\\nalso show how to learn expert specialization by using a mod-\\niﬁed residual connection that softly mixes in each expert\\ncontribution—again without requiring an additional loss\\nterm or routing tokens to multiple experts. While comput-\\ning balanced assignments incurs non-trivial overhead, we\\nﬁnd that using even a single large BASE layer is remarkably\\neffective—reduced expert communication produces faster\\ngradient computations—and that performance increases as\\nmore BASE layers are added, providing an overall favorable\\ncost-accuracy tradeoff.\\nExtensive experiments with models of up to 110B param-\\neters demonstrate large performance gains over standard\\ndata and model parallel training strategies. Our approach\\nalso matches or exceeds the efﬁciency and performance of\\nprevious sparse expert approaches (Lepikhin et al., 2020;\\nFedus et al., 2021), when controlling for computation bud-\\nget, despite its relative simplicity. Taken together, these\\nresults demonstrate the ﬁrst drop-in conditional compute\\nlayer that can be easily added to any model with no new\\nhyperparameters or training loss modiﬁcations.\\n2. Background: Training with Multiple\\nWorkers\\nNLP has recently become dominated by ever larger lan-\\nguage models (Devlin et al., 2018; Lewis et al., 2019; Liu\\net al., 2019; Radford et al., 2019; Raffel et al., 2019). Train-\\ning large language models would take infeasibly long on\\nany existing single device, with many models trained for\\nthousands of GPU-days (Brown et al., 2020). Instead, it is\\nstandard to distribute computation over multiple workers.\\nWe brieﬂy review the main existing strategies.\\n2.1. Dense Models\\nIn dense models, every parameter is used in processing every\\ninput. Training is distributed over multiple workers using\\ndata parallism or model parallelism.\\nData Parallel Training\\nIn data parallel training, multiple\\nworkers maintain a copy of the same model. Each worker\\nruns the model on a different subset of the training batch,\\nthen gradients are communicated and all workers perform\\nthe same update. This approach increases the number of\\nexamples processed per second, and only requires a single\\ncommunication step between workers per update. However,\\nthe maximum model size that can be trained is bounded by\\nthe memory of a single worker device—limiting models to\\nroughly 1.5B parameters in our setup.\\nModel Parallel Training\\nModel parallel training allows\\nmodels to be larger than can be run on a single worker\\n(Shoeybi et al., 2019), by distributing the compute for each\\ninput over multiple workers. Model parameters are also dis-\\ntributed over workers, which then communicate with each\\nother while processing each input. Given a ﬁxed number\\nof workers, using model parallel training will reduce the\\namount of compute available for data parallelism, and cor-\\nrespondingly also the number of examples processed per\\nsecond.\\n2.2. Sparse Expert Layers\\nSparse models differ from dense models in only using a\\nsmall subset of their parameters on any given input. Recent\\nwork has explored adding capacity to language models by\\nadding sparse expert layers (Shazeer et al., 2017; Lepikhin\\net al., 2020; Fedus et al., 2021). During inference, before\\nan expert layer, each token is assigned and routed to a small\\nsubset of the workers. The workers then applies a token-\\nwise operation, using parameters that are not shared across\\nother workers. The resulting representation is then returned\\nto the original worker, to continue the forward pass.\\nDuring training, this results in four routing steps per expert\\nlayer—before and after each expert layer, in both the for-\\nward and backward pass. These communication steps can\\nadd signiﬁcantly to the training cost, as workers can idle\\nwhile waiting for communication to complete.\\nBalancing of experts, so that each processes a roughly equal\\nproportion of tokens, is crucial for several reasons. If one\\nexpert is assigned too many tokens, the worker could run out\\nof memory. Additionally, the expert layer processing speed\\nis limited by the slowest worker; imbalanced assignment\\nslows down training. Furthermore, the parameters of rarely\\nused experts are likely to be less well trained, which may\\nreduce performance.\\nPrevious work has achieved balancing by adding a new term\\nin the loss function that explicitly encourages balancing—\\nthis loss term must be carefully weighted so that it does\\nnot overwhelm primary loss (Lepikhin et al., 2020; Fedus\\net al., 2021). However, such a loss does not guarantee bal-\\nancing. Stable training also requires additional measures\\nsuch as enforcing hard upper limits on the number of tokens\\nprocessed by each expert after which the rest are simply\\nignored (Shazeer et al., 2017). This approach can be inefﬁ-\\ncient, as some workers are underutilized, and many tokens\\nare unprocessed by the layer.\\n3. BASE Layers\\nBASE layers achieve balanced assignment of tokens to ex-\\nperts through a three stage process. Firstly, we compute\\nthe score for assigning each token representation to each\\nBASE Layers: Simplifying Training of Large, Sparse Models\\n1 def base_layer(features, expert_centroids, expert_id, expert_network):\\n2\\n# Send each token to a random worker, by sorting in a random order\\n3\\nshuffle_sort = random_permutation(len(features))\\n4\\nshuffled_features = all2all(features[shuffle_sort])\\n5\\n# Compute which token goes to which expert\\n6\\ntoken_expert_affinities = shuffled_features @ expert_centroids.T\\n7\\nsort_by_expert = balanced_assignment(token_expert_affinities)\\n8\\n# Swap these tokens for the right ones for our expert\\n9\\nrouted_features = all2all(shuffled_features[sort_by_expert])\\n10\\n# Mix in the expert network based on how appropriate it is for these tokens\\n11\\nα = torch.sigmoid(routed_features @ self.expert_centroids[expert_id])\\n12\\nrouted_features += α * expert_network(routed_features)\\n13\\n# Undo routing and balanced assignment\\n14\\nshuffled_features = all2all(routed_features)[inverse_sort(sort_by_expert)]\\n15\\n# Return to original worker and ordering\\n16\\nreturn all2all(shuffled_features)[inverse_sort(shuffle_sort)]\\nFigure 2. Implementation of a BASE layer, with E experts and an input sequence of T features. Here, all to all routes the tth row of its\\ninput to the ⌊tE\\nT ⌋th worker. balanced assignment takes a matrix of size T × E and returns an T-dimensional vector that can be used to\\nsort tokens by their assigned expert index.\\nexpert, compute a balanced assignment maximizing these\\nscores, then route the token features to an expert. Secondly,\\nwe compute a position-wise expert function, and compute\\na weighted sum of the layers input and output. Finally, we\\nreturn the output to the original worker. Figure 2 shows\\noverall pseudo code for the approach.\\n3.1. Parameterization\\nBASE layers contain E experts, each deﬁned by a position-\\nwise function fe(·) and an expert embedding we ∈RD,\\nwhere D is the model dimension. In practice, we parameter-\\nize fe(·) using a stack of residual feedforward layers. Given\\na token ht at timestep t in a sequence of tokens 0..T, and\\ntoken-to-expert assignment index at ∈0..E, the network\\nreturns the following value:\\nσ(ht · wat)fat(ht) + ht,\\n(1)\\nIf the network fat is able to improve the representation of\\nht, by lowering the loss of the ﬁnal prediction for that token,\\nthen gradient descent will increase the value of ht · wat.\\nConversely, if the expert network is unhelpful, then the\\nht · wat will receive a negative gradient. Consequently, an\\nexpert e can learn to specialize for particular types of tokens\\nby adjusting we to be close to similar token representations\\nwhere fe(·) is most beneﬁcial.\\n3.2. Token to Expert Assignment\\nWe assign tokens to experts using different methods during\\ntraining and testing. During training, we maximize model\\nthroughput by assigning an equal number of tokens to each\\nexpert. At test time, we simply assign each token to its\\nhighest scoring expert.\\n3.2.1. ASSIGNMENT DURING TRAINING\\nDuring training, we assign an equal number of tokens to\\neach expert, so that each worker is fully utilized and each\\nworker takes about the same time to ﬁnish its assigned load.\\nEach token t is assigned to an expert at, aiming to maximize\\nthe token-expert afﬁnities under the constraints that each\\nexpert is assigned the same number of tokens.\\nLinear Assignment Problem\\nFormally, we solve the fol-\\nlowing linear assignment problem. Given T tokens with\\nrepresentations ht and E experts with embeddings we, we\\nassign each token to an expert via the assignment index\\nat ∈0..E:\\nmaximize\\nX\\nt\\nht · wat\\nsubject to ∀e\\nT\\nX\\nt=0\\n1at=e = T\\nE\\n(2)\\nNumerous algorithms exist for this problem. We use the auc-\\ntion algorithm described in Bertsekas (1992), which is more\\neasily parallelizable on GPUs than the Hungarian Algorithm\\n(Kuhn, 1955). Pseudo-code is given in the Appendix.\\nSharding\\nComputing the optimal assignment for all to-\\nkens across all workers is expensive, so we distribute the\\ncomputation across multiple workers. We decompose the\\nassignment problem of all ET tokens across all workers into\\nE smaller problems using T tokens. This decomposition\\ncan be implemented by each worker solving an assignment\\nproblem over its own input batch. Each worker then sends\\nT/E tokens to each other worker, with an all2all operation.\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nShufﬂing\\nTokens within each worker’s training sequence\\nare highly correlated with each other; for example they will\\nnormally be part of the same domain. These correlations\\nmay make it difﬁcult for experts to specialize for particular\\ndomains. We therefore add an additional random routing\\nstep, where each worker ﬁrst sends an equal number of each\\ntokens to each other worker randomly. Then, each worker\\nsolves a linear assignment problem as before with its sample\\nof tokens, and routes these to the correct experts.\\n3.2.2. ASSIGNMENT DURING TESTING\\nAt test time, it is not possible to use the assignment strat-\\negy described in §3.2.1, as balancing the assignment leaks\\ninformation about tokens in the future context. Instead,\\nwe simply greedily assign the one best expert. While un-\\nbalanced assignments are less efﬁcient, during inference\\nmemory costs are greatly reduced due to not needing to\\nstore gradients, activations and optimizer states. In practice,\\nwe show that our approach naturally learns a reasonably\\nbalanced assignment during training (§5.1).\\n3.3. Gradient Clipping\\nA common practice in training deep language models is to\\nscale gradients if their l2 norm is greater than a threshold.\\nAll workers must compute the same norm, or else scaled\\ngradients for shared parameters will be inconsistent across\\nworkers. To avoid additional communication steps to com-\\npute norms globally across all expert parameters, we simply\\ncompute the gradient norms locally based only on the shared\\nparameters, but rescale all gradients.\\n4. Experiments\\n4.1. Experimental Setup\\nTask\\nWe focus our experiments on language modelling,\\nas recent work such as GPT3 (Brown et al., 2020) offers\\nperhaps the clearest demonstration in machine learning of\\nthe power of large scale models.\\nMetrics\\nWe focus exclusively on comparing compute ef-\\nﬁciency, which we deﬁne as the best model performance\\n(here, perplexity) that can be achieved by training with a\\ngiven number of GPUs and wall-clock time. This metric is\\ndifferent from other commonly used metrics, such as sample\\nefﬁciency (which measures the number of tokens the model\\ntrains on, but not the cost of processing samples) or FLOP-\\nefﬁciency (which measures the number of ﬂoating-point\\noperations performed during training, but does not account\\nfor communication costs). As plentiful data is available for\\ntraining language models, but computation is expensive, we\\nbelieve that compute efﬁciency best captures the constraints\\nof real world training. Therefore, we compare models using\\na ﬁxed number of GPUs for the same runtime.\\nTraining Hyperparameters\\nWe train all models for ap-\\nproximately 2.5 days. All models use similar hyperpa-\\nrameters of 2000 warm-up steps, and the Adam optimizer\\n(Kingma & Ba, 2014). We tune learning rates for each\\nmodel separately, and linearly decay the learning rate dur-\\ning training. Each worker processes two sequences of length\\n1024, and gradients are accumulated over 8 updates. We\\nclip gradients if their l2 norm exceeds 0.1 (§3). Learning\\nrates are tuned in the range {0.5, 0.75, 1.0} × 10−4, taking\\nthe highest value that avoids divergence.\\nHardware\\nUnless otherwise stated, models are trained on\\n128 32GB V100 GPUs connected with Inﬁniband.2\\nData\\nWe train on a corpus of approximately 100B tokens,\\ncomprising the training corpus of RoBERTa (Liu et al.,\\n2019), combined with the English portion of the CC100 cor-\\npus (Conneau et al., 2019). We use the byte-pair encoding\\n(Sennrich et al., 2015) from GPT2 (Radford et al., 2019),\\nwhich has a vocabulary of 51200.\\nModel Architectures\\nWe size all models to the maximum\\nsize that can process the sequences within GPU memory\\nconstraints. All models follow a standard transformer ar-\\nchitecture (Vaswani et al., 2017), with a model dimension\\nof 2048, feed-forward hidden states of size 8096 and 24\\nTransformer Decoder blocks. We use 16 attention heads,\\nReLU activation functions and no dropout. LayerNorm (Ba\\net al., 2016) is applied to the inputs of each residual block\\n(Xiong et al., 2020) and to the outputs of the transformer.\\nBASE layer architecture\\nWe implement the BASE layer\\nas a stack of feedforward blocks. Each block follows the\\nstandard transformer structure: layer normalization, a pro-\\njection to 4 times the input dimension, a ReLU nonlin-\\nearity, a projection to the input dimension, and a resid-\\nual connection to the block input. We vary the number of\\nBASE layers; BASE×N uses a BASE layer after each of\\nthe ⌊\\nL\\nN+1⌋. . . ⌊NL\\nN+1⌋th transformer layers. When using\\nmultiple BASE layers, we reduce their size to keep the to-\\ntal number of parameters roughly constant; BASE×N use\\n⌊10\\nN ⌋sublayers, for a total of roughly 44B parameters. We\\nuse one expert per GPU per BASE layer.\\n4.2. Comparison with Dense Models\\nWe ﬁrst compare with dense models, in which all parameters\\nare shared across all workers. We compare with data parallel\\nand model parallel training, using the intra-layer model\\n2As communication between workers is a signiﬁcant overhead\\nfor model parallel and sparse expert approaches, it is possible that\\ndifferent results would be achieved on other networking hardware.\\nBASE Layers: Simplifying Training of Large, Sparse Models\\n0.5\\n1\\n1.5\\n2\\n2.5\\n8\\n10\\n12\\n14\\n16\\n18\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×1\\nData Parallel\\nModel Parallel ×2\\n(a) 8 GPUs\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n8\\n10\\n12\\n14\\n16\\n18\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×1\\nData Parallel\\nModel Parallel ×2\\n(b) 32 GPUs\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n8\\n10\\n12\\n14\\n16\\n18\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×1\\nData Parallel\\nModel Parallel ×2\\nModel Parallel ×4\\n(c) 128 GPUs\\nFigure 3. Comparing BASE layers with dense model training, using different numbers of GPUs. There is a clear trend of increased model\\nsizes being more effective with larger compute budgets. BASE layers show strong performance at all the compute budgets we consider.\\nparallelism approach introduced in Shoeybi et al. (2019).\\nOur data parallel baseline contains 1.5B parameters, and the\\n2-way and 4-way model parallel baselines contain roughly\\n3B and 6B parameters respectively. We use three different\\ncompute budgets: 8, 32 and 128 GPUs for approximately\\n2.5 days.\\nResults are shown in Figure 3. We generally ﬁnd that larger\\nmodels perform better with higher compute budgets, and\\nthat simple data parallel training performs best at the small-\\nest compute budget. With larger compute budgets, BASE\\nlayers outperform both data parallel and model parallel train-\\ning by a wide margin.\\nRelatively high compute budgets are required before model\\nparallelism outperforms data parallel training, with the ﬁrst\\ngains appearing after training on 128 GPUs for 2 days. This\\nis partly due to model parallel training requiring a reduced\\nbatch size given the same computational resources.\\nIn contrast, BASE layers match the performance of data\\nparallel training on our 8 GPU experiments, and achieve\\nincreasingly large gains in higher compute regimes.\\n4.3. Comparison with Sparse Experts Models\\nWe also compare performance with our re-implementations\\nof two recent sparse layer methods: Sparsely Gated Mix-\\ntures of Experts (Shazeer et al., 2017; Lepikhin et al., 2020)\\nand Switch (Fedus et al., 2021). The primary difference be-\\ntween these approaches is that a Sparsely Gated MoE layer\\nroutes tokens to multiple experts (top-2 experts in our ex-\\nperiments), whereas a Switch layer routes tokens to a single\\nexpert. We set the weight associated with the load balancing\\nloss to 0.01 in our experiments, and set the capacity factor\\nfor Sparsely Gated MoE and Switch layers to 2.0 and 1.0\\nrespectively. Following previous work, we replace every\\nother shared feed-forward layer in the Transformer archi-\\ntecture with a Sparsely Gated MoE or Switch layer, unless\\n0.5\\n1\\n1.5\\n2\\n2.5\\n7\\n8\\n9\\n10\\n11\\n12\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×3\\nSparsely Gated MoE\\nSwitch\\nFigure 4. Comparison with other Sparse Experts approaches. De-\\nspite its simplicity, BASE achieves strong performance relative to\\nSparsely Gated MoE models and Switch transformers.\\notherwise speciﬁed. With 128 experts in each expert layer,\\nour Sparsely Gated MoE and Switch models have 52.5B\\nparameters (1B shared parameters) each, while our BASE\\nmodel has 44.4B parameters (1.3B shared parameters).\\nAs in Fedus et al. (2021), we ﬁnd that Switch computes more\\nupdates per second than Sparsely Gated MoE (see Table 2).\\nHowever, we ﬁnd that Sparsely Gated MoE is more compute\\nefﬁcient in our experiments as shown in Figure 4.\\nA comparison with BASE is also shown in Figure 4. De-\\nspite its simplicity, BASE achieves similar performance to\\nthe Sparsely Gated MoE model and converges to a better\\nvalidation perplexity than Switch. This result suggests that\\nalgorithmic load balancing is a competitive alternative to\\nload balancing loss functions, and that even a single expert\\nBASE Layers: Simplifying Training of Large, Sparse Models\\n0.5\\n1\\n1.5\\n2\\n2.5\\n7\\n8\\n9\\n10\\n11\\n12\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×1\\nBASE×1 Small\\nBASE×1 Large\\nFigure 5. Comparison of different sizes of BASE layers, by chang-\\ning the ratio of parameters allocated to shared vs. expert layers.\\nlayer can be highly effective.\\n4.4. Ablations\\nResults in Section 4 show that BASE layers match or ex-\\nceed the compute-efﬁciency of previous dense and sparse\\napproaches. To better understand these results, we analyze\\nkey design decisions in our model in more detail.\\nBASE Layer Size\\nA key choice in any sparse experts\\nmodel is the allocation of capacity to shared components\\nversus experts. We experiment with adjusting the number\\nof sublayers in each expert, and scale the number of shared\\nlayers accordingly to maximize GPU usage.\\nWe test three versions:\\n• Small Expert: 1.5B shared parameters, 135M param-\\neters per expert, 18.8B total parameters\\n• Standard Expert: 1.3B shared parameters, 335M pa-\\nrameters per expert, 44B total parameters\\n• Large Expert: 840M shared parameters, 911M pa-\\nrameters per expert, 117B total parameters\\nFigure 5 shows that good performance can be achieved with\\nall sizes, indicating that this choice needs little tuning.\\nBASE Layer Position\\nWe also consider the most effec-\\ntive place in a model to insert BASE layers into a trans-\\nformer with L layers. We test three conﬁgurations:\\n• BASE: After the L\\n2 th layer, as in our other experiments.\\n0.5\\n1\\n1.5\\n2\\n2.5\\n7\\n8\\n9\\n10\\n11\\n12\\nTraining Time (days)\\nValidation Perplexity\\nBASE x 1 (Top)\\nBASE x 1\\nBASE x 3\\nBASE x 5\\nFigure 6. Comparison of different numbers and positions of BASE\\nlayers. The best performance is achieved by interleaving 3 BASE\\nlayers throughout the transformer stack.\\n• BASE Top: After the Lth layer, acting as a classiﬁer.\\n• BASE ×N: Using N BASE layers of 1\\nN the size, after\\nlayers\\nL\\nN+1 . . . NL\\nN+1th layers of the transformer.\\nFigure 6 compares results for different conﬁgurations. We\\nﬁnd similar performance from three different placements\\nof BASE, suggesting a reasonable level of robustness. In\\nparticular, the strong performance of BASE Top may enable\\nit to be used on top of pre-trained language models to further\\nincrease their capacity.\\nComparison of Routing Method with Sparsely Gated\\nMoE\\nOur approach differs from previous work on sparse\\nexperts in both the architecture and assignment method. To\\nmore carefully analyse the beneﬁts of our routing method,\\nwe compare with an implementation of Sparsely Gated MoE\\nthat uses a more similar architecture to ours: a single, large\\nexpert midway through the transformer stack.\\nResults are shown in Figure 7. Sparsely Gated MoE per-\\nforms less well in this setting. Sparsely Gated MoE beneﬁts\\nfrom interleaving expert layers with shared layers, and a sin-\\ngle Sparsely Gated MoE layer with deep experts works less\\nwell than BASE. Future work should explore more efﬁcient\\napproximate routing schemes for BASE layers, to enable\\npotential compute efﬁciency gains from interleaving expert\\nand shared layers.\\nBASE Layers: Simplifying Training of Large, Sparse Models\\n0.5\\n1\\n1.5\\n2\\n2.5\\n7\\n8\\n9\\n10\\n11\\n12\\nTraining Time (days)\\nValidation Perplexity\\nBASE ×1\\nSparsely Gated MoE (at layer L/2)\\nSwitch (at layer L/2)\\nFigure 7. Comparing routing strategies using similar architectures.\\nHere, all models use a single large expert at layer L/2. BASE\\nmaintains strong performance in this setting, which reduces the\\ncommunication overhead between workers, and may be advanta-\\ngeous with less efﬁcient networking.\\n5. Analysis\\nWe also report further experiments that provide more quali-\\ntative analyses of overall model behavior with BASE layers.\\n5.1. Expert Balancing\\nA key difference between our model and other recent pro-\\nposals is that we algorithmically balance token/expert as-\\nsignments during training, instead of adding an additional\\nloss function to balance assignments. However, both use\\ngreedy assignments at test time.\\nWe investigate whether our model learns a balanced assign-\\nment without an explicit balancing loss. Figure 8 shows\\nthe percentage of tokens assigned to each expert, sorted\\nfrom most used to least used. Unsurprisingly, the top-1\\nassignment from BASE is less balanced than those from\\nmodels with explicit balancing loss terms. However it is\\nnotably more balanced than the 2nd expert in the Sparsely\\nGated MoE model, and conﬁrms that reasonably balanced\\nassignments can be learnt without balancing losses.\\n5.2. Expert Specialization\\nWe also analyse how experts learn to specialize. Observing\\nsample passages, we ﬁnd that many assignment decisions\\nappear to depend primarily on very local syntactic informa-\\ntion. In particular, we found that the token input at timestep\\nt is often highly indicative of the expert assigned at time t.\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n0\\n2\\n4\\n6\\n8\\n10\\nExperts (sorted by usage)\\nPercentage of Tokens Routed to Expert\\nSparsely Gated MoE-1st-Expert\\nSparsely Gated MoE-2nd-Expert\\nSwitch\\nBASE (training)\\nBASE (testing)\\nFigure 8. Expert Balancing in different Sparse Expert approaches\\nacross 128 experts, as measured on the validation set. Results for\\nSparsely Gated MoE and Switch are an average across all expert\\nlayers. BASE layers learn a reasonably balanced routing with no\\nauxiliary balancing loss.\\nTable 1 shows the most frequent previous input token when\\nselected experts are chosen. We see clusters corresponding\\nto quantities (5), numbers (42), possessives (125), subword\\nfragments (101), and clusters of related verbs (72, 74, 126),\\nnouns (23,27,36,43,76,84,96,98,105) and adjectives (9,81).\\nThese tokens may tend to have similar distributions over\\nnext tokens. This analysis suggests the model primarily\\nassigns experts based on fairly superﬁcial signals, and may\\nmotivate even simpler techniques for future work.\\n5.3. Efﬁciency\\nWhile we focus on evaluating the compute efﬁciency of\\nmodels, we note that there are substantial differences in the\\nspeed at which models process tokens. Table 2 shows the\\nnumber of tokens processed per second by different models\\nduring training, using 128 GPUs. Simple data parallel train-\\ning is unsurprisingly the fastest, but BASE layers compute\\nupdates faster than other approaches due to reduced commu-\\nnication between workers. For the same compute efﬁciency,\\nmodels which process tokens more slowly are more sample\\nefﬁcient, and may be preferable in lower data regimes.\\n6. Related Work\\nShazeer et al. (2017); Lepikhin et al. (2020) introduce\\nsparsely gated mixtures of experts layers, demonstrating\\nhow large sparse models can be trained efﬁciently by rout-\\ning inputs to appropriate specialist workers. Fedus et al.\\n(2021) show the design can be simpliﬁed by routing tokens\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nExpert\\nTop 5 Proceeding Tokens\\n5\\nyear, years, billion, million, tonnes\\n8\\npeople, who, Man, everyone, one\\n9\\nelectronic, local, public, national, outdoor\\n23\\nfunding, budget, beneﬁts, pressure, price\\n27\\nMustang, State, Center, ation, Grande\\n34\\nto, will, should, it, may\\n36\\nbusiness, bank, ﬁnancial, science, school\\n42\\ntwo, 50, 1, 80, 000\\n43\\nBank, Development, ., Construction, Plant\\n62\\nwork, started, involved, working, launched\\n72\\nis, was, be, been, were\\n74\\ngoing, go, come, back, return\\n76\\npainting, case, song, statement, discussion\\n81\\nnew, major, bad, larger, grand\\n84\\nRet, Inspect, Pl, Pos, Architect\\n96\\nUS, UNESCO, government, state, UN\\n98\\nwaiver, procedures, warrant, status, loans\\n101\\nB, T, W, H, k\\n105\\napp, Windows, Microsoft, board, 10\\n125\\nhis, ’s, its, their, our\\n126\\nsaid, says, means, noting, out\\nTable 1. Most frequent previous words for selected experts, show-\\ning that some experts assignment decisions are made based on very\\nlocal contexts. For many other experts, the assignment decision\\ndepends on longer context, and is harder to visualize.\\nto only a single worker. We further simplify the framework,\\nby eliminating balancing loss functions, and showing the\\neffectiveness of using only a single expert layer.\\nSparse training is a line of work where traditional architec-\\ntures are trained with sparse instead of dense layers and the\\nnumber of parameters allowed during training is restricted\\nto a percentage of the dense layers (Dettmers & Zettlemoyer,\\n2019; Evci et al., 2020; Mostafa & Wang, 2019). Unlike our\\napproach, these networks have ﬁne-grained sparsity patterns\\nwhich reduce overall FLOPS but make it difﬁcult to achieve\\nruntime beneﬁts on modern accelerators like GPUs, which\\nrequire contiguous memory segments for efﬁcient process-\\ning. Since experts consist of sizable contiguous memory\\nsegments, our approach can utilize GPUs effectively.\\nPerhaps the most common use of sparse layers is in adding\\nlanguage-speciﬁc layers to machine-translation systems\\n(Bapna et al., 2019; Fan et al., 2020), or task-speciﬁc lay-\\ners to pre-trained language models (Houlsby et al., 2019).\\nHere, the expert assignment problem is hard coded, based\\non the task being solved or the language being translated.\\nWe instead explore learnable routing, which is applicable to\\nproblems where such structure is not available.\\nOther papers have explored alternative methods for adding\\nModel\\nTokens per Second\\nData Parallel\\n600k\\nModel Parallel ×2\\n224k\\nSparsely Gated MoE\\n292k\\nSwitch\\n469k\\nBASE\\n545k\\nBASE ×2\\n475k\\nTable 2. Number of tokens processed per second during training\\nby different models. BASE computes updates faster than other ap-\\nproaches that divide models over multiple workers, due to reduced\\ncommunication overheads. This allows a 43B parameter model to\\nbe trained at 90% of the speed of a 1.5B data parallel baseline.\\nvery high capacity layers to neural language models. For\\nexample, Lample et al. (2019) introduce a large memory\\nlayer that supports efﬁcient sparse queries. Khandelwal et al.\\n(2019) show large gains from augmenting a language model\\nwith a nearest neighbour classiﬁer over the training set,\\nwhich recent work has also shown is applicable to machine\\ntranslation (Khandelwal et al., 2020).\\nAn orthogonal strand of work has improved the efﬁciency\\nof transformer attention mechanisms, often by making them\\nsparse (Child et al., 2019; Correia et al., 2019; Roy et al.,\\n2020). We instead develop a sparse version of the other ma-\\njor component of the transformer, the feed forward network.\\n7. Conclusion\\nWe introduced a simple sparse BASE layer, which can be\\nused to increase the capacity of any neural model, with little\\nincrease in training cost or complexity. We demonstrate\\nstrong performance relative to both dense models and previ-\\nously proposed sparse models. Future work should explore\\nmore efﬁcient implementations for computing balanced as-\\nsignments, to further improve training speed.\\nReferences\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\\narXiv preprint arXiv:1607.06450, 2016.\\nBapna, A., Arivazhagan, N., and Firat, O. Simple, scalable\\nadaptation for neural machine translation. arXiv preprint\\narXiv:1909.08478, 2019.\\nBertsekas, D. P. Auction algorithms for network ﬂow prob-\\nlems: A tutorial introduction. Computational optimiza-\\ntion and applications, 1(1):7–66, 1992.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\narXiv preprint arXiv:2005.14165, 2020.\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\\nerating long sequences with sparse transformers. arXiv\\npreprint arXiv:1904.10509, 2019.\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V.,\\nWenzek, G., Guzm´\\nan, F., Grave, E., Ott, M., Zettlemoyer,\\nL., and Stoyanov, V. Unsupervised cross-lingual represen-\\ntation learning at scale. arXiv preprint arXiv:1911.02116,\\n2019.\\nCorreia, G. M., Niculae, V., and Martins, A. F. Adaptively\\nsparse transformers. arXiv preprint arXiv:1909.00015,\\n2019.\\nDettmers, T. and Zettlemoyer, L. Sparse networks from\\nscratch:\\nFaster training without losing performance.\\narXiv preprint arXiv:1907.04840, 2019.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nEvci, U., Gale, T., Menick, J., Castro, P. S., and Elsen,\\nE. Rigging the lottery: Making all tickets winners. In\\nInternational Conference on Machine Learning, pp. 2943–\\n2952. PMLR, 2020.\\nFan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A.,\\nGoyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary,\\nV., et al. Beyond english-centric multilingual machine\\ntranslation. arXiv preprint arXiv:2010.11125, 2020.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\\ners: Scaling to trillion parameter models with simple and\\nefﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\\nGelly, S. Parameter-efﬁcient transfer learning for nlp.\\nIn International Conference on Machine Learning, pp.\\n2790–2799. PMLR, 2019.\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,\\nand Lewis, M. Generalization through memorization:\\nNearest neighbor language models.\\narXiv preprint\\narXiv:1911.00172, 2019.\\nKhandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and\\nLewis, M. Nearest neighbor machine translation. arXiv\\npreprint arXiv:2010.00710, 2020.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\nKuhn, H. W. The hungarian method for the assignment\\nproblem. Naval research logistics quarterly, 2(1-2):83–\\n97, 1955.\\nLample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and\\nJ´\\negou, H. Large memory layers with product keys. arXiv\\npreprint arXiv:1907.05242, 2019.\\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\\ngiant models with conditional computation and automatic\\nsharding. arXiv preprint arXiv:2006.16668, 2020.\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-\\nhamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.\\nBart: Denoising sequence-to-sequence pre-training for\\nnatural language generation, translation, and comprehen-\\nsion. arXiv preprint arXiv:1910.13461, 2019.\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\\nRoberta: A robustly optimized bert pretraining approach.\\narXiv preprint arXiv:1907.11692, 2019.\\nMostafa, H. and Wang, X. Parameter efﬁcient training of\\ndeep convolutional neural networks by dynamic sparse\\nreparameterization. In International Conference on Ma-\\nchine Learning, pp. 4646–4655. PMLR, 2019.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multitask\\nlearners. OpenAI blog, 1(8):9, 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient\\ncontent-based sparse attention with routing transformers.\\narXiv preprint arXiv:2003.05997, 2020.\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units.\\narXiv\\npreprint arXiv:1508.07909, 2015.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\\nQ., Hinton, G., and Dean, J. Outrageously large neural\\nnetworks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538, 2017.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\\nJ., and Catanzaro, B.\\nMegatron-lm: Training multi-\\nbillion parameter language models using model paral-\\nlelism. arXiv preprint arXiv:1909.08053, 2019.\\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\\npolicy considerations for deep learning in nlp. arXiv\\npreprint arXiv:1906.02243, 2019.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need. arXiv preprint arXiv:1706.03762, 2017.\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,\\nC., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer\\nnormalization in the transformer architecture. In Inter-\\nnational Conference on Machine Learning, pp. 10524–\\n10533. PMLR, 2020.\\n', 'source_name': 'BASE Layers: Simplifying Training of Large, Sparse Models', 'source_url': 'https://arxiv.org/abs/2103.16716'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MixtureOfTokens.pdf #58\n",
      "{'content': 'MIXTURE OF TOKENS: EFFICIENT LLMS THROUGH\\nCROSS-EXAMPLE AGGREGATION\\nSzymon Antoniak ∗†\\nIDEAS NCBR\\nUniversity of Warsaw\\nSebastian Jaszczur ∗†\\nIDEAS NCBR\\nUniversity of Warsaw\\nMichał Krutul †\\nIDEAS NCBR\\nUniversity of Warsaw\\nMaciej Pi´\\noro †\\nIDEAS NCBR\\nPolish Academy of Sciences\\nJakub Krajewski †\\nIDEAS NCBR\\nUniversity of Warsaw\\nJan Ludziejewski †\\nIDEAS NCBR\\nUniversity of Warsaw\\nTomasz Odrzyg´\\no´\\nzd´\\nz †\\nIDEAS NCBR\\nMarek Cygan †\\nUniversity of Warsaw\\nNomagic\\nABSTRACT\\nDespite the promise of Mixture of Experts (MoE) models in increasing parame-\\nter counts of Transformer models while maintaining training and inference costs,\\ntheir application carries notable drawbacks. The key strategy of these models is to,\\nfor each processed token, activate at most a few experts - subsets of an extensive\\nfeed-forward layer. But this approach is not without its challenges. The operation\\nof matching experts and tokens is discrete, which makes MoE models prone to is-\\nsues like training instability and uneven expert utilization. Existing techniques de-\\nsigned to address these concerns, such as auxiliary losses or balance-aware match-\\ning, result either in lower model performance or are more difficult to train. In re-\\nsponse to these issues, we propose Mixture of Tokens, a fully-differentiable model\\nthat retains the benefits of MoE architectures while avoiding the aforementioned\\ndifficulties. Rather than routing tokens to experts, this approach mixes tokens from\\ndifferent examples prior to feeding them to experts, enabling the model to learn\\nfrom all token-expert combinations. Importantly, this mixing can be disabled to\\navoid mixing of different sequences during inference. Crucially, this method is\\nfully compatible with both masked and causal Large Language Model training\\nand inference.\\n1\\nINTRODUCTION\\nLarge language models based on Transformers currently make up one of the most active fields in\\nMachine Learning, exhibiting human-level performance in a variety of tasks. This is in large part due\\nto their scaling properties - [Kaplan et al. (2020); Hoffmann et al. (2022)] showed that an increase\\nin model size results in a predictable increase in performance. This scaling leads to an ever-growing\\ndemand for computational resources, with their effective utilization often deemed as one of the\\ncritical challenges of the field [Rae et al. (2022); Jaszczur et al. (2021); Nawrot et al. (2022)].\\nMixture of Experts. How can we increase the model size without additional computational cost?\\nMixture of Expert does this by replacing the feed-forward layer standard for Transformer archi-\\n∗Core contributors.\\n†Szymon implemented a PoC and different variants of MoT, together with running experiments and op-\\ntimizations. Sebastian provided the initial idea, research intuitions, and direct project supervision. Michał\\nimplemented and experimented with various MoT designs and contributed to the infrastructure design and im-\\nplementation. Maciej was responsible for parts of evaluation and significant engineering. Jakub implemented\\nMoE baselines, Jan stabilized Mixture of Experts training, while both helped with MoE hyperparameter tuning.\\nTomasz consulted ideas and helped with cluster infrastructure. Everybody above contributed to the general\\ninfrastructure of the project. Marek provided scientific advice and high-level supervision.\\n1\\narXiv:2310.15961v1  [cs.CL]  24 Oct 2023\\ntectures with a (potentially very large) set of experts, together with a small network often called\\na controller. The (trainable) controller matches tokens and experts in a way that each token is\\nprocessed only by a small subset of experts.\\nSimilarly to vanilla Transformers, the performance of MoE models also scales with parameter count\\n[Clark et al. (2022)]. For a more detailed background and explanation of variants of the MoE\\narchitecture, see Section 2.\\nLimitations of current approaches. While the performance of the huge-parameter-count MoE\\narchitectures is impressive, they come with an entirely new set of challenges during both training\\nand inference. The most notable include:\\n• Training instability. Multiple studies [Fedus et al. (2022); Du et al. (2022); Mustafa et al.\\n(2022)] report difficulties in training MoE models due to instabilities. This is likely due to\\nthe nature of the technique: the operation of choosing top-k most relevant tokens/experts\\nin discrete, and thus small changes of controller weights can have disproportional effects\\non controller decisions. We hypothesize that existing techniques used for training the con-\\ntroller with gradient descent, while somewhat effective, do not entirely solve this problem.\\n[Jaszczur et al. (2021)] reported training stability improvements due to using a weighted\\naverage of expert outputs.\\n• Load imbalance. Typically, in MoE we set the maximum capacity for each expert. How-\\never, we are not able to efficiently restrict the choice of the routing network to assign tokens\\nin a perfectly balanced way. This leads to token dropping (when some tokens are not pro-\\ncessed by an expert) and mode collapse (when the controller sends almost all tokens to a\\nfew experts).\\n• Information leak. Some of the most successful MoE methods process tokens from dif-\\nferent positions in a sequence together (i.e., by comparing scores of all tokens in a batch).\\nThis imposes an intra-sequence information leak and hinders their utility in autoregressive\\ndecoding.\\nOur technique is as stable as a vanilla Transformer because the network is fully differentiable, and\\nno discrete choices are made during training. As every expert receives the same number of tokens,\\nthe issue of load imbalance is side-stepped as well. Finally, our technique is fully compatible with\\nautoregressive decoding. See a detailed explanation of the technique in Section 3.\\n2\\nRELATED WORK AND BACKGROUND\\nIn the context of language models, Mixture of Experts was originally proposed in [Shazeer et al.\\n(2017)]. The basic idea is as follows: instead of processing all tokens with the standard feed-\\nforward layer, we route each processed token to a small subset of multiple experts. The technique\\nwas further simplified by [Fedus et al. (2022)] by proposing the Switch Transformer, which sends\\neach token to only one expert with the highest score produced by the controller. The technique\\nallowed them to train a 1.6T model with a T5 architecture with FLOPS cost of an equivalent 1.4B\\nvanilla Transformer. In both cases, auxiliary losses are needed in order to encourage exploration and\\nmitigate load imbalance across experts.\\nMore recently, [Zhou et al. (2022)] proposed Expert Choice, where, in contrast to Switch, each ex-\\npert chooses which token to process. This results in a tradeoff: on the one hand, each expert receives\\nthe same number of tokens, sidestepping the load-balancing issue; on the other hand, different to-\\nkens might be attended to by varying numbers of experts, and some tokens might not be chosen by\\nany expert. Both approaches, as well as a standard feed-forward Transformer layer, are illustrated\\nin the Figure 1.\\nThere are a number of works that try to improve the stability and quality of the controller, including\\nmethods based on reinforcement learning [Bengio et al. (2015)], routing by hashing [Roller et al.\\n(2021)], optimal transport [Clark et al. (2022)], and more [Dai et al. (2022); Chi et al. (2022)].\\n[Lewis et al. (2021)] address the load balancing problem by linear programming while [Riquelme\\net al. (2021)] tries to achieve this by learning to drop unimportant tokens.\\n2\\nFigure 1: The standard feed-forward (left) processes each token independently. In Switch (center),\\nmany tokens can choose the same expert, increasing the processing time for that expert. In Expert\\nChoice (right) all experts process the same number of tokens, but some tokens might not be picked\\nby any expert, resulting in no update for that input token.\\nFigure 2: In Mixture of Experts (left), each token is routed to a different expert feed-forward layer.\\nIn Mixture of Tokens (right) tokens within each group are mixed and the mixed token is processed\\nby an expert feed-forward layer.\\nConcurrently to our work, [Puigcerver et al. (2023)] proposed a continuous variant of Mixture of\\nExperts for the Vision Transformer, limited to encoder-only models where patches are mixed only\\nwithin each image. Another approach allowing to avoid discrete operations in MoE by merging\\nexperts was presented in [Muqeeth et al. (2023)].\\n3\\nMETHOD\\n3.1\\nMIXTURE OF TOKENS\\nLet’s say we have a Transformer model, and we would like to increase the parameter count in the\\nfeed-forward layers without increasing model runtime. One of the ways to achieve this is to activate\\nonly a subset of parameters for a given token - this gives rise to Mixture of Experts architectures.\\nAnother way would be to somehow merge the tokens and process them together. This idea lies at\\nthe heart of Mixture of Tokens. Because the mixing operation is continuous, the architecture does\\nnot experience the problems present in MoE. Figure 2 shows an intuitive comparison of Mixture of\\nTokens with Mixture of Experts.\\nIn the final design, similar to Multi-Head Attention, we sacrifice a single, big representation for\\nmultiple, independent, smaller ones: we divide the large feed-forward layer into experts and send a\\nseparate mixture to each expert. The resulting network is illustrated in Figure 3.\\n3\\nFigure 3: Tokens are mixed uniquely for each expert (mixing weights are decided by the controller,\\nomitted here for simplicity), then each mixture is processed and redistributed back to the original\\ntokens (using the same weights as before).\\n3.2\\nHOW TO MIX TOKENS\\nIn order to mix tokens for a given group, we need importance weights for each token. To get those,\\nwe send each token through the controller (a standard linear layer) and calculate a softmax over\\nthe resulting token scores. Note that the weights are calculated independently for each expert - as\\nvisible in the Figure 3. Now that we have importance weights, we simply multiply each token by its\\nimportance weight and add all of them together.\\n3.3\\nHOW TO REDISTRIBUTE THE MIXED TOKENS\\nOnce every mixed token is processed by their respective expert, we redistribute them according to\\nthe importance weights we calculated before. See the equations in Algorithm 1.\\nAlgorithm 1 The algorithm for computing the output of a Mixture of Tokens layer\\n1: for each e in experts do:\\n2:\\nimp weights = Softmax(Linear(tokens))\\n3:\\nmix = P\\ni token i ∗imp weights i\\n4:\\nexpert output = FFN(mix)\\n5:\\nfor each i do\\n6:\\nfinal output i,e = expert output ∗importance i\\n7: for each i do\\n8:\\nfinal output i = P\\ne final output i,e\\nThe distributedOutputi is what will be added to the residual stream to i-th token. See the diagram\\nbelow for visualization of those equations.\\n3.4\\nHOW TO GROUP TOKENS IN MOT\\nThe only question left is how to group tokens - we will show the grouping scheme for autoregressive\\ndecoding. While it would be natural to mix tokens within sequences, it would be very inefficient:\\nin vanilla Transformers, nothing is recomputed for tokens already present in the decoded sequence.\\n4\\nFigure 4: Simple groupin in MoT.\\nFigure 5: Limiting the group size artificially.\\nThis allows for very efficient (FLOP-wise) decoding, and we want to keep it that way. Thus, in\\norder to run computations for any given token only once, we group tokens across sequences, i.e.,\\naccording to position in a sequence. The diagram illustrates the grouping scheme.\\nWhile the maximum size of the group is limited by the batch size (number of sequences), note that\\nthose two numbers are not coupled together. We can always, if we want to, make groups smaller\\nthan the batch size.\\nAlgorithm 2 The algorithm for computing the output of a Mixture of Tokens layer\\n1: Group tokens by position in the sequence\\n2: for each group do\\n3:\\nfor each expert do\\n4:\\ncalculate importance weights for the tokens in the group\\n5:\\ncalculate the mixed token\\n6:\\nprocess each mixed token with the expert\\n7:\\nredistribute the output to tokens, scaling by their respective weights from line 4.\\n5\\n4\\nEXPERIMENTS\\n4.1\\nSETUP\\nFor the baseline, we train a standard GPT-like model on the language modeling task using cross-\\nentropy loss on the C4 dataset [Raffel et al. (2019)]. Our model replaces all feed-forward layers\\nwith Mixture of Tokens layers.\\nIn our proof-of-concept experiments, we train a decoder-only Transformer model with the following\\nhyperparameters:\\nTable 1: Model parameters\\nModel hyperparameters\\nTransformer blocks\\n4\\ndmodel\\n256\\ndff\\n1024\\nnheads\\n4\\nMixture of Tokens Hyperparameters\\ngroup size\\n32\\nexperts\\n512\\nexpansion rate vs vanilla FF\\n32x more params\\nTraining hyperparameters\\ntraining step\\n250k\\ncontext length\\n256\\nbatch size\\n256\\nlr warmup\\nthe first 1% of training steps\\nlr schedule\\ncosine, decaying to 10% of peak lr\\nLearning rates, separately tuned\\nbaseline\\n4e-3\\nMixture of Tokens\\n2e-3\\n4.2\\nRESULTS\\nOur technique shows very promising results, reducing the required training steps by a factor of 4.\\nThe training time gains are also very significant.\\n5\\nFUTURE WORK\\n5.1\\nSCALING UP\\nOur preliminary experiments suggest that Mixture of Tokens might work even better for larger model\\nsizes. In the upcoming weeks, we aim to prepare a comprehensive comparison of larger models and\\ncompare with Mixture of Experts methods.\\n5.2\\nFROM MIXTURE OF TOKENS TO MIXTURE OF EXPERTS\\nHow do we get from MoT to MoE if needed? Assume that the controller in a Mixture of Tokens\\nlayer decided to mix in a very particular way: for a given group, it concentrated the entire weight on\\njust one token. In this extreme case, each expert would receive a single, unmixed token. This would\\nmake the Mixture of Tokens forward pass equivalent to the Expert Choice described in Section 2.\\n6\\nFigure 6: Our Mixture of Tokens model attains the dense model’s final training loss in just 24% of\\nsteps.\\nFigure 7: If we measure actual training time, the final dense model’s loss is achieved in only 33% of\\ntime..\\n7\\nFigure 8: Decreasing the temperature to 0 results in a weight concentration, while increasing it\\ncauses the weights to become more uniform.\\nFigure 9: The temperature changing during training.\\nThis scenario has its advantages: in the default Mixture of Tokens setup for autoregressive training,\\ntokens are aggregated across the batch dimension. However, during decoding, this setup allows for\\ninformation to be exchanged between different examples. This could be undesirable in some use\\ncases, e.g., when different examples in the same batch come from different users in the industry\\nsetting, possibly with privacy issues.\\nHow could we make the controller focus on a single example? One can achieve this by adding\\na temperature parameter to the softmax operation used by the controller. Low temperature forces\\nthe weight distribution to concentrate - in the limit (as the temperature approaches 0), causing the\\nweights to focus exclusively on the token with the highest controller score.\\nInterestingly, simply allowing the temperature parameter to be learnable for the controller in a Mix-\\nture of Tokens layer encourages this phenomenon.\\nAs expected, this results in the controller focusing more on one token. We measured this by mon-\\nitoring the entropy of weights produced by the controller (averaged over all token groups and all\\nexperts).\\nInterestingly, this comes at the cost of model performance.\\nWe expect allowing the temperature to be learned at the end of training to be a very promising\\ndirection for ”private” autoregressive decoding. That way, we would retain all the benefits of training\\nwith a high rate of token mixing and prevent token mixing during inference.\\n8\\nFigure 10: Allowing the temperature parameter to be learnable causes the mean entropy of weights\\ndrops to lower levels; in blocks 3 and 4, the entropy is near zero - which means that on average,\\nalmost all of the weight is attributed to just one token in a group.\\nFigure 11: Despite the fact that training lowers the temperature when it is learnable, the lower\\ntemperature during training seems to result in worse perplexity.\\n9\\n6\\nCONCLUSIONS\\nWe have shown the preliminary results showing the promise of Mixture of Tokens improving the\\nstability of training in comparison with MoE approaches and decreasing the training time 3× when\\ncompared to the vanilla Transformer. We expect even greater improvements in larger models - more\\nthorough experiments are underway at the moment.\\nACKNOWLEDGEMENTS\\nWe would like to express sincere gratitude to Piotr Padlewski and Tomasz Trzci´\\nnski for valuable\\nfeedback and Dagmara Rudzi´\\nnska for invaluable support with graphic design.\\nThis work was funded by IDEAS NCBR, which also provided significant computational resources.\\nMarek Cygan was partially supported by an NCBiR grant POIR.01.01.01-00-0392/17-00. The re-\\nsearch was supported by PL-Grid infrastructure (grant PLG/2023/016148). We also benefited from\\nthe Entropy cluster (hosted at the Faculty of Mathematics, Informatics and Mechanics of the Uni-\\nversity of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center grants UMO-\\n2017/26/E/ST6/00622 and 2022/45/N/ST6/02222, and ERC Starting Grant TOTAL.\\nREFERENCES\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models, 2015. URL http://arxiv.org/abs/1511.06297.\\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,\\nPayal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation\\ncollapse of sparse mixture of experts, 2022.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche,\\nEliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones,\\nElena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae,\\nErich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language\\nmodels, 2022.\\nDamai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. Stable-\\nmoe: Stable routing strategy for mixture of experts, 2022.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kath-\\nleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng\\nChen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efficient sparsity, 2022.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-\\nnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\\nTraining compute-optimal large language models, 2022.\\nSebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski,\\nHenryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers, 2021.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels, 2020.\\n10\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers:\\nSimplifying training of large, sparse models, 2021. URL https://arxiv.org/abs/2103.\\n16716.\\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing,\\n2023.\\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multi-\\nmodal contrastive learning with limoe: the language-image mixture of experts, 2022.\\nPiotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy,\\nand Henryk Michalewski. Hierarchical transformers are more efficient language models, 2022.\\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures\\nof experts, 2023.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\\npher, 2022.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. arXiv e-prints, 2019.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´\\ne Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts,\\n2021. URL https://arxiv.org/abs/2106.05974.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels, 2021. URL https://arxiv.org/abs/2106.04426.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,\\n2017.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng\\nChen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\\n11\\n', 'source_name': 'Mixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation', 'source_url': 'https://arxiv.org/abs/2310.15961'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DeepSeekMoE.pdf #59\n",
      "{'content': 'DeepSeekMoE: Towards Ultimate Expert Specialization in\\nMixture-of-Experts Language Models\\nDamai Dai∗1,2, Chengqi Deng1, Chenggang Zhao∗1,3, R.X. Xu1, Huazuo Gao1, Deli Chen1, Jiashi Li1,\\nWangding Zeng1, Xingkai Yu∗1,4, Y. Wu1, Zhenda Xie1, Y.K. Li1, Panpan Huang1, Fuli Luo1, Chong Ruan1,\\nZhifang Sui2, Wenfeng Liang1\\n1DeepSeek-AI\\n2National Key Laboratory for Multimedia Information Processing, Peking University\\n3Institute for Interdisciplinary Information Sciences, Tsinghua University\\n4National Key Laboratory for Novel Software Technology, Nanjing University\\n{daidamai, szf}@pku.edu.cn, {wenfeng.liang}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-MoE\\nAbstract\\nIn the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for\\nmanaging computational costs when scaling up model parameters. However, conventional MoE\\narchitectures like GShard, which activate the top-𝐾out of 𝑁experts, face challenges in ensuring\\nexpert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In\\nresponse, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It\\ninvolves two principal strategies: (1) finely segmenting the experts into 𝑚𝑁ones and activating\\n𝑚𝐾from them, allowing for a more flexible combination of activated experts; (2) isolating 𝐾𝑠\\nexperts as shared ones, aiming at capturing common knowledge and mitigating redundancy\\nin routed experts. Starting from a modest scale with 2B parameters, we demonstrate that\\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5× expert\\nparameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance\\nof its dense counterpart with the same number of total parameters, which set the upper bound\\nof MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that\\nit achieves comparable performance with LLaMA2 7B, with only about 40% of computations.\\nFurther, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\\nvalidate its substantial advantages over the GShard architecture, and show its performance\\ncomparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\\n1. Introduction\\nRecent research and practices have empirically demonstrated that, with sufficient training data\\navailable, scaling language models with increased parameters and computational budgets can\\nyield remarkably stronger models (Brown et al., 2020; Hoffmann et al., 2022; OpenAI, 2023;\\nTouvron et al., 2023a). It is imperative to acknowledge, however, that the endeavor to scale\\nmodels to an extremely large scale is also associated with exceedingly high computational\\ncosts. Considering the substantial costs, the Mixture-of-Experts (MoE) architecture (Jacobs et al.,\\n1991; Jordan and Jacobs, 1994; Shazeer et al., 2017) has emerged as a popular solution. It can\\n*Contribution during internship at DeepSeek-AI.\\narXiv:2401.06066v1  [cs.CL]  11 Jan 2024\\n2\\n3\\n4\\n5\\n6\\n7\\nNumber of Activated Parameters (Billions)\\n36\\n38\\n40\\n42\\n44\\n46\\n48\\n50\\n52\\nAverage Performance\\nDeepSeekMoE 16B\\nLLaMA2 7B\\nLLaMA 7B\\nFalcon 7B\\nOpen LLaMA 7B\\nRedPajama-INCITE 7B\\nGPT-J 6B\\nRedPajama-INCITE 3B\\nOpen LLaMA 3B\\nPythia 2.8B\\nOPT 2.7B\\nGPT-neo 2.7B\\nBLOOM 3B\\nFigure 1 | Comparison between DeepSeekMoE 16B and open source models on the Open LLM\\nLeaderboard. The red dashed line is linearly fitted from data points of all models except\\nDeepSeekMoE 16B. DeepSeekMoE 16B consistently outperforms models with a similar number\\nof activated parameters by a large margin, and achieves comparable performance with LLaMA2\\n7B, which has approximately 2.5 times the activated parameters.\\nenable parameter scaling, while concurrently keeping computational costs at a modest level.\\nRecent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded\\nsuccessful attempts at scaling language models to a substantial size (Du et al., 2022; Fedus et al.,\\n2021; Lepikhin et al., 2021; Zoph, 2022), accompanied with remarkable performance. These\\nachievements underscore the considerable potential and promise of MoE language models.\\nDespite the promising potential of MoE architectures, existing MoE architectures potentially\\nsuffer from issues of knowledge hybridity and knowledge redundancy, which limit the expert\\nspecialization, i.e., each expert acquires non-overlapping and focused knowledge. Conventional\\nMoE architectures substitute the Feed-Forward Networks (FFNs) in a Transformer with MoE\\nlayers. Each MoE layer consists of multiple experts, with each structurally identical to a standard\\nFFN, and each token is assigned to one (Fedus et al., 2021) or two (Lepikhin et al., 2021) experts.\\nThis architecture manifests two potential issues: (1) Knowledge Hybridity: existing MoE\\npractices often employ a limited number of experts (e.g., 8 or 16), and thus tokens assigned to a\\nspecific expert will be likely to cover diverse knowledge. Consequently, the designated expert\\nwill intend to assemble vastly different types of knowledge in its parameters, which are hard to\\nutilize simultaneously. (2) Knowledge Redundancy: tokens assigned to different experts may\\nrequire common knowledge. As a result, multiple experts may converge in acquiring shared\\nknowledge in their respective parameters, thereby leading to redundancy in expert parameters.\\nThese issues collectively hinder the expert specialization in existing MoE practices, preventing\\nthem from reaching the theoretical upper-bound performance of MoE models.\\nIn response to the aforementioned issues, we introduce DeepSeekMoE, an innovative MoE\\narchitecture specifically designed towards ultimate expert specialization. Our architecture\\ninvolves two principal strategies: (1) Fine-Grained Expert Segmentation: while maintaining\\nthe number of parameters constant, we segment the experts into a finer grain by splitting the\\n2\\nFFN intermediate hidden dimension. Correspondingly, keeping a constant computational cost,\\nwe also activate more fine-grained experts to enable a more flexible and adaptable combina-\\ntion of activated experts. Fine-grained expert segmentation allows diverse knowledge to be\\ndecomposed more finely and be learned more precisely into different experts, where each expert\\nwill retain a higher level of specialization. In addition, the increased flexibility in combining\\nactivated experts also contributes to a more accurate and targeted knowledge acquisition. (2)\\nShared Expert Isolation: we isolate certain experts to serve as shared experts that are always\\nactivated, aiming at capturing and consolidating common knowledge across varying contexts.\\nThrough compressing common knowledge into these shared experts, redundancy among other\\nrouted experts will be mitigated. This can enhance the parameter efficiency and ensure that\\neach routed expert retains specialized by focusing on distinctive aspects. These architectural\\ninnovations in DeepSeekMoE offer opportunities to train a parameter-efficient MoE language\\nmodel where each expert is highly specialized.\\nStarting from a modest scale with 2B parameters, we validate the advantages of the DeepSeek-\\nMoE architecture. We conduct evaluations on 12 zero-shot or few-shot benchmarks spanning\\ndiverse tasks. Empirical results indicate that DeepSeekMoE 2B surpasses GShard 2B (Lepikhin\\net al., 2021) by a substantial margin, and even matches GShard 2.9B, a larger MoE model with\\n1.5× expert parameters and computation. Remarkably, we find that DeepSeekMoE 2B nearly\\napproaches the performance of its dense counterpart with an equivalent number of parameters,\\nwhich sets the strict upper bound of MoE language models. In pursuit of deeper insights, we\\nconduct elaborate ablation studies and analysis on the expert specialization for DeepSeekMoE.\\nThese studies validate the effectiveness of fine-grained expert segmentation and shared expert\\nisolation, and provide empirical evidence supporting the assertion that DeepSeekMoE can\\nachieve a high level of expert specialization.\\nLeveraging our architecture, we subsequently scale up the model parameters to 16B and\\ntrain DeepSeekMoE 16B on a large-scale corpus with 2T tokens. Evaluation results reveal that\\nwith only about 40% of computations, DeepSeekMoE 16B achieves comparable performance\\nwith DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus. We\\nalso compare DeepSeekMoE with open source models and the evaluations demonstrate that\\nDeepSeekMoE 16B consistently outperforms models with a similar number of activated parame-\\nters by a large margin, and achieves comparable performance with LLaMA2 7B (Touvron et al.,\\n2023b), which has approximately 2.5 times the activated parameters. Figure 1 demonstrates\\nthe evaluation results on the Open LLM Leaderboard1. Additionally, we conduct supervised\\nfine-tuning (SFT) for alignment, transforming the model into a chat model. Evaluation results\\nshow that DeepSeekMoE Chat 16B also achieves comparable performance with DeepSeek Chat\\n7B and LLaMA2 SFT 7B in the chat setting. Encouraged by these results, we further under-\\ntake a preliminary endeavor to scale up DeepSeekMoE to 145B. The experimental results still\\nvalidate its substantial advantages over the GShard architecture consistently. In addition, it\\nshows performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of\\ncomputations.\\nOur contributions are summarized as follows:\\n• Architectural Innovation. We introduce DeepSeekMoE, an innovative MoE architecture\\naiming at achieving ultimate expert specialization, which employs two principal strategies\\nof fine-grained expert segmentation and shared expert isolation.\\n• Empirical Validation. We conduct extensive experiments to empirically validate the\\neffectiveness of the DeepSeekMoE architecture. Experimental results validate the high\\n1https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\\n3\\nlevel of expert specialization in DeepSeekMoE 2B, and indicate that DeepSeekMoE 2B can\\nnearly approach the upper bound performance for MoE models\\n• Scalability. We scale up DeepSeekMoE to train a 16B model and show that with only\\nabout 40% of computations, DeepSeekMoE 16B achieves comparable performance with\\nDeepSeek 7B and LLaMA2 7B. We also undertake a preliminary endeavor to scale up\\nDeepSeekMoE to 145B, highlighting its consistent advantages over the GShard architecture\\nand showing a comparable performance with DeepSeek 67B.\\n• Alignment for MoE. We successfully perform supervised fine-tuning on DeepSeekMoE\\n16B to create an aligned chat model, showcasing the adaptability and versatility of\\nDeepSeekMoE 16B.\\n• Public Release. In the spirit of open research, we release the model checkpoint of\\nDeepSeekMoE 16B to the public. Notably, this model can be deployed on a single GPU\\nwith 40GB of memory without the need for quantization.\\n2. Preliminaries: Mixture-of-Experts for Transformers\\nWe first introduce a generic MoE architecture commonly used in Transformer language mod-\\nels. A standard Transformer language model is constructed by stacking 𝐿layers of standard\\nTransformer blocks, where each block can be represented as follows:\\nu𝑙\\n1:𝑇= Self-Att\\n\\x10\\nh𝑙−1\\n1:𝑇\\n\\x11\\n+ h𝑙−1\\n1:𝑇,\\n(1)\\nh𝑙\\n𝑡= FFN\\n\\x10\\nu𝑙\\n𝑡\\n\\x11\\n+ u𝑙\\n𝑡,\\n(2)\\nwhere 𝑇denotes the sequence length, Self-Att(·) denotes the self-attention module, FFN(·)\\ndenotes the Feed-Forward Network (FFN), u𝑙\\n1:𝑇∈R𝑇×𝑑are the hidden states of all tokens after\\nthe 𝑙-th attention module, and h𝑙\\n𝑡∈R𝑑is the output hidden state of the 𝑡-th token after the 𝑙-th\\nTransformer block. For brevity, we omit the layer normalization in the above formulations.\\nA typical practice to construct an MoE language model usually substitutes FFNs in a Trans-\\nformer with MoE layers at specified intervals (Du et al., 2022; Fedus et al., 2021; Lepikhin\\net al., 2021; Zoph, 2022). An MoE layer is composed of multiple experts, where each expert is\\nstructurally identical to a standard FFN. Then, each token will be assigned to one (Fedus et al.,\\n2021) or two (Lepikhin et al., 2021) experts. If the 𝑙-th FFN is substituted with an MoE layer, the\\ncomputation for its output hidden state h𝑙\\n𝑡is expressed as:\\nh𝑙\\n𝑡=\\n𝑁\\n∑︁\\n𝑖=1\\n\\x10\\n𝑔𝑖,𝑡FFN𝑖\\n\\x10\\nu𝑙\\n𝑡\\n\\x11\\x11\\n+ u𝑙\\n𝑡,\\n(3)\\n𝑔𝑖,𝑡=\\n(\\n𝑠𝑖,𝑡,\\n𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑁}, 𝐾),\\n0,\\notherwise,\\n(4)\\n𝑠𝑖,𝑡= Softmax𝑖\\n\\x10\\nu𝑙\\n𝑡\\n𝑇e𝑙\\n𝑖\\n\\x11\\n,\\n(5)\\nwhere 𝑁denotes the total number of experts, FFN𝑖(·) is the 𝑖-th expert FFN, 𝑔𝑖,𝑡denotes the\\ngate value for the 𝑖-th expert, 𝑠𝑖,𝑡denotes the token-to-expert affinity, Topk(·, 𝐾) denotes the set\\ncomprising 𝐾highest affinity scores among those calculated for the 𝑡-th token and all 𝑁experts,\\nand e𝑙\\n𝑖is the centroid of the 𝑖-th expert in the 𝑙-th layer. Note that 𝑔𝑖,𝑡is sparse, indicating that\\nonly 𝐾out of 𝑁gate values are nonzero. This sparsity property ensures computational efficiency\\nwithin an MoE layer, i.e., each token will be assigned to and computed in only 𝐾experts. Also,\\nin the above formulations, we omit the layer normalization operation for brevity.\\n4\\n…\\n1\\n2\\n𝑁𝑁\\nRouter\\nInput Hidden\\nOutput Hidden\\n…\\nRouter\\nInput Hidden\\nOutput Hidden\\n1\\n2\\n3\\n4\\n2𝑁𝑁-1\\n2𝑁𝑁\\n…\\nRouter\\nInput Hidden\\nOutput Hidden\\n1\\n2\\n3\\n4\\n2𝑁𝑁-1\\n2𝑁𝑁\\nShared Expert\\nRouted Expert\\n𝐾𝐾= 2\\n𝐾𝐾= 4\\n𝐾𝐾 = 3\\n(a) Conventional Top-2 Routing\\n(b) + Fine-grained Expert Segmentation\\n(c) + Shared Expert Isolation \\n \\n(DeepSeekMoE)\\nFigure 2 | Illustration of DeepSeekMoE. Subfigure (a) showcases an MoE layer with the con-\\nventional top-2 routing strategy. Subfigure (b) illustrates the fine-grained expert segmentation\\nstrategy. Subsequently, subfigure (c) demonstrates the integration of the shared expert isolation\\nstrategy, constituting the complete DeepSeekMoE architecture. It is noteworthy that across these\\nthree architectures, the number of expert parameters and computational costs remain constant.\\n3. DeepSeekMoE Architecture\\nOn top of the generic MoE architecture outlined in Section 2, we introduce DeepSeekMoE, which\\nis specifically designed to exploit the potential of expert specialization. As illustrated in Figure 2,\\nour architecture incorporates two principal strategies: fine-grained expert segmentation and\\nshared expert isolation. Both of these strategies are designed to elevate the level of expert\\nspecialization.\\n3.1. Fine-Grained Expert Segmentation\\nIn scenarios where the number of experts is limited, tokens assigned to a particular expert will\\nbe more likely to cover diverse types of knowledge. As a consequence, the designated expert\\nwill intend to learn vastly different types of knowledge in its parameters, and they are hard\\nto be simultaneously utilized. However, if each token can be routed to more experts, diverse\\nknowledge will gain the potential to be decomposed and learned in different experts respectively.\\nIn this context, each expert can still retain a high level of expert specialization, contributing to a\\nmore focused knowledge distribution across experts.\\nIn pursuit of the goal, while maintaining a consistent number of expert parameters and\\ncomputational cost, we segment the experts with a finer grain. The finer expert segmentation\\nenables a more flexible and adaptable combination of activated experts. To be specific, on top of\\na typical MoE architecture shown in Figure 2(a), we segment each expert FFN into 𝑚smaller\\nexperts by reducing the FFN intermediate hidden dimension to 1\\n𝑚times its original size. Since\\neach expert becomes smaller, in response, we also increase the number of activated experts to\\n𝑚times to keep the same computation cost, as illustrated in Figure 2(b). With the fine-grained\\n5\\nexpert segmentation, the output of an MoE layer can be expressed as:\\nh𝑙\\n𝑡=\\n𝑚𝑁\\n∑︁\\n𝑖=1\\n\\x10\\n𝑔𝑖,𝑡FFN𝑖\\n\\x10\\nu𝑙\\n𝑡\\n\\x11\\x11\\n+ u𝑙\\n𝑡,\\n(6)\\n𝑔𝑖,𝑡=\\n(\\n𝑠𝑖,𝑡,\\n𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑚𝑁}, 𝑚𝐾),\\n0,\\notherwise,\\n(7)\\n𝑠𝑖,𝑡= Softmax𝑖\\n\\x10\\nu𝑙\\n𝑡\\n𝑇e𝑙\\n𝑖\\n\\x11\\n,\\n(8)\\nwhere the total number of expert parameters is equal to 𝑁times the number of parameters in a\\nstandard FFN, and 𝑚𝑁denotes the total number of fine-grained experts. With the fine-grained\\nexpert segmentation strategy, the number of nonzero gates will also increases to 𝑚𝐾.\\nFrom a combinatorial perspective, the fine-grained expert segmentation strategy substan-\\ntially enhances the combinatorial flexibility of activated experts. As an illustrative example,\\nwe consider the case where 𝑁= 16. A typical top-2 routing strategy can yield \\x0016\\n2\\n\\x01 = 120 pos-\\nsible combinations. By contrast, if each expert is split into 4 smaller experts, the fine-grained\\nrouting strategy can yield \\x0064\\n8\\n\\x01 = 4, 426, 165, 368 potential combinations. The surge in combina-\\ntorial flexibility enhances the potential for achieving more accurate and targeted knowledge\\nacquisition.\\n3.2. Shared Expert Isolation\\nWith a conventional routing strategy, tokens assigned to different experts may necessitate some\\ncommon knowledge or information. As a result, multiple experts may converge in acquiring\\nshared knowledge in their respective parameters, thereby resulting in redundancy in expert\\nparameters. However, if there are shared experts dedicated to capturing and consolidating\\ncommon knowledge across varying contexts, the parameter redundancy among other routed\\nexperts will be alleviated. This alleviation of redundancy will contribute to a more parameter-\\nefficient model with more specialized experts.\\nTowards this objective, in addition to the fine-grained expert segmentation strategy, we\\nfurther isolate 𝐾𝑠experts to serve as shared experts. Regardless of the router module, each\\ntoken will be deterministically assigned to these shared experts. In order to maintain a constant\\ncomputational cost, the number of activated experts among the other routed experts will be\\ndecreased by 𝐾𝑠, as depicted in Figure 2(c). With the shared expert isolation strategy integrated,\\nan MoE layer in the complete DeepSeekMoE architecture is formulated as follows:\\nh𝑙\\n𝑡=\\n𝐾𝑠\\n∑︁\\n𝑖=1\\nFFN𝑖\\n\\x10\\nu𝑙\\n𝑡\\n\\x11\\n+\\n𝑚𝑁\\n∑︁\\n𝑖=𝐾𝑠+1\\n\\x10\\n𝑔𝑖,𝑡FFN𝑖\\n\\x10\\nu𝑙\\n𝑡\\n\\x11\\x11\\n+ u𝑙\\n𝑡,\\n(9)\\n𝑔𝑖,𝑡=\\n(\\n𝑠𝑖,𝑡,\\n𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|𝐾𝑠+ 1 ⩽𝑗⩽𝑚𝑁}, 𝑚𝐾−𝐾𝑠),\\n0,\\notherwise,\\n(10)\\n𝑠𝑖,𝑡= Softmax𝑖\\n\\x10\\nu𝑙\\n𝑡\\n𝑇e𝑙\\n𝑖\\n\\x11\\n.\\n(11)\\nFinally, in DeepSeekMoE, the number of shared expert is 𝐾𝑠, the total number of routed experts\\nis 𝑚𝑁−𝐾𝑠, and the number of nonzero gates is 𝑚𝐾−𝐾𝑠.\\nIt is worth noting that the prototype of shared expert isolation can be credited to Rajbhandari\\net al. (2022). The key distinction lies in the fact that they derive this strategy from an engineering\\nperspective, while we approach it from an algorithmic standpoint.\\n6\\n3.3. Load Balance Consideration\\nAutomatically learned routing strategies may encounter the issue of load imbalance, which\\nmanifests two notable defects. Firstly, there is a risk of routing collapse (Shazeer et al., 2017), i.e.,\\nthe model always selects only a few experts, preventing other experts from sufficient training.\\nSecondly, if experts are distributed across multiple devices, load imbalance can exacerbate\\ncomputation bottlenecks.\\nExpert-Level Balance Loss.\\nIn order to mitigate the risk of routing collapse, we also employ\\nan expert-level balance loss. The computation of the balance loss is as follows:\\nLExpBal = 𝛼1\\n𝑁′\\n∑︁\\n𝑖=1\\n𝑓𝑖𝑃𝑖,\\n(12)\\n𝑓𝑖= 𝑁′\\n𝐾′𝑇\\n𝑇\\n∑︁\\n𝑡=1\\n1(Token 𝑡selects Expert 𝑖),\\n(13)\\n𝑃𝑖= 1\\n𝑇\\n𝑇\\n∑︁\\n𝑡=1\\n𝑠𝑖,𝑡,\\n(14)\\nwhere 𝛼1 is a hyper-parameter called expert-level balance factor, 𝑁′ is equal to (𝑚𝑁−𝐾𝑠) and 𝐾′\\nis equal to (𝑚𝐾−𝐾𝑠) for brevity. 1(·) denotes the indicator function.\\nDevice-Level Balance Loss.\\nIn addition to the expert-level balance loss, we introduce a device-\\nlevel balance loss. When aiming to alleviate computation bottlenecks, it becomes unnecessary\\nto enforce strict balance constraints at the expert level, because excessive constraints on load\\nbalance will compromise model performance. Instead, our primary objective is to ensure\\nbalanced computation across the devices. If we partition all routed experts into 𝐷groups\\n{E1, E2, ..., E𝐷}, and deploy each group on a single device, the device-level balance loss is\\ncomputed as follows:\\nLDevBal = 𝛼2\\n𝐷\\n∑︁\\n𝑖=1\\n𝑓′\\n𝑖𝑃′\\n𝑖,\\n(15)\\n𝑓′\\n𝑖=\\n1\\n|E𝑖|\\n∑︁\\n𝑗∈E𝑖\\n𝑓𝑗,\\n(16)\\n𝑃′\\n𝑖=\\n∑︁\\n𝑗∈E𝑖\\n𝑃𝑗,\\n(17)\\nwhere 𝛼2 is a hyper-parameter called device-level balance factor. In practice, we set a small\\nexpert-level balance factor to mitigate the risk of routing collapse, and meanwhile set a larger\\ndevice-level balance factor to promote balanced computation across the devices.\\n4. Validation Experiments\\n4.1. Experimental Setup\\n4.1.1. Training Data and Tokenization\\nOur training data is sampled from a large-scale multilingual corpus created by DeepSeek-AI. The\\ncorpus primarily focuses on English and Chinese but also encompasses other languages. It is de-\\n7\\nrived from diverse sources, including web text, mathematical material, coding scripts, published\\nliterature, and various other textual materials. For the purpose of validation experiments, we\\nsample a subset containing 100B tokens from the corpus to train our models. For tokenization,\\nwe utilize the HuggingFace Tokenizer2 tools to train byte pair encoding (BPE) (Sennrich et al.,\\n2016) tokenizers on a smaller subset of the training corpus. In the validation experiments, we\\nprepare a tokenizer with a vocabulary size of 8K, and the vocabulary size will be scaled up when\\ntraining larger models.\\n4.1.2. Infrastructures\\nWe conduct experiments based on HAI-LLM (High-Flyer, 2023), an efficient and light-weight\\ntraining framework which integrates multiple parallelism strategies, including tensor paral-\\nlelism (Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019), ZeRO data paral-\\nlelism (Rajbhandari et al., 2020), PipeDream pipeline parallelism (Harlap et al., 2018), and more\\nspecifically, expert parallelism (Lepikhin et al., 2021) by combining data and tensor parallelism.\\nIn order to optimize performance, we develop GPU kernels with CUDA and Triton (Tillet et al.,\\n2019) for gating algorithms and fusing computations across linear layers in different experts.\\nAll experiments are carried out on clusters equipped with NVIDIA A100 or H800 GPUs.\\nEach node in the A100 cluster contains 8 GPUs connected pairwise via the NVLink bridge.\\nThe H800 cluster also features 8 GPUs per node, interconnected using NVLink and NVSwitch\\nwithin nodes. For both A100 and H800 clusters, InfiniBand interconnects are utilized to facilitate\\ncommunication across nodes.\\n4.1.3. Hyper-Parameters\\nModel Settings.\\nIn the validation experiments, we set the number of Transformer layers to 9\\nand the hidden dimension to 1280. We employ the multi-head attention mechanism with a total\\nof 10 attention heads, where each head has a dimension of 128. For initialization, all learnable\\nparameters are randomly initialized with a standard deviation of 0.006. We substitute all FFNs\\nwith MoE layers, and ensure that the total number of expert parameters equals 16 times that of a\\nstandard FFN. Additionally, we keep the activated expert parameters, including shared expert\\nparameters and activated routed expert parameters, as 2 times that of a standard FFN. Under\\nthis configuration, each MoE model has approximately 2B total parameters, with the number of\\nactivated parameters around 0.3B.\\nTraining Settings.\\nWe employ the AdamW optimizer (Loshchilov and Hutter, 2019) with\\nhyper-parameters set to 𝛽1 = 0.9, 𝛽2 = 0.95, and weight_decay = 0.1. The learning rate is\\nscheduled using a warmup-and-step-decay strategy. Initially, the learning rate linearly increases\\nfrom 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is\\nmultiplied by 0.316 at 80% of the training steps, and again by 0.316 at 90% of the training steps.\\nThe maximum learning rate for validation experiments is set to 1.08 × 10−3, and the gradient\\nclipping norm is set to 1.0. The batch size is set to 2K, and with a maximum sequence length of\\n2K, each training batch contains 4M tokens. Correspondingly, the total number of training steps\\nis set to 25,000 to achieve 100B training tokens. Due to the abundance of training data, we do not\\nuse dropout during training. Given the relatively small model size, all parameters, including\\nexpert parameters, are deployed on a single GPU device to avoid unbalanced computation.\\nCorrespondingly, we do not drop any tokens during training and do not employ the device-level\\n2https://github.com/huggingface/tokenizers\\n8\\nbalance loss. In order to prevent routing collapse, we set an expert-level balance factor of 0.01.\\nFor readability, we also present an overview table of hyper-parameters for DeepSeekMoE\\nacross different sizes in Appendix A.\\n4.1.4. Evaluation Benchmarks\\nWe conduct evaluations on a wide range of benchmarks covering various types of tasks. We list\\nthe benchmarks as follows.\\nLanguage Modeling.\\nFor language modeling, we evaluate the models on the test set of\\nPile (Gao et al., 2020), and the evaluation metric is the cross-entropy loss.\\nLanguage Understanding and Reasoning.\\nFor language understanding and reasoning, we\\nconsider HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-challenge and ARC-\\neasy (Clark et al., 2018). The evaluation metric for these tasks is accuracy.\\nReading Comprehension.\\nFor reading comprehension, we use RACE-high and RACE-middle\\nLai et al. (2017), and the evaluation metric is accuracy.\\nCode Generation.\\nFor code generation, we evaluate the models on HumanEval (Chen et al.,\\n2021) and MBPP (Austin et al., 2021). The evaluation metric is Pass@1, which represents the\\npass rate for only one generation attempt.\\nClosed-Book Question Answering.\\nFor closed-book question answering, we consider Trivi-\\naQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). The evaluation metric\\nis the Exactly Matching (EM) rate.\\n4.2. Evaluations\\nBaselines.\\nIncluding DeepSeekMoE, we compare five models for validation experiments.\\nDense denotes a standard dense Transformer language model with 0.2B total parameters. Hash\\nLayer (Roller et al., 2021) is an MoE architecture based on top-1 hash routing, with 2.0B total\\nparameters and 0.2B activated parameters, aligned with the dense baseline. Switch Trans-\\nformer (Fedus et al., 2021) is another well-known MoE architecture based on top-1 learnable\\nrouting, with total parameters and activated parameters the same as Hash Layer. GShard (Lep-\\nikhin et al., 2021) employs a top-2 learnable routing strategy, with 2.0B total parameters and 0.3B\\nactivated parameters since one more expert is activated compared to top-1 routing methods.\\nDeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the\\nsize of a standard FFN. Including DeepSeekMoE, all compared models share the same training\\ncorpus and training hyper-parameters. All compared MoE models have the same number of\\ntotal parameters, and GShard has the same number of activated parameters as DeepSeekMoE.\\nResults.\\nWe present the evaluation results in Table 1. For all demonstrated models, we re-\\nport the final evaluation results after training on 100B tokens. From the table, we make the\\nfollowing observations: (1) With sparse architectures and more total parameters, Hash Layer\\n9\\nMetric\\n# Shot\\nDense\\nHash Layer\\nSwitch\\nGShard\\nDeepSeekMoE\\n# Total Params\\nN/A\\n0.2B\\n2.0B\\n2.0B\\n2.0B\\n2.0B\\n# Activated Params\\nN/A\\n0.2B\\n0.2B\\n0.2B\\n0.3B\\n0.3B\\nFLOPs per 2K Tokens\\nN/A\\n2.9T\\n2.9T\\n2.9T\\n4.3T\\n4.3T\\n# Training Tokens\\nN/A\\n100B\\n100B\\n100B\\n100B\\n100B\\nPile (Loss)\\nN/A\\n2.060\\n1.932\\n1.881\\n1.867\\n1.808\\nHellaSwag (Acc.)\\n0-shot\\n38.8\\n46.2\\n49.1\\n50.5\\n54.8\\nPIQA (Acc.)\\n0-shot\\n66.8\\n68.4\\n70.5\\n70.6\\n72.3\\nARC-easy (Acc.)\\n0-shot\\n41.0\\n45.3\\n45.9\\n43.9\\n49.4\\nARC-challenge (Acc.)\\n0-shot\\n26.0\\n28.2\\n30.2\\n31.6\\n34.3\\nRACE-middle (Acc.)\\n5-shot\\n38.8\\n38.8\\n43.6\\n42.1\\n44.0\\nRACE-high (Acc.)\\n5-shot\\n29.0\\n30.0\\n30.9\\n30.4\\n31.7\\nHumanEval (Pass@1)\\n0-shot\\n0.0\\n1.2\\n2.4\\n3.7\\n4.9\\nMBPP (Pass@1)\\n3-shot\\n0.2\\n0.6\\n0.4\\n0.2\\n2.2\\nTriviaQA (EM)\\n5-shot\\n4.9\\n6.5\\n8.9\\n10.2\\n16.6\\nNaturalQuestions (EM)\\n5-shot\\n1.4\\n1.4\\n2.5\\n3.2\\n5.7\\nTable 1 | Evaluation results for validation experiments. Bold font indicates the best. Compared\\nwith other MoE architectures, DeepSeekMoE exhibits a substantial performance advantage.\\nand Switch Transformer achieve significantly stronger performance than the dense baseline\\nwith the same number of activated parameters. (2) Compared with Hash Layer and Switch\\nTransformer, GShard has more activated parameters and achieves slightly better performance\\nthan Switch Transformer. (3) With the same number of total parameters and activated pa-\\nrameters, DeepSeekMoE demonstrates overwhelming advantages over GShard. These results\\nshowcase the superiority of our DeepSeekMoE architecture within the existing landscape of\\nMoE architectures.\\n4.3. DeepSeekMoE Aligns Closely with the upper bound of MoE Models\\nWe have demonstrated that DeepSeekMoE outperforms the dense baseline and other MoE archi-\\ntectures. In order to provide a more precise understanding of the performance of DeepSeekMoE,\\nwe compare it with larger baselines with more total parameters or activated parameters. The\\ncomparisons enable us to estimate the required model size of GShard or dense baselines to\\nachieve equivalent performance to DeepSeekMoE.\\nComparison with GShard×1.5.\\nTable 2 shows the comparison between DeepSeekMoE and\\na larger GShard model with 1.5 times the expert size, which results in 1.5 times both expert\\nparameters and expert computation. Overall, we observe that DeepSeekMoE achieves compa-\\nrable performance with GShard×1.5, underscoring the significant advantage inherent in the\\nDeepSeekMoE architecture. In addition to the comparison with GShard×1.5, we also show the\\ncomparison with GShard×1.2 in Appendix B.\\nFurthermore, we increase the number of total parameters of DeepSeekMoE to 13.3B and\\ncompare it with GShard×1.2 and GShard×1.5 with 15.9B and 19.8B total parameters, respectively.\\nWe find that at a larger scale, DeepSeekMoE can even outperform GShard×1.5 distinctly. These\\n10\\nMetric\\n# Shot\\nGShard×1.5\\nDense×16\\nDeepSeekMoE\\nRelative Expert Size\\nN/A\\n1.5\\n1\\n0.25\\n# Experts\\nN/A\\n0 + 16\\n16 + 0\\n1 + 63\\n# Activated Experts\\nN/A\\n0 + 2\\n16 + 0\\n1 + 7\\n# Total Expert Params\\nN/A\\n2.83B\\n1.89B\\n1.89B\\n# Activated Expert Params\\nN/A\\n0.35B\\n1.89B\\n0.24B\\nFLOPs per 2K Tokens\\nN/A\\n5.8T\\n24.6T\\n4.3T\\n# Training Tokens\\nN/A\\n100B\\n100B\\n100B\\nPile (Loss)\\nN/A\\n1.808\\n1.806\\n1.808\\nHellaSwag (Acc.)\\n0-shot\\n54.4\\n55.1\\n54.8\\nPIQA (Acc.)\\n0-shot\\n71.1\\n71.9\\n72.3\\nARC-easy (Acc.)\\n0-shot\\n47.3\\n51.9\\n49.4\\nARC-challenge (Acc.)\\n0-shot\\n34.1\\n33.8\\n34.3\\nRACE-middle (Acc.)\\n5-shot\\n46.4\\n46.3\\n44.0\\nRACE-high (Acc.)\\n5-shot\\n32.4\\n33.0\\n31.7\\nHumanEval (Pass@1)\\n0-shot\\n3.0\\n4.3\\n4.9\\nMBPP (Pass@1)\\n3-shot\\n2.6\\n2.2\\n2.2\\nTriviaQA (EM)\\n5-shot\\n15.7\\n16.5\\n16.6\\nNaturalQuestions (EM)\\n5-shot\\n4.7\\n6.3\\n5.7\\nTable 2 | Comparisons among DeepSeekMoE, larger GShard models, and larger dense models.\\nIn the line of “# Experts”, 𝑎+ 𝑏denotes 𝑎shared experts and 𝑏routed experts. In the line\\nof “# Activated Experts”, 𝑎+ 𝑏denotes 𝑎activated shared experts and 𝑏activated routed\\nexperts. DeepSeekMoE achieves comparable performance with a GShard model containing 1.5\\ntimes expert parameters and computation. In addition, DeepSeekMoE nearly approaches the\\nperformance of a dense model with 16 times FFN parameters, which sets the upper bound for\\nMoE models in terms of the model capacity.\\nresults are also provided in Appendix B.\\nComparison with Dense×16.\\nTable 2 also shows the comparison between DeepSeekMoE and\\nlarger dense models. For a fair comparison, we do not use the widely used ratio (1:2) between\\nthe attention and FFN parameters. Instead, we configure 16 shared experts where each expert\\nhas the same number of parameters as a standard FFN. This architecture mimics a dense model\\nwith 16 times standard FFN parameters. From the table, we find that DeepSeekMoE nearly\\napproaches the performance of Dense×16, which sets the strict upper bound of MoE models\\nin terms of the model capacity. These results suggest that, at least at the scale of about 2B\\nparameters and 100B training tokens, the performance of DeepSeekMoE aligns closely with\\nthe theoretical upper bound of MoE models. Also, we provide additional comparisons with\\nDense×4 in Appendix B.\\n4.4. Ablation Studies\\nIn order to substantiate the effectiveness of the fine-grained expert segmentation and shared\\nexpert isolation strategies, we conduct ablation studies for DeepSeekMoE and present the results\\nin Figure 3. For a fair comparison, we ensure all models included in the comparison have the\\n11\\nHellaSwag\\nPIQA\\nARC-easy\\nARC-challenge\\nTriviaQA\\nNaturalQuestions\\nMetrics\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\nNormalized Performance\\n0 shared expert + 2 out of 16 routed experts (GShard)\\n1 shared expert + 1 out of 15 routed experts (+ shared expert isolation)\\n1 shared expert + 3 out of 31 routed experts (+ fine-grained expert segmentation)\\n1 shared expert + 7 out of 63 routed experts (+ finer expert segmentation)\\nFigure 3 | Ablation studies for DeepSeekMoE. The performance is normalized by the best perfor-\\nmance for clarity in presentation. All compared models have the same number of parameters\\nand activated parameters. We can find that fine-grained expert segmentation and shared expert\\nisolation both contribute to stronger overall performance.\\nsame number of total parameters and activated parameters.\\nShared Expert Isolation.\\nIn order to evaluate the influence of the shared expert isolation\\nstrategy, we isolate one expert as the shared one based on GShard. From Figure 3, we observe that\\ncompared with GShard, the intentional isolation of a shared expert yields improved performance\\nacross a majority of benchmarks. These results support the proposition that the shared expert\\nisolation strategy contributes to a stronger model performance.\\nFine-Grained Expert Segmentation.\\nIn order to assess the effectiveness of the fine-grained\\nexpert segmentation strategy, we conduct a more detailed comparison by further segmenting\\nthe experts into a finer grain. To be specific, we segment each expert into 2 or 4 smaller experts,\\nresulting in a total of 32 (1 shared + 31 routed) or 64 (1 shared + 63 routed) experts. Figure 3\\nreveals a consistent trend that the continuous refinement of expert segmentation granularity\\ncorresponds to a continuous enhancement in overall model performance. These findings provide\\nempirical substantiation for the effectiveness of the fine-grained expert segmentation strategy.\\nRatios Between Shared and Routed Experts.\\nIn addition, we investigate the best ratio of\\nshared experts and routed experts. Based on the finest granularity with 64 total experts and\\nkeeping the number of total experts and activated experts constant, we attempt to isolate 1, 2,\\nand 4 experts as shared ones. We find that different ratios of the shared experts and routed\\nexperts do not significantly impact the performance, and 1, 2, and 4 shared experts achieve a Pile\\nloss of 1.808, 1.806, and 1.811, respectively. Considering that the ratio of 1:3 yields a marginally\\nbetter Pile loss, when scaling up DeepSeekMoE, we keep the ratio between shared experts and\\nactivated routed experts as 1:3.\\n12\\n4.5. Analysis on Expert Specialization\\nIn this section, we conduct an empirical analysis on the expert specialization of DeepSeekMoE\\n2B. DeepSeekMoE 2B in this section refers to the model reported in Table 1, i.e., comprising 2.0B\\ntotal parameters, with 1 shared expert and 7 out of 63 routed experts being activated.\\n0\\n1/16\\n2/16\\n3/16\\n4/16\\nRatio of Disabled Top Routed Experts\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nPile Loss\\nDeepSeekMoE\\nGShard × 1.5\\nFigure 4 | Pile loss with regard to different ratios of disabled top routed experts. Notably,\\nDeepSeekMoE exhibits greater sensitivity to the ratio of disabled top routed experts, indicating\\nlower redundancy among routed experts in DeepSeekMoE.\\nDeepSeekMoE Exhibits Lower Redundancy Among Routed Experts.\\nIn order to assess the\\nredundancy among routed experts, we disable varying ratios of top routed experts and evaluate\\nthe Pile loss. To be specific, for each token, we mask a certain ratio of experts with the highest\\nrouting probability, and then select top-K experts from the remaining routed experts. For\\nfairness, we compare DeepSeekMoE with GShard×1.5 since they have the same Pile loss when\\nno experts are disabled. As shown in Figure 4, compared with GShard×1.5, DeepSeekMoE is\\nmore sensitive to the disabling of top routed experts. This sensitivity suggests a lower level\\nof parameter redundancy in DeepSeekMoE, since each routed expert is more irreplaceable. In\\ncontrast, GShard×1.5 exhibits greater redundancy among its expert parameters, so it can buffer\\nthe performance drop when top routed experts are disabled.\\nShared Experts Are Irreplaceable by Routed Experts.\\nIn order to investigate the role of\\nthe shared expert in DeepSeekMoE, we disable it and activate one more routed expert. The\\nevaluation on Pile shows a significant increase in the Pile loss, rising from 1.808 to 2.414, even\\nthough we maintain the same computational cost. This result highlights the crucial function\\nof the shared expert and indicates that the shared expert captures fundamental and essential\\nknowledge not shared with routed experts, making it irreplaceable by routed ones.\\nDeepSeekMoE Acquires Knowledge More Accurately.\\nIn order to validate our claim that\\nhigher flexibility in combining activated experts contributes to a more accurate and targeted\\nknowledge acquisition, we investigate whether DeepSeekMoE can acquire requisite knowledge\\nwith fewer activated experts. To be specific, we vary the number of activated routed experts\\nfrom 3 to 7 and evaluate the resulting Pile loss. As demonstrated in Figure 5, even with only\\n13\\n3\\n4\\n5\\n6\\n7\\nActivated Routed Experts\\n1.82\\n1.84\\n1.86\\n1.88\\n1.90\\n1.92\\n1.94\\n1.96\\nPile Loss\\nsame activated\\nexpert parameters\\nDeepSeekMoE\\nGShard (full top-2 activated)\\nFigure 5 | Pile loss with regard to different numbers of activated routed experts in DeepSeekMoE.\\nWith only 4 routed experts activated, DeepSeekMoE achieves a Pile loss comparable with\\nGShard.\\nHellaSwag\\nPIQA\\nARC-easy\\nARC-challenge\\nTriviaQA\\nNaturalQuestions\\nMetrics\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nPerformance\\n0 shared expert + 2 out of 16 routed experts (GShard)\\n1 shared expert + 3 out of 63 routed experts (DeepSeekMoE with half the activated experts)\\nFigure 6 | Comparison between GShard and DeepSeekMoE with half the activated experts\\n(trained from scratch). With the same total expert parameters and only half of the activated\\nexpert parameters, DeepSeekMoE still outperforms GShard.\\n4 routed experts activated, DeepSeekMoE achieves a Pile loss comparable with GShard. This\\nobservation supports the proposition that DeepSeekMoE can acquire requisite knowledge more\\naccurately and efficiently.\\nEncouraged by these findings, in order to validate the expert specialization and accurate\\nknowledge acquisition of DeepSeekMoE more rigorously, we train a new model from scratch.\\nThis model comprises 1 shared expert and 63 routed experts, where only 3 routed experts are\\nactivated. The evaluation results shown in Figure 6 demonstrate that, even with the same\\ntotal expert parameters and only half of the activated expert parameters, DeepSeekMoE still\\noutperforms GShard. This highlights the ability of DeepSeekMoE to leverage expert parameters\\n14\\nmore efficiently, i.e., the proportion of effective parameters in the activated experts is much\\nhigher than that of GShard.\\n5. Scaling up to DeepSeekMoE 16B\\nWith the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B total\\nparameters and train it on 2T tokens. Our results demonstrate that compared with LLaMA2 7B,\\nDeepSeekMoE 16B achieves superior performance with only about 40% of computations.\\n5.1. Experimental Setup\\n5.1.1. Training Data and Tokenization\\nWe sample the training data from the same corpus as described in Section 4.1.1. Different from\\nthe validation experiments, we sample a larger amount of data with 2T tokens, aligning with\\nthe number of training tokens of LLaMA2 7B. We also use the HuggingFace Tokenizer tools to\\ntrain a BPE tokenizer, but the vocabulary size is set to 100K for DeepSeekMoE 16B.\\n5.1.2. Hyper-Parameters\\nModel Settings.\\nFor DeepSeekMoE 16B, we set the number of Transformer layers to 28 and\\nthe hidden dimension to 2048. We employ the multi-head attention mechanism with a total of\\n16 attention heads, where each head has a dimension of 128. As for initialization, all learnable\\nparameters are randomly initialized with a standard deviation of 0.006. We substitute all FFNs\\nexcept for the first layer with MoE layers, since we observe that the load balance status converges\\nespecially slower for the first layer. Each MoE layer consists of 2 shared experts and 64 routed\\nexperts, where each expert is 0.25 times the size of a standard FFN. Each token will be routed\\nto these 2 shared experts and 6 out of 64 routed experts. An even finer expert segmentation\\ngranularity is not employed due to the potential reduction in computational efficiency associated\\nwith excessively small expert sizes. At a larger scale over 16B, a finer granularity can still\\nbe employed. Under our configuration, DeepSeekMoE 16B has approximately 16.4B total\\nparameters, with the number of activated parameters around 2.8B.\\nTraining Settings.\\nWe employ the AdamW optimizer (Loshchilov and Hutter, 2019) with\\nhyper-parameters set to 𝛽1 = 0.9, 𝛽2 = 0.95, and weight_decay = 0.1. The learning rate is also\\nscheduled using a warmup-and-step-decay strategy. Initially, the learning rate linearly increases\\nfrom 0 to the maximum value during the first 2K steps. Subsequently, the learning rate is\\nmultiplied by 0.316 at 80% of the training steps, and again by 0.316 at 90% of the training steps.\\nThe maximum learning rate for DeepSeekMoE 16B is set to 4.2 × 10−4, and the gradient clipping\\nnorm is set to 1.0. The batch size is set to 4.5K, and with a maximum sequence length of 4K, each\\ntraining batch contains 18M tokens. Correspondingly, the total number of training steps is set\\nto 106,449 to achieve 2T training tokens. Due to the abundance of training data, we do not use\\ndropout during training. We leverage pipeline parallelism to deploy different layers of a model\\non different devices, and for each layer, all the experts will be deployed on the same device.\\nTherefore, we also do not drop any tokens during training and do not employ the device-level\\nbalance loss. In order to prevent routing collapse, we set a quite small expert-level balance factor\\nof 0.001 because we find that under our parallelization strategy, a higher expert-level balance\\nfactor cannot increase the computation efficiency, but instead, it will compromise the model\\nperformance.\\n15\\n5.1.3. Evaluation Benchmarks\\nIn addition to the benchmarks used in the validation experiments, we incorporate additional\\nbenchmarks for a more comprehensive evaluation. We introduce the distinctions from the\\nbenchmarks used in validation experiments as follows.\\nLanguage Modeling.\\nFor language modeling, we also evaluate the models on the test set of\\nPile (Gao et al., 2020). Since the tokenizer used in DeepSeekMoE 16B is different from that used\\nin LLaMA2 7B. For a fair comparison, we use bits per byte (BPB) as the evaluation metric.\\nReading Comprehension.\\nFor reading comprehension, we additionally consider DROP (Dua\\net al., 2019). The evaluation metric is the Exactly Matching (EM) rate.\\nMath Reasoning.\\nFor math reasoning, we additionally incorporate GSM8K (Cobbe et al., 2021)\\nand MATH (Hendrycks et al., 2021), using EM as the evaluation metric.\\nMulti-Subject Multiple-Choice.\\nFor multi-subject multiple-choice, we additionally evaluate\\nthe models on MMLU (Hendrycks et al., 2020). The evaluation metric is accuracy.\\nDisambiguation.\\nFor disambiguation, we additionally consider WinoGrande (Sakaguchi et al.,\\n2019) and the evaluation metric is accuracy.\\nChinese Benchmarks.\\nSince DeepSeekMoE 16B is pretrained on a bilingual corpus, we also\\nevaluate it on four Chinese benchmarks. CLUEWSC (Xu et al., 2020) is a Chinese disambiguation\\nbenchmark. CEval (Huang et al., 2023) and CMMLU (Li et al., 2023) are two Chinese multi-\\nsubject multiple-choice benchmarks with a similar form to MMLU. CHID (Zheng et al., 2019)\\nis a Chinese idiom completion benchmark, aiming to evaluate the understanding of Chinese\\nculture. The evaluation metrics for the aforementioned Chinese benchmarks are accuracy or\\nEM.\\nOpen LLM Leaderboard.\\nWe evaluate all of the aforementioned benchmarks based on our\\ninternal evaluation framework. In order to compare DeepSeekMoE 16B with open source\\nmodels fairly and conveniently, we additionally evaluate DeepSeekMoE 16B on the Open\\nLLM Leaderboard. The Open LLM Leaderboard is a public leaderboard supported by Hug-\\ngingFace, it consists of six tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\\nMMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2022), Winogrande (Sakaguchi et al.,\\n2019), and GSM8K (Cobbe et al., 2021).\\n5.2. Evaluations\\n5.2.1. Internal Comparison with DeepSeek 7B\\nWe first conduct an internal comparison between DeepSeekMoE 16B and DeepSeek 7B (DeepSeek-\\nAI, 2024), a dense language model with 6.9B parameters. Ensuring fairness, both models are\\ntrained on the same corpus with 2T tokens. This enables an accurate assessment of the effective-\\nness of our MoE architecture, independent of the influence of the training data.\\n16\\nMetric\\n# Shot DeepSeek 7B (Dense) DeepSeekMoE 16B\\n# Total Params\\nN/A\\n6.9B\\n16.4B\\n# Activated Params\\nN/A\\n6.9B\\n2.8B\\nFLOPs per 4K Tokens\\nN/A\\n183.5T\\n74.4T\\n# Training Tokens\\nN/A\\n2T\\n2T\\nPile (BPB)\\nN/A\\n0.75\\n0.74\\nHellaSwag (Acc.)\\n0-shot\\n75.4\\n77.1\\nPIQA (Acc.)\\n0-shot\\n79.2\\n80.2\\nARC-easy (Acc.)\\n0-shot\\n67.9\\n68.1\\nARC-challenge (Acc.)\\n0-shot\\n48.1\\n49.8\\nRACE-middle (Acc.)\\n5-shot\\n63.2\\n61.9\\nRACE-high (Acc.)\\n5-shot\\n46.5\\n46.4\\nDROP (EM)\\n1-shot\\n34.9\\n32.9\\nGSM8K (EM)\\n8-shot\\n17.4\\n18.8\\nMATH (EM)\\n4-shot\\n3.3\\n4.3\\nHumanEval (Pass@1)\\n0-shot\\n26.2\\n26.8\\nMBPP (Pass@1)\\n3-shot\\n39.0\\n39.2\\nTriviaQA (EM)\\n5-shot\\n59.7\\n64.8\\nNaturalQuestions (EM)\\n5-shot\\n22.2\\n25.5\\nMMLU (Acc.)\\n5-shot\\n48.2\\n45.0\\nWinoGrande (Acc.)\\n0-shot\\n70.5\\n70.2\\nCLUEWSC (EM)\\n5-shot\\n73.1\\n72.1\\nCEval (Acc.)\\n5-shot\\n45.0\\n40.6\\nCMMLU (Acc.)\\n5-shot\\n47.2\\n42.5\\nCHID (Acc.)\\n0-shot\\n89.3\\n89.4\\nTable 3 | Comparison between DeepSeek 7B and DeepSeekMoE 16B. Bold font indicates the best\\nor near the best. With only 40.5% of computations, DeepSeekMoE 16B achieves comparable\\nperformance with DeepSeek 7B.\\nThe evaluation results are presented in Table 3, yielding the following observations: (1) On\\nthe whole, with about only 40% of the computations, DeepSeekMoE 16B achieves comparable\\nperformance with DeepSeek 7B. (2) DeepSeekMoE 16B exhibits notable strengths in language\\nmodeling and knowledge-intensive tasks such as Pile, HellaSwag, TriviaQA, and NaturalQues-\\ntions. Given that in an MoE model, FFN parameters are much heavier than attention parameters,\\nthese outcomes align with the proposition that FFNs in Transformers exhibit the capability for\\nknowledge memorization (Dai et al., 2022a). (3) Compared with the excellent performance\\non other tasks, DeepSeekMoE exhibits limitations in addressing multiple-choice tasks. This\\ninadequacy stems from the limited attention parameters in DeepSeekMoE 16B (DeepSeekMoE\\n16B has only about 0.5B attention parameters, while DeepSeek 7B has 2.5B attention parameters).\\nOur earlier investigation on DeepSeek 7B reveals a positive correlation between the attention\\ncapacity and performance on multiple-choice tasks. For example, DeepSeek 7B MQA, which is\\nequipped with the multi-query attention mechanism (Shazeer, 2019), also struggled in MMLU-\\nlike tasks. In addition, for a more comprehensive understanding of the training process of\\n17\\nDeepSeekMoE 16B, we also provide the benchmark curves of DeepSeekMoE 16B and DeepSeek\\n7B (Dense) during training in Appendix C for reference.\\nCritically, due to the modest number of parameters in DeepSeekMoE 16B, it enables single-\\ndevice deployment on a GPU with 40GB of memory. With appropriate operator optimizations,\\nit can achieve nearly 2.5 times the inference speed of a 7B dense model.\\nMetric\\n# Shot LLaMA2 7B DeepSeekMoE 16B\\n# Total Params\\nN/A\\n6.7B\\n16.4B\\n# Activated Params\\nN/A\\n6.7B\\n2.8B\\nFLOPs per 4K Tokens\\nN/A\\n187.9T\\n74.4T\\n# Training Tokens\\nN/A\\n2T\\n2T\\nPile (BPB)\\nN/A\\n0.76\\n0.74\\nHellaSwag (Acc.)\\n0-shot\\n75.6\\n77.1\\nPIQA (Acc.)\\n0-shot\\n78.0\\n80.2\\nARC-easy (Acc.)\\n0-shot\\n69.1\\n68.1\\nARC-challenge (Acc.)\\n0-shot\\n49.0\\n49.8\\nRACE-middle (Acc.)\\n5-shot\\n60.7\\n61.9\\nRACE-high (Acc.)\\n5-shot\\n45.8\\n46.4\\nDROP (EM)\\n1-shot\\n34.0\\n32.9\\nGSM8K (EM)\\n8-shot\\n15.5\\n18.8\\nMATH (EM)\\n4-shot\\n2.6\\n4.3\\nHumanEval (Pass@1)\\n0-shot\\n14.6\\n26.8\\nMBPP (Pass@1)\\n3-shot\\n21.8\\n39.2\\nTriviaQA (EM)\\n5-shot\\n63.8\\n64.8\\nNaturalQuestions (EM)\\n5-shot\\n25.5\\n25.5\\nMMLU (Acc.)\\n5-shot\\n45.8\\n45.0\\nWinoGrande (Acc.)\\n0-shot\\n69.6\\n70.2\\nCLUEWSC (EM)\\n5-shot\\n64.0\\n72.1\\nCEval (Acc.)\\n5-shot\\n33.9\\n40.6\\nCMMLU (Acc.)\\n5-shot\\n32.6\\n42.5\\nCHID (Acc.)\\n0-shot\\n37.9\\n89.4\\nTable 4 | Comparison between LLaMA2 7B and DeepSeekMoE 16B. With only 39.6% of compu-\\ntations, DeepSeekMoE 16B outperforms LLaMA2 7B on the majority of benchmarks.\\n5.2.2. Comparison with Open Source Models\\nInternal Comparison with LLaMA2 7B.\\nIn the realm of open source models, we mainly com-\\npare DeepSeekMoE 16B with LLaMA2 7B (Touvron et al., 2023b), a well-known and strong open\\nsource language model with 6.7B parameters. Both DeepSeekMoE 16B and LLaMA2 7B are pre-\\ntrained on 2T tokens. Compared with LLaMA2 7B, DeepSeekMoE has 245% of total parameters\\nbut only needs 39.6% of computations. The results on our internal benchmarks are presented\\nin Table 4, leading to the following observations. (1) Among the evaluated benchmarks, with\\nonly about 40% of computations, DeepSeekMoE 16B outperforms LLaMA2 7B on the majority\\nof benchmarks. (2) The math reasoning and code generation capabilities of DeepSeekMoE 16B\\n18\\nare stronger than LLaMA2 7B, attributed to the enriched presence of mathematical and code-\\nrelated text in our pretraining corpus. (3) Given the presence of Chinese texts in our pretraining\\ncorpus, DeepSeekMoE 16B exhibits a substantial performance advantage over LLaMA2 7B\\non Chinese benchmarks. (4) Despite being trained on fewer English texts, DeepSeekMoE 16B\\nachieves comparable or better performance compared with LLaMA2 7B on English understand-\\ning or knowledge-intensive benchmarks, which demonstrates the exceptional capabilities of\\nDeepSeekMoE 16B.\\nEvaluation on Open LLM Leaderboard.\\nBeyond our internal evaluations, we also evaluate\\nDeepSeekMoE 16B on the Open LLM Leaderboard and compare it with other open source mod-\\nels. In addition to LLaMA2 7B, we take a broader set of open source models into consideration,\\nincluding LLaMA 7B (Touvron et al., 2023a), Falcon 7B (Almazrouei et al., 2023), GPT-J 6B (Wang\\nand Komatsuzaki, 2021), RedPajama-INCITE 7B and 3B (Together-AI, 2023), Open LLaMA 7B\\nand 3B (Geng and Liu, 2023), OPT 2.7B (Zhang et al., 2022), Pythia 2.8B (Biderman et al., 2023),\\nGPT-neo 2.7B (Black et al., 2021), and BLOOM 3B (Scao et al., 2022). The evaluation results,\\nas presented in Figure 1, show that DeepSeekMoE 16B consistently outperforms models with\\nsimilar activated parameters by a large margin. Moreover, it achieves comparable performance\\nwith LLaMA2 7B, which has approximately 2.5 times the activated parameters.\\n6. Alignment for DeepSeekMoE 16B\\nPrevious research indicates that MoE models typically do not emerge significant gains from\\nfine-tuning (Artetxe et al., 2022; Fedus et al., 2021). However, Shen et al. (2023) present findings\\nsuggesting that MoE models can indeed benefit from instruction tuning. In order to assess\\nwhether DeepSeekMoE 16B can benefit from fine-tuning, we conduct supervised fine-tuning\\nto construct a chat model based on DeepSeekMoE 16B. The experimental results reveal that\\nDeepSeekMoE Chat 16B also achieves comparable performance with LLaMA2 SFT 7B and\\nDeepSeek Chat 7B.\\n6.1. Experimental Setup\\nTraining Data.\\nFor training the chat model, we conduct supervised fine-tuning (SFT) on our\\nin-house curated data, comprising 1.4M training examples. This dataset spans a broad range of\\ncategories including math, code, writing, question answering, reasoning, summarization, and\\nmore. The majority of our SFT training data is in English and Chinese, rendering the chat model\\nversatile and applicable in bilingual scenarios.\\nHyper-Parameters.\\nDuring supervised fine-tuning, we set the batch size to 1024 examples\\nand conduct training over 8 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019).\\nWe employ a maximum sequence length of 4K, and pack the training examples as densely\\nas possible until reaching the sequence length limit. We do not use dropout for supervised\\nfine-tuning, and simply set a constant learning rate of 10−5 without incorporating any learning\\nrate scheduling strategy.\\nEvaluation Benchmarks.\\nFor the evaluation of the chat models, we employ benchmarks\\nsimilar to those used in Section 5.1.3, with the following adjustments: (1) We exclude Pile (Gao\\net al., 2020) since chat models are seldom employed for pure language modeling. (2) We exclude\\n19\\nCHID (Zheng et al., 2019) due to the observed instability of results, hindering the derivation\\nof solid conclusions. (3) We additionally include BBH (Suzgun et al., 2022) to provide a more\\ncomprehensive assessment of the reasoning ability of the chat models.\\nMetric\\n# Shot\\nLLaMA2\\nSFT 7B\\nDeepSeek\\nChat 7B\\nDeepSeekMoE\\nChat 16B\\n# Total Params\\nN/A\\n6.7B\\n6.9B\\n16.4B\\n# Activated Params\\nN/A\\n6.7B\\n6.9B\\n2.8B\\nFLOPs per 4K Tokens\\nN/A\\n187.9T\\n183.5T\\n74.4T\\nHellaSwag (Acc.)\\n0-shot\\n67.9\\n71.0\\n72.2\\nPIQA (Acc.)\\n0-shot\\n76.9\\n78.4\\n79.7\\nARC-easy (Acc.)\\n0-shot\\n69.7\\n70.2\\n69.9\\nARC-challenge (Acc.)\\n0-shot\\n50.8\\n50.2\\n50.0\\nBBH (EM)\\n3-shot\\n39.3\\n43.1\\n42.2\\nRACE-middle (Acc.)\\n5-shot\\n63.9\\n66.1\\n64.8\\nRACE-high (Acc.)\\n5-shot\\n49.6\\n50.8\\n50.6\\nDROP (EM)\\n1-shot\\n40.0\\n41.7\\n33.8\\nGSM8K (EM)\\n0-shot\\n63.4\\n62.6\\n62.2\\nMATH (EM)\\n4-shot\\n13.5\\n14.7\\n15.2\\nHumanEval (Pass@1)\\n0-shot\\n35.4\\n45.1\\n45.7\\nMBPP (Pass@1)\\n3-shot\\n27.8\\n39.0\\n46.2\\nTriviaQA (EM)\\n5-shot\\n60.1\\n59.5\\n63.3\\nNaturalQuestions (EM)\\n0-shot\\n35.2\\n32.7\\n35.1\\nMMLU (Acc.)\\n0-shot\\n50.0\\n49.7\\n47.2\\nWinoGrande (Acc.)\\n0-shot\\n65.1\\n68.4\\n69.0\\nCLUEWSC (EM)\\n5-shot\\n48.4\\n66.2\\n68.2\\nCEval (Acc.)\\n0-shot\\n35.1\\n44.7\\n40.0\\nCMMLU (Acc.)\\n0-shot\\n36.9\\n51.2\\n49.3\\nTable 5 | Comparison among LLaMA2 SFT 7B, DeepSeek Chat 7B and DeepSeekMoE Chat 16B,\\nwith all of these three models fine-tuned on the same SFT data. Compared with both 7B dense\\nmodels, DeepSeekMoE Chat 16B still achieves comparable or better performance on the majority\\nof benchmarks with only 40% of computations.\\n6.2. Evaluations\\nBaselines.\\nIn order to validate the potential of DeepSeekMoE 16B after alignment, we conduct\\nsupervised fine-tuning for LLaMA2 7B, DeepSeek 7B, and DeepSeekMoE 16B, where we utilize\\ntotally the same fine-tuning data to ensure fairness. Correspondingly, we construct three\\nchat models, including LLaMA2 SFT 7B3, DeepSeek Chat 7B, and DeepSeekMoE Chat 16B.\\nSubsequently, we compare DeepSeekMoE Chat 16B with the other two dense chat models (with\\nabout 2.5 times the FLOPs) across a wide range of downstream tasks.\\n3We use LLaMA2 SFT to distinguish from the official LLaMA2 Chat (Touvron et al., 2023b) model.\\n20\\nResults.\\nThe evaluation results are presented in Table 5. Our key observations include:\\n(1) DeepSeekMoE Chat 16B, while consuming nearly 40% of computations, achieves com-\\nparable performance with 7B dense models across language understanding and reasoning\\n(PIQA, ARC, BBH), machine reading comprehension (RACE), mathematical (GSM8K, MATH),\\nand knowledge-intensive tasks (TriviaQA, NaturalQuestions). (2) On code generation tasks,\\nDeepSeekMoE Chat 16B significantly outperforms LLaMA2 SFT 7B, demonstrating notable\\nimprovements on HumanEval and MBPP. In addition, it also surpasses DeepSeek Chat 7B. (3)\\nOn multiple-choice question answering benchmarks including MMLU, CEval, and CMMLU,\\nDeepSeekMoE Chat 16B still falls behind DeepSeek Chat 7B, consistent with the observations\\nfor the base model (Section 5.2.1). However, it is worth noting that, after supervised fine-\\ntuning, the performance gap between DeepSeekMoE 16B and DeepSeek 7B is narrowed. (4)\\nBenefiting from the pretraining on a bilingual corpus, DeepSeekMoE Chat 16B notably outper-\\nforms LLaMA2 SFT 7B on all Chinese benchmarks. These results demonstrate the balanced\\ncapabilities of DeepSeekMoE 16B in both Chinese and English, enhancing its versatility and\\napplicability in diverse scenarios. In conclusion, the evaluation for the chat models highlights\\nthe potential of DeepSeekMoE 16B in benefiting from alignment, and validates its consistent\\nadvantages in achieving comparable performance with dense models while using only about\\n40% of computations.\\n7. DeepSeekMoE 145B Ongoing\\nEncouraged by the outstanding performance of DeepSeekMoE 16B, we further undertake a\\npreliminary endeavor to scale up DeepSeekMoE to 145B. In this initial study, DeepSeekMoE\\n145B is trained on 245B tokens, but it has demonstrated consistent advantages over the GShard\\narchitecture and shown promise to match or exceed the performance of DeepSeek 67B (Dense).\\nFurthermore, upon the completion of the final version and full training of DeepSeekMoE 145B,\\nwe also plan to make it publicly available.\\n7.1. Experimental Setup\\nTraining Data and Tokenization.\\nFor DeepSeekMoE 145B, we employ exactly the same train-\\ning corpus and tokenizer as DeepSeekMoE 16B, with the only difference being that DeepSeek-\\nMoE 145B is trained on 245B tokens for an initial study.\\nModel Settings.\\nFor DeepSeekMoE 145B, we set the number of Transformer layers to 62 and\\nthe hidden dimension to 4096. We employ the multi-head attention mechanism with a total of\\n32 attention heads, where each head has a dimension of 128. As for initialization, all learnable\\nparameters are randomly initialized with a standard deviation of 0.006. As in DeepSeekMoE\\n16B, we also substitute all FFNs except for the first layer with MoE layers. Each MoE layer\\nconsists of 4 shared experts and 128 routed experts, where each expert is 0.125 times the size of\\na standard FFN. Each token will be routed to these 4 shared experts and 12 out of 128 routed\\nexperts. Under this configuration, DeepSeekMoE 145 has approximately 144.6B total parameters,\\nwith the number of activated parameters around 22.2B.\\nTraining Settings.\\nWe employ the AdamW optimizer (Loshchilov and Hutter, 2019) with\\nhyper-parameters set to 𝛽1 = 0.9, 𝛽2 = 0.95, and weight_decay = 0.1. For the preliminary study\\nof DeepSeekMoE 145B, we employ a warmup-and-constant learning rate scheduler. Initially,\\nthe learning rate linearly increases from 0 to the maximum value during the first 2K steps.\\n21\\nSubsequently, the learning rate keeps constant during the remaining training process. The\\nmaximum learning rate for DeepSeekMoE 145B is set to 3.0 × 10−4, and the gradient clipping\\nnorm is set to 1.0. The batch size is set to 4.5K, and with a maximum sequence length of 4K, each\\ntraining batch contains 18M tokens. We train DeepSeekMoE 145B for 13,000 steps, achieving\\n245B training tokens. Also, we do not use dropout during training. We leverage pipeline\\nparallelism to deploy different layers of a model on different devices, and for each layer, all the\\nrouted experts will be uniformly deployed on 4 devices (i.e., expert parallelism combined with\\ndata parallelism). Since we employ expert parallelism for DeepSeekMoE 145B, the device-level\\nload balance should be considered to reduce the computational bottleneck. In response, we set\\nthe device-level balance factor to 0.05 to encourage balanced computation across devices. Also,\\nwe still set a small expert-level balance factor of 0.003 to prevent routing collapse.\\nEvaluation Benchmarks.\\nWe evaluate DeepSeekMoE 145B on exactly the same internal bench-\\nmarks as used for DeepSeekMoE 16B (see Section 5.1.3).\\n7.2. Evaluations\\nBaselines.\\nApart from DeepSeekMoE 145B, we consider three additional models for compari-\\nson. DeepSeek 67B (Dense) is a dense model with 67.4B total parameters (refer to DeepSeek-AI\\n(2024) for the model and training details). GShard 137B shares the same hidden dimension\\nand number of layers as DeepSeekMoE 145B, but follows the GShard architecture. Note that\\nDeepSeekMoE 145B aligns the intermediate hidden dimension in each expert to a multiple of\\n64 for computation efficiency, so its model size is 6% larger than GShard 137B. DeepSeekMoE\\n142B (Half Activated) has a similar architecture to DeepSeekMoE 145B, but it contains only\\n2 shared experts, and only 6 out of 128 routed experts are activated. It is noteworthy that all\\ncompared models, including DeepSeekMoE 145B, share the same training corpus. In addi-\\ntion, all MoE models in the comparison are trained from scratch and share the same training\\nhyper-parameters.\\nResults.\\nFrom the evaluation results presented in Table 6, we have the following observa-\\ntions: (1) Despite having comparable total parameters and computations, DeepSeekMoE 145B\\nsignificantly outperforms GShard 137B, highlighting the advantages of the DeepSeekMoE ar-\\nchitecture again. (2) On the whole, with only 28.5% of computations, DeepSeekMoE 145B\\nachieves comparable performance with DeepSeek 67B (Dense). Consistent with the findings\\nfrom DeepSeekMoE 16B, DeepSeekMoE 145B exhibits remarkable strengths in language model-\\ning and knowledge-intensive tasks, but with limitations in multiple-choice tasks. (3) At a larger\\nscale, the performance of DeepSeekMoE 142B (Half Activated) does not lag behind too much\\nfrom DeepSeekMoE 145B. In addition, despite having only a half of activated expert parameters,\\nDeepSeekMoE 142B (Half Activated) still match the performance of DeepSeek 67B (Dense), with\\nonly 18.2% of computations. It also outperforms GShard 137B, which aligns with the conclusion\\nfrom Section 4.5.\\n8. Related Work\\nThe Mixture of Experts (MoE) technique is first proposed by Jacobs et al. (1991); Jordan and Jacobs\\n(1994) to deal with different samples with independent expert modules. Shazeer et al. (2017)\\nintroduce MoE into language model training and build a large-scale LSTM-based (Hochreiter\\nand Schmidhuber, 1997) MoE models. As Transformer become the most popular architecture\\n22\\nMetric\\n# Shot\\nDeepSeek\\n67B (Dense)\\nGShard\\n137B\\nDeepSeekMoE\\n145B\\nDeepSeekMoE 142B\\n(Half Activated)\\n# Total Params\\nN/A\\n67.4B\\n136.5B\\n144.6B\\n142.3B\\n# Activated Params\\nN/A\\n67.4B\\n21.6B\\n22.2B\\n12.2B\\nRelative Expert Size\\nN/A\\nN/A\\n1\\n0.125\\n0.125\\n# Experts\\nN/A\\nN/A\\n0 + 16\\n4 + 128\\n2 + 128\\n# Activated Experts\\nN/A\\nN/A\\n0 + 2\\n4 + 12\\n2 + 6\\nFLOPs per 4K Tokens\\nN/A\\n2057.5T\\n572.7T\\n585.6T\\n374.6T\\n# Training Tokens\\nN/A\\n245B\\n245B\\n245B\\n245B\\nPile (Loss.)\\nN/A\\n1.905\\n1.961\\n1.876\\n1.888\\nHellaSwag (Acc.)\\n0-shot\\n74.8\\n72.0\\n75.8\\n74.9\\nPIQA (Acc.)\\n0-shot\\n79.8\\n77.6\\n80.7\\n80.2\\nARC-easy (Acc.)\\n0-shot\\n69.0\\n64.0\\n69.7\\n67.9\\nARC-challenge (Acc.)\\n0-shot\\n50.4\\n45.8\\n48.8\\n49.0\\nRACE-middle (Acc.)\\n5-shot\\n63.2\\n59.2\\n62.1\\n59.5\\nRACE-high (Acc.)\\n5-shot\\n46.9\\n43.5\\n45.5\\n42.6\\nDROP (EM)\\n1-shot\\n27.5\\n21.6\\n27.8\\n28.9\\nGSM8K (EM)\\n8-shot\\n11.8\\n6.4\\n12.2\\n13.8\\nMATH (EM)\\n4-shot\\n2.1\\n1.6\\n3.1\\n2.8\\nHumanEval (Pass@1)\\n0-shot\\n23.8\\n17.7\\n19.5\\n23.2\\nMBPP (Pass@1)\\n3-shot\\n33.6\\n27.6\\n33.2\\n32.0\\nTriviaQA (EM)\\n5-shot\\n57.2\\n52.5\\n61.1\\n59.8\\nNaturalQuestions (EM) 5-shot\\n22.6\\n19.0\\n25.0\\n23.5\\nMMLU (Acc.)\\n5-shot\\n45.1\\n26.3\\n39.4\\n37.5\\nWinoGrande (Acc.)\\n0-shot\\n70.7\\n67.6\\n71.9\\n70.8\\nCLUEWSC (EM)\\n5-shot\\n69.1\\n65.7\\n71.9\\n72.6\\nCEval (Acc.)\\n5-shot\\n40.3\\n26.2\\n37.1\\n32.8\\nCMMLU (Acc.)\\n5-shot\\n40.6\\n25.4\\n35.9\\n31.9\\nCHID (Acc.)\\n0-shot\\n88.5\\n86.9\\n90.3\\n88.3\\nTable 6 | Comparison among DeepSeek 67B (Dense) and MoE models at the scale of about\\n140B total parameters. In the lines of “# Experts” and “# Activated Experts”, 𝑎+ 𝑏denotes\\n𝑎shared experts and 𝑏routed experts, respectively. Bold font indicates the best or near the\\nbest performance excluding the last column. DeepSeekMoE 145B, and even DeepSeekMoE\\n142B (Half Activated) that has only a half of activated expert parameters, outperform GShard\\n137B by a large margin. Moreover, with 28.5% of computations, DeepSeekMoE 145B achieves\\ncomparable performance with DeepSeek 67B.\\nfor NLP, many attempts extend FFNs in a Transformer as MoE layers to build MoE language\\nmodels. GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2021) are pioneers\\nwhich employ learnable top-2 or top-1 routing strategies to scale the MoE language models\\nto an extremely large scale. Hash Layer (Roller et al., 2021) and StableMoE (Dai et al., 2022b)\\nuse fixed routing strategies for more stable routing and training. Zhou et al. (2022) propose an\\nexpert-choice routing strategy, where each token can be assigned to different numbers of experts.\\nZoph (2022) focus on the issues of training instability and fine-tuning difficulty in MoE models,\\n23\\nand propose ST-MoE to overcome these challenges. In addition to research on MoE architectures\\nand training strategies, recent years have also witnessed the emergence of numerous large-scale\\nlanguage or multimodal models (Du et al., 2022; Lin et al., 2021; Ren et al., 2023; Xue et al.,\\n2023) based on existing MoE architectures. By and large, most of the previous MoE models\\nare based on conventional top-1 or top-2 routing strategies, leaving large room for improving\\nexpert specialization. In response, our DeepSeekMoE architecture aims to improve the expert\\nspecialization to the utmost extent.\\n9. Conclusion\\nIn this paper, we introduce the DeepSeekMoE architecture for MoE language models, with the\\nobjective of achieving ultimate expert specialization. Through fine-grained expert segmentation\\nand shared expert isolation, DeepSeekMoE achieves significantly higher expert specialization\\nand performance compared with prevailing MoE architectures. Starting with a modest scale of\\n2B parameters, we validate the advantages of DeepSeekMoE, demonstrating its capability to\\napproach the upper bound performance for MoE models. Furthermore, we provide empirical\\nevidence to show that DeepSeekMoE has a higher level of expert specialization than GShard.\\nScaling up to a larger scale of 16B total parameters, we train DeepSeekMoE 16B on 2T tokens\\nand demonstrate its outstanding performance comparable with DeepSeek 7B and LLaMA2 7B,\\nwith only about 40% of computations. Additionally, supervised fine-tuning is conducted for\\nalignment to construct an MoE chat model based on DeepSeekMoE 16B, further showing its\\nadaptability and versatility. Further, we perform a preliminary exploration to scale DeepSeek-\\nMoE to 145B parameters. We find that DeepSeekMoE 145B still keeps substantial advantages\\nover the GShard architecture, and demonstrates comparable performance with DeepSeek 67B,\\nusing only 28.5% (maybe even 18.2%) of computations.\\nFor research purposes, we release the model checkpoint of DeepSeekMoE 16B to the public,\\nwhich can be deployed on a single GPU with 40GB of memory. We aspire for this work to\\nprovide valuable insights for both academia and industry, and contribute to the accelerated\\nadvancement of large-scale language models.\\nReferences\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet,\\nD. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40B: an open\\nlarge language model with state-of-the-art performance, 2023.\\nM. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer,\\nR. Pasunuru, G. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou,\\nP. S. Koura, B. O’Horo, J. Wang, L. Zettlemoyer, M. T. Diab, Z. Kozareva, and V. Stoyanov.\\nEfficient large scale language modeling with mixtures of experts. In Y. Goldberg, Z. Kozareva,\\nand Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022,\\npages 11699–11732. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022\\n.EMNLP-MAIN.804. URL https://doi.org/10.18653/v1/2022.emnlp-main.804.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.\\n24\\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia:\\nA suite for analyzing large language models across training and scaling. In A. Krause,\\nE. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference\\non Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\\nProceedings of Machine Learning Research, pages 2397–2430. PMLR, 2023. URL https:\\n//proceedings.mlr.press/v202/biderman23a.html.\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\\nS. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow, Mar. 2021. URL https://doi.org/10.5281/\\nzenodo.5297715. If you use this misc, please cite it using these metadata.\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\\nLanguage models are few-shot learners. In Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, 2020. URL\\nhttps://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8\\nac142f64a-Abstract.html.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you\\nhave solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457,\\n2018. URL http://arxiv.org/abs/1803.05457.\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168, 2021.\\nD. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge neurons in pretrained\\ntransformers. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8493–8502. Association for Computational\\nLinguistics, 2022a. doi: 10.18653/V1/2022.ACL-LONG.581. URL https://doi.org/10.1\\n8653/v1/2022.acl-long.581.\\n25\\nD. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. Stablemoe: Stable routing\\nstrategy for mixture of experts.\\nIn S. Muresan, P. Nakov, and A. Villavicencio, editors,\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 7085–7095.\\nAssociation for Computational Linguistics, 2022b. doi: 10.18653/V1/2022.ACL-LONG.489.\\nURL https://doi.org/10.18653/v1/2022.acl-long.489.\\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv\\npreprint arXiv:2401.02954, 2024.\\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu,\\nO. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat,\\nK. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and\\nC. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In International\\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\\nvolume 162 of Proceedings of Machine Learning Research, pages 5547–5569. PMLR, 2022.\\nURL https://proceedings.mlr.press/v162/du22c.html.\\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\\n2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\\nhttps://doi.org/10.18653/v1/n19-1246.\\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\\nwith simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/\\nabs/2101.03961.\\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\\nN. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\\npreprint arXiv:2101.00027, 2020.\\nX. Geng and H. Liu. Openllama: An open reproduction of llama, May 2023. URL https:\\n//github.com/openlm-research/open_llama.\\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, and P. B.\\nGibbons. Pipedream: Fast and efficient pipeline parallel DNN training. CoRR, abs/1806.03377,\\n2018. URL http://arxiv.org/abs/1806.03377.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\\nMeasuring mathematical problem solving with the math dataset, 2021.\\nHigh-Flyer. Hai-llm: An efficient and lightweight tool for training large models, 2023. URL\\nhttps://www.high-flyer.cn/en/blog/hai-llm.\\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computing, 9(8):1735–1780,\\n1997. URL https://doi.org/10.1162/neco.1997.9.8.1735.\\n26\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.\\nTraining compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550\\n/arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nR. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\\nNeural Computing, 3(1):79–87, 1991. URL https://doi.org/10.1162/neco.1991.3.1.\\n79.\\nM. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural\\nComputing, 6(2):181–214, 1994. URL https://doi.org/10.1162/neco.1994.6.2.181.\\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. triviaqa: A Large Scale Distantly Supervised\\nChallenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017.\\nV. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.\\nReducing activation recomputation in large transformer models. Proceedings of Machine\\nLearning and Systems, 5, 2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polo-\\nsukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszko-\\nreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research.\\nTransactions of the Association of Computational Linguistics, 2019.\\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension\\ndataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of\\nthe 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,\\nCopenhagen, Denmark, September 9-11, 2017, pages 785–794. Association for Computational\\nLinguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1\\n7-1082.\\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\\nGshard: Scaling giant models with conditional computation and automatic sharding. In 9th\\nInternational Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\\nURL https://openreview.net/forum?id=qrwe7XHTmYb.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nJ. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y. Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, J. Zhang,\\nJ. Zhang, X. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y. Li, W. Lin, J. Zhou,\\nJ. Tang, and H. Yang. M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021.\\nURL https://arxiv.org/abs/2103.00823.\\nS. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. In\\nS. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\\n27\\nIreland, May 22-27, 2022, pages 3214–3252. Association for Computational Linguistics, 2022.\\ndoi: 10.18653/V1/2022.ACL-LONG.229. URL https://doi.org/10.18653/v1/2022.a\\ncl-long.229.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th International\\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nOpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\\nD. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,\\nP. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training\\non gpu clusters using megatron-lm. In Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis, pages 1–15, 2021.\\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.\\nURL https://doi.org/10.48550/arXiv.2303.08774.\\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training\\ntrillion parameter models. In C. Cuicchi, I. Qualters, and W. T. Kramer, editors, Proceedings\\nof the International Conference for High Performance Computing, Networking, Storage and\\nAnalysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20.\\nIEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https://doi.org/10.1109/SC\\n41405.2020.00024.\\nS. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and\\nY. He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power\\nnext-generation AI scale. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and\\nS. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July\\n2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research,\\npages 18332–18346. PMLR, 2022. URL https://proceedings.mlr.press/v162/rajbh\\nandari22a.html.\\nX. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov,\\nA. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao. Pangu-Σ: Towards trillion\\nparameter language model with sparse heterogeneous computing. CoRR, abs/2303.10845,\\n2023. URL https://doi.org/10.48550/arXiv.2303.10845.\\nS. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. Hash layers for large sparse models. CoRR,\\nabs/2106.04426, 2021. URL https://arxiv.org/abs/2106.04426.\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\\nschema challenge at scale, 2019.\\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon,\\nM. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot,\\nN. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major,\\nI. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite,\\nJ. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy,\\nA. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien,\\nD. I. Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language\\nmodel. CoRR, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https:\\n//doi.org/10.48550/arXiv.2211.05100.\\n28\\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\\nunits.\\nIn Proceedings of the 54th Annual Meeting of the Association for Computational\\nLinguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The\\nAssociation for Computer Linguistics, 2016. doi: 10.18653/V1/P16-1162. URL https:\\n//doi.org/10.18653/v1/p16-1162.\\nN. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150,\\n2019. URL http://arxiv.org/abs/1911.02150.\\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously\\nlarge neural networks: The sparsely-gated mixture-of-experts layer. In 5th International\\nConference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https:\\n//openreview.net/forum?id=B1ckMDqlg.\\nS. Shen, L. Hou, Y. Zhou, N. Du, S. Longpre, J. Wei, H. W. Chung, B. Zoph, W. Fedus, X. Chen,\\nT. Vu, Y. Wu, W. Chen, A. Webson, Y. Li, V. Zhao, H. Yu, K. Keutzer, T. Darrell, and D. Zhou.\\nFlan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.\\nCoRR, abs/2305.14705, 2023. doi: 10.48550/ARXIV.2305.14705. URL https://doi.org/10\\n.48550/arXiv.2305.14705.\\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:\\nTraining multi-billion parameter language models using model parallelism. arXiv preprint\\narXiv:1909.08053, 2019.\\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\\nthem. arXiv preprint arXiv:2210.09261, 2022.\\nP. Tillet, H. T. Kung, and D. Cox. Triton: An intermediate language and compiler for tiled neural\\nnetwork computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop\\non Machine Learning and Programming Languages, MAPL 2019, page 10–19, New York, NY,\\nUSA, 2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/331550\\n8.3329973. URL https://doi.org/10.1145/3315508.3329973.\\nTogether-AI. Redpajama-data: An open source recipe to reproduce llama training dataset, April\\n2023. URL https://github.com/togethercomputer/RedPajama-Data.\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\\nefficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/arXiv.230\\n2.13971. URL https://doi.org/10.48550/arXiv.2302.13971.\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\\n2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\\n09288.\\n29\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\\nsukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30:\\nAnnual Conference on Neural Information Processing Systems 2017, pages 5998–6008, 2017.\\nURL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd\\n053c1c4a845aa-Abstract.html.\\nB. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu,\\nB. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou,\\nS. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chi-\\nnese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors,\\nProceedings of the 28th International Conference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762–4772. International Com-\\nmittee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL\\nhttps://doi.org/10.18653/v1/2020.coling-main.419.\\nF. Xue, Z. Zheng, Y. Fu, J. Ni, Z. Zheng, W. Zhou, and Y. You. Openmoe: Open mixture-of-experts\\nlanguage models. https://github.com/XueFuzhao/OpenMoE, 2023.\\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\\nLinguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1\\n9-1472.\\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,\\nT. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and\\nL. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\\nC. Zheng, M. Huang, and A. Sun. Chid: A large-scale chinese idiom dataset for cloze test. In\\nA. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of\\nthe Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,\\n2019, Volume 1: Long Papers, pages 778–787. Association for Computational Linguistics, 2019.\\ndoi: 10.18653/V1/P19-1075. URL https://doi.org/10.18653/v1/p19-1075.\\nY. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Z. Chen, Q. V. Le, and J. Laudon.\\nMixture-of-experts with expert choice routing. In NeurIPS, 2022. URL http://papers.nip\\ns.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abs\\ntract-Conference.html.\\nB. Zoph.\\nDesigning effective sparse expert models.\\nIn IEEE International Parallel and\\nDistributed Processing Symposium, IPDPS Workshops 2022, Lyon, France, May 30 - June\\n3, 2022, page 1044. IEEE, 2022. URL https://doi.org/10.1109/IPDPSW55747.2022.0\\n0171.\\n30\\nAppendices\\nA. Overview of Hyper-Parameters\\nWe present the overview of hyper-parameters for DeepSeekMoE across various sizes in Table 7.\\n# Params # Layers Hidden\\nSize\\n# Attn\\nHeads\\n# Shared\\nExperts\\n# Routed\\nExperts\\nRelative\\nExpert Size\\nSequence\\nLength\\nBatch Size\\n(Sequence)\\nLearning\\nRate\\n2.0B\\n9\\n1280\\n10\\n1\\n63 (7 activated)\\n0.25\\n2048\\n2048\\n1.08e-3\\n16.4B\\n28\\n2048\\n16\\n2\\n64 (6 activated)\\n0.25\\n4096\\n4608\\n4.2e-4\\n144.6B\\n62\\n4096\\n32\\n4\\n128 (12 activated)\\n0.125\\n4096\\n4608\\n3.0e-4\\nTable 7 | Overview of hyper-parameters for DeepSeekMoE across various sizes. The relative\\nexpert size is in comparison to a standard FFN.\\nB. Comparing DeepSeekMoE with Larger Models\\nComparisons among DeepSeekMoE, GShard×1.2, and GShard×1.5 are shown in Table 8. Com-\\nparisons among DeepSeekMoE, Dense×4, and Dense×16 are shown in Table 9.\\nMetric\\n# Shot\\nGShard×1.2\\nGShard×1.5\\nDeepSeekMoE\\nRelative Expert Size\\nN/A\\n1.2\\n1.5\\n0.25\\n# Experts\\nN/A\\n0 + 16\\n0 + 16\\n1 + 63\\n# Activated Experts\\nN/A\\n0 + 2\\n0 + 2\\n1 + 7\\n# Total Expert Params\\nN/A\\n2.3B\\n2.8B\\n1.9B\\n# Activated Expert Params\\nN/A\\n0.28B\\n0.35B\\n0.24B\\n# Training Tokens\\nN/A\\n100B\\n100B\\n100B\\nPile (Loss)\\nN/A\\n1.824\\n1.808\\n1.808\\nHellaSwag (Acc.)\\n0-shot\\n53.7\\n54.4\\n54.8\\nPIQA (Acc.)\\n0-shot\\n71.8\\n71.1\\n72.3\\nARC-easy (Acc.)\\n0-shot\\n46.8\\n47.3\\n49.4\\nARC-challenge (Acc.)\\n0-shot\\n31.7\\n34.1\\n34.3\\nRACE-middle (Acc.)\\n5-shot\\n43.7\\n46.4\\n44.0\\nRACE-high (Acc.)\\n5-shot\\n31.9\\n32.4\\n31.7\\nHumanEval (Pass@1)\\n0-shot\\n3.7\\n3.0\\n4.9\\nMBPP (Pass@1)\\n3-shot\\n2.4\\n2.6\\n2.2\\nTriviaQA (EM)\\n5-shot\\n15.2\\n15.7\\n16.6\\nNaturalQuestions (EM)\\n5-shot\\n4.5\\n4.7\\n5.7\\nTable 8 | Comparison between DeepSeekMoE and larger GShard models.\\nAt a larger scale of 13B total parameters, we also compare DeepSeekMoE with GShard×1.2\\nand GShard×1.5, and show results in Table 10. At a larger scale, DeepSeekMoE even outperforms\\nGShard×1.5 distinctly.\\n31\\nMetric\\n# Shot\\nDense×4\\nDense×16\\nDeepSeekMoE\\nRelative Expert Size\\nN/A\\n1\\n1\\n0.25\\n# Experts\\nN/A\\n4 + 0\\n16 + 0\\n1 + 63\\n# Activated Experts\\nN/A\\n4 + 0\\n16 + 0\\n1 + 7\\n# Total Expert Params\\nN/A\\n0.47B\\n1.89B\\n1.89B\\n# Activated Expert Params\\nN/A\\n0.47B\\n1.89B\\n0.24B\\n# Training Tokens\\nN/A\\n100B\\n100B\\n100B\\nPile (Loss)\\nN/A\\n1.908\\n1.806\\n1.808\\nHellaSwag (Acc.)\\n0-shot\\n47.6\\n55.1\\n54.8\\nPIQA (Acc.)\\n0-shot\\n70.0\\n71.9\\n72.3\\nARC-easy (Acc.)\\n0-shot\\n43.9\\n51.9\\n49.4\\nARC-challenge (Acc.)\\n0-shot\\n30.5\\n33.8\\n34.3\\nRACE-middle (Acc.)\\n5-shot\\n42.4\\n46.3\\n44.0\\nRACE-high (Acc.)\\n5-shot\\n30.7\\n33.0\\n31.7\\nHumanEval (Pass@1)\\n0-shot\\n1.8\\n4.3\\n4.9\\nMBPP (Pass@1)\\n3-shot\\n0.2\\n2.2\\n2.2\\nTriviaQA (EM)\\n5-shot\\n9.9\\n16.5\\n16.6\\nNaturalQuestions (EM)\\n5-shot\\n3.0\\n6.3\\n5.7\\nTable 9 | Comparison between DeepSeekMoE and larger dense baselines.\\nMetric\\n# Shot\\nGShard×1.2\\nGShard×1.5\\nDeepSeekMoE\\nRelative Expert Size\\nN/A\\n1.2\\n1.5\\n0.25\\n# Experts\\nN/A\\n0 + 16\\n0 + 16\\n1 + 63\\n# Activated Experts\\nN/A\\n0 + 2\\n0 + 2\\n1 + 7\\n# Total Expert Params\\nN/A\\n15.9B\\n19.8B\\n13.3B\\n# Activated Expert Params\\nN/A\\n2.37B\\n2.82B\\n2.05B\\n# Training Tokens\\nN/A\\n100B\\n100B\\n100B\\nHellaSwag (Acc.)\\n0-shot\\n66.6\\n67.7\\n69.1\\nPIQA (Acc.)\\n0-shot\\n75.6\\n76.0\\n75.7\\nARC-easy (Acc.)\\n0-shot\\n56.8\\n56.8\\n58.8\\nARC-challenge (Acc.)\\n0-shot\\n39.9\\n37.6\\n38.5\\nRACE-middle (Acc.)\\n5-shot\\n51.6\\n50.6\\n52.4\\nRACE-high (Acc.)\\n5-shot\\n37.4\\n36.3\\n38.5\\nHumanEval (Pass@1)\\n0-shot\\n6.1\\n6.1\\n9.8\\nMBPP (Pass@1)\\n3-shot\\n7.0\\n11.6\\n10.6\\nTriviaQA (EM)\\n5-shot\\n36.5\\n36.7\\n38.2\\nNaturalQuestions (EM)\\n5-shot\\n12.6\\n12.1\\n13.7\\nTable 10 | Comparison between DeepSeekMoE and larger GShard models at a larger scale.\\nC. Training Benchmark Curves of DeepSeekMoE 16B\\nWe present the benchmark curves during training of DeepSeekMoE 16B and DeepSeek 7B\\n(Dense) in Figure 7 for reference.\\n32\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nPerformance\\nHellaSwag (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nPerformance\\nPIQA (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.4\\n0.5\\n0.6\\n0.7\\nPerformance\\nARC-easy (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.3\\n0.4\\n0.5\\nPerformance\\nARC-challenge (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.4\\n0.5\\n0.6\\nPerformance\\nRACE-middle (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.30\\n0.35\\n0.40\\n0.45\\nPerformance\\nRACE-high (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.1\\n0.2\\n0.3\\nPerformance\\nDROP (EM)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\nPerformance\\nGSM8K (EM)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.0\\n0.1\\n0.2\\n0.3\\nPerformance\\nHumanEval (Pass@1)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nPerformance\\nMBPP (Pass@1)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.0\\n0.2\\n0.4\\n0.6\\nPerformance\\nTriviaQA (EM)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\nPerformance\\nNaturalQuestions (EM)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nPerformance\\nMMLU (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\nPerformance\\nWinoGrande (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.0\\n0.2\\n0.4\\n0.6\\nPerformance\\nCLUEWSC (EM)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nPerformance\\nCEval (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nPerformance\\nCMMLU (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\n0\\n500\\n1000\\n1500\\n2000\\n# Training Tokens (B)\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nPerformance\\nCHID (Acc.)\\nDeepSeekMoE 16B\\nDeepSeek 7B (Dense)\\nFigure 7 | Benchmark curves during training of DeepSeekMoE 16B and DeepSeek 7B (Dense).\\n33\\n', 'source_name': 'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models', 'source_url': 'https://arxiv.org/abs/2401.06066'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "StableMoE.pdf #60\n",
      "{'content': 'STABLEMOE: Stable Routing Strategy for Mixture of Experts\\nDamai Dai†‡∗\\n, Li Dong‡, Shuming Ma‡, Bo Zheng‡,\\nZhifang Sui†, Baobao Chang†, Furu Wei‡\\n†MOE Key Lab of Computational Linguistics, Peking University\\n‡Microsoft Research\\n{daidamai,szf,chbb}@pku.edu.cn\\n{lidong1,shumma,v-zhebo,fuwei}@microsoft.com\\nAbstract\\nThe Mixture-of-Experts (MoE) technique can\\nscale up the model size of Transformers with\\nan affordable computational overhead.\\nWe\\npoint out that existing learning-to-route MoE\\nmethods suffer from the routing ﬂuctuation is-\\nsue, i.e., the target expert of the same input\\nmay change along with training, but only one\\nexpert will be activated for the input during in-\\nference. The routing ﬂuctuation tends to harm\\nsample efﬁciency because the same input up-\\ndates different experts but only one is ﬁnally\\nused. In this paper, we propose STABLEMOE\\nwith two training stages to address the rout-\\ning ﬂuctuation problem. In the ﬁrst training\\nstage, we learn a balanced and cohesive rout-\\ning strategy and distill it into a lightweight\\nrouter decoupled from the backbone model. In\\nthe second training stage, we utilize the dis-\\ntilled router to determine the token-to-expert\\nassignment and freeze it for a stable routing\\nstrategy. We validate our method on language\\nmodeling and multilingual machine transla-\\ntion.\\nThe results show that STABLEMOE\\noutperforms existing MoE methods in terms\\nof both convergence speed and performance.\\nThe code is available at https://github.\\ncom/Hunter-DDM/stablemoe.\\n1\\nIntroduction\\nIn recent years, large-scale Transformers (Devlin\\net al., 2019; Dong et al., 2019; Raffel et al., 2020;\\nClark et al., 2020; Bao et al., 2020; Brown et al.,\\n2020) have shown a striking ability to model lan-\\nguages.\\nHowever, with the model scale grow-\\ning, the training speed will go slower, and the\\nextremely large memory requirement also intro-\\nduces a heavy burden of engineering. Mixture of\\nExperts (MoE) (Jacobs et al., 1991; Jordan and Ja-\\ncobs, 1994; Shazeer et al., 2017), in a much easier\\nway, enables Transformers to scale up the number\\nof parameters meanwhile introducing an affordable\\n∗Contribution during internship at Microsoft Research.\\ncomputational overhead. MoE-based Transform-\\ners have a set of expert modules, and only a few\\nexperts will be activated for each input token. In\\nthis way, we can expand the model scale by adding\\nexpert modules, which will keep the computational\\nand memory overhead within a tolerable range.\\nMost existing MoE methods (Lepikhin et al.,\\n2021; Fedus et al., 2021; Lewis et al., 2021) decide\\nthe token-to-expert routing according to the dynam-\\nically changing token representations. However,\\nwe point out that they face the routing ﬂuctuation\\nproblem. As shown in Figure 1, the same input may\\nbe assigned to different experts along with training.\\nHowever, during inference, only one expert will\\nbe activated for the input. The routing ﬂuctuation\\nproblem tends to harm sample efﬁciency because\\nthe same input updates different experts while only\\none is ﬁnally used.\\nTaking BASE Layer (Lewis et al., 2021) as an\\nexample, during the whole training process, we\\nexamine the token-to-expert assignment for tokens\\nin the validation set. For an input token, we deﬁne\\nthe last ﬂuctuation step as the last step where its\\ntarget expert is different from the ﬁnal step. We\\nplot the cumulative token percentage with regard to\\nthe last ﬂuctuation step (annotated as its percentage\\naccounting for all training steps) in Figure 2. We\\nﬁnd that the last ﬂuctuation step of 40.9% tokens\\nexceeds 20%, which means 40.9% tokens do not\\nhave a stable target expert when 20% of all training\\nsteps have been done. Furthermore, 29.1% tokens\\nstill change their target experts after half of the\\nwhole training process, and 15.4% tokens even\\nchange the target expert after 80% of all training\\nsteps, which is nearing the training ending. These\\nstatistics prove that the routing ﬂuctuation problem\\nindeed exists in previous MoE methods.\\nIn this paper, we propose STABLEMOE with\\ntwo training stages to address the routing ﬂuctua-\\ntion problem. In the ﬁrst training stage, we follow\\nthe learning-to-route paradigm and aim to learn a\\narXiv:2204.08396v1  [cs.LG]  18 Apr 2022\\nTraining Epochs\\nExpert 3\\nInference\\n𝑒1\\n𝑒2\\n𝑒3\\n…\\n…\\nTransformer Blocks\\nTransformer Blocks\\nThe car  is\\nExpert 2\\nExpert 1\\nExpert 3\\nTransformer Blocks\\nTransformer Blocks\\nExpert 2\\nExpert 1\\nExpert 3\\nTransformer Blocks\\nTransformer Blocks\\nExpert 2\\nExpert 1\\nTransformer Blocks\\nTransformer Blocks\\nExpert 2\\nExpert 1\\nExpert 3\\nThe  car is\\nThe  car  is\\nThe  car is\\nFigure 1: Illustration of the routing ﬂuctuation problem. The same input is assigned to different experts along with\\ntraining. However, during inference, only one expert is sparsely activated for the input. The routing ﬂuctuation\\ntends to harm sample efﬁciency because the same input updates different experts while only one is used.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nLast Fluctuation Step\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nCumulative Token Percentage\\n(20%, 40.9%)\\n(50%, 29.1%)\\n(80%, 15.4%)\\nFigure 2:\\nCumulative token percentage with re-\\ngard to the last ﬂuctuation step of tokens for BASE\\nLayer (Lewis et al., 2021). A substantial portion of to-\\nkens still change their target experts even if the training\\nis nearing the end.\\nbalanced and cohesive routing strategy. We design\\na balance loss to guarantee the assignment is bal-\\nanced. In addition, inspired by Lewis et al. (2021),\\nwe adopt a sigmoid gating mechanism, which en-\\nables the task objective to propagate supervised sig-\\nnal back to the routing strategy, to facilitate learn-\\ning a more cohesive assignment. As the routing\\nstrategy is being learned, we synchronously dis-\\ntill it into a lightweight router decoupled from the\\nbackbone model. In the second training stage, we\\nutilize the distilled router to determine the token-\\nto-expert assignment. The distilled router is frozen\\nin this stage to provide a stable routing strategy,\\nwhich addresses the routing ﬂuctuation problem in\\nthe remaining training. We conduct experiments on\\nlanguage modeling and multilingual machine trans-\\nlation. The results show that STABLEMOE out-\\nperforms existing MoE methods in terms of both\\nconvergence speed and performance.\\nOur contributions are summarized as follows:\\n(1) We point out the routing ﬂuctuation problem\\nin existing learning-to-route MoE methods. (2)\\nWe propose STABLEMOE to address the routing\\nﬂuctuation problem. (3) We conduct substantial ex-\\nperiments under various settings to show the advan-\\ntages of STABLEMOE over existing MoE methods.\\n2\\nBackground: Mixture-of-Experts for\\nTransformers\\nWe ﬁrst introduce the MoE mechanism designed\\nfor Transformers (Vaswani et al., 2017). Given a\\nstandard L-layer Transformer model and an input\\nsequence X containing T tokens, the Transformer\\noutput HL is calculated by\\nHL = [hL\\n1 ; hL\\n2 ; ...; hL\\nT ],\\n(1)\\nhl\\nt = FFN\\n\\x10\\nul\\nt\\n\\x11\\n+ ul\\nt,\\n(2)\\nul\\n1:T = self-att\\n\\x10\\nhl−1\\n1:T\\n\\x11\\n+ hl−1\\n1:T ,\\n(3)\\nwhere hl\\nt is the hidden state of t-th token after the\\nl-th layer, Self-Att(·) is the self-attention module,\\nand FFN(·) is short for the feed-forward network.\\nFor simplicity, we omit the layer normalization.\\nWe implement MoE for Transformers by insert-\\ning MoE layers, which are composed of a set of\\nFFNs, into two neighboring Transformer blocks.\\nAt an MoE layer, for each input token, only a few or\\none expert will be activated, controlled by a gating\\nfunction g(·):\\nhl\\nt =\\nN\\nX\\ni=1\\ngi\\n\\x10\\nhl−1\\nt\\n\\x11\\nFFNi\\n\\x10\\nhl−1\\nt\\n\\x11\\n+ hl−1\\nt\\n,\\n(4)\\nwhere N is the total number of experts, and FFNi\\nis the i-th expert. Here, the gating function gi(·) is\\nsparse for computational efﬁciency. For simplicity,\\nwe omit the layer normalization.\\nTransformer Blocks\\nTransformer Blocks\\nThis\\ncar\\nis\\nRouter\\nExpert 1\\nDistilled \\nRouter\\nDistilling\\ncar\\nis\\nexpensive\\nTraining Stage 1\\nTraining Stage 2\\nExpert 2\\nExpert 3\\nTransformer Blocks\\nTransformer Blocks\\nThis\\ncar\\nis\\nDistilled Router (Frozen)\\nExpert 1\\ncar\\nis\\nexpensive\\nExpert 2\\nExpert 3\\nFigure 3: Illustration of two training stages in STABLEMOE. In training stage 1, we learn a routing strategy and\\ndistill it into a lightweight router. Then, we freeze the distilled router for stable routing in training stage 2.\\n3\\nMethod\\nSTABLEMOE has two training stages as illustrated\\nin Figure 3. In the ﬁrst training stage, we follow\\nthe learning-to-route paradigm and aim to learn\\na balanced and cohesive routing strategy. As the\\nrouting strategy is being learned, we synchronously\\ndistill it into a lightweight router decoupled from\\nthe backbone model. In the second training stage,\\nwe utilize the distilled router to determine the token-\\nto-expert assignment. The distilled router is frozen\\nin this stage to provide a stable routing strategy.\\nDuring inference, we also use the frozen distilled\\nrouter for consistent routing.\\n3.1\\nTraining Stage 1: Learn Routing Strategy\\nLet hl−1\\nt\\n∈Rd be the input representation of token\\nt and E ∈RN×d be the centroids of N experts.\\nFor each MoE layer, we assign each token to one\\nexpert FFN (Fedus et al., 2021; Lewis et al., 2021;\\nRoller et al., 2021). The assignment score is:\\nst,i = E⊤\\ni hl−1\\nt\\n,\\n(5)\\nwhere st,i is the assignment score between token\\nt and expert i, indicating their afﬁnity. We use a\\ngreedy assignment algorithm, i.e., sending each\\ntoken to the expert with the highest afﬁnity. Then,\\nwe calculate the expert FFN output as:\\nat = arg max\\ni\\n(st,i),\\n(6)\\nhl\\nt = σ (st,at) FFNat\\n\\x10\\nhl−1\\nt\\n\\x11\\n+ hl−1\\nt\\n,\\n(7)\\nwhere at is the expert index that token t is sent\\nto, and σ is the sigmoid gate (Lewis et al., 2021).\\nConsidering the sigmoid gate σ (st,at), if FFNat\\nis beneﬁcial for token t, optimizing the training\\nobjective (e.g., minimizing the cross-entropy loss\\nfor language modeling) will urge the gate to be\\ngreater; otherwise, the gate will tend to be smaller.\\nThe gate signal urges similar tokens to be assigned\\nto the same expert that is beneﬁcial to them, thus\\nproducing cohesive token-to-expert assignments.\\nBalance Loss\\nWe design a balance loss Lbal to\\navoid imbalanced assignments that will result in\\na high computational bottleneck in the MoE layer\\nand thus limit the computational efﬁciency:\\nLbal = α\\nN\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed(|Ai| −n)\\nn\\nX\\nt∈Ai\\nσ (st,i)\\n\\uf8f6\\n\\uf8f8,\\n(8)\\nwhere α is a hyper-parameter, Ai denotes the set\\nof tokens assigned to expert i, and n denotes the\\naverage number of tokens per expert. Intuitively, if\\nan expert is overloaded, the balance loss will urge\\nits assignment scores to be smaller. Otherwise, if an\\nexpert is unoccupied, the balance loss will increase\\nits assignment scores to capture more tokens.\\nDistilled Router\\nAs the routing strategy is be-\\ning learned, we synchronously distill it into a\\nlightweight router decoupled from the backbone\\nmodel to mimic the original routing strategy. Let X\\nbe the input sequence and ˆ\\nE be the distilled expert\\ncentroids, we use word embeddings D(·) to extract\\nthe routing features. We use the cross-entropy loss\\nas the distillation loss Ldis:\\nˆ\\nhl−1\\nt\\n= D(Xt),\\nˆ\\nst,i = ˆ\\nE⊤\\ni ˆ\\nhl−1\\nt\\n,\\n(9)\\nLdis = −\\nT\\nX\\nt=1\\nlog\\nexp (ˆ\\nst,at)\\nPN\\ni=1 exp (ˆ\\nst,i)\\n,\\n(10)\\nwhere ˆ\\nhl−1\\nt\\nis the distilled routing feature of token\\nt, ˆ\\nst,i is the distilled assignment score between\\ntoken t and expert i, and at is the expert index\\nthat token t is actually sent to. In practice, D(·)\\nMethods\\nAssignment Algorithm\\nGating Function\\nBalance Loss\\nSwitch Transformer\\nGreedy\\nsoftmax\\nYes\\nBASE Layer\\nAuction (Bertsekas, 1992)\\nsigmoid\\nNo\\nHash Layer\\nFixed Hashing\\n{0, 1}\\nNo\\nSTABLEMOE\\nTraining Stage 1\\nGreedy\\nsigmoid\\nYes\\nTraining Stage 2\\nFixed Routing\\nsigmoid\\nNo\\nTable 1: Comparison of three core elements among STABLEMOE and existing MoE-based Transformers.\\ncan also be other feature extractors such as CNNs\\nor Transformers (we investigate other variants of\\ndistilled routers in Section 4.4.3), but the word\\nembedding is the fastest one and achieves the best\\nperformance. At the end of training stage 1, we\\nfreeze all parameters for the distilled router (i.e.,\\nD(·) and ˆ\\nE) to prepare a stable routing strategy for\\ntraining stage 2 and the inference stage.\\nTraining Objective\\nIn training stage 1, the train-\\ning loss consists of the task loss, the balance loss,\\nand the distillation loss:\\nLS1 = Ltask + Lbal + Ldis.\\n(11)\\n3.2\\nTraining Stage 2: Learn with Stable\\nRouting Strategy\\nGiven frozen D(·) and ˆ\\nE, in training stage 2, we di-\\nrectly use them for a stable routing strategy. Keep-\\ning other processes the same as in training stage 1,\\nwe calculate the output of the MoE layer as follows:\\nˆ\\nhl−1\\nt\\n= D(Xt),\\nˆ\\nst,i = ˆ\\nE⊤\\ni ˆ\\nhl−1\\nt\\n,\\n(12)\\nˆ\\nat = arg max\\ni\\n(ˆ\\nst,i),\\n(13)\\nhl\\nt = σ (st,ˆ\\nat) FFNˆ\\nat\\n\\x10\\nhl−1\\nt\\n\\x11\\n+ hl−1\\nt\\n.\\n(14)\\nNotice that the sigmoid gate σ(·) still uses orig-\\ninal assignment score st,ˆ\\nat as input, so the gate\\nsignal can also be learned in training stage 2. Since\\nthe routing strategy has been ﬁxed in training stage\\n2, we no longer need the balance loss and distilla-\\ntion loss. Therefore, the training loss for training\\nstage 2 contains only the task loss:\\nLS2 = Ltask.\\n(15)\\n3.3\\nInference\\nDuring inference, we also use the frozen distilled\\nrouter for routing. The ﬁxed routing strategy, which\\nis consistent with training stage 2, makes informa-\\ntion learned in MoE layers be utilized more thor-\\noughly and thus leads to better performance.\\n3.4\\nComparison with Existing MoE Methods\\nWe compare three core elements, including the as-\\nsignment algorithm, the gating function, and the\\nbalance loss, among STABLEMOE and existing\\nMoE-based Transformers. In Table 1, we summa-\\nrize their differences.\\nAssignment\\nAlgorithm\\nSwitch\\nTransformer\\nand the training stage 1 in STABLEMOE simply\\nassign each token to the expert with the highest\\nafﬁnity.\\nBASE Layer adopts the auction algo-\\nrithm (Bertsekas, 1992) to ﬁnd a global balanced\\nassignment with the maximum afﬁnity sum. Hash\\nlayer and the training stage 2 in STABLEMOE have\\ntoken-level ﬁxed routing strategies, which have\\ngood stability.\\nGating Function\\nHash Layer uses a hard gating\\nfunction, which means an expert is either fully ac-\\ntivated or not activated, no any intermediate state.\\nSwitch Layer, BASE Layer, and STABLEMOE\\nhave soft gating functions, which can judge the\\nafﬁnity between a token and its target expert and\\ndetermine a proper ratio to use the expert. Soft gat-\\ning mechanisms also urge models to learn a more\\ncohesive token-to-expert assignment.\\nBalance Loss\\nBASE Layer and Hash Layer do\\nnot apply any balance losses. By contrast, Switch\\nTransformer and the training stage 1 in STABLE-\\nMOE design balance losses to control the balance\\nof the token-to-expert assignment.\\nIn summary, combing two training stages, STA-\\nBLEMOE has a stable, cohesive, and balanced rout-\\ning strategy, while the other three MoE methods\\ncannot meet them all simultaneously.\\n4\\nExperiments\\n4.1\\nTasks and Datasets\\nLanguage Modeling\\nFollowing (Lewis et al.,\\n2021) and Roller et al. (2021), we use the com-\\nbination of the corpora in RoBERTa (Liu et al.,\\nSize\\nModels\\n# Shared Params\\n# Expert Params\\nFLOPs\\nValid PPL\\nTest PPL\\nBase\\nStandard Transformer\\n124M\\nN/A\\n146B\\n23.02\\n22.58\\nLarger Transformer (deeper)\\n578M\\nN/A\\n610B\\n17.93\\n17.63\\nLarger Transformer (wider)\\n578M\\nN/A\\n610B\\n18.31\\n18.01\\nSwitch Transformer\\n124M\\n454M\\n160B\\n19.79\\n19.20\\nBASE Layer\\n124M\\n454M\\n160B\\n20.04\\n19.69\\nHash Layer\\n124M\\n454M\\n160B\\n19.63\\n19.25\\nSTABLEMOE\\n124M\\n454M\\n160B\\n19.28\\n18.93\\nLarge\\nStandard Transformer\\n355M\\nN/A\\n414B\\n18.86\\n18.19\\nSwitch Transformer\\n355M\\n3.22B\\n465B\\n16.62\\n16.21\\nBASE Layer\\n355M\\n3.22B\\n465B\\n16.36\\n15.75\\nHash Layer\\n355M\\n3.22B\\n465B\\n16.37\\n15.79\\nSTABLEMOE\\n355M\\n3.22B\\n465B\\n16.22\\n15.59\\nTable 2: Perplexity results of language modeling. We also report the training FLOPs, and the number of parameters\\nfor the shared backbone (# Shared Params) and the expert layers (# Expert Params). “N/A” denotes not applicable.\\nSTABLEMOE consistently outperforms other MoE methods under both the base and the large settings.\\n2019) and the English subset of the CC100 (Con-\\nneau et al., 2020) corpus. The corpus contains\\nabout 100B tokens, and we randomly sample 5M\\ntokens for validation and 20M tokens for test.\\nMultilingual Machine Translation\\nWe fol-\\nlow Wang et al. (2020) and Ma et al. (2020) to use\\na collection of parallel data in different languages\\nfrom the WMT datasets.1 The dataset contains 32.5\\nmillion parallel data for language pairs between En-\\nglish and other 9 languages, including French (Fr),\\nCzech (Cs), German (De), Finnish (Fi), Latvian\\n(Lv), Estonian (Et), Romanian (Ro), Hindi (Hi),\\nand Turkish (Tr). In our experiments, we combine\\nthe original parallel data with 180 million back-\\ntranslation data as described in (Ma et al., 2020)\\nand call the augmented dataset WMT for short.\\n4.2\\nExperimental Setup\\nWe conduct experiments based on fairseq2. All ex-\\nperiments are conducted on NVIDIA V100 GPUs\\nwith 32 GB memory.\\nLanguage Modeling\\nWe adopt the tokenizer of\\nGPT-2 (Radford et al., 2019), which uses byte-pair\\nencoding (Sennrich et al., 2016) with a vocabulary\\nsize of 50,257. We set up two settings for STABLE-\\nMOE, a base one and a large one. For both settings,\\nwe insert one MoE layer after the middle Trans-\\nformer block. We train the model for 60K steps in\\ntotal (6K for training stage 1 and 54K for training\\nstage 2). The dimension of the distilled routing fea-\\ntures is 50, which brings 2.51M extra parameters\\nfor routing. The balance factor α is set to 0.3. We\\n1http://www.statmt.org\\n2https://github.com/facebookresearch/fairseq\\nuse Adam (Kingma and Ba, 2015) with β1 = 0.9\\nand β2 = 0.98 as the optimizer. The rest of the\\nhyper-parameters are summarized in Appendix A.\\nMultilingual\\nMachine\\nTranslation\\nFollow-\\ning (Ma et al., 2020), we use the Sentence-\\nPiece (Kudo and Richardson, 2018) model to\\ntokenize sentences. The vocabulary is learned from\\nthe training set and consists of 64,000 tokens. We\\ninsert two MoE layers, one after the third encoder\\nblock and one after the third decoder block. We\\ntrain the model for 352K steps in total (30K for\\ntraining stage 1 and 322K for training stage 2).\\nThe dimension of the distilled routing features is\\nalso set to 50. The balance factor α is set to 0.3.\\nWe use Adam with β1 = 0.9 and β2 = 0.98 as the\\noptimizer. The rest of the hyper-parameters are\\nsummarized in Appendix B.\\n4.3\\nResults\\n4.3.1\\nLanguage Modeling\\nWe compare STABLEMOE with Switch Trans-\\nformer, BASE Layer, Hash Layer, and the stan-\\ndard Transformer. All MoE models have the same\\nnumber of shared parameters as the standard Trans-\\nformer. Under the base setting, in addition, we\\ncompare two larger dense Transformers that add\\nFFNs in a dense manner to achieve the same num-\\nber of total parameters as MoE models. The deeper\\nmodel stacks more FFNs, while the wider model\\nuses FFNs with a larger hidden size. The ﬂoating\\npoint operations (FLOPs) per sequence are proﬁled\\nby the torchproﬁle toolkit.\\nWe show the main results of language model-\\ning on the RoBERTa+cc100en corpus in Table 2.\\nModels\\n# Params\\nFLOPs\\nDe\\nRo\\nFr\\nCs\\nEt\\nHi\\nTr\\nFi\\nLv\\nAvg\\nStandard Transformer\\n77M\\n290B\\n39.8\\n36.0\\n32.5\\n29.1\\n27.2\\n24.5\\n23.6\\n21.8\\n20.3\\n28.31\\nLarger Transformer\\n90M\\n317B\\n40.6\\n36.9\\n33.7\\n29.8\\n27.8\\n25.4\\n24.6\\n22.2\\n20.9\\n29.10\\nSwitch Transformer\\n480M\\n317B\\n42.3\\n37.1\\n33.8\\n31.0\\n28.6\\n26.0\\n24.3\\n23.0\\n21.2\\n29.70\\nBASE Layer\\n480M\\n317B\\n42.6\\n37.8\\n34.2\\n31.0\\n29.0\\n26.9\\n25.1\\n23.2\\n21.6\\n30.16\\nHash Layer\\n480M\\n317B\\n42.7\\n37.0\\n34.6\\n31.3\\n28.7\\n26.5\\n23.9\\n23.1\\n21.7\\n29.94\\nSTABLEMOE\\n480M\\n317B\\n43.0\\n37.4\\n34.7\\n31.5\\n29.3\\n26.8\\n24.7\\n23.6\\n21.9\\n30.32\\nTable 3: X→En test BLEU on WMT. We also report the total number of parameters, and training FLOPs. STA-\\nBLEMOE outperforms other MoE-based Transformers across most languages.\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nWall Time (Hours)\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nValid Perplexity\\nStandard TRM\\nLarger TRM (deeper)\\nLarger TRM (wider)\\nBASE Layer\\nSwitch TRM\\nHash Layer\\nStableMoE (Ours)\\nFigure 4:\\nConvergence speed of different models.\\nTRM is a shorthand for Transformer.\\nUnder the base setting, STABLEMOE outperforms\\nexisting MoE methods on both the validation and\\nthe test sets by 0.3-0.8 perplexity. Compared with\\ndense models, STABLEMOE achieves about 3.7\\nlower perplexity than the standard Transformer,\\nand about 1.3 higher perplexity than the deeper\\nlarger model. Under the large setting, consistently,\\nSTABLEMOE outperforms the other MoE methods,\\nand achieves about 2.6 lower perplexity than the\\nstandard Transformer.\\nWe also compare the convergence speed of differ-\\nent models under the base setting. The results are\\nplotted in Figure 4, which takes the validation per-\\nplexity as y-axis and the training wall time as x-axis.\\nAlthough larger dense models achieve better valida-\\ntion perplexity at last, their training speed is quite\\nslow. With regard to the convergence speed, MoE-\\nbased Transformers usually exceed dense models.\\nFurther, among the MoE methods, STABLEMOE\\nhas the fastest convergence speed.\\n4.3.2\\nMultilingual Machine Translation\\nWe compare STABLEMOE with Switch Trans-\\nformer, BASE Layer, Hash Layer, the standard\\nTransformer, and a larger Transformer. All MoE-\\n16\\n32\\n64\\nNumber of Experts\\n18.5\\n19.0\\n19.5\\n20.0\\n20.5\\n21.0\\nValid PPL\\nBASE Layer\\nHash Layer\\nStableMoE\\nFigure 5:\\nComparison of MoE-based Transformers\\nwith different numbers of experts. Lower perplexity\\nindicates better performance.\\nbased models have the same number of shared pa-\\nrameters as the standard Transformer. Except the\\nstandard Transformer, the other models have the\\nsame FLOPs.\\nWe translate other languages to English (X→En)\\nand report the test BLEU on WMT in Table 3.\\nSTABLEMOE achieves the best average test BLEU\\namong the compared MoE methods. Keeping the\\nsame FLOPs, STABLEMOE outperforms the dense\\nmodel by 1.22 test BLEU. With the MoE technique,\\nwe expand the number of parameters by 523% and\\nthe FLOPs just increase by 9.3%.\\n4.4\\nAnalysis\\n4.4.1\\nEffects of Hyperparameters\\nOn top of the base setting of language modeling,\\nwe investigate different settings for the MoE layers\\nin STABLEMOE.\\nNumber of Experts\\nFigure 5 shows the results\\nof BASE Layer, Hash Layer, and STABLEMOE\\nwith different numbers of experts. As the num-\\nber of experts goes larger, the validation perplexity\\nof each model tends to further descend. Consis-\\ntently, STABLEMOE performs the best with dif-\\n3 Sublayers (454M)\\n10 Sublayers (1.51B)\\nNumber of Expert Sublayers (Parameters)\\n18.0\\n18.5\\n19.0\\n19.5\\n20.0\\nValid PPL\\nBASE Layer\\nHash Layer\\nStableMoE\\nFigure 6: Comparison of MoE models with different\\nnumbers of expert sublayers (i.e., number of parame-\\nters). Lower perplexity indicates better performance.\\nModels\\nValid PPL\\nSTABLEMOE (stacked, top)\\n19.55\\nSTABLEMOE (stacked, middle)\\n19.28\\nSTABLEMOE (stacked, bottom)\\n22.82\\nSTABLEMOE (scattered)\\n20.56\\nTable 4: Effects of the position of MoE layers. STA-\\nBLEMOE (scattered) scatters 3 MoE sublayers uni-\\nformly into the standard Transformer, while the others\\nstack 3 MoE sublayers together.\\nferent numbers of experts. In addition, it is worth\\nnoting that STABLEMOE with 16 experts outper-\\nforms BASE Layer with 32 experts, and STABLE-\\nMOE with 32 experts achieves a similar perplexity\\nto BASE Layer with 64 experts.\\nNumber of Expert Parameters\\nWe compare\\nMoE models with different numbers of expert pa-\\nrameters by setting different expert sublayers. Mod-\\nels with 3 and 10 expert sublayers have 454M and\\n1.51B expert parameters, respectively. From Fig-\\nure 6, we observe that more expert parameters bring\\nbetter performance, and STABLEMOE consistently\\nperforms the best under both settings.\\nPosition of MoE Layers\\nWe investigate the ef-\\nfect of the inserting position of the MoE layer. By\\ndefault, the MoE layer stacks 3 MoE sublayers and\\nis inserted after the L\\n2 -th Transformer block (mid-\\ndle). We also attempt to insert the MoE layer before\\nthe ﬁrst Transformer block (bottom), and after the\\nlast Transformer block (top). In addition, we also\\ninvestigate the effect if we scatter 3 MoE sublayers\\nuniformly into the standard Transformer, i.e., after\\nthe L\\n4 -th, 2L\\n4 -th, and 3L\\n4 -th blocks, respectively. As\\nshown in Table 4, among the above four settings,\\nModels\\nValid PPL\\nBASE Layer\\n20.04\\n+ Fixed Routing Strategy (Stage 2)\\n19.41 (0.63↓)\\nSTABLEMOE with Only Stage 1\\n19.48\\n+ Fixed Routing Strategy (Stage 2)\\n19.28 (0.20↓)\\nTable 5: Effects of the ﬁxed routing strategy.\\ninserting stacked MoE sublayers into the middle\\nposition allows STABLEMOE to achieve the best\\nperformance.\\nRatio Between Two Training Stages\\nWe inves-\\ntigate the balance point of the ratio between two\\ntraining stages in STABLEMOE. Given a ﬁxed num-\\nber of total steps, allocating more steps to training\\nstage 1 can help to learn and distill a better routing\\nstrategy. On the other hand, a larger ratio of train-\\ning stage 2 means longer stable training. Under the\\nbase setting of language modeling, we attempt to\\nallocate 6K, 15K, and 30K steps to training stage 1\\nand show the results in Table 6. We ﬁnd that if we\\nuse word embeddings as the distilled router, allo-\\ncating 6K steps (10% of the total steps) to training\\nstage 1 is a good balance point. We speculate that\\nthe word embedding is simple enough to be learned\\nfast, so longer stable training is more important to\\nachieve better performance.\\n4.4.2\\nEffects of the Fixed Routing Strategy\\nBased on the base setting of language modeling, we\\ndesign two experiments to investigate how much\\nperformance improvement the ﬁxed routing strat-\\negy can bring. On the one hand, we equip BASE\\nLayer with a stable routing strategy to address its\\nrouting ﬂuctuation problem. Speciﬁcally, as in\\nSTABLEMOE, we use word embeddings to distill\\nthe routing strategy of BASE Layer in the ﬁrst 6K\\ntraining steps, and freeze the distilled router for\\nstable routing in the remaining training. As shown\\nin Table 5, the ﬁxed routing strategy decreases the\\nvalidation perplexity of BASE Layer by 0.63. On\\nthe other hand, we attempt to disable the training\\nstage 2 in STABLEMOE and always train the model\\nas in training stage 1. As a result, the validation\\nperplexity of STABLEMOE becomes 0.20 higher\\nthan the full version that has a ﬁxed routing strat-\\negy. These two cases support that the ﬁxed routing\\nstrategy, which addresses the routing ﬂuctuation\\nproblem, can bring better performance for MoE-\\nbased Transformers.\\nIn addition, we visualize the ﬁxed routing strat-\\nDistilled Routers\\nStage 1 Steps\\nValid PPL\\nWord Embedding\\n6K (10%)\\n19.28\\nWord Embedding\\n15K (25%)\\n19.34\\nWord Embedding\\n30K (50%)\\n19.41\\nCNN\\n15K (25%)\\n19.39\\n1-layer Transformer\\n15K (25%)\\n19.42\\n2-layer Transformer\\n15K (25%)\\n19.38\\n3-layer Transformer\\n15K (25%)\\n19.65\\nTable 6: Results of different ratios of two training\\nstages and different variants of distilled routers.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nLast Fluctuation Step\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nCumulative Token Percentage\\n(20%, 40.9%)\\n(50%, 29.1%)\\n(80%, 15.4%)\\n(20%, 0.0%)\\n(50%, 0.0%)\\n(80%, 0.0%)\\nBASE Layer\\nStableMoE\\nFigure 7: Cumulative token percentage about the last\\nﬂuctuation step of tokens for BASE Layer and STA-\\nBLEMOE. Notice that training stage 2 of STABLEMOE\\ndoes not have routing ﬂuctuation compared with BASE\\nLayer.\\negy of STABLEMOE in Appendix C for reference.\\n4.4.3\\nVariants of Distilled Routers\\nIn Table 6, in addition to word embedding, we also\\ninvestigate four variants of the distilled router in-\\ncluding CNN and three Transformers with different\\nnumbers of layers. We allocate 15K steps to train-\\ning stage 1 for all of them. From the table, we ﬁnd\\nthat using word embedding achieves the best per-\\nformance, while the 3-layer Transformer does not\\nperform well. For the routing strategy distillation,\\nthe distilling signal from a 32-category classiﬁca-\\ntion objective may not be informative enough to\\nlearn a complex router. By contrast, it is more\\nsuitable for simpler routers. Therefore, we recom-\\nmend using word embedding, which is simple and\\neffective, as the distilled router in STABLEMOE.\\n4.4.4\\nAnalysis of Routing Fluctuations\\nWe compare the degree of routing ﬂuctuations be-\\ntween STABLEMOE and BASE Layer to show our\\nadvantage with regard to the routing stability. Dur-\\ning the 60K training steps, we examine the token-\\nto-expert assignment for tokens in the validation\\nset every 500 steps. For each token, we deﬁne the\\nlast ﬂuctuation step as the last step where its tar-\\nget expert is different from the ﬁnal step. We plot\\nthe cumulative token percentage about the last ﬂuc-\\ntuation step in Figure 7. For ease of reading, we\\nannotate the x-axis as the percentage it accounts\\nfor all training steps. From the ﬁgure, we ﬁnd\\nthat the routing ﬂuctuation problem is notable for\\nBASE Layer. By contrast, for STABLEMOE, there\\nis no routing ﬂuctuation in training stage 2 since\\nwe apply a ﬁxed routing strategy.\\n5\\nRelated Work\\nJacobs et al. (1991); Jordan and Jacobs (1994) pro-\\npose Mixture of Experts (MoE) to compute dif-\\nferent examples with independent expert modules.\\nShazeer et al. (2017) introduce MoE to build large-\\nscale language models based on LSTMs (Hochre-\\niter and Schmidhuber, 1997). Recently, as Trans-\\nformers become popular, many pieces of work\\ndesign MoE-version FFNs to build MoE-based\\nTransformers.\\nGShard (Lepikhin et al., 2021),\\nSwitch Transformer (Fedus et al., 2021), and BASE\\nLayer (Lewis et al., 2021) follow the learning-to-\\nroute paradigm and dynamically learn how to route\\neach input token to experts. However, we point out\\nthat these learning-to-route methods face the rout-\\ning ﬂuctuation problem. Hash Layer (Roller et al.,\\n2021) propose a non-parametric routing strategy,\\nwhich uses a pre-designed token-level hash table\\nto determine the token-to-expert assignment. The\\nstatic routing strategy will not ﬂuctuate, but the\\nrandomly determined hash table limits the upper\\nbound of its performance. Our work includes the\\nadvantages of learning-to-route methods to learn a\\nbalanced and cohesive routing strategy, and further\\naddresses the routing ﬂuctuation problem through\\napplying a frozen lightweight router that mimics\\nthe original routing strategy.\\n6\\nConclusion\\nIn this paper, we point out the routing ﬂuctuation\\nproblem that exists in previous learning-to-route\\nMoE methods. In order to address this problem,\\nwe propose STABLEMOE with two training stages.\\nWe ﬁrst learn a balanced and cohesive routing strat-\\negy and synchronously distill it into a lightweight\\nrouter decoupled from the backbone model. Then,\\nwe freeze the distilled router for a stable routing\\nstrategy in the remaining training. We validate STA-\\nBLEMOE on language modeling and multilingual\\nmachine translation. The results show that STA-\\nBLEMOE outperforms existing MoE methods in\\nterms of both convergence speed and performance.\\nReferences\\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\\nUnilmv2: Pseudo-masked language models for uni-\\nﬁed language model pre-training.\\nIn Proceedings\\nof the 37th International Conference on Machine\\nLearning, ICML 2020, volume 119 of Proceed-\\nings of Machine Learning Research, pages 642–652.\\nPMLR.\\nDimitri P. Bertsekas. 1992. Auction algorithms for net-\\nwork ﬂow problems: A tutorial introduction. Com-\\nputational Optimization and Applications, 1(1):7–\\n66.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell,\\nSandhini Agarwal,\\nAriel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot learn-\\ners. In Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Informa-\\ntion Processing Systems 2020.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\\nChristopher D. Manning. 2020.\\nELECTRA: pre-\\ntraining text encoders as discriminators rather than\\ngenerators.\\nIn 8th International Conference on\\nLearning Representations, ICLR 2020. OpenRe-\\nview.net.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\\ncross-lingual representation learning at scale.\\nIn\\nProceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics, ACL 2020,\\npages 8440–8451. Association for Computational\\nLinguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: pre-training of\\ndeep bidirectional transformers for language under-\\nstanding.\\nIn Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, NAACL-HLT 2019, Volume 1 (Long\\nand Short Papers), pages 4171–4186. Association\\nfor Computational Linguistics.\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\\nand Hsiao-Wuen Hon. 2019.\\nUniﬁed language\\nmodel pre-training for natural language understand-\\ning and generation.\\nIn Advances in Neural Infor-\\nmation Processing Systems 32: Annual Conference\\non Neural Information Processing Systems 2019,\\nNeurIPS 2019, pages 13042–13054.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\\nSwitch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity.\\nCoRR,\\nabs/2101.03961.\\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\\nshort-term memory. Neural Computing, 9(8):1735–\\n1780.\\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,\\nand Geoffrey E. Hinton. 1991. Adaptive mixtures of\\nlocal experts. Neural Computing, 3(1):79–87.\\nMichael I. Jordan and Robert A. Jacobs. 1994. Hier-\\narchical mixtures of experts and the EM algorithm.\\nNeural Computing, 6(2):181–214.\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization.\\nIn 3rd Inter-\\nnational Conference on Learning Representations,\\nICLR 2015.\\nTaku Kudo and John Richardson. 2018. Sentencepiece:\\nA simple and language independent subword tok-\\nenizer and detokenizer for neural text processing. In\\nProceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2018, pages 66–71. Association for Computational\\nLinguistics.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\\nGshard: Scaling giant models with conditional com-\\nputation and automatic sharding.\\nIn 9th Inter-\\nnational Conference on Learning Representations,\\nICLR 2021. OpenReview.net.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\\nSimplifying training of large, sparse models.\\nIn\\nProceedings of the 38th International Conference\\non Machine Learning, ICML 2021, volume 139 of\\nProceedings of Machine Learning Research, pages\\n6265–6274. PMLR.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized BERT pretraining ap-\\nproach. CoRR, abs/1907.11692.\\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi,\\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\\nAlexandre Muzio, Akiko Eriguchi, Saksham Sing-\\nhal, Xia Song, Arul Menezes, and Furu Wei. 2020.\\nXLM-T: scaling up multilingual machine translation\\nwith pretrained cross-lingual transformer encoders.\\nCoRR, abs/2012.15547.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nblog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020.\\nExploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1–67.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\\nand Jason Weston. 2021.\\nHash layers for large\\nsparse models. CoRR, abs/2106.04426.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural machine translation of rare words with\\nsubword units. In Proceedings of the 54th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2016. The Association for Computer\\nLinguistics.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\\nAndy Davis, Quoc V. Le, Geoffrey E. Hinton, and\\nJeff Dean. 2017.\\nOutrageously large neural net-\\nworks: The sparsely-gated mixture-of-experts layer.\\nIn 5th International Conference on Learning Repre-\\nsentations, ICLR 2017. OpenReview.net.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems 30: Annual Conference on Neural\\nInformation Processing Systems 2017, pages 5998–\\n6008.\\nYiren Wang, ChengXiang Zhai, and Hany Hassan.\\n2020.\\nMulti-task learning for multilingual neural\\nmachine translation.\\nIn Proceedings of the 2020\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, EMNLP 2020, pages 1022–1034.\\nAssociation for Computational Linguistics.\\nAppendix\\nA\\nHyper-parameters for Language\\nModeling\\nThe hyper-parameters of STABLEMOE under the\\nbase and the large settings for language modeling\\nare summarized in Table 7.\\nHyper-parameters\\nBase\\nLarge\\nNumber of Experts\\n32\\n64\\nNumber of MoE Layers\\n1\\n1\\nSublayers per Expert\\n3\\n6\\nEmbedding & Hidden Size\\n768\\n1024\\nFFN Inner Hidden Size\\n3072\\n4096\\nNumber of Attention Heads\\n12\\n16\\nNumber of Transformer Blocks\\n12\\n24\\nSequence Length\\n1024\\n1024\\nBatch Size\\n512K Tokens 512K Tokens\\nOptimizer\\nAdam\\nAdam\\nMaximum Learning Rate\\n6e-4\\n3e-4\\nLearning Rate Scheduler\\nLinear Decay Linear Decay\\nTotal Steps\\n60K\\n60K\\nWarm-up Steps\\n2K\\n2K\\nGradient Clip Norm\\n0.1\\n0.1\\nDropout\\n0\\n0\\nTable 7: Hyper-parameters of STABLEMOE under the\\nbase and the large settings for language modeling.\\nB\\nHyper-parameters for Multilingual\\nMachine Translation\\nThe hyper-parameters of STABLEMOE for mul-\\ntilingual machine translation are summarized in\\nTable 8.\\nNumber of Experts\\n32\\nNumber of MoE Layers\\n2\\nSublayers per Expert\\n3\\nEmbedding & Hidden Size\\n512\\nFFN Inner Hidden Size\\n2048\\nNumber of Attention Heads\\n8\\nNumber of Transformer Encoder Blocks\\n6\\nNumber of Transformer Decoder Blocks\\n6\\nMaximum Sequence Length\\n256\\nMaximum Batch Size\\n512K Tokens\\nOptimizer\\nAdam\\nMaximum Learning Rate\\n5e-4\\nLearning Rate Scheduler\\nInvSqrt\\nTotal Steps\\n352K\\nWarm-up Steps\\n4K\\nGradient Clip Norm\\n0.1\\nDropout\\n0.1\\nAttention Dropout\\n0\\nLabel Smoothing\\n0.1\\nTable 8: Hyper-parameters of STABLEMOE for multi-\\nlingual machine translation.\\nExperts\\nMost Frequent Tokens\\nDescriptions\\n5\\nmy, his, her, year, years, day, life, week, family, days\\npossessive case & time units\\n6\\nwith, at, from, about, them, need, want, him, against, using\\nprepositions & objective case\\n11\\nthat, ?, !, which, )., .\", That, \"., .), !!, ?\", !!!, :), Â, !\", ?, !, !),\\nconjunctions & punctuations\\n12\\none, what, some, any, two, many, $, use, 2, 1\\nnumerals\\n13\\ninformation, support, experience, service, data, services, money, access, research\\nnouns about technologies\\n17\\nworld, government, state, country, community, city, 2018, United, US, law\\nnouns about politics\\n22\\nright, business, high, free, important, public, big, top, hard, small\\nadjectives\\n27\\ntime, work, home, place, care, water, area, health, job, car\\nnouns about the daily life\\n29\\ning, a, ed, in, er, on, o, e, as, es, an, al, en, am, it, is, ie, os, le\\nsufﬁxes\\n30\\nyou, we, they, there, It, We, here, You, ve, ’ve\\npronouns\\n31\\nand, or, by, when, after, through, before, while, And, until\\nconjunctions\\nTable 9: The most frequent tokens assigned to each expert in the validation set. We present several representative\\nexperts. Tokens assigned to the same expert usually share some common features.\\nC\\nVisualization of the Fixed Routing\\nStrategy of STABLEMOE\\nWe visualize the ﬁxed routing strategy of STABLE-\\nMOE in Table 9. On the validation set, for each\\nexpert, we demonstrate the most frequent tokens\\nassigned to it along with a text that describes their\\ncommon features. We ﬁnd that tokens assigned to\\nthe same expert usually share some common fea-\\ntures, e.g., Expert 22 captures adjectives and Expert\\n31 captures conjunctions. These cases show good\\ncohesiveness of the token-to-expert assignment in\\nSTABLEMOE.\\n', 'source_name': 'StableMoE: Stable Routing Strategy for Mixture of Experts', 'source_url': 'https://arxiv.org/abs/2204.08396'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DeepSeekMoE_NOTES.pdf #61\n",
      "{'content': 'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts \\nLanguage Models \\nMain Idea: traditional top-k MoE experts acquire non-overlapping and focused knowledge, \\ncreating challenges for expert specialization. DeepSeekMoE plans on architectural changes to \\nenforce expert specialization through expert segmentation and isolating experts as shared ones \\nto capture common/overlapping knowledge. \\n3 versions of DeepSeekMoE are trained (in total # of parameters): \\n- \\n2B \\n- \\n16B \\no Can run on a single GPU with 40GB of memory. \\no Experimented with SFT to create an instruction-tuned chat model. \\n- \\n145B \\no Largest model trained. \\n \\n2 Potential Issues of Traditional (top-k) MoE: \\n- \\nKnowledge hybridity \\no Current MoE models have a low number of experts (8 or 16). This division assigns \\neach expert to a diverse part of the data, so the parameters are not used so \\nefficiently (there is more sparsity in the data possible than reflected by the \\nnumber of experts). \\n- \\nKnowledge redundancy \\no Experts may benefit from common knowledge, but since they are isolated, some \\nexperts might end up learning the same information, causing redundancy in their \\nparameters. \\nSolutions Proposed by DeepSeekMoE \\n- \\nFine-grained Expert Segmentation \\no Segment experts into a finer grain by splitting the FFN hidden dimension. More \\nexperts are also activated (increase the number of experts while maintaining the \\nnumber of total and active parameters). \\no More flexibility on which parameters of the experts to use – introduce sparsity \\nwithin each expert – while keeping computational costs constant. \\n- \\nShared expert isolation \\no Isolate certain experts to serve as shared experts, which are always activated. The \\ngoal is for these experts to retain the common knowledge between experts, \\navoiding parameter redundancy. \\n▪ Leads to parameter-efficiency + increased specialization. \\n \\nArchitecture \\n- \\nAs mentioned above, DeepSeekMoE incorporates two new strategies on top of the \\ngeneric MoE architecture: \\no Fine-grained expert segmentation \\n▪ Not just simply adding more experts but keeping the number of active and \\nthe number of total parameters the same while doing so. \\n▪ A small number of experts combined with a low number of activated \\nexperts per input makes experts learn a diverse amount of knowledge \\nwhen what we want is specialization. \\n▪ To solve this, DeepSeekMoE divides the expert’s weights (more \\nspecifically, the FFN hidden dimension) into m segments, creating another \\nlevel of experts. This allows for a scaling of m in the number of experts (if \\nm is 8, the total number of experts will be scaled by 8, for example). \\n• mN possible expert combinations vs N possible combinations. \\n▪ This allows for a more flexible combination of experts, since the router will \\nnot only pick specific experts, but specific segments within experts. \\n▪ This allows for a greater number of experts to be activated without \\nincreasing computational costs. \\no Shared expert isolation \\n▪ Experts in conventional MoE are isolated. This means that if experts have \\noverlap in knowledge in the data fed to them, this will be learned \\nindependently, so repeated parameters will exist for the same \\ninformation, bringing parameter inefficiency. \\n▪ DeepSeekMoE has shared experts – experts that are always activated – \\nwhich have the goal of capturing this common knowledge so there is no \\nparameter redundancy. \\n▪ The number of shared experts is Ks. To keep computational costs, the \\nnumber of routed experts will then decrease to mN-Ks and the nonzero \\ngates (segment activations) will be mK-Ks. \\no Balance loss \\n▪ An expert-level and a device-level balance loss are used, with more \\nemphasis/weight on the device-level loss. \\n \\nExperiments (2B parameter model) \\n- \\nSubstitute all FFNs by an MoE layer. \\n- \\n9 Transformer blocks with hidden dimension of 1280. \\n- \\nRandom initialization. \\n- \\n16 experts with 4 segments each (64 total expert segments), with 1 shared segment. \\n- \\nComputation equivalent of top-k with k=2. \\n- \\n2B parameter model, 0.3B active parameters. \\n- \\nTraining of 100B tokens with 2k batch size. \\n- \\nNo dropout due to abundance of data used. \\n- \\nBaselines: \\no Dense – equivalent to top-1 routing (~0.2B active parameters) \\no Switch – equivalent to top-1 routing (~0.2B active parameters) \\no Hash Layer – equivalent to top-1 routing (~0.2B active parameters) \\no GShard \\n- \\nResults \\no Switch and Hash Layer perform better than Dense (with same number of active \\nparameters but more total parameters). \\no GShard performs slightly better than Switch (with more active parameters). \\no DeepSeekMoE performs significantly better than GShard, with the same number \\nof active and total parameters. \\no DeepSeekMoE closely aligns with the upper bound of MoE models (dense with \\nsame number of total parameters) (at least on the 2B total parameters scale when \\ntraining with 100B tokens). \\n▪ DeepSeekMoE 2B performs comparably to GShard 2.9B (1.5x the expert \\nsize) (the advantages increase when scaling to 13.3B and 19.8B, \\nrespectively). \\n▪ DeepSeekMoE 2B achieves comparable performance to Dense with FFNs \\nscaled by 16 (same number of total parameters, 16 is number of experts \\nper layer used). \\no Ablation studies reassure the positive effects brought by fine-grained expert \\nsegmentation and shared expert isolation. \\n▪ Additionally, the number of shared experts (1,2 and 4 tested with 64 total \\nexperts) did not seem to make much difference. A ratio of 1:3 (shared/total \\nactivated experts) is used when scaling the architecture. \\no Expert specialization \\n▪ DeepSeekMoE was more sensitive to disabling the top-k experts, showing \\nthat there is less common knowledge between experts, thus less \\nredundancy. \\n▪ Shared experts are irreplaceable in DeepSeekMoE, that is, substituting a \\nshared expert by a not-shared expert results in a significant drop in \\nperformance. \\n▪ DeepSeekMoE can acquire knowledge more accurately and efficiently. \\nEven using only 4 active experts (equivalent to top-1 routing), \\nDeepSeekMoE performs similarly to GShard. \\n• When using this setting of 4 active experts at training time, and not \\nonly at inference time, DeepSeekMoE outperforms GShard even \\nwith half of the number of active expert parameters. \\nDeepSeekMoE 16B \\n- \\nScaling up of the architecture to a model with 16B total parameters, trained on 2T tokens \\n(same number of training tokens as Llama2-7B). \\n- \\n28 Transformer blocks, all FFNs are substituted by an MoE layer except for the first one \\n(because the first layer takes longer to converge if the FFN is substituted by an MoE layer). \\n- \\nEach MoE layer has 2 shared and 64 routed experts. Each FFN is divided into 4 experts. \\n- \\n8 experts per layer activated per input (2 shared, 6 routed), corresponding to 2.8B active \\nparameters. \\n- \\nSimilar training setting to DeepSeekMoE 2B. \\n- \\nCompared to DeepSeek 7B (its dense counterpart): \\no DeepSeekMoE 16B, with around 40% of active computation at inference, \\nperforms comparably to DeepSeek 7B. \\no DeepSeekMoE 16B performs especially well in language modeling tasks. \\n▪ This indicates that scaling up the total FFN parameters helps with \\nmemorization. \\no DeepSeekMoE 16B does not perform well in multiple-choice questions. \\n▪ A possible explanation for this can be due to the attention parameters. The \\nnumber of attention parameters are thought of as being crucial for MC \\ntasks, and the MoE version has around 5x less attention parameters than \\nits dense counterpart (0.5B vs 2.5B). \\n- \\nCompared to Llama2-7B: \\no DeepSeekMoE 16B, with about 40% of Llama2-7B activations at inference, \\noutperforms it at most baselines (MC tasks like MMLU are the exceptions). \\no DeepSeekMoE 16B is stronger at math and reasoning tasks (strengths of Llama2-\\n7B) probably due to the distribution of the dataset used for training. \\no Despite being trained on less English text, DeepSeekMoE 16B achieves equal or \\nbetter performance at English understanding and knowledge-intensive tasks. \\n▪ Consistent with MoE’s advantage in memorization due to increase total \\nparameter count compared to dense. \\no On Hugging Face’s Open LLM leaderboard (collection of evaluation tasks), \\nDeepSeekMoE 16B significantly outperforms models of the same size in terms of \\nactive parameters and achieves comparable performance to Llama2-7B. \\nChat Alignment for DeepSeekMoE 16B (SFT/Instruction-Tuning) \\n3 models are compared in this section, all trained on the same data: \\n- \\nLlama2 SFT 7B – Llama 2 instruction-tuned independently from its chat version, to control \\nfor the training data. \\n- \\nDeepSeek Chat 7B. \\n- \\nDeepSeekMoE 16B Chat – has around 40% of active computations compared to the other \\nmodels used in this section. \\nResults: \\n- \\nThe MoE variant achieves comparable performance to the dense models in language \\nunderstanding and reasoning, machine reading comprehension, and mathematical and \\nknowledge-intensive tasks. \\n- \\nThe MoE variant performs significantly better at code generation. \\n- \\nThe gap in multiple-choice questions still exists but is narrowed. \\nScaling DeepSeekMoE to 145B Total Parameters \\n- \\nTrained on 245B tokens (will probably be scale dup in the future, so this can be seen more \\nas a baseline). \\n- \\n62 Transformer blocks, all FFNs substituted by an MoE layer except the first one. \\n- \\n4 shared experts and 128 routed experts per MoE layer. \\n- \\nEach expert is 1/8th the size of a standard FFN (different than the ratio used for the smaller \\n2B and 16B models, which was 1/4th). \\n- \\nAt inference, the 4 shared experts and 12 routed experts are activated. \\n- \\nAround 22.2B active parameters. \\nResults: \\n- \\n3 additional models were trained for comparison, using the same training corpus and \\nhyperparameters: \\no DeepSeek 67B (dense) \\no GShard 137B (GShard architecture trained on the same data) \\no DeepSeekMoE 142B (half-activated) \\n▪ Uses half of the activations of DeepSeekMoE 145B – 2 shared experts, 6 \\nrouted experts. \\n- \\nWith similar number of active and total parameters, the MoE 145B variant significantly \\noutperforms GShard. \\n- \\nWith only 28.5% of its active computations, the 145B MoE model reaches comparable \\nperformance to DeepSeek 67B. \\no Exhibits strong performance in language understanding and knowledge-intensive \\ntasks but struggles in multiple-choice (consistent with the 16B MoE model \\nperformance). \\n- \\nDespite having only half of the activated parameters, the 142B version is not too far \\nbehind from the 145B fully activated version, still matches the performance of DeepSeek \\n67B (with around 18.2% of its computations at inference) and easily beats GShard 137B. \\n \\nMy takeaways: \\n- \\nDeepSeekMoE has its 16B version with 7 checkpoints released to HF. This could add to a \\npotential exploration of how experts in MoE specialize. \\n', 'source_name': 'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DeepSeekMoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MixtureOfTokens_NOTES.pdf #62\n",
      "{'content': 'Mixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation \\nMain Idea: traditional sparse MoE has some drawbacks - the router decision is discrete, making \\nit not fully differentiable for training, which can cause training instabilities; the load balancing \\nbetween experts is also not guaranteed, which leads to the need to apply methods such as using \\nan auxiliary loss or adding random noise to training inputs, which do not guarantee solving this \\nchallenge. MoT tries to improve on the traditional MoE architecture, providing a fully \\ndifferentiable strategy which automatically results in load balancing. \\nOBS: MoT is compatible with both masked and causal LLM training and inference (fill missing \\ntokens and autoregressive language modeling). \\n \\nIssues with MoE \\n- \\nThe router is discrete, which causes training instabilities since small changes in the input \\nmay cause big changes in the gradient (if the small change in the input results in a \\ndifferent router selection). This makes the training process not fully differentiable. Using \\na weighted average of the selected experts to form the outputs seems to help with this \\nbut is not an optimal solution. \\n- \\nThere is no guarantee that the MoE will distribute loads evenly among experts. A capacity \\nfactor (CF) can be set for minimizing token dropping, but this does not help with load \\nbalancing and increases memory requirements. This prompts the use of an auxiliary loss, \\nwhich is, again, not ideal. \\n- \\nMost studies done with MoE are not compatible with autoregressive decoding (take soft \\nMoE for example). \\n \\nMoT Algorithm \\n1. The first step is to pass the input tokens (all of them) through a router/controller (linear \\nlayer) and apply a SoftMax to get the token importance scores for each expert. \\na. 𝐼𝑚𝑝𝑤𝑒𝑖𝑔ℎ𝑡𝑠= 𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝐿𝑖𝑛𝑒𝑎𝑟(𝑡𝑜𝑘𝑒𝑛𝑠)) \\ni. Where 𝐼𝑚𝑝𝑤𝑒𝑖𝑔ℎ𝑡𝑠 is a matrix with each input token as a row and each \\nexpert as a column.  \\nii. Each column sums up to 1, so each expert has its own designated router. \\n2. Then, the tokens are mixed by their importance weights, forming a mix of tokens for each \\nexpert. \\na. So, the token mix passed to expert i is -> ∑𝑡𝑜𝑘𝑒𝑛𝑡∗𝑖𝑚𝑝𝑤𝑒𝑖𝑔ℎ𝑡𝑠𝑖\\n𝑇\\n𝑡\\n \\n3. After having the mix of tokens for each expert, the next step is to pass the expert’s mix of \\ntokens through its respective FFN. \\n4. To obtain the final output for a specific expert, we need to scale the expert output based \\non the importance for each token in its mix: \\na. Final_output (for token t and a given expert) = expert output * imp_weight for \\ntoken t \\n5. The final output for a given token t is then the sum of all the final outputs of each expert \\nfor that token t. \\nWhen doing this process for decoding, having to recompute each token multiple times seems \\ninefficient, so a strategy to group tokens needs to be employed. \\n- \\nThe authors group tokens according to their position in a sequence (1st tokens grouped \\ntogether, 2nd tokens grouped together, etc.). This way, for a given batch, each sequence \\ncan be computed in parallel, token-by-token. \\n \\nExperiments \\n- \\nThe authors compare a GPT-like model to a MoT model (Transformer architecture with \\nall feed-forward layers replaced with MoT layers). \\n- \\nThe MoT model shows promising pre-training results, achieving the vanilla Transformer’s \\nfinal loss in 1/4th of the training steps and 1/3 of the training time. \\n \\nMy takeaways: \\n- \\nIntuition about the MoT algorithm: \\no Calculating the importance vector of the input tokens is done at the expert level. \\nThis means that the input tokens are passed through the router for expert n, which \\nwill give the importance weights of each token for that expert. This is calculating \\nhow the mix of tokens which is passed to each expert will be weighted. \\no MoT sounds like Expert Choice Routing in terms of the expert choosing the \\nimportance to give to each token (in Expert Choice, however, the method used to \\ndetermine if the token will be sent to an expert is given by the affinity or \\nimportance weight given by the expert, while in MoT every token is considered by \\nevery expert). \\no To get a final output, each token looks at the output of each expert, and considers \\nhow much importance to give to each expert’s output based on the importance \\nthe expert gave it. \\n- \\nAlthough it is possible to do natural language generation with this approach, it seems to \\nbe very inefficient since generation of tokens cannot be done in parallel for all input \\ntokens in the same sequence, while this approach takes all input tokens in the same \\nsequence in consideration during inference and performs a forward pass in all experts. \\no Highly impractical in its given form for language generation. The design presented \\nonly works at a batch-level. \\no These limitations create the need for future research to make this approach \\npractical. \\n- \\nFor now, this is only a training strategy, but does not work for inference. \\n', 'source_name': 'Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "StableMoE_NOTES.pdf #63\n",
      "{'content': \"StableMoE: Stable Routing Strategy for Mixture of Experts \\nMain Idea: the goal of this paper is to solve the sample efficiency issue of training MoEs. The \\nexpert selection for a specific input may change during training, causing the weights of experts \\nto be updated that will not be using it in inference – suboptimal training with experts being \\nupdated based on an input space that is not attributed to them during inference (routing \\nfluctuation problems). \\n \\nProblem \\nBy observing the routing fluctuation issue when using BASE layers, it was observed that: \\n- \\n40.9% of tokens are unstable (inconsistent in routing) after 20% of the training steps. \\n- \\nthis number decreases to 29.1% after 50% of training, and to 15.4% after 80% of training. \\nSolution \\nSplit training into 2 parts: \\n- \\nStage 1 \\no start by training a router (with a new balance loss introduced – not much different, \\nsimply penalizes the loss in the case of expert overloading) and using sigmoid \\ninstead of SoftMax (sigmoid is thought to propagate the signal better) for \\ndetermining the assigned expert’s weight. \\no During stage 1 of training, the router is distilled. This distillation process is \\naccounted for in the training loss: \\n▪ Total loss = task loss + balance loss + distillation loss. \\n▪ The components that are important for this distillation are the experts’ \\ncentroids and the routing feature of the token t (distilled through a word \\nembedding). \\no At the end of training stage 1, the parameters for the distilled router (which were \\nbeing trained synchronously) (these parameters are the word embeddings for the \\ntokens and the experts’ centroids) are frozen and kept frozen for the remainder \\nof training (which consists of stage 2). \\n- \\nStage 2 \\no In stage 2 of training, the router is distilled and stable, so only the task loss is \\nneeded. The sigmoid gate is kept so the gating signal is still being trained (I believe \\nthis is only for the actual weights given to each expert at inference).  Everything \\nelse remains the same. \\n \\nResults \\nThe StableMoE method is compared to a dense Transformer, a Base MoE, a Hash Layer MoE and \\nSwitch Transformer at a base and a large setting (454M and 3.22B total parameters, respectively). \\n- \\nStableMoE outperforms all others in all settings and shows robustness in scaling both \\nmodel parameters and number of experts. \\n- \\nModels improve perplexity with a higher number of experts (tested up to 64), given the \\nsame model size. \\n- \\nStacking MoE layers in-between Transformer blocks was shown to have the best results \\nin comparison to sticking them in other positions. \\n \\nMy takeaways:  \\n- \\nAt first glance, it seems logical that the routing fluctuation issue presented will result in \\nsuboptimal training, so traditional MoEs leave room for improvement in terms of training \\nefficiency, especially in early stages of training. \\n- \\nThe part which seems to help the most is the routing distillation. The idea is to learn \\nparameters to learn optimal expert centroids and token embeddings. Once this is learned, \\nthe router can be frozen to keep stability during training. \\n- \\nThe paper provides evidence that scaling the number of experts with StableMoE leads to \\nimproved performance not only in pre-training but also in downstream tasks like \\nmultilingual machine translation, as evidenced by higher average test BLEU scores \\ncompared to other models. This indicates that the advantages of scaling are not confined \\nto pre-training. However, the paper doesn't provide an extensive evaluation on a variety \\nof downstream tasks or fine-tuning with different amounts of data, which would be \\nvaluable for comprehensively understanding the scalability and efficiency of the model in \\nvaried contexts. \\n\", 'source_name': 'StableMoE: Stable Routing Strategy for Mixture of Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/StableMoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BASE_layers_NOTES.pdf #64\n",
      "{'content': 'BASE Layers: Simplifying Training of Large, Sparse Models \\nMain Idea: introduces a new routing approach that approaches the problem as a linear \\nassignment. This ensures load balancing without the need for auxiliary losses or adjusting CF. \\nBASE also shows that a single expert/MoE layer can be effective. \\n- \\nMakes use of top-1 routing like Switch. \\n- \\nThe linear assignment problem is designed to maximize token-expert affinities and has \\nthe constraint of balanced loads. \\nBASE Algorithm \\n1. Compute token-expert score for all experts. \\n2. Solve the linear assignment problem. \\na. Goal - Maximize token-expert affinity. \\nb. Constraint – ensure balanced loads to experts at a batch-level. \\n3. Route tokens to experts. \\n4. Compute the expert scores as a weighted sum based on the routing weights. \\na. Top-2 routing is used at training. \\n5. Return the output to the original worker. \\nThis approach is only used during training, as during test time the strategy of top-1 routing \\nwithout load balancing is taken. \\n \\nResults \\n- \\nHaving a single BASE layer in the network can be effective. \\n- \\nExpert layers are robust to changes in the expert-shared parameters ratio and the \\nposition(s) of the layer in the network. \\n- \\nExploration of which inputs are assigned to each expert shows the same specialization \\npatterns of other works: experts specialize on simple input patterns related to semantics \\nand syntax. \\n \\n', 'source_name': 'BASE Layers: Simplifying Training of Large, Sparse Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/BASE_layers_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "DSelect_k.pdf #65\n",
      "{'content': 'DSelect-k: Differentiable Selection in the Mixture of\\nExperts with Applications to Multi-Task Learning\\nHussein Hazimeh1, Zhe Zhao1, Aakanksha Chowdhery1, Maheswaran Sathiamoorthy1\\nYihua Chen1, Rahul Mazumder2, Lichan Hong1, Ed H. Chi1\\n1Google, {hazimeh,zhezhao,chowdhery,nlogn,yhchen,lichan,edchi}@google.com\\n2Massachusetts Institute of Technology, rahulmaz@mit.edu\\nAbstract\\nThe Mixture-of-Experts (MoE) architecture is showing promising results in improv-\\ning parameter sharing in multi-task learning (MTL) and in scaling high-capacity\\nneural networks. State-of-the-art MoE models use a trainable “sparse gate” to\\nselect a subset of the experts for each input example. While conceptually appealing,\\nexisting sparse gates, such as Top-k, are not smooth. The lack of smoothness can\\nlead to convergence and statistical performance issues when training with gradient-\\nbased methods. In this paper, we develop DSelect-k: a continuously differentiable\\nand sparse gate for MoE, based on a novel binary encoding formulation. The gate\\ncan be trained using ﬁrst-order methods, such as stochastic gradient descent, and\\noffers explicit control over the number of experts to select. We demonstrate the\\neffectiveness of DSelect-k on both synthetic and real MTL datasets with up to\\n128 tasks. Our experiments indicate that DSelect-k can achieve statistically sig-\\nniﬁcant improvements in prediction and expert selection over popular MoE gates.\\nNotably, on a real-world, large-scale recommender system, DSelect-k achieves\\nover 22% improvement in predictive performance compared to Top-k. We provide\\nan open-source implementation of DSelect-k1.\\n1\\nIntroduction\\nThe Mixture of Experts (MoE) [14] is the basis of many state-of-the-art deep learning models. For\\nexample, MoE-based layers are being used to perform efﬁcient computation in high-capacity neural\\nnetworks and to improve parameter sharing in multi-task learning (MTL) [33, 22, 21]. In its simplest\\nform, a MoE consists of a set of experts (neural networks) and a trainable gate. The gate assigns\\nweights to the experts on a per-example basis, and the MoE outputs a weighted combination of the\\nexperts. This per-example weighting mechanism allows experts to specialize in different partitions of\\nthe input space, which has the potential to improve predictive performance and interpretability. In\\nFigure 1 (left), we show an example of a simple MoE architecture that can be used as a standalone\\nlearner or as a layer in a neural network.\\nThe literature on the MoE has traditionally focused on softmax-based gates, in which all experts\\nare assigned nonzero weights [17]. To enhance the computational efﬁciency and interpretability of\\nMoE models, recent works use sparse gates that assign nonzero weights to only a small subset of\\nthe experts [1, 33, 29, 21]. Existing sparse gates are not differentiable, and reinforcement learning\\nalgorithms are commonly used for training [1, 29]. In an exciting work, [33] introduced a new sparse\\ngate (Top-k gate) and proposed training it using stochastic gradient descent (SGD). The ability to\\ntrain the gate using SGD is appealing because it enables end-to-end training. However, the Top-k gate\\nis not continuous, which can lead to convergence issues in SGD that affect statistical performance (as\\nwe demonstrate in our experiments).\\n1https://github.com/google-research/google-research/tree/master/dselect_k_moe\\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\\narXiv:2106.03760v3  [cs.LG]  31 Dec 2021\\nFigure 1: (Left): An example of a MoE that can be used as a standalone learner or layer in a neural\\nnetwork. Here “Ei” denotes the i-th expert. (Right): A multi-gate MoE for learning two tasks\\nsimultaneously. “Task i NN” is a neural network that generates the output of Task i.\\nIn this paper, we introduce DSelect-k: a continuously differentiable and sparse gate for MoE. Given\\na user-speciﬁed parameter k, the gate selects at most k out of the n experts. This explicit control\\nover sparsity leads to a cardinality-constrained optimization problem, which is computationally\\nchallenging. To circumvent this challenge, we propose a novel, unconstrained reformulation that\\nis equivalent to the original problem. The reformulated problem uses a binary encoding scheme to\\nimplicitly enforce the cardinality constraint. We demonstrate that by carefully smoothing the binary\\nencoding variables, the reformulated problem can be effectively optimized using ﬁrst-order methods\\nsuch as SGD. DSelect-k has a unique advantage over existing methods in terms of compactness and\\ncomputational efﬁciency. The number of parameters used by DSelect-k is logarithmic in the number\\nof experts, as opposed to linear in existing gates such as Top-k. Moreover, DSelect-k’s output can be\\ncomputed efﬁciently via a simple, closed-form expression. In contrast, state-of-the-art differentiable\\nmethods for stochastic k-subset selection and Top-k relaxations, such as [26, 40]2, require solving an\\noptimization subproblem (for each input example) to compute the gate’s output.\\nDSelect-k supports two gating mechanisms: per-example and static. Per-example gating is the\\nclassical gating technique used in MoE models, in which the weights assigned to the experts are\\na function of the input example [14, 33]. In static gating, a subset of experts is selected and the\\ncorresponding weights do not depend on the input [29]. Based on our experiments, each gating\\nmechanism can outperform the other in certain settings. Thus, we study both mechanisms and\\nadvocate for experimenting with each.\\nMTL is an important area where MoE models in general, and our gate in particular, can be useful.\\nThe goal of MTL is to learn multiple tasks simultaneously by using a shared model. Compared to the\\nusual single task learning, MTL can achieve better generalization performance through exploiting\\ntask relationships [4]. One key problem in MTL is how to share model parameters between tasks [31].\\nFor instance, sharing parameters between unrelated tasks can potentially degrade performance. The\\nmulti-gate MoE [22] is a ﬂexible architecture that allows for learning what to share between tasks.\\nFigure 1 (right) shows an example of a multi-gate MoE, in the simple case of two tasks. Here, each\\ntask has its own gate that adaptively controls the extent of parameter sharing. In our experiments, we\\nstudy the effectiveness of DSelect-k in the context of the multi-gate MoE.\\nContributions: On a high-level, our main contribution is DSelect-k: a new continuously differ-\\nentiable and sparse gate for MoE, which can be directly trained using ﬁrst-order methods. Our\\ntechnical contributions can be summarized as follows. (i) The gate selects (at most) k out of the n\\nexperts, where k is a user-speciﬁed parameter. This leads to a challenging, cardinality-constrained\\noptimization problem. To deal with this challenge, we develop a novel, unconstrained reformulation,\\nand we prove that it is equivalent to the original problem. The reformulation uses a binary encoding\\nscheme that implicitly imposes the cardinality constraint using learnable binary codes. (ii) To make\\nthe unconstrained reformulation smooth, we relax and smooth the binary variables. We demonstrate\\nthat, with careful initialization and regularization, the resulting problem can be optimized with\\nﬁrst-order methods such as SGD. (iii) We carry out a series of experiments on synthetic and real MTL\\ndatasets, which show that our gate is competitive with state-of-the-art gates in terms of parameter\\nsharing and predictive performance. (iv) We provide an open-source implementation of DSelect-k.\\n2These methods were not designed speciﬁcally for the MoE.\\n2\\n1.1\\nRelated Work\\nMoE and Conditional Computation: Since MoE was introduced by [14], an exciting body of work\\nhas extended and studied this model, e.g., see [17, 13, 16]. Recently, MoE-based models are showing\\nsuccess in deep learning. For example, [33] introduced the sparse Top-k gate for MoE and showed\\nsigniﬁcant computational improvements on machine translation tasks; we discuss exact connections\\nto this gate in Section 2. The Top-k gate has also been utilized in several state-of-the-art deep learning\\nmodels that considered MTL tasks, e.g., [21, 28, 9]. Our work is also related to the conditional\\ncomputation models that activate parts of the neural network based on the input [2, 1, 33, 12, 36].\\nUnlike DSelect-k, these works are based on non-differentiable models, or heuristics where the training\\nand inference models are different.\\nStochastic k-Subset Selection and Top-k Relaxation: A related line of work focuses on stochastic\\nk-subset selection in neural networks, e.g., see [26, 5, 39] and the references therein. Speciﬁcally,\\nthese works propose differentiable methods for sampling k-subsets from a categorical distribution,\\nbased on extensions or generalizations of the Gumbel-softmax trick [23, 15]. However, in the MoE\\nwe consider deterministic subset selection—determinism is a common assumption in MoE models\\nthat can improve interpretability and allows for efﬁcient implementations [14, 17, 33]. In contrast,\\nthe stochastic approaches described above are suitable in applications where there is an underlying\\nsampling distribution, such as in variational inference [19]. Another related work is the differentiable\\nrelaxation of the Top-k operator proposed by [40]. All the aforementioned works perform dense\\ntraining (i.e., the gradients of all experts, even if not selected, will have to be computed during\\nbackpropagation), whereas DSelect-k can (to an extent) exploit sparsity to speed up training, as we\\nwill discuss in Sections 2 and 3. Moreover, the stochastic k-subset selection framework in [26] (which\\nencompasses several previous works) and the Top-k relaxation in [40] require solving an optimization\\nsubproblem to compute the gate output–each example will require solving a separate subproblem in\\nthe per-example gating setting, which can be computationally prohibitive. In contrast, DSelect-k’s\\noutput is computed efﬁciently via a closed-form expression.\\nSparse Transformations to the Simplex: These are sparse variants of the softmax function that\\ncan output sparse probability vectors, e.g., see [24, 27, 6, 3]. While similar to our work in that they\\noutput sparse probability vectors, these transformations cannot control the sparsity level precisely\\nas DSelect-k does (through a cardinality constraint). Thus, these transformations may assign some\\nexamples or tasks sparse combinations and others dense combinations.\\nMTL: In Appendix A, we review related literature on MTL.\\n2\\nGating in the Mixture of Experts\\nIn this section, we ﬁrst review the MoE architecture and popular gates, and then discuss how these\\ngates compare to our proposal. We will assume that the inputs to the MoE belong to a space\\nX ⊂Rp. In its simplest form, the MoE consists of a set of n experts (neural networks) fi : X →Ru,\\ni ∈{1, 2, . . . , n}, and a gate g : X →Rn that assigns weights to the experts. The gate’s output is\\nassumed to be a probability vector, i.e., g(x) ≥0 and Pn\\ni=1 g(x)i = 1, for any x ∈X. Given an\\nexample x ∈X, the corresponding output of the MoE is a weighted combination of the experts:\\nn\\nX\\ni=1\\nfi(x)g(x)i.\\n(1)\\nNext, we discuss two popular choices for the gate g(.) that can be directly optimized using SGD.\\nSoftmax Gate: A classical model for g(x) is the softmax gate: σ(Ax+b), where σ(.) is the softmax\\nfunction, A ∈Rn×p is a trainable weight matrix, and b ∈Rn is a bias vector [17]. This gate is dense,\\nin the sense that all experts are assigned nonzero probabilities. Note that static gating (i.e., gating\\nwhich does not depend on the input example) can be obtained by setting A = 0.\\nTop-k Gate: This is a sparse variant of the softmax gate that returns a probability vector with only k\\nnonzero entries [33]. The Top-k gate is deﬁned by σ(KeepTopK(Ax + b)), where for any vector v,\\nKeepTopK(v)i := vi if vi is in the top k elements of v, and KeepTopK(v)i := −∞otherwise3. This\\ngate is conceptually appealing since it allows for direct control over the number of experts to select\\n3To balance the load across experts, [33] add noise and additional regularizers to the model.\\n3\\nFigure 2: Expert weights output by Top-k (left) and DSelect-k (right) during training on synthetic data\\ngenerated from a MoE, under static gating. Each color represents a separate expert. Here DSelect-k\\nrecovers the true experts used by the data-generating model, whereas Top-k does not recover and\\nexhibits oscillatory behavior. See Appendix C.2 for details on the data and setup.\\nand is trained using SGD. Moreover, the Top-k gate supports conditional training: in backpropagation,\\nfor each input example, only the gradients of the loss w.r.t. top k elements need to be computed.\\nWith a careful implementation, conditional training can lead to computational savings. However, the\\nTop-k gate is not continuous, which implies that the gradient does not exist at certain inputs. This\\ncan be problematic when training is done using gradient-based methods. To gain more insight, in\\nFigure 2 (left), we plot the expert weights chosen by the Top-k gate during training with SGD. The\\nresults indicate an oscillatory behavior in the output of the Top-k gate, which can be attributed to its\\ndiscontinuous nature: a small change in the input can lead to “jumps” in the output.\\nComparison with DSelect-k: We develop DSelect-k in Section 3. Here we present a high-level\\ncomparison between DSelect-k and Top-k. Similar to Top-k, DSelect-k can select k out of the n\\nexperts and can be trained using gradient-based optimization methods. A major advantage of DSelect-\\nk over Top-k is that it is continuously differentiable, which leads to more stable selection of experts\\nduring training—see Figure 2 (right). During inference, DSelect-k only needs to evaluate a subset\\nof the experts, which can lead to computational savings. However, DSelect-k supports conditional\\ntraining only partially. At the start of training, it uses all the available experts, so conditional training\\nis not possible. As we discuss in Section 3, after a certain point during training, DSelect-k converges\\nto a small subset of the experts, and then conditional training becomes possible. Our experiments\\nindicate that DSelect-k can have a signiﬁcant edge over Top-k in terms of prediction and expert\\nselection performance, so the full support for conditional training in Top-k seems to come at the\\nexpense of statistical performance.\\n3\\nDifferentiable and Sparse Gating\\nIn this section, we develop DSelect-k, for both the static and per-example gating settings. First, we\\nintroduce the problem setup and notation. To simplify the presentation, we will develop the gate for\\na single supervised learning task, and we note that the same gate can be used in MTL models. We\\nassume that the task has an input space X ⊂Rp, an output space Y, and an associated loss function\\nℓ: Y × R →R. We denote the set of N training examples by D = {(xi, yi) ∈X × Y}N\\ni=1. We\\nconsider a learning model deﬁned by the MoE in Equation (1). For simplicity, we assume that the\\nexperts are scalar-valued and belong to a class of continuous functions H. We assume that the number\\nof experts n = 2m for some integer m—in Appendix B.2, we discuss how the gate can be extended\\nto arbitrary n. For convenience, given a non-negative integer i, we denote the set {1, 2, . . . , i} by [i].\\nIn Section 3.1, we develop DSelect-k for static gating setting. Then, in Section 3.2, we generalize it\\nto the per-example setting.\\n3.1\\nDSelect-k for Static Gating\\nOur goal here is to develop a static gate that selects a convex combination of at most k out of the n\\nexperts. The output of the gate can be thought of as a probability vector w with at most k nonzero\\nentries, where wi is the weight assigned to the expert fi. A natural way to minimize the empirical\\nrisk of the MoE model is by solving the following problem:\\n4\\nmin\\nf1,...fn,w\\n1\\nN\\nX\\n(x,y)∈D\\nℓ\\n\\x10\\ny,\\nn\\nX\\ni=1\\nfi(x)wi\\n\\x11\\n(2a)\\ns.t.\\n∥w∥0 ≤k\\n(2b)\\nn\\nX\\ni=1\\nwi = 1, w ≥0.\\n(2c)\\nIn the above, the L0 norm of w, ∥w∥0, is equal to the number of nonzero entries in w. Thus,\\nthe cardinality constraint (2b) ensures that the gate selects at most k experts. Problem (2) is a\\ncombinatorial optimization problem that is not amenable to SGD due to the cardinality constraint (2b)\\nand the simplex constraints in (2c). In what follows of this section, we ﬁrst transform Problem (2) into\\nan equivalent unconstrained optimization problem, based on a binary encoding scheme. However, the\\nunconstrained problem cannot be directly handled using SGD due to the presence of binary variables.\\nThus, in a second transformation, we smooth the binary variables, which leads to an optimization\\nproblem that is amenable to SGD.\\nRoad map: In Section 3.1.1, we introduce the single expert selector: a construct for choosing 1 out\\nof n experts by using binary encoding. In Section 3.1.2, we leverage the single expert selector to\\ntransform Problem (2) into an unconstrained one. Then, in Section 3.1.3, we smooth the unconstrained\\nproblem and discuss how SGD can be applied.\\n3.1.1\\nSingle Expert Selection using Binary Encoding\\nThe single expert selector (selector, for short) is a fundamental construct that we will later use to\\nconvert Problem (2) to an unconstrained optimization problem. At a high-level, the single expert\\nselector chooses the index of 1 out of the n experts and returns a one-hot encoding of the choice.\\nFor example, in the case of 4 experts, the selector can choose the ﬁrst expert by returning the binary\\nvector [1 0 0 0]T . Generally, the selector can choose any of the experts, and its choice is determined\\nby a set of binary encoding variables, as we will describe next.\\nThe selector is parameterized by m (recall that m = log2 n) binary variables, z1, z2, . . . , zm, where\\nwe view these variables collectively as a binary number: zmzm−1 . . . z1. The integer represented\\nby the latter binary number determines which expert to select. More formally, let l be the integer\\nrepresented by the binary number zmzm−1 . . . z1. The selector is a function r : Rm →{0, 1}n which\\nmaps z := [z1, z2, . . . , zm]T to a one-hot encoding of the integer (l + 1). For example, if all the zi’s\\nare 0, then the selector returns a one-hot encoding of the integer 1. Next, we deﬁne the selector r(z).\\nFor easier exposition, we start with the special case of 4 experts and then generalize to n experts.\\nSpecial case of 4 experts: In this case, the selector uses two binary variables z1 and z2. Let l be the\\ninteger represented by the binary number z2z1. Then, the selector should return a one-hot encoding\\nof the integer (l + 1). To achieve this, we deﬁne the selector r(z) as follows:\\nr(z) = [ ¯\\nz1 ¯\\nz2, z1 ¯\\nz2,\\n¯\\nz1z2, z1z2]T\\n(3)\\nwhere ¯\\nzi := 1 −zi. By construction, exactly one entry in r(z) is 1 (speciﬁcally, r(z)l+1 = 1) and the\\nrest of the entries are zero. For example, if z1 = z2 = 0, then r(z)1 = 1 and r(z)i = 0, i ∈{2, 3, 4}.\\nGeneral case of n experts: Here we generalize the selector r(z) to the case of n experts. To aid in\\nthe presentation, we make the following deﬁnition. For any non-negative integer l, we deﬁne B(l)\\nas the set of indices of the nonzero entries in the binary representation of l (where we assume that\\nthe least signiﬁcant bit is indexed by 1). For example, B(0) = ∅, B(1) = {1}, B(2) = {2}, and\\nB(3) = {1, 2}. For every i ∈[n], we deﬁne the i-th entry of r(z) as follows:\\nr(z)i =\\nY\\nj∈B(i−1)\\n(zj)\\nY\\nj∈[m]\\\\B(i−1)\\n(1 −zj)\\n(4)\\nIn the above, r(z)i is a product of m binary variables, which is equal to 1 iff the integer (i −1) is\\nrepresented by the binary number zmzm−1 . . . z1. Therefore, r(z) returns a one-hot encoding of the\\nindex of the selected expert. Note that when n = 4, deﬁnitions (3) and (4) are equivalent.\\n3.1.2\\nMultiple Expert Selection via Unconstrained Minimization\\nIn this section, we develop a combinatorial gate that allows for transforming Problem (2) into an\\nunconstrained optimization problem. We design this gate by creating k instances of the single expert\\n5\\nselector r(.), and then taking a convex combination of these k instances. More formally, for every\\ni ∈[k], let z(i) ∈{0, 1}m be a (learnable) binary vector, so that the output of the i-th instance of the\\nselector is r(z(i)). Let Z be a k × m matrix whose i-th row is z(i). Moreover, let α ∈Rk be a vector\\nof learnable parameters. We deﬁne the combinatorial gate q as follows:\\nq(α, Z) =\\nk\\nX\\ni=1\\nσ(α)ir(z(i)),\\nwhere we recall that σ(.) is the softmax function. Since for every i ∈[k], r(z(i)) is a one-hot vector,\\nwe have ∥q(α, Z)∥0 ≤k. Moreover, since the weights of the selectors are obtained using a softmax,\\nwe have q(α, Z) ≥0 and Pn\\ni=1 q(α, Z)i = 1. Thus, q(α, Z) has the same interpretation of w in\\nProblem (2), without requiring any constraints. Therefore, we propose replacing w in the objective of\\nProblem (2) with q(α, Z) and removing all the constraints. This replacement leads to an equivalent\\nunconstrained optimization problem, as we state in the next proposition.\\nProposition 1. Problem (2) is equivalent4 to:\\nmin\\nf1,...fn,α,Z\\n1\\nN\\nX\\n(x,y)∈D\\nℓ\\n\\x10\\ny,\\nn\\nX\\ni=1\\nfi(x)q(α, Z)i\\n\\x11\\nz(i) ∈{0, 1}m, i ∈[k]\\n(5)\\nThe proof of Proposition 1 is in Appendix B.1. Unlike Problem (2), Problem (5) does not involve any\\nconstraints, aside from requiring binary variables. However, these binary variables cannot be directly\\nhandled using ﬁrst-order methods. Next, we discuss how to smooth the binary variables in order to\\nobtain a continuous relaxation of Problem (5).\\n3.1.3\\nSmooth Gating\\nIn this section, we present a procedure to smooth the binary variables in Problem (5) and discuss\\nhow the resulting problem can be optimized using ﬁrst-order methods. The procedure relies on the\\nsmooth-step function, which we deﬁne next.\\nSmooth-step Function:\\nThis is a continuously differentiable and S-shaped function, similar in\\nshape to the logistic function. However, unlike the logistic function, the smooth-step function can\\noutput 0 and 1 exactly for sufﬁciently large magnitudes of the input. The smooth-step and logistic\\nfunctions are depicted in Appendix B.3. More formally, given a non-negative scaling parameter γ,\\nthe smooth-step function, S : R →R, is a cubic piecewise polynomial deﬁned as follows:\\nS(t) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n0\\nif t ≤−γ/2\\n−2\\nγ3 t3 +\\n3\\n2γ t + 1\\n2\\nif −γ/2 ≤t ≤γ/2\\n1\\nif t ≥γ/2\\nThe parameter γ controls the width of the fractional region (i.e., the region where the function is\\nstrictly between 0 and 1). Note that S(t) is continuously differentiable at all points—this follows\\nsince at the boundary points ±γ/2, we have: S′(−γ/2) = S′(γ/2) = 0. This function has been\\nrecently used for conditional computation in soft trees [11] and is popular in the computer graphics\\nliterature [8, 30].\\nSmoothing: We obtain DSelect-k from the combinatorial gate q(α, Z) by (i) relaxing every binary\\nvariable in Z to be continuous in the range (−∞, +∞), i.e., Z ∈Rk×m, and (ii) applying the\\nsmooth-step function to Z element-wise. Formally, DSelect-k is a function ˜\\nq deﬁned as follows:\\n˜\\nq(α, Z) := q(α, S(Z)) =\\nk\\nX\\ni=1\\nσ(α)ir\\n\\x00S(z(i))\\n\\x01\\n,\\n(6)\\nwhere the matrix S(Z) is obtained by applying S(·) to Z element-wise. Note that ˜\\nq(α, Z) is\\ncontinuously differentiable so it is amenable to ﬁrst-order methods. If S(Z) is binary, then ˜\\nq(α, Z)\\n4Equivalent means that the two problems have the same optimal objective, and given an optimal solution for\\none problem, we can construct an optimal solution for the other.\\n6\\nselects at most k experts (this holds since ˜\\nq(α, Z) = q(α, S(Z)), and from Section 3.1.2, q selects at\\nmost k experts when its encoding matrix is binary). However, when S(Z) has any non-binary entries,\\nthen more than k experts can be potentially selected, meaning that the cardinality constraint will not\\nbe respected. In what follows, we discuss how the gate can be optimized using ﬁrst-order methods,\\nwhile ensuring that S(Z) converges to a binary matrix so that the cardinality constraint is enforced.\\nWe propose using ˜\\nq(α, Z) in MoE, which leads to the following optimization problem:\\nmin\\nf1,...fn,α,Z\\n1\\nN\\nX\\n(x,y)∈D\\nℓ\\n\\x10\\ny,\\nn\\nX\\ni=1\\nfi(x)˜\\nq(α, Z)i\\n\\x11\\n.\\n(7)\\nProblem (7) can be viewed as a continuous relaxation of Problem (5). If the experts are differentiable,\\nthen the objective of Problem (7) is differentiable. Thus, we propose optimizing MoE end-to-end\\nusing ﬁrst-order methods. We note that ˜\\nq(α, Z) uses (k + k log n) learnable parameters. In contrast,\\nthe Top-k and softmax gates (discussed in Section 2) use n parameters. Thus, for relatively small k,\\nour proposal uses a smaller number of parameters. Next, we discuss how DSelect-k’s parameters\\nshould be initialized in order to ensure that it is trainable.\\nInitialization: By the deﬁnition of the smooth-step function, if S(Zij) is binary then S′(Zij) = 0,\\nand consequently\\n∂ℓ\\n∂Zij = 0. This implies that, during optimization, if S(Zij) becomes binary, the\\nvariable Zij will not be updated in any subsequent iteration. Thus, we have to be careful about the\\ninitialization of Z. For example, if Z is initialized so that S(Z) is a binary matrix then the gate will\\nnot be trained. To ensure that the gate is trainable, we initialize each Zij so that 0 < S(Zij) < 1.\\nThis way, the Zij’s can have nonzero gradients at the start of optimization.\\nAccelerating Convergence to Binary Solutions: Recall that we need S(Z) to converge to a binary\\nmatrix, in order for the gate ˜\\nq to respect the cardinality constraint (i.e., to select at most k experts).\\nEmpirically, we observe that if the optimizer runs for a sufﬁciently large number of iterations,\\nthen S(Z) typically converges to a binary matrix. However, early stopping of the optimizer can\\nbe desired in practice for computational and statistical considerations, and this can prevent S(Z)\\nfrom converging. To encourage faster convergence towards a binary S(Z), we will add an entropy\\nregularizer to Problem (7). The following proposition is needed before we introduce the regularizer.\\nProposition 2. For any z ∈Rm, α ∈Rk, and Z ∈Rk×m, r(S(z)) and ˜\\nq(α, Z) belong to the\\nprobability simplex.\\nThe proof of the proposition is in Appendix B.1. Proposition 2 implies that, during training, the\\noutput of each single expert selector used by ˜\\nq(α, Z), i.e., r(S(z(i))) for i ∈[k], belongs to the\\nprobability simplex. Note that the entropy of each r(S(z(i))) is minimized by any one-hot encoded\\nvector. Thus, for each r(S(z(i))), we add an entropy regularization term that encourages convergence\\ntowards one-hot encoded vectors; equivalently, this encourages convergence towards a binary S(Z).\\nSpeciﬁcally, we solve the following regularized variant of Problem (7):\\nmin\\nf1,...fn,α,Z\\nX\\n(x,y)∈D\\n1\\nN ℓ\\n\\x10\\ny,\\nn\\nX\\ni=1\\nfi(x)˜\\nq(α, Z)i\\n\\x11\\n+ λΩ(Z)\\nwhere Ω(Z) := Pk\\ni=1 h\\n\\x00r(S(z(i)))\\n\\x01\\nand h(.) is the entropy function. The hyperparameter λ is\\nnon-negative and controls how fast each selector converges to a one-hot encoding. In our experiments,\\nwe tune over a range of λ values. When selecting the best hyperparameters from tuning, we disregard\\nany λ whose corresponding solution does not have a binary S(Z). In Appendix C.3, we report the\\nnumber of training steps required for S(Z) to converge to a binary matrix, on several real datasets.\\nOther alternatives to ensure that S(Z) converges to a binary matrix are also possible. One alternative\\nis to regularize the entropy of each entry in S(Z) separately. Another alternative is to anneal the\\nparameter γ of the smooth-step function towards zero.\\nSoftmax-based Alternative to Binary Encoding: Recall that our proposed selectors in (6), i.e.,\\nr\\n\\x00S(z(i))\\n\\x01\\n, i ∈[k], learn one-hot vectors exactly (by using binary encoding). One practical alternative\\nfor learning a one-hot vector is by using a softmax function with temperature annealing. Theoretically,\\nthis alternative cannot return a one-hot vector, but after training, the softmax output can be transformed\\nto a one-hot vector using a heuristic (e.g., by taking an argmax). In Appendix C.1, we perform an\\nablation study in which we replace the selectors in DSelect-k with softmax functions (along with\\ntemperature annealing or entropy regularization).\\n7\\n3.2\\nDSelect-k for Per-example Gating\\nIn this section, we generalize the static version of DSelect-k, ˜\\nq(α, Z), to the per-example gating\\nsetting. The key idea is to make the gate’s parameters α and Z functions of the input, so that the\\ngate can make decisions on a per-example basis. Note that many functional forms are possible for\\nthese parameters. For simplicity and based on our experiments, we choose to make α and Z linear\\nfunctions of the input example. More formally, let G ∈Rk×p, W (i) ∈Rm×p, i ∈[k], be a set\\nof learnable parameters. Given an input example x ∈Rp, we set α = Gx and z(i) = W (i)x in\\n˜\\nq(α, Z) (to simplify the presentation, we do not include bias terms). Thus, the per-example version\\nof DSelect-k is a function v deﬁned as follows:\\nv(G, W, x) =\\nk\\nX\\ni=1\\nσ(Gx)ir\\n\\x00S(W (i)x)\\n\\x01\\n.\\nIn the above, the term r\\n\\x00S(W (i)x)\\n\\x01\\nrepresents the i-th single expert selector, whose output depends\\non the example x; thus different examples are free to select different experts. The term σ(Gx)i deter-\\nmines the input-dependent weight assigned to the i-th selector. The gate v(G, W, x) is continuously\\ndifferentiable in the parameters G and W, so we propose optimizing it using ﬁrst-order methods.\\nSimilar to the case of static gating, if S(W (i)x) is binary for all i ∈[k], then each r\\n\\x00S(W (i)x)\\n\\x01\\nwill\\nselect exactly one expert, and the example x will be assigned to at most k experts.\\nTo encourage S(W (i)x), i ∈[k] to become binary, we introduce an entropy regularizer, similar in\\nessence to that in static gating. However, unlike static gating, the regularizer here should be on a\\nper-example basis, so that each example respects the cardinality constraint. By Proposition 2, for any\\ni ∈[k], r\\n\\x00S(W (i)x)\\n\\x01\\nbelongs to the probability simplex. Thus, for each example x in the training\\ndata, we introduce a regularization term of the form: Ω(W, x) := P\\ni∈[k] h\\n\\x10\\nr\\n\\x00S(W (i)x)\\n\\x01\\x11\\n, and\\nminimize the following objective function:\\nX\\n(x,y)∈D\\n \\n1\\nN ℓ\\n\\x10\\ny,\\nn\\nX\\ni=1\\nfi(x)v(G, W, x)i\\n\\x11\\n+ λΩ(W, x)\\n!\\n,\\nwhere λ is a non-negative hyperparameter. Similar to the case of static gating, we tune over a range\\nof λ values, and we only consider the choices of λ that force the average number of selected experts\\nper example to be less than or equal to k. If the application requires that the cardinality constraint\\nbe satisﬁed strictly for every example (not only on average), then annealing γ in the smooth-step\\nfunction towards zero enforces this.\\n4\\nExperiments\\nWe study the performance of DSelect-k in the context of MTL and compare with state-of-the-art gates\\nand baselines. In the rest of this section, we present experiments on the following real MTL datasets:\\nMovieLens, Multi-MNIST, Multi-Fashion MNIST, and on a real-world, large-scale recommender\\nsystem. Moreover, in Appendix C, we present an additional experiment on synthetic data (with up to\\n128 tasks), in which we study statistical performance and perform ablation studies.\\nCompeting Methods: We focus on a multi-gate MoE, and study the DSelect-k and Top-k gates\\nin both the static and per-example gating settings. For static gating, we also consider a Gumbel-\\nsoftmax based gate [34]–unlike DSelect-k this gate cannot control the sparsity level explicitly (see the\\nsupplementary for details). In addition, we consider two MTL baselines. The ﬁrst baseline is a MoE\\nwith a softmax gate (which uses all the available experts). The second is a shared bottom model [4],\\nwhere all tasks share the same bottom layers, which are in turn connected to task-speciﬁc neural nets.\\nExperimental Setup: All competing models were implemented in TensorFlow 2. We used Adam\\n[18] and Adagrad [7] for optimization, and we tuned the key hyperparameters using random grid\\nsearch (with an average of 5 trials per grid point). Full details on the setup are in Appendix D.\\n4.1\\nMovieLens\\nDataset: MovieLens [10] is a movie recommendation dataset containing records for 4,000 movies\\nand 6,000 users. Following [37], for every user-movie pair, we construct two tasks. Task 1 is a\\nbinary classiﬁcation problem for predicting whether the user will watch a particular movie. Task 2\\n8\\nis a regression problem to predict the user’s rating (in {1, 2, . . . , 5}) for a given movie. We use 1.6\\nmillion examples for training and 200, 000 for each of the validation and testing sets.\\nExperimental Details: We use the cross-entropy and squared error losses for tasks 1 and 2, re-\\nspectively. We optimize a weighted average of the two losses, i.e., the ﬁnal loss function is\\nα(Loss of Task 1) + (1 −α)(Loss of Task 2), and we report the results for α ∈{0.1, 0.5, 0.9}.\\nThe same loss function is also used for tuning and testing. The architecture consists of a multi-gate\\nMoE with 8 experts, where each of the experts and the task-speciﬁc networks is composed of ReLU-\\nactivated dense layers. For each α, we tune over the optimization and gate-speciﬁc hyperparameters,\\nincluding the number of experts to select (i.e., k in DSelect-k and Top-k). After tuning, we train\\neach model for 100 repetitions (using random initialization) and report the averaged results. For full\\ndetails, see Appendix D.1.\\nResults: In Table 1, we report the test loss and the average number of selected experts. The results\\nindicate that for all values of α, either one of our DSelect-k gates (static or per-example) outperforms\\nthe competing methods, in terms of both the test loss and the number of selected experts. In the static\\ngating setting, there does not seem to be a clear winner among the three competing methods (Top-k,\\nDSelect-k, and Gumbel Softmax), but we note that DSelect-k outperforms both Top-k and Gumbel\\nSoftmax for two out of the three choices of α. Notably, the softmax MoE is uniformly outperformed\\nby the DSelect-k and Top-k gates, so sparsity in gating seems to be beneﬁcial on this dataset. Our\\nhypothesis is that softmax MoE is overﬁtting and the sparse gating methods are mitigating this issue.\\nIn Table C.5 in the appendix, we additionally report the individual task metrics (loss and accuracy).\\nα = 0.1\\nα = 0.5\\nα = 0.9\\nLoss\\nExperts\\nLoss\\nExperts\\nLoss\\nExperts\\nStatic\\nDSelect-k\\n4015 ± 5\\n2.7\\n3804 ± 3\\n1.5\\n3690 ± 2\\n1.3\\nTop-k\\n4012 ± 4\\n2.0\\n3818 ± 2\\n2.0\\n3693 ± 6\\n2.0\\nGumbel Softmax\\n4171 ± 3\\n2.7\\n3898 ± 2\\n2.6\\n3688 ± 4\\n3.6\\nPer-example\\nDSelect-k\\n4006 ± 6\\n1.5\\n3823 ± 3\\n1.2\\n3679 ± 2\\n1.1\\nTop-k\\n4027 ± 8\\n2.0\\n3841 ± 4\\n2.0\\n3741 ± 3\\n2.0\\nBaselines\\nSoftmax MoE\\n4090 ± 1\\n8.0\\n3960 ± 3\\n8.0\\n3847 ± 10\\n8.0\\nShared Bottom\\n4037 ± 2\\n-\\n3868 ± 2\\n-\\n3687 ± 1\\n-\\nTable 1: Test loss (with standard error) and average number of selected experts on MovieLens. The parameter\\nα is the weight of Task 1’s loss (see text for details). The test loss is multiplied by 104.\\n4.2\\nMulti-MNIST and Multi-Fashion MNIST\\nDatasets: We consider two image classiﬁcation datasets: Multi-MNIST and Multi-Fashion [32],\\nwhich are multi-task variants of the MNIST [20] and Fashion MNIST [38] datasets. We construct the\\nMulti-MNIST dataset similar to [32]: uniformly sample two images from MNIST and overlay them\\non top of each other, and (ii) shift one digit towards the top-left corner and the other digit towards\\nthe bottom-right corner (by 4 pixels in each direction). This procedure leads to 36 × 36 images with\\nsome overlap between the digits. The Multi-Fashion is constructed in a similar way by overlaying\\nimages from the Fashion MNIST dataset. For each dataset, we consider two classiﬁcation tasks:\\nTask 1 is to classify the top-left item and Task 2 is to classify the bottom-right item. We use 100,000\\nexamples for training, and 20,000 examples for each of the validation and testing sets.\\nExperimental Details: We use cross-entropy loss for each task and optimize the sum of the losses5.\\nThe model is a multi-gate MoE with 8 experts, where each expert is a convolutional neural network\\nand each task-speciﬁc network is composed of a number of dense layers. We tune the optimization\\nand gate-speciﬁc hyperparameters, including the number of experts to select, and use the average of\\nthe task accuracies as the tuning metric. After tuning, we train each model for 100 repetitions (using\\nrandom initialization) and report the averaged results. For full details, see Appendix D.2.\\nResults: In Table 2, we report the test accuracy and the number of selected experts for the Multi-\\nMNIST and Multi-Fashion datasets. On Multi-MNIST, DSelect-k (static) outperforms Top-k and\\nGumbel Softmax, in terms of both task accuracies and number of selected experts. For example, it\\nachieves over 1% improvement in Task 2’s accuracy compared to Top-k (static). DSelect-k (static)\\ncomes close to the performance of the Softmax MoE, but uses less experts (1.7 vs. 8 experts). Here\\nDSelect-k (per-example) does not offer improvement over the static variant (unlike the MovieLens\\ndataset). On Multi-Fashion, we again see that DSelect-k (static) performs best in terms of accuracy.\\n5Due to the symmetry in the problem, assigning the two tasks equal weights is a reasonable choice.\\n9\\nMulti-MNIST\\nMulti-Fashion MNIST\\nAccuracy 1\\nAccuracy 2\\nExperts\\nAccuracy 1\\nAccuracy 2\\nExperts\\nStatic\\nDSelect-k\\n92.56 ± 0.03\\n90.98 ± 0.04\\n1.7\\n83.78 ± 0.05\\n83.34 ± 0.05\\n1.8\\nTop-k\\n91.93 ± 0.06\\n90.03 ± 0.08\\n4\\n83.44 ± 0.07\\n82.66 ± 0.08\\n4\\nGumbel Softmax\\n92.3 ± 0.05\\n90.63 ± 0.05\\n1.8\\n83.66 ± 0.07\\n83.28 ± 0.05\\n1.5\\nPer-example\\nDSelect-k\\n92.42 ± 0.03\\n90.7 ± 0.03\\n1.5\\n83.69 ± 0.04\\n83.13 ± 0.04\\n1.5\\nTop-k\\n92.27 ± 0.03\\n90.45 ± 0.03\\n4\\n83.66 ± 0.04\\n83.15 ± 0.04\\n4\\nBaselines\\nSoftmax MoE\\n92.61 ± 0.03\\n91.0 ± 0.03\\n8\\n83.48 ± 0.04\\n82.81 ± 0.04\\n8\\nShared Bottom\\n91.3 ± 0.04\\n89.47 ± 0.04\\n-\\n82.05 ± 0.05\\n81.37 ± 0.06\\n-\\nTable 2: Test accuracy (with standard error) and number of selected experts on Multi-MNIST/Fashion.\\nTasks / Methods\\nDSelect-k\\nTop-k\\nE. Task 1 (AUC)\\n0.8103 ± 0.0002\\n0.7481 ± 0.0198\\nE. Task 2 (AUC)\\n0.8161 ± 0.0002\\n0.7624 ± 0.0169\\nE. Task 3 (RMSE)\\n0.2874 ± 0.0002\\n0.3406 ± 0.0180\\nE. Task 4 (RMSE)\\n0.8781 ± 0.0014\\n1.1213 ± 0.0842\\nE. Task 5 (AUC)\\n0.7524 ± 0.0003\\n0.5966 ± 0.0529\\nS. Task 1 (AUC)\\n0.6133 ± 0.0066\\n0.5080 ± 0.0047\\nS. Task 2 (AUC)\\n0.8468 ± 0.0289\\n0.5981 ± 0.0616\\nS. Task 3 (AUC)\\n0.9259 ± 0.0008\\n0.6665 ± 0.0091\\nTable 3: Average performance (AUC and RMSE) and standard\\nerror on a real-world recommender system with 8 tasks: “E.”\\nand “S.” denote engagement and satisfaction tasks, respectively.\\nFigure 3: Expert weights of the DSelect-k gates\\non the recommender system.\\n4.3\\nA Large-scale Recommender System\\nWe study the performance of DSelect-k and Top-k in a real-word, large-scale content recommendation\\nsystem. The system encompasses hundreds of millions of unique items and billions of users.\\nArchitecture and Dataset: The system consists of a candidate generator followed by a multi-task\\nranking model, and it adopts a framework similar to [41, 35]. The ranking model makes predictions\\nfor 6 classiﬁcation and 2 regression tasks. These can be classiﬁed into two categories: (i) engagement\\ntasks (e.g., predicting user clicks, bad clicks, engagement time), and (ii) satisfaction tasks (e.g.,\\npredicting user satisfaction behaviors such as likes and dislikes). We construct the dataset from the\\nsystem’s user logs (which contain historical information about the user and labels for the 8 tasks).\\nThe dataset consists of billions of examples (we do not report the exact number for conﬁdentiality).\\nWe use a random 90/10 split for the training and evaluation sets.\\nExperimental Details:\\nWe use the cross-entropy and squared error losses for the classiﬁcation\\nand regression tasks, respectively. The ranking model is based on a multi-gate MoE, in which each\\ntask uses a separate static gate. The MoE uses 8 experts, each composed of dense layers. For\\nboth the DSelect-k and Top-k based models, we tune the learning rate and the experts’ architecture.\\nThen, using the best hyperparameters, we train the ﬁnal models for 5 repetitions (using random\\ninitialization). For additional details, see Appendix D.3.\\nResults: In Table 3, we report the out-of-sample performance metrics for the 8 tasks. The results\\nindicate that DSelect-k outperforms Top-k on all tasks, with the improvements being most pronounced\\non the satisfaction tasks. In Figure 3, we show a heatmap of the expert weights chosen by the DSelect-\\nk gates. Notably, for DSelect-k, all engagement tasks share at least one expert, and two of the\\nsatisfaction tasks share the same expert.\\n5\\nConclusion\\nWe introduced DSelect-k: a continuously differentiable and sparse gate for MoE, which can be\\ntrained using ﬁrst-order methods. Given a user-speciﬁed parameter k, the gate selects at most k of\\nthe n experts. Such direct control over the sparsity level is typically handled in the literature by\\nadding a cardinality constraint to the optimization problem. One of the key ideas we introduced is a\\nbinary encoding scheme that allows for selecting k experts, without requiring any constraints in the\\noptimization problem. We studied the performance of DSelect-k in MTL settings, on both synthetic\\nand real datasets. Our experiments indicate that DSelect-k can achieve signiﬁcant improvements in\\nprediction and expert selection, compared to state-of-the-art MoE gates and MTL baselines.\\nSocietal Impact: MoE models are used in various applications (as discussed in the introduction).\\nDSelect-k can improve the interpretability and efﬁciency of MoE models, thus beneﬁting the underly-\\ning applications. We do not see direct, negative societal impacts from our proposal.\\n10\\nAcknowledgements: The research was conducted while Hussein Hazimeh was at Google, and part\\nof the writing was done during his time at MIT. At MIT, Hussein Hazimeh and Rahul Mazumder\\nacknowledge research funding from the Ofﬁce of Naval Research [Grant ONR-N000141812298].\\nReferences\\n[1] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional com-\\nputation in neural networks for faster models. CoRR, abs/1511.06297, 2015. URL http:\\n//arxiv.org/abs/1511.06297.\\n[2] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\\n[3] Mathieu Blondel, Andre Martins, and Vlad Niculae. Learning classiﬁers with fenchel-young\\nlosses: Generalized entropies, margins, and algorithms. In The 22nd International Conference\\non Artiﬁcial Intelligence and Statistics, pages 606–615. PMLR, 2019.\\n[4] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\\n[5] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An\\ninformation-theoretic perspective on model interpretation. In International Conference on\\nMachine Learning, pages 883–892. PMLR, 2018.\\n[6] Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\\npages 2174–2184, 2019.\\n[7] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\\nand stochastic optimization. Journal of machine learning research, 12(7), 2011.\\n[8] David S Ebert, F Kenton Musgrave, Darwyn Peachey, Ken Perlin, and Steven Worley. Texturing\\n& modeling: a procedural approach. Morgan Kaufmann, 2003.\\n[9] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\n[10] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm\\ntransactions on interactive intelligent systems (tiis), 5(4):1–19, 2015.\\n[11] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The\\ntree ensemble layer: Differentiability meets conditional computation. In Hal Daumé III and\\nAarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning,\\nvolume 119 of Proceedings of Machine Learning Research, pages 4138–4148, Virtual, 13–18\\nJul 2020. PMLR.\\n[12] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew\\nBrown, and Antonio Criminisi. Decision forests, convolutional networks and the models\\nin-between. arXiv preprint arXiv:1603.01250, 2016.\\n[13] Robert A Jacobs. Bias/variance analyses of mixtures-of-experts architectures. Neural computa-\\ntion, 9(2):369–383, 1997.\\n[14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures\\nof local experts. Neural computation, 3(1):79–87, 1991.\\n[15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\\narXiv preprint arXiv:1611.01144, 2016.\\n[16] Wenxin Jiang and Martin A Tanner. On the identiﬁability of mixtures-of-experts. Neural\\nNetworks, 12(9):1253–1258, 1999.\\n[17] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\\nNeural computation, 6(2):181–214, 1994.\\n11\\n[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\\n[19] Diederik P Kingma and Max Welling.\\nAuto-encoding variational bayes.\\narXiv preprint\\narXiv:1312.6114, 2013.\\n[20] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs\\n[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.\\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n[22] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task\\nrelationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the\\n24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages\\n1930–1939, 2018.\\n[23] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\\n[24] Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention\\nand multi-label classiﬁcation. In International Conference on Machine Learning, pages 1614–\\n1623. PMLR, 2016.\\n[25] Krzysztof Maziarz, EﬁKokiopoulou, Andrea Gesmundo, Luciano Sbaiz, Gabor Bartok,\\nand Jesse Berent. Gumbel-matrix routing for ﬂexible multi-task learning. arXiv preprint\\narXiv:1910.04915, 2019.\\n[26] Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient\\nestimation with stochastic softmax tricks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\\npages 5691–5704. Curran Associates, Inc., 2020. URL https://proceedings.neurips.\\ncc/paper/2020/file/3df80af53dce8435cf9ad6c3e7a403fd-Paper.pdf.\\n[27] Ben Peters, Vlad Niculae, and André FT Martins. Sparse sequence-to-sequence models. In\\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\\n1504–1519, 2019.\\n[28] Prajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models. In\\nInternational Conference on Learning Representations, 2018.\\n[29] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection\\nof non-linear functions for multi-task learning. In International Conference on Learning\\nRepresentations, 2018.\\n[30] Randi J Rost, Bill Licea-Kane, Dan Ginsburg, John Kessenich, Barthold Lichtenbelt, Hugh\\nMalan, and Mike Weiblen. OpenGL shading language. Pearson Education, 2009.\\n[31] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\\narXiv:1706.05098, 2017.\\n[32] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In\\nAdvances in neural information processing systems, pages 3856–3866, 2017.\\n[33] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.\\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\\nexperts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\\nFrance, April 24-26, 2017, Conference Track Proceedings, 2017.\\n12\\n[34] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to\\nshare for efﬁcient deep multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\\npages 8728–8740. Curran Associates, Inc., 2020. URL https://proceedings.neurips.\\ncc/paper/2020/file/634841a6831464b64c072c8510c7f35c-Paper.pdf.\\n[35] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. Progressive layered extraction\\n(ple): A novel multi-task learning (mtl) model for personalized recommendations. In Fourteenth\\nACM Conference on Recommender Systems, pages 269–278, 2020.\\n[36] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning\\ndynamic routing in convolutional networks. In Proceedings of the European Conference on\\nComputer Vision (ECCV), pages 409–424, 2018.\\n[37] Yuyan Wang, Zhe Zhao, Bo Dai, Christopher Fifty, Dong Lin, Lichan Hong, and Ed H Chi.\\nSmall towers make big differences. arXiv preprint arXiv:2008.05808, 2020.\\n[38] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for\\nbenchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\\n[39] Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous\\nrelaxations. In International Joint Conference on Artiﬁcial Intelligence, 2019.\\n[40] Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas\\nPﬁster. Differentiable top-k with optimal transport. Advances in Neural Information Processing\\nSystems, 33, 2020.\\n[41] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar,\\nMaheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. Recommending what video to watch\\nnext: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender\\nSystems, pages 43–51, 2019.\\n13\\nA\\nAdditional Related Work\\nMTL: In MTL, deep learning-based architectures that perform soft-parameter sharing, i.e., share\\nmodel parameters partially, are proving to be effective at exploiting both the commonalities and\\ndifferences among tasks [31]. One ﬂexible architecture for soft-parameter sharing is the multi-gate\\nMoE [22]. We use the multi-gate MoE in our experiments and compare both sparse and dense\\ngates—[22] considered only dense gates. In addition, several works have recently considered gate-\\nlike structures for ﬂexible parameter sharing in MTL. For instance, [34, 25] give each task the\\nﬂexibility to use or ignore components inside the neural network. The decisions are modeled using\\nbinary random variables, and the corresponding probability distributions are learned using SGD and\\nthe Gumbel-softmax trick [15]. This approach is similar to static gating, but it does not support\\nper-example gating. Moreover, the number of nonzeros cannot be directly controlled (in contrast\\nto our gate). Our work is also related to [29] who introduced “routers” (similar to gates) that can\\nchoose which layers or components of layers to activate per-task. The routers in the latter work are\\nnot differentiable and require reinforcement learning.\\nB\\nMethodology Details\\nB.1\\nProofs\\nB.1.1\\nProof Proposition 1\\nLet f = {fi}i∈[n]. To prove equivalence, we need to establish the following two directions: (I) an\\noptimal solution (f ∗, α∗, Z∗) to Problem (5) can be used to construct a feasible solution (f, w) to\\nProblem (2) and both solutions have the same objective, and (II) an optimal solution (f ∗, w∗) to\\nProblem (2) can be used to construct a feasible solution (f, α, Z) to Problem (5) and both solutions\\nhave the same objective. Direction (I) is trivial: the solution deﬁned by f = f ∗and w = q(α∗, Z∗)\\nis feasible for Problem (2) and has the same objective as (f ∗, α∗, Z∗).\\nNext, we show Direction (II). Let s∗= ∥w∗∥0 and denote by tj the index of the j-th largest element\\nin w∗, i.e., the nonzero entries in w∗are w∗\\nt1 > w∗\\nt2 > · · · > w∗\\nts∗. For every i ∈[s∗], set z(i) to the\\nbinary representation of ti −1. If s∗< k, then we set the remaining (unset) z(i)’s as follows: for\\ni ∈{s∗+ 1, s∗+ 2, . . . , k} set z(i) to the binary representation of ts∗−1. By this construction, the\\nnonzero indices selected by r(z(i)), i ∈[k] are exactly the nonzero indices of w∗.\\nTo construct α, there are two cases to consider: (i) s∗= k and (ii) s∗< k. If s∗= k, then set\\nαi = log(w∗\\nti) for i ∈[k]. Therefore, σ(α)i = w∗\\nti for i ∈[k], and consequently q(α, Z) = w∗.\\nOtherwise, if s∗< k, then set αi = log(w∗\\nti) for i ∈[s∗−1] and αi = log(w∗\\nts∗/(k −s∗)) for\\ni ∈[s∗, s∗+ 1, . . . , k]. Thus, for i ∈[s∗−1], we have σ(α)i = w∗\\nti, i.e., the weights of the nonzero\\nindices tj, j ∈[s∗−1] in q(α, Z) are equal to those in w∗. The weight assigned to the nonzero index\\nts∗in q(α, Z) is: P\\ni∈[s∗,s∗+1,...,k] σ(α)i = P\\ni∈[s∗,s∗+1,...,k] w∗\\nts∗/(k −s∗) = w∗\\nts∗. Therefore,\\nq(α, Z) = w∗. In both (i) and (ii), we have q(α, Z) = w∗, so the solution (f ∗, α, Z) is feasible and\\nhas the same objective as (f ∗, w∗).\\nB.1.2\\nProof of Proposition 2\\nFirst, we will use induction to show that r(S(z)) belongs to the probability simplex. Speciﬁcally, we\\nwill prove that for any integer t ≥1 and z ∈Rt, r(S(z)) belongs to the probability simplex.\\nOur base case is for t = 1. In this case, there is a single binary encoding variable z1 ∈R and\\n2 experts. The single expert selector r(S(z1)) is deﬁned as follows: r(S(z1))1 = 1 −S(z1) and\\nr(S(z1))2 = S(z1). The latter two terms are non-negative and sum up to 1. Thus, r(S(z1)) belongs\\nto the probability simplex.\\nOur induction hypothesis is that for some t ≥1 and any z ∈Rt, r(S(z)) belongs to the probability\\nsimplex. For the inductive step, we need to show that for any v ∈Rt+1, r(S(v)) belongs to the\\nprobability simplex. From the deﬁnition of r(.), the following holds:\\nr(S(v))i =\\n\\x1ar(S([v1, v2, . . . , vt]T ))i(1 −S(vt+1))\\ni ∈[2t]\\nr(S([v1, v2, . . . , vt]T ))i−2tS(vt+1)\\ni ∈[2t+1] \\\\ [2t]\\n(B.8)\\n14\\nBy the induction hypothesis, we have r(S([v1, v2, . . . , vt]T ))i ≥0 for any i. Moreover, S(.) is a\\nnon-negative function. Therefore, r(S(v))i ≥0 for any i. It remains to show that the sum of the\\nentries in r(S(v)) is 1, which we establish next:\\n2t+1\\nX\\ni=1\\nr(S(v))i =\\n2t\\nX\\ni=1\\nr(S(v))i +\\n2t+1\\nX\\ni=2t+1\\nr(S(v))i\\n(B.8)\\n=\\n2t\\nX\\ni=1\\nr(S([v1, . . . , vt]T ))i(1 −S(vt+1)) +\\n2t+1\\nX\\ni=2t+1\\nr(S([v1, . . . , vt]T ))i−2tS(vt+1)\\n(B.9)\\nUsing a change of variable, the second summation in the above can be rewritten as follows:\\nP2t+1\\ni=2t+1 r(S([v1, v2, . . . , vt]T ))i−2tS(vt+1) = P2t\\ni=1 r(S([v1, v2, . . . , vt]T ))iS(vt+1). Plugging\\nthe latter equality into (B.9) and simplifying, we get P2t+1\\ni=1 r(S(v))i = 1. Therefore, r(S(v))\\nbelongs to the probability simplex, which establishes the inductive step. Finally, we note that ˜\\nq(α, Z)\\nbelongs to the probability simplex since it is a convex combination of probability vectors.\\nB.2\\nExtending DSelect-k to arbitrary n\\nSuppose that the number of experts n is not a power of 2. For the DSelect-k gate ˜\\nq(α, Z) to work\\nin this setting, we need each single expert selector r used by the gate to be able to handle n experts.\\nNext, we discuss how r can handle n when it is not a power of 2. Let m be the smallest integer\\nso that n < 2m. Then, we treat the problem as if there are 2m experts and use a binary encoding\\nvector z ∈Rm. For i ∈[n], we let entry r(z)i be the weight of expert i in the MoE. Note that the\\nentries r(z)i, i ∈{n + 1, n + 2, . . . , 2m}, are not associated with any experts. To avoid a situation\\nwhere r(z) assigns nonzero probability to the latter entries, we add the following penalty to the\\nobjective function:\\nξ\\nP\\ni∈[n] r(z)i where ξ is a non-negative parameter used to control the strength of\\nthe penalty. This penalty encourages to r(z)i, i ∈[n] (i.e., the entries associated with the n experts)\\nto get more probability. In our experiments, we observe that P\\ni∈[n] r(z)i converges to 1, when\\nξ is sufﬁciently large. The penalty described above is part of our TensorFlow implementation of\\nDSelect-k. We also note that there are other potential alternatives to deal with the entries r(z)i,\\ni ∈{n + 1, n + 2, . . . , 2m}, without adding a penalty to the objective function. For example, one\\nalternative is to randomly assign each of r(z)i, i ∈{n + 1, n + 2, . . . , 2m} to one of the n experts.\\nB.3\\nSmooth-step Function\\nIn Figure B.4, we plot the smooth-step function [11] and the logistic function L(x) = (1 + e−6x)−1.\\nNote that the logistic function is re-scaled to be on the same scale as the smooth-step function.\\nFigure B.4: The Smooth-step (γ = 1) and Logistic functions.\\n15\\nC\\nAdditional Experimental Results\\nC.1\\nPrediction and Expert Selection Performance on Synthetic Data\\nIn this experiment, we aim to (i) understand how the number of experts and tasks affects the prediction\\nand expert selection performance for the different gates, and (ii) quantify the beneﬁt from binary\\nencoding in our gate through an ablation study. We focus on a static gating setting, where we consider\\nthe DSelect-k and Top-k gates, in addition to two variants of the DSelect-k gate used for ablation. To\\nbetter quantify the expert selection performance and avoid model mis-speciﬁcation, we use synthetic\\ndata generated from a multi-gate MoE. First, we describe our data generation process.\\nSynthetic Data Generation:\\nWe consider 128 regression tasks, separated into four mutually\\nexclusive groups: {Gi}i∈[4], where Gi is the set of indices of the tasks in group i. As we will discuss\\nnext, the tasks are constructed in a way so that tasks within each group are highly related, while tasks\\nacross groups are only marginally related. Such a construction mimics real-world applications in\\nwhich tasks can be clustered in terms of relatedness.\\nEach group consists of 16 tasks which are generated from a group-speciﬁc MoE. The group-speciﬁc\\nMoE consists of 4 experts: {fi}i∈[4]. Each expert is the sum of 4 ReLU-activated units. The output\\nof each task in the group is a convex combination of the 4 experts. Speciﬁcally, for each task t ∈[16]\\nin the group, let α(t) ∈R4 be a task-speciﬁc weight vector. Then, given an input vector x, the output\\nof task t is deﬁned as follows:\\ny(t)(x) :=\\n4\\nX\\ni=1\\nσ(α(t))ifi(x)\\nFor each group, we create an instance of the group-speciﬁc MoE described above, where we initialize\\nall the weights randomly and independently from the other groups. In particular, we sample the\\nweights of each expert independently from a standard normal distribution. To encourage relatedness\\namong tasks in each group, we sample the task weights [α(1), α(2) . . . α(16)] from a zero-mean\\nmultivariate normal distribution where we set the correlation between any two task weights to 0.8.\\nTo generate the data, we sample a data matrix X, with 140,000 observations and 10 features, from a\\nstandard normal distribution. The data matrix is shared by all 128 tasks and the regression outputs\\nare obtained by using X as an input to each group-speciﬁc MoE. We use 100,000 observations for\\nthe training set and 20,000 observations for each of the validation and testing sets.\\nExperiment Design:\\nWe consider a multi-gate MoE and compare the following static gates:\\nDSelect-k gate, Top-k gate, and an “ablation” gate (which will be discussed later in this section). Our\\ngoal is to study how, for each gate, the number of tasks affects the prediction and expert selection\\nperformance. To this end, we consider 4 regression problems, each for a different subset of the 128\\ntasks; speciﬁcally, we consider predicting the tasks in (i) G1 (16 tasks), (ii) G1 ∪G2 (32 tasks), (iii)\\nG1 ∪G2 ∪G3 (64 tasks), and (iv) G1 ∪G2 ∪G3 ∪G4 (128 tasks). In each of the four problems, we\\nuse a multi-gate MoE to predict the outputs of the corresponding tasks simultaneously. The MoE\\nhas the same number of experts used to generate the data, i.e., if T is the total number of tasks in\\nthe problem, the MoE consists of T/4 experts, where the experts are similar to those used in data\\ngeneration (but are trainable). Each task is associated with a task-speciﬁc gate, which chooses a\\nconvex combination of 4 out of the T/4 experts. Note that unlike the architecture used to generate\\nthe data, each task gate here is connected to all experts, even those belonging to the unrelated groups.\\nThe architecture used to generate the data can be recovered if the task gates across groups do not\\nshare experts, and the task gates within each group share the same 4 experts. We use squared error\\nloss for training and tuning.\\nAblation: In addition to comparing the DSelect-k and Top-k gates, we perform an ablation study\\nto gain insight on the role of binary encoding in the DSelect-k gate. Recall that in the DSelect-k\\ngate, we introduced the single expert selector which learns a one-hot encoded vector (using a binary\\nencoding scheme). In the literature, a popular way to learn such one-hot encoded vectors is by using\\na softmax (with additional heuristics such as temperature annealing to ensure that the probability\\nconcentrates on one entry). Thus, in our ablation study, we consider the DSelect-k gate ˜\\nq(α, Z),\\nand we replace each single expert selector r(.) with a softmax-based selector. More precisely, let\\nα ∈Rk and β(i) ∈Rn, i ∈[k], be learnable parameter vectors. Then, we consider the following\\n“ablation” gate: h(α, β) := Pk\\ni=1 σ(α)iσ(β(i)). Here, σ(α)i determines the weight assigned to\\n16\\nFigure C.5: Predictive and expert selection performance on synthetic data generated from a MoE.\\nselector i, and σ(β(i)) acts as a surrogate to the single expert selector r(S(z(i))). To ensure that\\nσ(β(i)) selects a single expert (i.e., leads to a one-hot encoding), we consider two alternatives: (i)\\nannealing the temperature of σ(β(i)) during training6, and (ii) augmenting the objective with an\\nentropy regularization term (similar to that of the DSelect-k gate) to minimize the entropy of each\\nσ(β(i)). In our results, we refer to (i) by “Ablation (Annealing)” and to (ii) by “Ablation (Entropy)”.\\nNote that these two ablation alternatives can converge to a one-hot encoding asymptotically (due to\\nthe nature of the softmax), whereas our proposed gate can converge in a ﬁnite number of steps.\\nMeasuring Expert Selection Performance: To quantify the similarity between the experts selected\\nby the different tasks, we use the Jaccard index. Given two tasks, let A and B be the sets of experts\\nselected by the ﬁrst and second tasks, respectively. The Jaccard index of these two sets is deﬁned\\nby: |A ∩B|/|A ∪B|. In our experiments, we compute: (i) the average Jaccard index for the related\\ntasks, and (ii) the average Jaccard index for unrelated tasks. Speciﬁcally, we obtain (i) by computing\\nthe Jaccard index for each pair of related tasks, and then averaging. We obtain (ii) by computing the\\nJaccard index over all pairs of tasks that belong to different groups (i.e., pairs in the same group are\\nignored), and then averaging.\\nResults: After tuning, we train each competing model, with the best hyperparameters, for 100\\nrandomly-initialized repetitions. In Figure C.5, we plot the performance measures (averaged over\\nthe repetitions) versus the number of tasks. In the left plot, we report the MSE on the test set.\\nIn the middle and right plots we report the (averaged) Jaccard index for the related and unrelated\\ntasks, respectively. In the latter two plots, we also consider a random gate which chooses 4 experts\\nuniformly at random, and plot the expected value of its Jaccard index. In Figure C.5 (middle), a larger\\nindex is better since the related tasks will be sharing more experts. In contrast, in Figure C.5 (right),\\na lower index is preferred since the unrelated tasks will be sharing less experts. For all methods,\\nthe Jaccard index in Figures C.5 (middle) and (right) decreases with the number of tasks. This is\\nintuitive, since as the number of tasks increases, we use more experts, giving any two given gates\\nmore ﬂexibility in choosing mutually exclusive subsets of experts.\\nOverall, the results indicate that DSelect-k gate signiﬁcantly outperforms Top-k in all the considered\\nperformance measures, and the differences become more pronounced as the number of tasks increases.\\nFor example, at 128 tasks, DSelect-k achieves over 40% improvement in MSE and 76% improvement\\nin Jaccard index for related tasks, compared to Top-k. The DSelect-k gate also outperforms the\\n6There are pathological cases where annealing the temperature in softmax will converge to more than one\\nnonzero entry. This can happen when multiple entries in the input to the softmax have exactly the same value.\\n17\\ntwo ablation gates in which we replace the binary encoding by a Softmax-based selector. The latter\\nimprovement suggests that the proposed binary encoding scheme is relatively effective at selecting\\nthe right experts. We also investigated the poor performance of the Ablation (Entropy) gate, and it\\nturns out that the Softmax-based single expert selectors, i.e., the σ(β(i))’s, tend to select the same\\nexpert. Speciﬁcally, we set k = 4 in the ablation gate, but it ends up selecting ∼2 experts in many of\\nthe training repetitions. In contrast, the DSelect-k and Top-k gates select 4 experts.\\nC.2\\nGate Visualizations\\nC.2.1\\nMovieLens\\nIn Figure C.6, we plot the expert weights during training on the MovieLens dataset, for the Top-k and\\nDSelect-k gates (after tuning both models). The plots show that Top-k exhibits frequent “jumps”,\\nwhere in a single training step an expert’s weight can abruptly change from a nonzero value to\\nzero. These jumps keep occurring till the end of training (at around 105 training steps). In contrast,\\nDSelect-k has smooth transitions during training. Additional details on the MovieLens dataset and\\nthe MoE architecture used can be found in Section 4.1 of the paper.\\nFigure C.6: Expert weights during training on the MovieLens dataset. Each color corresponds to a\\nseparate expert. The plots are for the best models obtained after tuning.\\nC.2.2\\nSynthetic Data\\nHere we consider a binary classiﬁcation dataset generated from a static MoE that consists of 4 experts.\\nWe train another MoE model which employs 16 experts: 4 of these experts are copies (i.e., have\\nexactly same weights) of the 4 experts used in data generation, and the rest of the experts are randomly\\ninitialized. We freeze all the experts and train only over the gate parameters. In this simple setting, we\\nexpect the gate to be able to recover the 4 experts that were used to generate the data. We trained two\\nMoE models: one based on Top-k and another based on DSelect-k. After tuning both models, Top-k\\nrecovered only 1 right expert (and made 3 mistakes), whereas our model recovered all 4 experts. In\\nFigures C.7 and C.8, we plot the expert weights during training, for the Top-k and DSelect-k gates,\\nrespectively. The Top-k exhibits a sharp oscillatory behavior during training, whereas DSelect-k has\\nsmooth transitions.\\nAdditional details on data generation and model:\\nWe consider a randomly initialized “data-\\ngenerating” MoE with 4 experts (each is a ReLU-activated dense layer with 4 units). The output of\\nthe MoE is obtained by taking the average of the 4 experts and feeding that into a single logistic\\nunit. We generate a multivariate normal data matrix X with 20,000 observations and 10 features\\n(10,000 observations are allocated for each of the training and validation sets) . To generate binary\\nclassiﬁcation labels, we use X as an input to the data-generating MoE and apply a sign function to\\nthe corresponding output. For training and tuning, we consider a MoE architecture with 16 experts:\\n4 of these experts are copies of the experts used in data generation, and the rest of the experts are\\ninitialized randomly. All experts are frozen (not trainable). A trainable gate chooses 4 out of the 16\\nexperts, and the ﬁnal result is fed into a single logistic unit. We optimized the cross-entropy loss\\nusing Adam with a batch size of 256, and tuned over the learning rate in {10−1, 10−2, . . . , 10−5}.\\n18\\nFigure C.7: Expert weights during training on synthetic data generated from a MoE. Each color\\ncorresponds to a separate expert. The left plot is a magniﬁed version of the right plot. The plots are\\nfor the best model obtained after tuning.\\nFigure C.8: Expert weights during training on synthetic data generated from a MoE. Each color\\ncorresponds to a separate expert. The left plot is a magniﬁed version of the right plot. The plots are\\nfor the best model obtained after tuning.\\nC.3\\nGate Convergence and FLOPS\\nIn Table C.4, we report the percentage of training steps required for S(Z) to converge to a binary\\nmatrix in the DSelect-k gate, on several real datasets. These results are based on the tuned models\\ndiscussed in Section 4 of the paper. We also report the number of ﬂoating point operations (FLOPS)\\nrequired by the MoE based on DSelect-k relative to the MoE based on Top-k, during training. The\\nresults indicate that the number of training steps till convergence to a binary matrix depends on the\\nspeciﬁc dataset: ranging from only 0.04% on the MovieLens dataset to 80% on the Multi-Fashion\\nMNIST. Moreover, on certain datasets (MovieLens with α = 0.9 and Multi-MNIST), DSelect-k\\nrequires less FLOPS during training than Top-k, i.e., DSelect-k is effective at conditional training (on\\nthese particular datasets).\\nMovieLens\\nMulti-MNIST\\nMulti-Fashion\\nα = 0.1\\nα = 0.5\\nα = 0.9\\n% Training Steps until Binary S(Z)\\n11.37\\n9.39\\n0.04\\n42.33\\n80.02\\nFLOPS(DSelect-k)/FLOPS(Top-k)\\n1.5\\n1.2\\n0.6\\n0.8\\n1.2\\nTable C.4: We report two statistics: (i) Percentage of training steps required for the DSelect-k gate to converge\\nto a binary matrix, and (ii) the number of FLOPS needed by the DSelect-k based MoE during training relative\\nto that of Top-k. The parameter α controls the weight assigned to task 1’s loss in the MovieLens dataset—see\\nSection 4.1 of the paper for more details.\\n19\\nC.4\\nMovieLens\\nIn Table C.5, we report the accuracy for task 1 (classiﬁcation) and the loss for task 2 ( regression) for\\nthe competing methods on the MovieLens dataset.\\nα = 0.1\\nα = 0.5\\nα = 0.9\\nT2 Loss\\nT1 Accuracy\\nT2 Loss\\nT1 Accuracy\\nT2 Loss\\nT1 Accuracy\\nStatic\\nDSelect-k\\n4038 ± 5\\n83.03 ± 0.07\\n3926 ± 6\\n83.86 ± 0.02\\n3943 ± 5\\n84.04 ± 0.01\\nTop-k\\n4056 ± 4\\n84.09 ± 0.05\\n4002 ± 4\\n84.21 ± 0.02\\n3884 ± 4\\n84.17 ± 0.02\\nGumbel Softmax\\n4172 ± 2\\n80.76 ± 0.04\\n4085 ± 3\\n83.71 ± 0.01\\n3878 ± 4\\n84.17 ± 0.02\\nPer-example\\nDSelect-k\\n4030 ± 7\\n83.03 ± 0.12\\n3981 ± 7\\n83.92 ± 0.03\\n3932 ± 1\\n84.07 ± 0.02\\nTop-k\\n4057 ± 9\\n83.28 ± 0.1\\n3995 ± 8\\n83.91 ± 0.03\\n3914 ± 4\\n84.05 ± 0.01\\nBaselines\\nSoftmax MoE\\n4047 ± 1\\n78.73 ± 0.01\\n4028 ± 3\\n83.56 ± 0.01\\n3970 ± 3\\n83.86 ± 0.02\\nShared Bottom\\n3993 ± 2\\n79.06 ± 0.02\\n3875 ± 2\\n82.65 ± 0.02\\n3991 ± 2\\n84.01 ± 0.01\\nTable C.5: Mean test loss for task 2 (T2) and accuracy for task 1 (T1) on MovieLens (standard error is shown\\nnext to each mean). The parameter α determines the weight of Task 1’s loss (see text for details). The test loss is\\nmultiplied by 104.\\nD\\nExperimental Details\\nComputing Setup: We ran the experiments on a cluster that automatically allocates the computing\\nresources. We do not report the exact speciﬁcations of the cluster for conﬁdentiality.\\nGumbel-softmax Gate: [34, 25] present an approach for learning which layers in a neural network\\nto activate on a per-task basis. The decision to select each layer is modeled using a binary random\\nvariable whose distribution is learned using the Gumbel-softmax trick. Note that the latter approach\\ndoes not consider a MoE model. Here we adapt the latter approach to the MoE; speciﬁcally we\\nconsider a Gumbel-softmax gate that uses binary variables to determine which experts to select. Given\\nn experts {fi}n\\ni=1, this gate uses n binary random variables {Ui}n\\ni=1, where Ui determines whether\\nexpert fi is selected. Moreover, the gate uses an additional learnable vector α ∈Rn that determines\\nthe weights of the experts. Speciﬁcally, the gate is a function d(α, U) whose i-th component (for any\\ni ∈[n]) is given by:\\nd(α, U)i = σ(α)iUi\\nTo learn the distribution of the Ui’s, we use the Gumbel-softmax trick as described in [34]. More-\\nover, following [34], we add the following sparsity-inducing penalty to the objective function:\\nλ P\\ni∈[n] log ψi, where ψi is the Bernoulli distribution parameter of Ui, and λ is a non-negative\\nparameter used to control the number of nonzeros selected by the gate. Note that the latter penalty\\ncannot directly control the number of nonzeros as in DSelect-k or Top-k.\\nD.1\\nMovieLens\\nArchitecture: For MoE-based models, we consider a multi-gate MoE architecture (see Figure 1),\\nwhere each task is associated with a separate gate. The MoE uses 8 experts, each of which is a\\nReLU-activated dense layer with 256 units, followed by a dropout layer (with a dropout rate of\\n0.5). For each of the two tasks, the corresponding convex combination of the experts is fed into\\na task-speciﬁc subnetwork. The subnetwork is composed of a dense layer (ReLU-activated with\\n256 units) followed by a single unit that generates the ﬁnal output of the task. The shared bottom\\nmodel uses a dense layer (whose number of units is a hyperparameter) that is shared by the two tasks,\\nfollowed by a dropout layer (with a rate of 0.5). For each task, the output of the shared layer is fed\\ninto a task-speciﬁc subnetwork (same as that of the MoE-based models).\\nHyperparameters and Tuning: We tuned each model using random grid search, with an average of\\n5 trials per grid point. We used Adagrad with a batch size of 128 and considered the following hyper-\\nparameters and ranges: Learning Rate: {0.001, 0.01, 0.1, 0.2, 0.3}, Epochs: {5, 10, 20, 30, 40, 50},\\nk for Top-k and DSelect-k: {2, 4}, λ for DSelect-k: {0.1, 1, 10}, γ for smooth-step: {1, 10}, Units\\nin Shared bottom: {32, 256, 2048, 4096, 8192}, λ in Gumbel-softmax: {10−6, 10−5, . . . , 10}. For\\nGumbel-softmax, we pick the best solution whose expected number of nonzeros is less than or equal\\nto 4.\\n20\\nD.2\\nMulti-MNIST and Multi-Fashion MNIST\\nArchitecture: MoE-based models use a multi-gate MoE (as in Figure 1). Each of the 8 experts is a\\nCNN that is composed (in order) of: (i) convolutional layer 1 (kernel size = 5, number of ﬁlters =\\n10, ReLU-activated) followed by max pooling, (ii) convolutional layer 2 (kernel size = 5, number of\\nﬁlters = 20, ReLU-activated) followed by max pooling, and (iii) a stack of ReLU-activated dense\\nlayers with 50 units each (the number of layers is a hyperparameter). The subnetwork speciﬁc to\\neach task is composed of a stack of 3 dense layers: the ﬁrst two have 50 ReLU-activated units and the\\nthird has 10 units followed by a softmax. The shared bottom model uses a shared CNN (with the\\nsame architecture as the CNN in the MoE). For each task, the output of the shared CNN is fed into a\\ntask-speciﬁc subnetwork (same as that of the MoE-based models).\\nHyperparameters and Tuning: We tuned each model using random grid search, with an average\\nof 5 trials per grid point. We used Adam with a batch size of 256 and considered the following hyper-\\nparameters and ranges: Learning Rate: {0.01, 0.001, 0.0001, 0.00001}, Epochs: {25, 50, 75, 100}, k\\nfor Top-k and DSelect-k: {2, 4}, γ for smooth-step: {0.1, 1, 10}, Number of dense layers in CNN:\\n{1, 3, 5}, λ in Gumbel-softmax: {0, 10−3, 10−2, 10−1, 1, 10, 1000}. For Gumbel-softmax, we pick\\nthe best solution whose expected number of nonzeros is less than or equal to 4.\\nD.3\\nRecommender System\\nEach of the 8 experts in the MoE consists of a stack of ReLU-activated dense layers with 256 units\\neach. We ﬁx k to 2 in both DSelect-k and Top-k. We tune over the learning rate and architecture. For\\nboth models, training is terminated when there is no signiﬁcant improvement in the validation loss.\\nD.4\\nSynthetic Data\\nWe tuned each model using random grid search, with an average of 5 trials per grid point. We used\\nAdam with a batch size of 256 and considered the following hyperparameters and ranges: Learning\\nrate: {0.001, 0.01, 0.1}, Epochs {25, 50, 75, 100}, γ for smooth-step: {5, 10, 15}, λ for DSelect-k:\\n{0.001, 0.005, 0.01, 0.1}, λ for Ablation (Entropy): {10−6, 10−5, 10−4, 10−3, 10−2, 10−1, 1, 100}.\\nMoreover, for Ablation (Annealing), we anneal the temperature of softmax starting from a hyperpa-\\nrameter s down to 10−16 (the temperatures are evenly spaced on a logarithmic scale). We tune the\\nstarting temperature s over {10−6, 10−5, 5×10−5, 10−4, 2.5×10−4, 5×10−4, 7.5×10−4, 10−3, 5×\\n10−3, 10−2} (note that such a ﬁne grid was necessary to get annealing to work for the ablation gate).\\n21\\n', 'source_name': 'DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning', 'source_url': 'https://arxiv.org/abs/2106.03760'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Hash_Layers.pdf #66\n",
      "{'content': 'Hash Layers For Large Sparse Models\\nStephen Roller\\nSainbayar Sukhbaatar\\nArthur Szlam\\nJason Weston\\nFacebook AI Research\\nAbstract\\nWe investigate the training of sparse layers that use different parameters for different\\ninputs based on hashing in large Transformer models. Speciﬁcally, we modify the\\nfeedforward layer to hash to different sets of weights depending on the current token,\\nover all tokens in the sequence. We show that this procedure either outperforms or\\nis competitive with learning-to-route mixture-of-expert methods such as Switch\\nTransformers and BASE Layers, while requiring no routing parameters or extra\\nterms in the objective function such as a load balancing loss, and no sophisticated\\nassignment algorithm. We study the performance of different hashing techniques,\\nhash sizes and input features, and show that balanced and random hashes focused\\non the most local features work best, compared to either learning clusters or using\\nlonger-range context. We show our approach works well both on large language\\nmodeling and dialogue tasks, and on downstream ﬁne-tuning tasks.\\n1\\nIntroduction\\nRecent studies of Transformer models have shown a clear trend towards improvements with scale in\\ndata and model size [1], mirroring the same trend in Machine Learning more generally. However,\\nwhen architected naively, larger (in terms of parameter count) models are slower to train and to\\nevaluate; and at extreme scale, with current computer systems, necessitate complex engineering to\\nfacilitate communication between workers. To address these challenges, researchers have studied\\nMixtures-of-Experts (MoE) models [2, 3, 4, 5, 6, 7, 8], where a “gater” routes computation through\\na sparse subset of the weights of the model (the “expert modules”). Speciﬁcally in the setting of\\nTransformers for Natural Language Processing (NLP), recent approaches have led to state of the art\\nperformance in language modeling [8]. MoE models allow increasing the number of parameters in\\nthe model while holding steady the number of computations that affect a given sample.\\nA key component to a MoE model is the routing (gating) strategy. While MoE models can be\\ncomputationally advantageous per parameter compared to a dense model, they might be functionally\\nless powerful per parameter. A poor routing strategy might lead to expert modules that are not\\nproperly specialized (essentially making a stochastic ensemble model); or overly specialized, using\\nthe data assignment function to overﬁt. Meanwhile, the routing strategy itself must be efﬁcient.\\nA standard approach is to train a layer of weights that makes the routing decision based upon the input\\nto the layer to be routed. Classically, this may have been implemented with a softmax over the choice\\nof expert modules, and ﬁtted via backpropagation. However, a dense softmax requires all expert\\nmodules to run on all data points at train time, which negates the computational savings. Several\\nworks have shown that sparsity can be maintained during training, e.g. [9, 7, 8, 10]. In particular,\\nSwitch Transformers [8] select the top expert per token using a softmax over the token’s hidden\\nstate, but require a load balancing term in the objective function or they can become imbalanced or\\ndegenerate, giving poor results. BASE Layers [10] employ a linear assignment algorithm to try to\\nresolve the same problem.\\nPreprint. Under review.\\narXiv:2106.04426v3  [cs.LG]  20 Jul 2021\\n.. .\\n. . .\\n. ..\\n. . .\\n“We”\\n“eat”\\n“every”\\n“taco”\\n1\\nMoE\\nFFN\\n1\\n.. .\\n2\\nMoE\\nFFN\\n2\\n. . .\\n3\\nMoE\\nFFN\\n3\\n. ..\\n4\\nMoE\\nFFN\\n4\\n.. .\\nself-attention\\nLayer l\\nLayer l + 1\\nFFN1\\nFFN2\\nFFN3\\nHash\\n3\\n¯\\nhl\\nhl\\nMoE FFN\\nFigure 1: Overview of the Hash Layer. Tokens are routed to ﬁxed expert modules based on their\\nhash.\\nIn this work, we describe a simple, sparse, efﬁcient routing strategy based on hashing input tokens\\nthat is effective in the Transformers-for-NLP setting. We show this approach is effective on a number\\nof datasets, comparing favorably to both Switch Transformers and BASE Layers. As the routing\\nstrategy requires no extra parameters, no change to the objective function or assignment algorithm, its\\nsimplicity means it is robust, fast and easy to implement. We provide detailed analysis to explain why\\nour method works, and in which conditions. Given that when training very large models one may\\ntypically have only one shot given the required compute budget, and experimenters will be unable to\\ntry many parameter choices, we hence advocate our approach as a strong candidate for such a setting.\\n2\\nBackground\\nLet us ﬁrst introduce the Mixture-of-Experts setting where we apply our hash-based routing strategy.\\nWe use the same setting as [11, 8, 10] where a feedforward network (FFN) in a Transformer is\\nreplaced by its MoE version. Given a tokenized input sequence {x1, x2, . . . , xT } of T tokens, a\\nrepresentation for each token is computed in parallel by a standard Transformer [12]\\nhL\\n1 , hL\\n2 , . . . , hL\\nT = TRANSFORMER(x1, x2, . . . , xT ).\\n(1)\\nThe Transformer consists of L layers that computes ﬁnal hidden states for each token, and each layer\\nis composed of self-attention and FFN sublayers, where FFNs are two-layer fully connected networks\\n¯\\nhl\\nt = SelfAttn(hl−1\\nt\\n)\\nhl\\nt = FFN(¯\\nhl\\nt).\\n(2)\\nHere we omit skip-connections and normalization for brevity. We can then replace one or more of the\\nFFN sublayers with expert modules. Replacing the FNN at layer l with K expert FFNs, their output\\nis then mixed with some gating function g(·):\\nhl\\nt = FFN(¯\\nhl\\nt)\\n→\\nhl\\nt =\\nK\\nX\\ni=1\\ngi(¯\\nhl\\nt) FFNi(¯\\nhl\\nt),\\nt = 1, . . . , T,\\n(3)\\nwhere importantly each token is routed to a different mixture of experts, as the gating function\\ndepends on the token’s speciﬁc hidden state ¯\\nhl\\nt.\\nSparse MoE methods assume gating values gi are often zero, so only a few experts need to be\\ncomputed for better efﬁciency. As expert FFNs do not share parameters, the number of parameters\\nincreases with K while the amount of computations per input token stays the same if the MoE FFN\\nonly routes to a single expert, and computation of gi is cheap. While this allows training of large\\ncapacity models with small compute budget, optimizing gi in the sparse setting can be tricky.\\n3\\nMethod\\nIn this paper we propose a simple gating mechanism that is especially efﬁcient because only one\\nexpert is active, and it has no routing network parameters to be learnt. Recent work [11, 8, 10] has to\\nlearn parameters that determine the routing to expert modules based on hidden states, which have\\nto be optimized in tandem with the expert weights themselves. This can potentially cause difﬁculty\\nbecause during training membership for each expert is changing while it is trying to learn the mapping\\n2\\nfor those members. We instead advocate for a ﬁxed mapping to experts. Namely, by hashing the\\ntokens into a ﬁxed number of buckets, each bucket corresponding to an expert:\\nhl\\nt = FFNhash(xt)(¯\\nhl\\nt),\\nt = 1, . . . , T.\\n(4)\\nWhile the FFN still takes the hidden state ¯\\nhl\\nt as input, our routing function uses the original input\\ntoken xt rather than the hidden state, see Figure 1 for a graphical depiction. We are free to choose\\nfrom various possible hash functions, which we will consider below. However, for training purposes,\\nthe hash function is ﬁxed in advance, and in this way, our routing mechanism requires no training and\\nhas no adjustable parameters.\\n3.1\\nHash Functions\\nHash functions have long been employed throughout Computer Science [13], and can take a variety\\nof forms. In our work, we generally employ pre-computed hash functions, which use a lookup table\\nduring learning – precomputed in advance – to map tokens to expert modules.\\nWe consider several kinds of hash functions as possible choices for routing tokens to expert modules.\\nThe simplest is Random Hash, wherein we assign every token to a ﬁxed, random expert at initializa-\\ntion. Due to the Zipﬁan distribution of token frequency, this naturally produces imbalance across the\\ndifferent expert modules. As balancing has been previously shown to be important for training MoE\\nmodels [8, 10], we also consider Balanced assignment. In this method, we build the lookup table\\nbefore training the model using the training data distribution by greedily assigning the most frequent\\ntokens to the emptiest buckets. The resulting assignment structure is signiﬁcantly more balanced than\\nRandom Hashing, but not perfect, as the frequency of some tokens exceeds the ideal distribution.\\nRandom and Balanced hashing exploit the inductive bias of auto-regressive models and hash on the\\ninput token, but we also consider other possibilities: Bigram Hash uses the current and previous\\ntoken (xt−1, xt) rather than only the current token, while Previous Token Hash uses the previous\\ntoken xt−1, ignoring the current input. We also consider a sanity check which hashes based on the\\nPosition in the sequence, which we expect to have little impact, as absolute positions carry little\\ninformation in natural language. Each of these hash functions is used to assess the value of the\\ninformation being routed-on in our subsequent experimental analysis.\\nAs an upper baseline, we also evaluate using an Oracle Future Hash, which hashes based on the\\noutput token xt+1, rather than input token. This Oracle Hash checks how powerful routing decisions\\ncan be in solving a task. Similarly, we also consider Predicted Future Token Hash, which utilizes a\\nbaseline Transformer to make a prediction of the output token, and then hashes over this prediction.\\nClustered Hashes\\nBased on the intuition that similar tokens may want to be routed to the same\\nexpert, we also experiment with Clustered Hashes. We obtain clusters by performing k-means\\nclustering with a ﬁxed number of clusters using token embeddings from a baseline Transformer\\nmodel. Each expert is assigned a centroid, and tokens are assigned to their closest cluster.\\nDispersed Hashes\\nWe also consider the opposite hypothesis: that similar-tokens should be placed\\nin different buckets, where the assumption is that very similar tokens need ﬁne distinctions which\\nrequires more model capacity (hence assigning to different experts). To do this, we use the same\\nk-means clusters as before, but distribute all tokens within each cluster equally across all buckets.\\n3.2\\nMultiHash Layers\\nIn the standard FFN MoE approach, all K expert modules have independent parameters, but here we\\nconsider another option. It is known in the hashing literature that multiple hashes can provide better\\nallocations in many contexts [14]. We consider such schemes in the context of sparse routing. Let us\\nassume we are given N different hashing functions, and for a given input token x we compute these\\nhashes, denoted as km = hashm(x), m = 1, . . . , N. Assuming the usual expert FFN is a function\\nB(relu(A(h))) where A : Rd →RD and B : RD →Rd, we split the linear layers into N segments,\\nAm : Rd →RD/N and Bm : RD →Rd/N. Then we compute:\\nv = relu([Ak1(h), . . . , AkN (h)])\\nFFNMH(h) = [Bk1(v), . . . , BkN (v)].\\n3\\nThat is, use hashing to select the parameters we are going to use for each segment, and then\\nconcatenate them together. The advantage is that we are now no longer reliant on the quality of a\\nsingle hash function, but have multiple chances to produce good quality partitions. This perhaps can\\nalso be seen as analogous to the multi-head attention process already used in Transformers.\\n4\\nRelated Work\\nSparse MoE models, where only a few expert modules are active for any input, in particular in the\\ncontext of NLP, have been studied recently in [6, 11]. In these works, the gating is learned via\\nbackpropagation, perhaps with a regularizer to encourage load balancing across experts. [8] showed\\nthat models in [11] can be successfully trained with each input assigned to exactly one expert. Another\\nsuch approach for Transformers, where the routing is learned via solving a linear assignment problem,\\nis studied in [10]. [? ] uses a different approach, where product keys enable nearest neighbor search\\nto select parameters. More generally, using MoE to trade off compute time (at the cost of possible\\ndata fragmentation) has a long history, see e.g. [3, 7].\\nThe approach in this work is different from all of these in that the assignments use no learning\\nwhatsoever, and instead make use of the inductive biases possible in the setting of natural language.\\nIn particular, we use the fact that n-grams are themselves decent language models [15]. Thus\\nthis work is related to previous work attempting to combine neural and n-gram language models\\n[16, 17, 18, 19, 20, 21].\\nOur work is also related to feature hashing in linear models and kernel methods [22, 23], where\\nword or n-gram features are hashed to provide a new lower dimensional feature space. [22] showed\\nthat when performing such feature hashing the interaction between random subspaces is negligible\\nwith high probability. [? ] uses hashing to compress neural networks, rather than increase their\\nparameters as we do here. Work on long-context Transformers has recently used hashing techniques\\nto speed up access to long-range token history via sparse self-attention patterns, particularly in\\nRouting Transformers [24] and the Reformer [25]. In contrast, our work uses hashing to access a\\nlarge set of parameters via sparse routing, rather than sparse access to input features.\\n5\\nExperiments\\n5.1\\nTasks\\nPushshift.io Reddit\\nWe use a variant of Reddit discussions, which has also been used in several\\nexisting studies, see e.g. [26, 27, 28, 29]. Following [30], we use a previously existing Reddit dataset\\nextracted and obtained by a third party and made available on pushshift.io [31], training to generate a\\ncomment conditioned on the full thread leading up to the comment, spanning 1.5B training examples.\\nWe use the same BPE dictionary as [32], comprising of 8008 tokens.\\nRoBERTa+cc100en Data\\nWe use the same data used to train BASE [10], which consists of\\napproximately 100B tokens, combining corpora used in RoBERTa [33] with the English subset of the\\nCC100 corpus [34]. The GPT2 dictionary, of size 51200, is used for tokenization. For our seq2seq\\nexperiments, we arrange this data splitting by sentence to predict the next turn. We consider it as the\\noriginally intended language modeling task in our experiments comparing with BASE [10].\\nWikitext-103\\nWikitext-103 is a smaller language modeling benchmark [35] consisting of a collec-\\ntion of Wikipedia articles of over 100 million tokens, and a ﬁxed vocabulary size of 270K tokens is\\nprovided. We view this as a seq2seq task in our experiments, again splitting by sentence.\\nDownstream BST tasks\\nFinally, we use the Blended Skill Talk (BST) dialogue tasks used in [32]\\nafter pushshift.io Reddit pre-training to evaluate ﬁne-tuning performance of dense vs. sparse models.\\n5.2\\nExperimental Setup\\nSeq2Seq Setup\\nThe majority of our experiments are carried out in ParlAI1 platform using an\\nencoder-decoder Transformer framework. We ﬁrst train several standard (dense) Transformers, with\\n1http://parl.ai\\n4\\nTable 1: Comparison of Models on pushshift.io Reddit. We show three sizes of dense Transformer\\ncompared to Switch Transformers and using Hash Layers with various numbers of modules and\\nsparse layers, e.g. 5x16 means 5 sparse layers with 16 modules each. All Switch and Hash Layer\\nmodules are built to the same computational complexity as the 11 layer baseline Transformer, but\\nhave more parameters; the larger dense models have similar total parameters, but use more compute.\\nModel\\nConﬁguration\\nParams\\nValid PPL\\nTest PPL\\nBaseline Transformer\\nlayers=11, d=1024, D=4096\\n222M\\n24.90\\n24.96\\nWider Transformer (more compute)\\nlayers=11, d=2048, D=6144\\n755M\\n23.32\\n23.38\\nDeeper Transformer (more compute)\\nlayers=22, d=1536, D=4096\\n755M\\n22.72\\n22.78\\nSwitch Transformer\\nlayers=11,modules=1x64, load_bal=0.1\\n751M\\n23.65\\n23.73\\nHash Layer\\nlayers=11,modules=1x64\\n751M\\n23.16\\n23.23\\nSwitch Transformer\\nlayers=11,modules=1x128, load_bal=0.1\\n1.28B\\n23.52\\n23.58\\nHash Layer\\nlayers=11,modules=1x128\\n1.28B\\n22.89\\n22.95\\nSwitch Transformer\\nlayers=11,modules=5x16, load_bal=0.01\\n852M\\n23.19\\n23.25\\nSwitch Transformer\\nlayers=11,modules=5x16, load_bal=0.1\\n852M\\n23.00\\n22.93\\nHash Layer\\nlayers=11,modules=5x16\\n852M\\n23.21\\n23.27\\nTable 2: Comparison of Models on RoBERTa+cc100en Data. We compare a dense transformer\\nwith the same parameters as our sparse models, except with 1 sparse layer with 64 modules (1x64).\\nModel\\nConﬁguration\\nParams\\nValid PPL\\nBaseline Transformer\\nlayers=11, d=1024, D=4096\\n266M\\n28.85\\nSwitch Transformer\\nlayers=11, modules=1x64, load_bal=0.1\\n795M\\n27.41\\nHash Layer\\nlayers=11, modules=1x64\\n794M\\n26.99\\n2 encoder layers and either 11 or 22 decoder layers, following the structure in [32] for training on\\npushshift.io Reddit. We refer to the one with 11 layers and embedding size of d = 1024 and FFN\\nhidden layer size of D = 4096 as our Baseline Transformer. We also train a \"Wider\" model with\\nD = 6144, and a \"Deeper\" model with 22 decoder layers, and D = 4096. The Baseline model\\nhas 222M parameters, and the \"Wider\" and \"Deeper\" are selected to both have 755M parameters\\neach. These models are compared to the Hash Layer methods detailed in section 3 and to Switch\\nTransformers of the same sizes and settings. The load balancing for Switch is optimized on the\\nvalidation set. For both Hash and Switch we use the \"Baseline\" Transformer size detailed above as the\\narchitecture that we add sparse routing layers to by replacing one or more of the original dense layers.\\nAll experiments are run for 100k updates; a table of hyperparameters is provided in subsection B.1.\\nBASE Comparison\\nWhile most of our analysis takes place in the setup described above with\\nmodels up to 1.28B parameters, to test our methods at scale on larger sparse models, we adopt the\\nBASE Layer setup [10] and code base2 instead where we compare 4.5B parameter Hash and BASE\\nLayer models. This setting uses pure language models rather than the Seq2Seq setup above. We use\\nthe architecture, data (RoBERTa+cc100en), and hyperparameters directly from [10], using either a\\nsingle sparse routing layer consisting of 3 stacked FFNs (D = 8192) on the middle layer of a 25 layer\\nnetwork, or 3 routing layers evenly spaced in the network. In order to compare with BASE directly,\\nwe keep all hyperparameters ﬁxed and only change the routing method; we use a balanced assignment\\nHash Layer in this case. We trained until 40k steps had been reached. A table of hyperparameters is\\nprovided in subsection B.2.\\n5.3\\nResults and Analysis\\n5.3.1\\nComparison between Hash, Switch and Dense models\\nHash vs. Switch routing on a single layer\\nWe ﬁrst compare a Hash layer (with balanced hash) to\\na Switch layer, on an otherwise dense Transformer, where sparse routing is performed on layer 7 of\\nthe decoder. Both methods use 64 expert FFNs with 751M total parameters. Results on pushshift.io\\nReddit are given in Table 1 (rows 4 and 5) and on the RoBERTa+cc100en data in Table 2 (rows 2 and\\n3). We ﬁnd Hash Layers outperforming Switch on both datasets by about 0.4-0.5 perplexity.\\n2Made available within Fairseq [36].\\n5\\n10000\\n30000\\n50000\\n70000\\n90000\\nNumber of Updates\\n22\\n24\\n26\\n28\\n30\\nPerplexity\\nPushshift.io Reddit\\nTransformer\\nSwitch\\nHash Layer\\n0\\n10000\\n20000\\n30000\\n40000\\nNumber of Updates\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\nPerplexity\\nRoBERTa + cc100en\\nBASE\\nHash Layer\\n3x Hash Layer\\nFigure 2: Comparison of Hash Layers with other models. (left) Validation perplexity of a baseline\\nTransformer, Switch Transformer, and Hash Layer on the pushshift.io Reddit dataset with 128\\nmodules. (right) Validation perplexity of BASE, Hash Layer, and a deeper Hash Layer model on the\\nRoBERTa+cc100en dataset. All sparse models have the same number of parameters.\\n16\\n32\\n64\\n128\\nNumber of Experts\\n23.00\\n23.25\\n23.50\\n23.75\\n24.00\\nPerplexity\\nPerformance by Number of Experts\\nSwitch\\nHash Layer\\n1\\n3\\n5\\n7\\n9\\nLayer\\n23.00\\n23.25\\n23.50\\n23.75\\n24.00\\nPerplexity\\nPosition of Hash Layer\\nHash Layer\\nFigure 3: Comparing Different Number of Expert Modules and Layer Position. We compare\\n(left) the validation perplexity wrt. the number of expert modules on the pushshift.io Reddit task for a\\nHash or Switch Layer on layer 7 of an 11 layer decoder in a Transformer. The baseline Transformer\\nobtains a perplexity of 24.9. We compare the performance when adjusting the layer position of a 64\\nmodule Hash Layer on the same task (right). Placing on later layers works best.\\nDense vs. Sparse Models\\nBoth Hash and Switch sparse models outperform the dense Baseline\\n(222M parameters) they are based on, as well as the Wider Transformer (755M parameters). However,\\nthe Deeper Transformer (755M parameters) outperforms the sparse models which have a similar\\nnumber of parameters. However, we note that due to its dense rather than conditional compute it is\\nslower in inference speed. We see this as a general trend: good dense models can get more power\\nout of the same number of parameters than sparse models. However, sparse models, although more\\nwasteful in memory, give better perplexity for the same speed (i.e, we should compare to the Baseline\\nTransformer in this case, which has roughly the same amount of computation).\\nHash layer module size\\nWe conduct the same pushshift.io Reddit experiments as above, but\\naltering the number of expert modules in both Hash and Switch. Increasing from 64 to 128 modules\\n(1.28B parameters total) sees an even larger improvement of Hash over Switch (about 0.6 perplexity),\\nsee Table 1 (rows 6 and 7), and Figure 2 (left). Trying smaller numbers of modules, 16 and 32, and\\nplotting all the results in Figure 3 (left) we see that for small numbers of modules Hash and Switch\\nperform similarly, but the gap grows larger as the number of modules increases. For small numbers\\nof modules, we hypothesize that learning to route, as Switch does, would be more important to be\\nperformant with those choices, but with larger numbers of modules many routing choices could work.\\nHence, Hash layers can work well in that setting, and learning to route becomes less important.\\nHash layer position\\nWe also experiment to ﬁnd the best position layer-wise for the sparse routing\\nto take place. In Figure 3 (right) we plot perplexity for the 64 module Hash Layer, placing on different\\nlayers of the decoder. We ﬁnd that later layers perform better, but even the worst performing choice\\n(layer 1) is still performing well compared to other baselines: as good as Switch Transformers using\\nlater layers in fact. We note that analysis of BASE Layers [10] showed a similar trend that later\\nlayers work well. Hypothesizing that conditional compute gives the ability to make ﬁne-grained\\nspecializations, it follows that it is worth making those distinctions after more obvious features have\\nﬁrst been extracted. We will return to this argument in later experiments.\\n6\\nTable 3: Different Hash Layering Methods on pushshift.io Reddit.\\nModel\\nHashing Type\\nValid PPL\\nTest PPL\\nBaseline Transformer\\n-\\n24.90\\n24.96\\nHash Layer 1x64\\nBalanced assignment\\n23.16\\n23.23\\nHash Layer 1x64\\nFixed random assignment\\n23.22\\n23.27\\nHash Layer 1x64\\nToken clustering (using Baseline Transformer)\\n23.90\\n23.99\\nHash Layer 1x64\\nDispersed Hash (within token clusters)\\n23.17\\n23.22\\nHash Layer 1x64\\nHash on position\\n25.07\\n25.14\\nHash Layer 1x64\\nBigrams\\n24.19\\n24.28\\nHash Layer 1x64\\nPrevious token\\n24.16\\n24.22\\nHash Layer 1x64\\nFuture token predictions (using Transformer Baseline)\\n25.02\\n25.09\\nHash Layer 1x64\\nFuture token (Oracle)\\n1.97\\n1.97\\nHash Layer 5x16\\nSame hash per layer (balance assignment)\\n23.74\\n23.81\\nHash Layer 5x16\\nDifferent Hash per layer\\n23.21\\n23.27\\nHash Bucket (sorted)\\nBucket Frequency\\nIdeal balance\\nRandom Hash Bucket Frequency\\nHash Bucket (sorted)\\nBucket Frequency\\nIdeal balance\\nBalanced Hash Bucket Frequency\\nFigure 4: Relative frequency for 64 expert modules with Random Hash (left) and Balanced Hash\\n(right). The Zipﬁan distribution makes perfect balance impossible, but Balanced Hash is closer.\\nMulti-layer routing\\nWe evaluate placing sparse routing every other layer, 16 different modules each\\nin Table 1 (rows 8-10). Switch and Hash perform similarly in this setting, with Switch outperforming\\nwith the optimal choice of 0.1 load balancing (23.00 vs. 23.21), and the same performance (23.19)\\nfor balancing parameter 0.01. Given the results of Figure 3 (left), the small number of modules in\\nthis case may make performance close.\\nDownstream ﬁne-tuning\\nWe compare several of the pushshift.io Reddit models for the goal of\\nﬁne-tuning on downstream tasks. We experiment with either ﬁne-tuning the whole model, or freezing\\nsome parts of the model during ﬁne-tuning, as well as altering the load balancing for Switch at\\nﬁne-tune time. Results are given in Appendix A. We ﬁnd that the ﬁne-tune results generally agree\\nwith the original performance on the pre-training pushshift.io Reddit task, and the order of methods\\nis retained. Hash outperforms Switch slightly, both outperform the Baseline model, and the larger\\ndense models perform better, as expected. Freezing parts of the model generally hurts ﬁne-tuning,\\nunless the part frozen is the sparse part of the model. It appears in that case just ﬁne-tuning the dense\\nparts of the model is sufﬁcient for good performance. Only tuning the sparse part of the model, on the\\nother hand, hurts performance, perhaps because the majority of the capacity of the model lies there.\\n5.3.2\\nHash Function Analysis\\nWe evaluate the different choices of hashing function detailed in subsection 3.1. The overall results\\nare given in Table 3 on the pushshift.io Reddit dataset using a 64 module Hash Layer.\\nRandom and Balanced Hash Functions\\nWe ﬁnd that ﬁxed random assignment (row 3) and\\nbalanced assignment (row 2) perform similarly well in terms of perplexity (23.22 vs. 23.16 valid\\nperplexity). However, balanced assignment, as its name suggests, is more balanced, see Figure 4,\\nwhich may render it more efﬁcient in terms of distributed training schemes.\\nClustering Hash Functions\\nInterestingly, using cluster based hashes (“Token clustering”, row 4)\\nperforms clearly worse than randomized hashes (23.90 vs. 23.22). We hypothesize that if the goal\\nof conditional computation is to make ﬁne distinctions, then those distinctions are more likely to\\nappear between tokens within the same cluster, hence they should be in different hashes (parts of\\nthe compute graph), not the same one. We provide partial evidence for this by hashing within token\\nclusters instead (“Dispersed Hash”, row 5), which restores the performance to be similar to random\\n7\\nTable 4: Comparison of Models on Wikitext-103. We compare a baseline dense Transformer to\\nour sparse models, which have 1 sparse layer with 16 modules (1x16). We show results with two\\ndifferent dictionaries, the BB [32] BPE dictionary (8008 tokens) and the standard one for the task\\n(267,739 tokens). As these are different dictionaries, perplexities are not comparable across columns.\\nStd. Dict\\nBB Dict\\nModel\\nConﬁguration\\nValid PPL\\nValid PPL\\nBaseline Transformer\\nlayers=8, d=512, D=512\\n33.09\\n12.58\\nSwitch Transformer\\nlayers=8, modules=1x16, load_bal=0.1\\n31.76\\n11.67\\nHash Layer\\nlayers=8, modules=1x16\\n32.32\\n11.58\\nhashes (23.17 vs. 23.22). We note that learn-to-route methods such as Switch Transformers and\\nBASE use simple functions of the hidden state to perform routing, which generally provide clustered\\nexpert modules [10], which could hence be a disadvantage for those methods.\\nPosition-based Hash Function\\nWe conduct experiments hashing based on sequence position only.\\nWe consider this experiment as a sanity check, we did not expect choosing conditional compute based\\non position in the output sequence to help. Indeed, it turns out that this is no better than the dense\\nTransformer baseline. Thus it appears that routing based on input content is much more important.\\nBigram Hash Function\\nHashing based on the last two tokens (bigrams) performs worse than using\\nonly the last token (24.19 vs. 23.16). We hypothesize there are two reasons for this: (1) ﬁrst, the last\\ntoken is clearly the most pertinent, and bigrams add a less relevant feature; (2) this creates too many\\nhashes, which performs less well. Subsequent experiments will help test these claims.\\nPrevious Token Hashing\\nHashing based on the previous token is clearly worse than using the\\ncurrent token (24.16 vs. 23.16), and gives similar performance to using bigrams, helping conﬁrm the\\nﬁrst part of our above bigram hypothesis.\\nDictionary size\\nWe perform experiments on Wikitext-103 in two settings: using the given dictionary\\nof 267k tokens, or using the 8k dictionary we use in our pushshift.io Reddit experiments, following\\n[32]. The results, comparing to Switch and a baseline Transformer, are given in Table 4. We ﬁnd\\nthat Hash works well for the small dictionary, slightly outperforming Switch. However, on the larger\\ndictionary, it performs worse than Switch. As this is the same data but just the tokenization has\\nchanged we conclude the hashing induced from the smaller dictionary is easier to learn from, helping\\nconﬁrm the second part of our above bigram hypothesis.\\nOracle Future Token Hashing\\nWe evaluate hashing using the oracle next token that is to be\\npredicted. This yields a perplexity of 1.9. Using oracle information just to choose between modules\\nis sufﬁcient to essentially solve a task.\\nPredicted Future Token Hashing\\nThe last result poses the question: if we can predict the next\\ntoken, and hash based on that prediction instead – will it be better than hashing on the current token?\\nWe thus tried hashing using the Baseline Transformer to predict labels, yielding a perplexity of 25.02\\n– which does not actually beat the Baseline itself. It appears that the bias of the token predictions\\nlimits the ability of the sparse routing to improve.\\nMulti-hashing\\nWe evaluate the multi-hashing technique described in subsection 3.2. Results are\\ngiven in Appendix A, comparing to Switch and standard hashing. Even though the same number of\\nparameters is used in all cases, we see improvements for splitting the hash into 2, 4 or 8 different\\nhashes compared to a single hash, with steadily improving results for both 16 or 32 modules.\\n5.3.3\\nSwitch Transformer Analysis\\nSwitch load balancing\\nWe show the performance of Switch for different values of the load\\nbalancing parameter on pushshift.io Reddit in Appendix A. Clearly the choice of parameter is\\nimportant, with results varying over a 1 perplexity point range.\\nSwitch with Token-based Routing\\nGiven our analysis of oracle and predicted token hashing in\\nsubsection 5.3.2, we hypothesize that the hidden representations in layers of the Transformer, being\\nbiased towards the predictions of the model, may be suboptimal for routing. We therefore experiment\\nwith a hybrid between Switch and Hash Layers: on the sparse layer, instead of using hidden state as\\n8\\nTable 5: Multi-hashing experiments on pushshift.io Reddit. When multi-hashing, the same number\\nof parameters is used, but the FFN weights are split and indexed into multiple hashes and then\\nconcatenated together for the forward step.\\nModel\\nConﬁguration\\nParams\\nValid PPL\\nTest PPL\\nSwitch Transformer\\nlayers=11,modules=1x32, load_bal=0.1\\n483M\\n23.79\\n23.84\\nHash Layer\\nlayers=11,modules=1x32\\n483M\\n23.58\\n23.65\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=2\\n483M\\n23.48\\n23.53\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=4\\n483M\\n23.38\\n23.45\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=8\\n483M\\n23.28\\n23.34\\nTable 6: Switch Transformers with Token-Based Routing on pushshift.io Reddit. We compare\\nstandard Switch which routes based on the hidden state to token feature-routing (‘Token Switch’).\\nModel\\nConﬁguration\\nParams\\nValid PPL\\nTest PPL\\nSwitch Transformer\\nlayers=11,modules=1x64, load_bal=0.1\\n751M\\n23.65\\n23.73\\nToken Switch\\nlayers=11,modules=1x64, load_bal=0.1\\n751M\\n23.43\\n23.43\\nSwitch Transformer\\nlayers=11,modules=1x128, load_bal=0.1\\n1.28B\\n23.52\\n23.58\\nToken Switch\\nlayers=11,modules=1x128, load_bal=0.1\\n1.28B\\n23.26\\n23.32\\nthe Switch router input, we use the current token instead. To convert the token to a vector we use an\\nextra lookup table, i.e., an extra set of learnable parameters that is the size of the dictionary. These\\nparameters are independent of the hidden state and are only used by the router to learn the best route.\\nResults are given in Table 6. We ﬁnd this brings some small improvements to Switch for 64 and 128\\nmodules on a single layer, afﬁrming the usefulness of token-based routing.\\n5.3.4\\nComparison to BASE Layers\\nWe next compare to BASE Layers. Using the BASE Layer code base, we implement Hash Layers in\\nexactly the same setup, changing only the routing method, and leaving everything else ﬁxed. Figure 2\\n(right) shows results comparing Hash with BASE for 4.5B parameter models. Across the entire run,\\nwe see that Hash outperforms BASE at each training step. During early parts of training, Hash would\\npresumably have an advantage in being able to specialize expert modules earlier, while BASE must\\nlearn membership for each of the expert modules. Later in training, BASE becomes mildly unstable\\npresumably as expert assignments shift, while Hash performance continues to improve smoothly.\\nAdditionally, to demonstrate Hash Layers remain performant when stacked, we trained a model with\\n3 Hash Layers (using random hashes), but fewer parameters per expert module so the total parameters\\nremained constant at 4.5B (see subsection B.2). We ﬁnd that using multiple Hash Layers gives a\\nsmall but consistent improvement, suggesting Hash Layers will be effective at even more depth.\\nIn addition to performance gains compared to BASE, we also ﬁnd that Hash Layers are more\\nefﬁcient in total computation. In particular, BASE requires two all-to-all communications: the ﬁrst\\nde-correlates batches in order to make assignment balancing more stochastic, and the second routes\\nstates to their assigned expert. As Hash Layers use ﬁxed, pre-computed assignments they avoid the de-\\ncorrelation step. In practice, we ﬁnd this gives an improvement of about 11% in updates-per-second.\\nAs the number of expert layers increases, this difference will become more exaggerated.\\n6\\nConclusion\\nWe have introduced a simple and efﬁcient approach to sparse models in the Transformers-for-NLP\\nsetting based on hash layers. We showed on a variety of datasets and with analysis in various\\nsettings that this approach is highly competitive with existing methods such as Switch Transformers\\nand BASE Layers, whilst being robust and far simpler – requiring no extra learning parameters,\\nassignment algorithm or changes to the objective function. Given that researchers typically have only\\none opportunity to train very large models, this makes our approach a strong candidate for such runs.\\nWhile our experiments scale up to 4.5B parameters, we do not reach the scales of large industrial\\nworks such as [8], and we hope to see future work conduct such experiments. Finally, given that our\\nrouting approach is learning free, our results perhaps suggest that none of the current approaches are\\n9\\nrouting particularly well. We thus believe learning-to-route should continue to be the study of future\\nwork, and consider our work a strong baseline for such research.\\n10\\nReferences\\n[1] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361, 2020.\\n[2] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures\\nof local experts. Neural computation, 3(1):79–87, 1991.\\n[3] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic\\nlanguage model. The journal of machine learning research, 3:1137–1155, 2003.\\n[4] Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of experts.\\nIEEE transactions on neural networks and learning systems, 23(8):1177–1193, 2012.\\n[5] David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a\\ndeep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\n[6] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[7] Sam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale\\nweakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 6865–6873, 2017.\\n[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\n[9] Ronan Collobert, Yoshua Bengio, and Samy Bengio. Scaling large learning problems with hard\\nparallel mixtures. International Journal of pattern recognition and artiﬁcial intelligence, 17\\n(03):349–365, 2003.\\n[10] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.\\n[11] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems, pages 5998–6008, 2017.\\n[13] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to\\nalgorithms. MIT press, 2009.\\n[14] Andrei Z Broder and Anna R Karlin. Multilevel adaptive hashing. In Proceedings of the ﬁrst\\nannual ACM-SIAM symposium on Discrete algorithms, pages 43–53, 1990.\\n[15] Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In\\n1995 international conference on acoustics, speech, and signal processing, volume 1, pages\\n181–184. IEEE, 1995.\\n[16] Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan ˇ\\nCernock`\\ny, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Eleventh annual conference of the international\\nspeech communication association, 2010.\\n[17] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and\\nTony Robinson. One billion word benchmark for measuring progress in statistical language\\nmodeling. arXiv preprint arXiv:1312.3005, 2013.\\n[18] Graham Neubig and Chris Dyer. Generalizing and hybridizing count-based and neural language\\nmodels. arXiv preprint arXiv:1606.00499, 2016.\\n11\\n[19] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large\\ntarget vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014.\\n[20] Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou, et al. Efﬁcient softmax\\napproximation for gpus. In International Conference on Machine Learning, pages 1302–1310.\\nPMLR, 2017.\\n[21] Anton Bakhtin, Arthur Szlam, Marc’Aurelio Ranzato, and Edouard Grave. Lightweight adaptive\\nmixture of neural and n-gram language models. arXiv preprint arXiv:1804.07705, 2018.\\n[22] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature\\nhashing for large scale multitask learning. In Proceedings of the 26th annual international\\nconference on machine learning, pages 1113–1120, 2009.\\n[23] Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi,\\nOlivier Chapelle, and Kilian Weinberger. Learning to rank with (a lot of) word features.\\nInformation retrieval, 13(3):291–314, 2010.\\n[24] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based\\nsparse attention with routing transformers. Transactions of the Association for Computational\\nLinguistics, 9:53–68, 2021.\\n[25] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer.\\narXiv preprint arXiv:2001.04451, 2020.\\n[26] Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge,\\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Learning semantic textual similarity from\\nconversations. In Proceedings of The Third Workshop on Representation Learning for NLP,\\npages 164–174, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[27] Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison, and Antoine Bordes. Training\\nmillions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing, pages 2775–2779, Brussels, Belgium, October-\\nNovember 2018. Association for Computational Linguistics.\\n[28] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\\nCTRL: A conditional transformer language model for controllable generation. arXiv preprint\\narXiv:1909.05858, 2019.\\n[29] Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. The\\ndialogue dodecathlon: Open-domain knowledge and image grounded conversational agents,\\n2019.\\n[30] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders:\\nArchitectures and pre-training strategies for fast and accurate multi-sentence scoring.\\nIn\\nProceedings of the International Conference on Learning Representations, 2019.\\n[31] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn.\\nThe pushshift reddit dataset. arXiv preprint arXiv:2001.08435, 2020.\\n[32] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu,\\nMyle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot.\\narXiv preprint arXiv:2004.13637, 2020.\\n[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692, 2019.\\n[34] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\\nzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\\nUnsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116,\\n2019.\\n12\\n[35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843, 2016.\\n[36] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint\\narXiv:1904.01038, 2019.\\n13\\nA\\nAdditional Results\\nTable 7: Multi-hashing experiments on pushshift.io Reddit. When multi-hashing, the same number\\nof parameters is used, but the FFN weights are split and indexed into multiple hashes and then\\nconcatenated together for the forward step.\\nModel\\nConﬁguration\\nParams\\nValid\\nTest\\nSwitch Transformer\\nlayers=11,modules=1x16, load_bal=0.1\\n348M\\n24.00\\n24.13\\nHash Layer\\nlayers=11,modules=1x16\\n348M\\n24.01\\n24.06\\nMultiHash Layer\\nlayers=11,modules=1x16,hashes=2\\n348M\\n23.88\\n23.93\\nMultiHash Layer\\nlayers=11,modules=1x16,hashes=4\\n348M\\n23.73\\n23.80\\nMultiHash Layer\\nlayers=11,modules=1x16,hashes=8\\n348M\\n23.83\\n23.88\\nSwitch Transformer\\nlayers=11,modules=1x32, load_bal=0.1\\n483M\\n23.79\\n23.84\\nHash Layer\\nlayers=11,modules=1x32\\n483M\\n23.58\\n23.65\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=2\\n483M\\n23.48\\n23.53\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=4\\n483M\\n23.38\\n23.45\\nMultiHash Layer\\nlayers=11,modules=1x32,hashes=8\\n483M\\n23.28\\n23.34\\nTable 8: Fine-tuning Dense and Sparse Models in various conﬁgurations on the BST Tasks.\\nModel\\nConﬁguration\\nParams\\nBST Valid\\nBaseline Transformer\\nlayers=11, d=1024, D=4096\\n222M\\n14.21\\nWider Transformer\\nlayers=11, d=2048, D=6144\\n755M\\n12.48\\nDeeper Transformer\\nlayers=22, d=1536, D=4096\\n755M\\n12.83\\nSwitch 1x64\\nNo weights frozen, load_bal=0.0\\n751M\\n13.67\\nSwitch 1x64\\nNo weights frozen, load_bal=0.1\\n751M\\n13.67\\nSwitch 1x64\\nSwitch weights frozen\\n751M\\n13.65\\nSwitch 1x64\\nRouter weights frozen\\n751M\\n13.61\\nSwitch 1x64\\nAll layers but last frozen\\n751M\\n14.42\\nSwitch 1x64\\nAll layers but Switch frozen\\n751M\\n14.37\\nHash 1x64\\nNo weights frozen\\n751M\\n13.45\\nHash 1x64\\nHash weights frozen\\n751M\\n13.56\\nHash 1x64\\nAll layers but last frozen\\n751M\\n14.29\\nHash 1x64\\nAll layers but Hash frozen\\n751M\\n14.12\\nTable 9: Switch Transformer Load Balancing. We show the perplexity with 64 modules on the\\npushshift.io Reddit task for different load balancing parameters. The choice of parameter is important;\\nwithout balancing the model performs worse.\\nModel\\nLoad balance\\nValid\\nTest\\nBaseline Transformer\\n-\\n24.90\\n24.96\\nSwitch\\n0\\n24.80\\n24.86\\nSwitch\\n0.01\\n23.95\\n24.01\\nSwitch\\n0.05\\n23.68\\n23.74\\nSwitch\\n0.1\\n23.65\\n23.73\\nSwitch\\n0.5\\n23.68\\n23.74\\n14\\nB\\nHyperparameters\\nB.1\\nComparisons to Switch\\nWe give here the parameters used in our standard pushshift.io Reddit and RoBERTa+cc100en setups.\\nOther experiments with parameter changes differing from these are indicated in the main text.\\nHyperparameter\\nSwitch\\nHash Layer\\nTotal parameters\\n751,224,896\\n751,159,296\\nExpert Modules per MoE layer\\n64\\n64\\nNumber of MoE layers\\n1\\n1\\nFFNs per Expert Module\\n1\\n1\\nEmbedding Size\\n1024\\n1024\\nFFN Size\\n4096\\n4096\\nAttention Heads\\n16\\n16\\nNumber of encoder layers\\n2\\n2\\nNumber of decoder layers\\n11\\n11\\nContext Length\\n128\\n128\\nLabel Length\\n128\\n128\\nBatchsize\\n40\\n40\\nGradient Accumulation\\n1\\n1\\nMaximum LR\\n0.002\\n0.002\\nWarmup\\n10,000 steps\\n10,000 steps\\nLR Scheduler\\nInvSqrt\\nInvSqrt\\nMaximum steps\\n100,000\\n100,000\\nOptimizer\\nADAM\\nADAM\\nGradient Clip\\n1.0\\n1.0\\nB.2\\nComparisons to Base\\nHyperparameter\\nBASE\\nHash Layer\\n3x Hash Layer\\nShared parameters\\n1,313,460,224\\n1,313,460,224\\n1,313,460,224\\nParameters per Expert\\n100,706,304\\n100,706,304\\n33,568,768\\nTotal parameters\\n4,536,061,952\\n4,536,061,952\\n4,536,061,952\\nExpert Modules per MoE layer\\n32\\n32\\n32\\nNumber of MoE layers\\n1\\n1\\n3\\nFFNs per Expert Module\\n3\\n3\\n1\\nEmbedding Size\\n2048\\n2048\\n2048\\nFFN Size\\n8192\\n8192\\n8192\\nAttention Heads\\n16\\n16\\n16\\nNumber of shared layers\\n24\\n24\\n24\\nContext Length\\n1024\\n1024\\n1024\\nBatchsize\\n2\\n2\\n2\\nGradient Accumulation\\n4\\n4\\n4\\nTotal tokens per update\\n512k\\n512k\\n512k\\nMaximum LR\\n7.5e-4\\n7.5e-4\\n7.5e-4\\nWarmup\\n2000 steps\\n2000 steps\\n2000 steps\\nLR Scheduler\\nPoly Decay\\nPoly Decay\\nPoly Decay\\nMaximum steps\\n62,500\\n62,500\\n62,500\\nOptimizer\\nADAM\\nADAM\\nADAM\\nGradient Clip\\n0.1\\n0.1\\n0.1\\nNote that within the comparisons to BASE, we utilize BASE’s gradient clipping method, which\\ncomputed gradient norm based only on shared parameters to avoid additional communication across\\ndevices.\\n15\\nC\\nComputational Resources\\nAll experiments were run on an internal cluster. Unless otherwise marked, all experiments used 8\\n32GB V100 GPUs for roughly 20 hours.\\nExceptions:\\n• Larger dense Transformer baselines and 128 module experiments used 16 V100s.\\n• The comparisons to BASE use 32 V100s for approximately 2 days.\\nD\\nSocietal Impact\\nImprovements to language modeling could have implications on a large number of surfaces across\\nhumanity. Hash Layer may also be used to train much larger models, which may have an increased\\nimpact on the environment, albeit at a fraction cost than the parameter-equivalent dense models. Hash\\nLayer also offers a nontrivial reduction in computational resources over the prior work of BASE.\\nThe datasets used in this work contain varied and potentially offensive text content, as they were\\noriginally procured from the Internet by third parties. Mitigating the negative effects of these efforts\\nis an important research area, but outside the scope of this paper. We expect (but do not show) that\\nsuch mitigation efforts are likely orthogonal and complementary to our own work on architecture\\nimprovements.\\n16\\n', 'source_name': 'Hash Layers for Large Sparse Models', 'source_url': 'https://arxiv.org/abs/2106.04426'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mistral.pdf #67\n",
      "{'content': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1\\nIntroduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.\\narXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2\\nArchitectural details\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\nlength, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\\nlatency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window\\nattention: each token can attend to at most W tokens from the previous layer (here, W = 3). Note that tokens\\noutside the sliding window still influence next word prediction. At each attention layer, information can move\\nforward by W tokens. Hence, after k attention layers, information can move forward by up to k × W tokens.\\nParameter\\nValue\\ndim\\n4096\\nn_layers\\n32\\nhead_dim\\n128\\nhidden_dim\\n14336\\nn_heads\\n32\\nn_kv_heads\\n8\\nwindow_size\\n4096\\ncontext_len\\n8192\\nvocab_size\\n32000\\nTable 1: Model architecture.\\nMistral 7B is based on a transformer architecture [27]. The main\\nparameters of the architecture are summarized in Table 1. Compared\\nto Llama, it introduces a few changes that we summarize below.\\nSliding Window Attention. SWA exploits the stacked layers of a trans-\\nformer to attend information beyond the window size W. The hidden\\nstate in position i of the layer k, hi, attends to all hidden states from\\nthe previous layer with positions between i −W and i. Recursively,\\nhi can access tokens from the input layer at a distance of up to W × k\\ntokens, as illustrated in Figure 1. At the last layer, using a window size\\nof W = 4096, we have a theoretical attention span of approximately\\n131K tokens. In practice, for a sequence length of 16K and W = 4096,\\nchanges made to FlashAttention [11] and xFormers [18] yield a 2x\\nspeed improvement over a vanilla attention baseline.\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling\\nbuffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored\\nin position i mod W of the cache. As a result, when the position i is larger than W, past values\\nin the cache are overwritten, and the size of the cache stops increasing. We provide an illustration\\nin Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage\\nby 8x, without impacting the model quality.\\n1https://github.com/mistralai/mistral-src\\n2https://github.com/skypilot-org/skypilot\\n3https://huggingface.co/mistralai\\n2\\nFigure 2: Rolling buffer cache. The cache has a fixed size of W = 4. Keys and values for position i are stored\\nin position i mod W of the cache. When the position i is larger than W, past values in the cache are overwritten.\\nThe hidden state corresponding to the latest generated tokens are colored in orange.\\nPre-fill and Chunking. When generating a sequence, we need to predict tokens one-by-one, as\\neach token is conditioned on the previous ones. However, the prompt is known in advance, and we\\ncan pre-fill the (k, v) cache with the prompt. If the prompt is very large, we can chunk it into smaller\\npieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as\\nour chunk size. For each chunk, we thus need to compute the attention over the cache and over the\\nchunk. Figure 3 shows how the attention mask works over both the cache and the chunk.\\ngo\\ndog\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\nthe\\nto\\nThe\\ncat\\nsat\\non\\nthe\\n1\\nmat\\nand\\n1\\n1\\n1\\nsaw\\nthe\\n1\\n0\\n0\\n0\\ndog\\ngo\\nto\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\nPast\\nCache\\nCurrent\\nFigure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory\\nusage. We process a sequence in three chunks, “The cat sat on”, “the mat and saw”, “the dog go to”. The figure\\nshows what happens for the third chunk (“the dog go to”): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3\\nResults\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\\nfair comparison. We measure performance on a wide variety of tasks categorized as follow:\\n• Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22],\\nOpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\n• World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\n• Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\n• Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\n• Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\n• Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29]\\n(3-5-shot, English multiple-choice questions only)\\nDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4\\ncompares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different\\ncategories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on\\nmost benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics,\\nand reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\nFigure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks. All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\nModel\\nModality MMLU HellaSwag WinoG PIQA\\nArc-e\\nArc-c\\nNQ\\nTriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B\\nPretrained 44.4%\\n77.1%\\n69.5% 77.9% 68.7% 43.2% 24.7%\\n63.8%\\n11.6%\\n26.1%\\n3.9%\\n16.0%\\nLLaMA 2 13B\\nPretrained 55.6%\\n80.7%\\n72.9% 80.8% 75.2% 48.8% 29.0%\\n69.6%\\n18.9%\\n35.4%\\n6.0%\\n34.3%\\nCode-Llama 7B Finetuned\\n36.9%\\n62.9%\\n62.3% 72.8% 59.4% 34.5% 11.0%\\n34.9%\\n31.1%\\n52.5%\\n5.2%\\n20.8%\\nMistral 7B\\nPretrained 60.1%\\n81.3%\\n75.3% 83.0% 80.0% 55.5% 28.8%\\n69.9%\\n30.5%\\n47.5% 13.1%\\n52.2%\\nTable 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and\\napproaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\nSize and Efficiency. We computed “equivalent model sizes” of the Llama 2 family, aiming to\\nunderstand Mistral 7B models’ efficiency in the cost-performance spectrum (see Figure 5). When\\nevaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B\\nmirrored performance that one might expect from a Llama 2 model with more than 3x its size. On\\nthe Knowledge benchmarks, Mistral 7B’s performance achieves a lower compression rate of 1.9x,\\nwhich is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\\non TriviaQA, we do not provide Wikipedia contexts.\\n4\\nInstruction Finetuning\\nModel\\nChatbot Arena\\nELO Rating\\nMT Bench\\nWizardLM 13B v1.2\\n1047\\n7.2\\nMistral 7B Instruct\\n1031\\n6.84 +/- 0.07\\nLlama 2 13B Chat\\n1012\\n6.65\\nVicuna 13B\\n1041\\n6.57\\nLlama 2 7B Chat\\n985\\n6.27\\nVicuna 7B\\n997\\n6.17\\nAlpaca 13B\\n914\\n4.53\\nTable 3: Comparison of Chat models. Mistral 7B –\\nInstruct outperforms all 7B models on MT-Bench, and\\nis comparable to 13B – Chat models.\\nTo evaluate the generalization capabilities of\\nMistral 7B, we fine-tuned it on instruction datasets\\npublicly available on the Hugging Face repository.\\nNo proprietary data or training tricks were utilized:\\nMistral 7B – Instruct model is a simple and\\npreliminary demonstration that the base model can\\neasily be fine-tuned to achieve good performance.\\nIn Table 3, we observe that the resulting model,\\nMistral 7B – Instruct, exhibits superior perfor-\\nmance compared to all 7B models on MT-Bench,\\nand is comparable to 13B – Chat models. An\\nindependent human evaluation was conducted on\\nhttps://llmboxing.com/leaderboard.\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n4\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5\\nAdding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1\\nSystem prompt to enforce guardrails\\nWe introduce a system prompt (see below) to guide the model to generate answers within specified\\nguardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the\\nPareto front of model utility / guardrails enforcement, as indicated in Table 4.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,\\nunethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\nGuardrails\\nMT Bench\\nNo system prompt\\n6.84 ± 0.07\\nLlama 2 system prompt\\n6.38 ± 0.07\\nMistral system prompt\\n6.58 ± 0.05\\nTable 4: System prompts. Mean official\\nMT Bench score over 10 iterations with\\nstandard deviation for Mistral 7B – Instruct.\\nFor reference, Llama 2 13B – Chat reports\\nofficial results of 6.65.\\nWe use a set of 175 unsafe prompts for evaluating safety.\\nWith the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the\\nquestion How to kill a linux process with system prompts\\nactivated. We observe that Mistral 7B provides a correct\\nresponse while Llama 2 declines to answer. Note that on\\nthis specific question, both models answer correctly when\\nsystem prompts are deactivated.\\n5.2\\nContent moderation with self-reflection\\nMistral 7B – Instruct can be used as a content moderator: the model itself is able to accurately\\nclassify a user prompt or its generated answer as being either acceptable or falling into one of the\\nfollowing categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing\\nor violent content such as discrimination, self-harm or bullying; Unqualified advice for instance\\nin legal, medical or financial domains.\\n5\\nModel\\nAnswer\\nMistral 7B – Instruct\\nwith Mistral system prompt\\nTo kill a Linux process, you can use the `kill`command followed by the process ID (PID)\\nof the process you want to terminate. For example, to kill process with PID 1234, you\\nwould run the command `kill 1234`. It’s important to note that killing a process can\\nhave unintended consequences, so it’s generally a good idea to only kill processes that\\nyou are certain you want to terminate. Additionally, it’s generally a good idea to use the\\n`kill`command with caution, as it can be used to terminate critical system processes that\\nare necessary for the operation of the system.\\nLlama 2 13B – Chat\\nwith Llama 2 system prompt\\nAs a helpful and respectful assistant, I cannot provide information on how to kill a Linux\\nprocess. It is not appropriate or ethical to intentionally harm or terminate processes, as it\\ncan cause damage to the system and potentially result in data loss. Additionally, it is\\nimportant to respect the autonomy and security of other users’ processes, as they may be\\ncritical to their work or personal activities. Instead, I suggest that you explore other\\noptions to address any issues or concerns you may have.\\nTable 5: Comparison between Mistral and Llama system prompts. Mistral provides the right answer whereas\\nLlama 2 declines to answer to the question How to kill a linux process.\\nTo do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated\\nanswer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial\\nand standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable\\nprompts as positives).\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring\\non the internet. In particular, the end user is able to select afterwards which categories to effectively\\nfilter based on their particular use-case.\\n6\\nConclusion\\nOur work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster.\\nWe thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.\\nWe thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance\\nin implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\\na tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help\\nin making our model compatible everywhere.\\n6\\nFigure 6: Human evaluation of Mistral 7B – Instruct vs Llama 2 13B – Chat Example. An example of\\nhuman evaluation from llmboxing.com. The question asks for recommendations of books in quantum physics.\\nLlama 2 13B – Chat recommends a general physics book, while Mistral 7B – Instruct recommends a more\\nrelevant book on quantum physics and describes in the contents in more detail.\\n7\\nReferences\\n[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and\\nSumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head\\ncheckpoints. arXiv preprint arXiv:2305.13245, 2023.\\n[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150, 2020.\\n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence, 2020.\\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n[7] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and\\nLuke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036,\\n2018.\\n[8] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\narXiv preprint arXiv:1905.10044, 2019.\\n[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[11] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast\\nand memory-efficient exact attention with IO-awareness. In Advances in Neural Information\\nProcessing Systems, 2022.\\n[12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt.\\nMeasuring massive multitask language understanding.\\narXiv preprint\\narXiv:2009.03300, 2020.\\n[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\\narXiv preprint arXiv:2103.03874, 2021.\\n[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas\\nHennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia\\nGuy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent\\nSifre. An empirical analysis of compute-optimal large language model training. In Advances in\\nNeural Information Processing Systems, volume 35, 2022.\\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\\nTriviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehension.\\narXiv preprint\\narXiv:1705.03551, 2017.\\n[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics, 7:453–466, 2019.\\n8\\n[17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-\\nguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles, 2023.\\n[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\\nxformers: A modular and hackable transformer modelling library. https://github.com/\\nfacebookresearch/xformers, 2022.\\n[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\\n2018.\\n[20] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\\nYossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models\\nfor code. arXiv preprint arXiv:2308.12950, 2023.\\n[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\\n2021.\\n[22] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-\\nmonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\\n[23] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\\nChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.\\nChallenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint\\narXiv:2210.09261, 2022.\\n[24] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937,\\n2018.\\n[25] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n[28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\\n[29] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\\nmodels. arXiv preprint arXiv:2304.06364, 2023.\\n9\\n', 'source_name': 'Mistral 7B', 'source_url': 'https://arxiv.org/abs/2310.06825'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mixtral.pdf #68\n",
      "{'content': 'Mixtral of Experts\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,\\nBlanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,\\nEmma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour,\\nGuillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,\\nThéophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed\\nAbstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language\\nmodel. Mixtral has the same architecture as Mistral 7B, with the difference\\nthat each layer is composed of 8 feedforward blocks (i.e. experts). For every\\ntoken, at each layer, a router network selects two experts to process the current\\nstate and combine their outputs. Even though each token only sees two experts,\\nthe selected experts can be different at each timestep. As a result, each token\\nhas access to 47B parameters, but only uses 13B active parameters during\\ninference. Mixtral was trained with a context size of 32k tokens and it outperforms\\nor matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks.\\nIn\\nparticular, Mixtral vastly outperforms Llama 2 70B on mathematics, code\\ngeneration, and multilingual benchmarks.\\nWe also provide a model fine-\\ntuned to follow instructions, Mixtral 8x7B – Instruct, that surpasses GPT-3.5\\nTurbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human bench-\\nmarks. Both the base and instruct models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/mixtral-of-experts/\\n1\\nIntroduction\\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,\\nlicensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As\\nit only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low\\nbatch-sizes, and higher throughput at large batch-sizes.\\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward\\nblock picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router\\nnetwork chooses two of these groups (the “experts”) to process the token and combine their output\\nadditively. This technique increases the number of parameters of a model while controlling cost and\\nlatency, as the model only uses a fraction of the total set of parameters per token.\\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches\\nor exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\\narXiv:2401.04088v1  [cs.LG]  8 Jan 2024\\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The\\nlayer’s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard\\nfeedforward block as in a vanilla transformer architecture.\\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require\\nmultilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments\\nshow that Mixtral is able to successfully retrieve information from its context window of 32k tokens,\\nregardless of the sequence length and the location of the information in the sequence.\\nWe also present Mixtral 8x7B – Instruct, a chat model fine-tuned to follow instructions using\\nsupervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses\\nthat of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human evaluation\\nbenchmarks. Mixtral – Instruct also demonstrates reduced biases, and a more balanced sentiment\\nprofile in benchmarks such as BBQ, and BOLD.\\nWe release both Mixtral 8x7B and Mixtral 8x7B – Instruct under the Apache 2.0 license1, free for\\nacademic and commercial usage, ensuring broad accessibility and potential for diverse applications.\\nTo enable the community to run Mixtral with a fully open-source stack, we submitted changes to\\nthe vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\\nallows the deployment of vLLM endpoints on any instance in the cloud.\\n2\\nArchitectural details\\nParameter\\nValue\\ndim\\n4096\\nn_layers\\n32\\nhead_dim\\n128\\nhidden_dim\\n14336\\nn_heads\\n32\\nn_kv_heads\\n8\\ncontext_len\\n32768\\nvocab_size\\n32000\\nnum_experts\\n8\\ntop_k_experts\\n2\\nTable 1: Model architecture.\\nMixtral is based on a transformer architecture [31] and uses the same\\nmodifications as described in [18], with the notable exceptions that Mix-\\ntral supports a fully dense context length of 32k tokens, and the feed-\\nforward blocks are replaced by Mixture-of-Expert layers (Section 2.1).\\nThe model architecture parameters are summarized in Table 1.\\n2.1\\nSparse Mixture of Experts\\nWe present a brief overview of the Mixture of Experts layer (Figure 1).\\nFor a more in-depth overview, see [12]. The output of the MoE module\\nfor a given input x is determined by the weighted sum of the outputs\\nof the expert networks, where the weights are given by the gating\\nnetwork’s output. i.e. given n expert networks {E0, Ei, ..., En−1}, the\\noutput of the expert layer is given by:\\nn−1\\nX\\ni=0\\nG(x)i · Ei(x).\\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)\\nis the output of the i-th expert network. If the gating vector is sparse, we can avoid computing\\nthe outputs of experts whose gates are zero. There are multiple alternative ways of implementing\\nG(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the\\nTop-K logits of a linear layer [28]. We use\\nG(x) := Softmax(TopK(x · Wg)),\\nwhere (TopK(ℓ))i := ℓi if ℓi is among the top-K coordinates of logits ℓ∈Rn and (TopK(ℓ))i := −∞\\notherwise. The value of K – the number of experts used per token – is a hyper-parameter that modu-\\nlates the amount of compute used to process each token. If one increases n while keeping K fixed, one\\n1https://mistral.ai/news/mixtral-of-experts/\\n2\\ncan increase the model’s parameter count while keeping its computational cost effectively constant.\\nThis motivates a distinction between the model’s total parameter count (commonly referenced as the\\nsparse parameter count), which grows with n, and the number of parameters used for processing an\\nindividual token (called the active parameter count), which grows with K up to n.\\nMoE layers can be run efficiently on single GPUs with high performance specialized kernels. For\\nexample, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large\\nsparse matrix multiplications, significantly enhancing the execution speed and naturally handling\\ncases where different experts get a variable number of tokens assigned to them. Moreover, the\\nMoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and\\nthrough a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE\\nlayer’s execution, tokens meant to be processed by a specific expert are routed to the corresponding\\nGPU for processing, and the expert’s output is returned to the original token location. Note that EP\\nintroduces challenges in load balancing, as it is essential to distribute the workload evenly across the\\nGPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\\nIn a Transformer model, the MoE layer is applied independently per token and replaces the\\nfeed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\\narchitecture as the expert function Ei(x) and set K = 2. This means each token is routed to two\\nSwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input\\ntoken x is computed as:\\ny =\\nn−1\\nX\\ni=0\\nSoftmax(Top2(x · Wg))i · SwiGLUi(x).\\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all\\nFFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\\nmore elaborate gating strategy for the second expert assigned to each token.\\n3\\nResults\\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\\ncomparison. We measure performance on a wide variety of tasks categorized as follow:\\n• Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27],\\nOpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\\n• World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\\n• Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\\n• Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\\n• Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\\n• Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\\n(3-5-shot, English multiple-choice questions only)\\nFigure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models\\nwere re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or\\nmatches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\\n3\\nModel\\nActive\\nParams MMLU\\nHellaS\\nWinoG\\nPIQA\\nArc-e\\nArc-c\\nNQ\\nTriQA HumanE MBPP\\nMath\\nGSM8K\\nLLaMA 2 7B\\n7B\\n44.4%\\n77.1%\\n69.5%\\n77.9%\\n68.7%\\n43.2%\\n17.5%\\n56.6%\\n11.6%\\n26.1%\\n3.9%\\n16.0%\\nLLaMA 2 13B\\n13B\\n55.6%\\n80.7%\\n72.9%\\n80.8%\\n75.2%\\n48.8%\\n16.7%\\n64.0%\\n18.9%\\n35.4%\\n6.0%\\n34.3%\\nLLaMA 1 33B\\n33B\\n56.8%\\n83.7%\\n76.2%\\n82.2%\\n79.6%\\n54.4%\\n24.1%\\n68.5%\\n25.0%\\n40.9%\\n8.4%\\n44.1%\\nLLaMA 2 70B\\n70B\\n69.9%\\n85.4%\\n80.4%\\n82.6%\\n79.9%\\n56.5%\\n25.4%\\n73.0%\\n29.3%\\n49.8%\\n13.8%\\n69.6%\\nMistral 7B\\n7B\\n62.5%\\n81.0%\\n74.2%\\n82.2%\\n80.5%\\n54.9%\\n23.2%\\n62.5%\\n26.2%\\n50.2%\\n12.7%\\n50.0%\\nMixtral 8x7B\\n13B\\n70.6%\\n84.4%\\n77.2%\\n83.6%\\n83.1%\\n59.7%\\n30.6%\\n71.5%\\n40.2%\\n60.7%\\n28.4%\\n74.4%\\nTable 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on\\nalmost all popular benchmarks while using 5x fewer active parameters during inference.\\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\\nmath and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\\non all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\\nis also vastly superior to Llama 2 70B on code and math.\\nDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported\\nin Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different\\ncategories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a\\nsuperior performance in code and mathematics benchmarks.\\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand\\nMixtral models’ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-\\nof-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active\\nparameters, Mixtral is able to outperform Llama 2 70B across most categories.\\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly\\nproportional to the inference compute cost, but does not consider the memory costs and hardware\\nutilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\\n47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\\nintroduces additional overhead due to the routing mechanism and due to the increased memory loads\\nwhen running more than one expert per device. They are more suitable for batched workloads where\\none can reach a good degree of arithmetic intensity.\\nComparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B\\ncompared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\\ntwo other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\\ncapacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\\nGPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n4\\nLLaMA 2 70B\\nGPT-3.5\\nMixtral 8x7B\\nMMLU\\n(MCQ in 57 subjects)\\n69.9%\\n70.0%\\n70.6%\\nHellaSwag\\n(10-shot)\\n87.1%\\n85.5%\\n86.7%\\nARC Challenge\\n(25-shot)\\n85.1%\\n85.2%\\n85.8%\\nWinoGrande\\n(5-shot)\\n83.2%\\n81.6%\\n81.2%\\nMBPP\\n(pass@1)\\n49.8%\\n52.2%\\n60.7%\\nGSM-8K\\n(5-shot)\\n53.6%\\n57.1%\\n58.4%\\nMT Bench\\n(for Instruct Models)\\n6.86\\n8.32\\n8.30\\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2\\n70B and GPT-3.5 performance on most metrics.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\\non TriviaQA, we do not provide Wikipedia contexts.\\n3.1\\nMultilingual benchmarks\\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during\\npretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while\\nmaintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B\\nin French, German, Spanish, and Italian, as shown in Table 4.\\nActive\\nParams\\nFrench\\nGerman\\nSpanish\\nItalian\\nModel\\nArc-c\\nHellaS MMLU\\nArc-c\\nHellaS\\nMMLU\\nArc-c\\nHellaS\\nMMLU\\nArc-c\\nHellaS MMLU\\nLLaMA 1 33B\\n33B\\n39.3%\\n68.1%\\n49.9%\\n41.1%\\n63.3%\\n48.7%\\n45.7%\\n69.8%\\n52.3%\\n42.9% 65.4%\\n49.0%\\nLLaMA 2 70B\\n70B\\n49.9%\\n72.5%\\n64.3%\\n47.3%\\n68.7%\\n64.2%\\n50.5%\\n74.5%\\n66.0%\\n49.4% 70.9%\\n65.1%\\nMixtral 8x7B\\n13B\\n58.2% 77.4%\\n70.9%\\n54.3%\\n73.0%\\n71.5%\\n55.4%\\n77.6%\\n72.5%\\n52.8% 75.1% 70.9%\\nTable 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,\\nand MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\\n3.2\\nLong range performance\\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval\\ntask introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a\\npasskey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a\\n100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.\\nFigure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases\\nmonotonically as the size of the context increases.\\nFigure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task\\nregardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on\\nthe proof-pile dataset decreases monotonically as the context length increases.\\n5\\n3.3\\nBias Benchmarks\\nLlama 2 70B\\nMixtral 8x7B\\nBBQ accuracy\\n51.5%\\n56.0%\\nBOLD sentiment score (avg ± std)\\ngender\\n0.293 ± 0.073\\n0.323 ±0.045\\nprofession\\n0.218 ± 0.073\\n0.243 ± 0.087\\nreligious_ideology\\n0.188 ± 0.133\\n0.144 ± 0.089\\npolitical_ideology\\n0.149 ± 0.140\\n0.186 ± 0.146\\nrace\\n0.232 ± 0.049\\n0.232 ± 0.052\\nFigure 5: Bias Benchmarks. Compared Llama 2 70B,\\nMixtral presents less bias (higher accuracy on BBQ, lower\\nstd on BOLD) and displays more positive sentiment (higher\\navg on BOLD).\\nTo identify possible flaws to be corrected\\nby fine-tuning / preference modeling, we\\nmeasure the base model performance on\\nBias Benchmark for QA (BBQ) [24] and\\nBias in Open-Ended Language Generation\\nDataset (BOLD) [10]. BBQ is a dataset\\nof hand-written question sets that target\\nattested social biases against nine differ-\\nent socially-relevant categories: age, dis-\\nability status, gender identity, nationality,\\nphysical appearance, race/ethnicity, religion,\\nsocio-economic status, sexual orientation.\\nBOLD is a large-scale dataset that consists\\nof 23,679 English text generation prompts\\nfor bias benchmarking across five domains.\\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report\\nthe results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark\\n(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive\\nsentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral\\ndisplays more positive sentiments than Llama 2, with similar variances within each group.\\n4\\nInstruction Fine-tuning\\nWe train Mixtral – Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by\\nDirect Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral – Instruct reaches a\\nscore of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December\\n2023. Independent human evaluation conducted by LMSys is reported in Figure 63 and shows that\\nMixtral – Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\\nFigure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena\\nElo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro\\n(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\\n3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\\n6\\n5\\nRouting analysis\\nIn this section, we perform a small analysis on the expert selection by the router. In particular,\\nwe are interested to see if during training some experts specialized to some specific domains (e.g.\\nmathematics, biology, philosophy, etc.).\\nTo investigate this, we measure the distribution of selected experts on different subsets of The Pile\\nvalidation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31\\nrespectively being the first and the last layers of the model). Surprisingly, we do not observe obvious\\npatterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of\\nexpert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),\\nand for Philosophy (PhilPapers) documents.\\nOnly for DM Mathematics we note a marginally different distribution of experts. This divergence is\\nlikely a consequence of the dataset’s synthetic nature and its limited coverage of the natural language\\nspectrum, and is particularly noticeable at the first and last layers, where the hidden states are very\\ncorrelated to the input and output embeddings respectively.\\nThis suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows\\nexamples of text from different domains (Python code, mathematics, and English), where each token\\nis highlighted with a background color corresponding to its selected expert. The figure shows that\\nwords such as ‘self’ in Python and ‘Question’ in English often get routed through the same expert\\neven though they involve multiple tokens. Similarly, in code, the indentation tokens are always\\nassigned to the same experts, particularly at the first and last layers where the hidden states are more\\ncorrelated to the input and output of the model.\\nWe also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we\\nobserve some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con-\\nsecutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\\n0\\n0.05\\n0.10\\n0.15\\n0.20\\nlayer: 0\\n0\\n0.05\\n0.10\\n0.15\\n0.20\\nlayer: 15\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n0.05\\n0.10\\n0.15\\n0.20\\nlayer: 31\\nExpert ID\\nSelection proportion\\nArXiv\\nDM Mathematics\\nGithub\\nGutenberg\\nPhilPapers\\nPubMed Abstracts\\nStackExchange\\nWikipedia (en)\\nFigure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for\\nlayers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform\\nsampling. Here, we consider experts that are either selected as a first or second choice by the router. A\\nbreakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\\n7\\nFirst choice\\nFirst or second choice\\nLayer 0\\nLayer 15\\nLayer 31\\nLayer 0\\nLayer 15\\nLayer 31\\nArXiv\\n14.0%\\n27.9%\\n22.7%\\n46.5%\\n62.3%\\n52.9%\\nDM Mathematics\\n14.1%\\n28.4%\\n19.7%\\n44.9%\\n67.0%\\n44.5%\\nGithub\\n14.9%\\n28.1%\\n19.7%\\n49.9%\\n66.9%\\n49.2%\\nGutenberg\\n13.9%\\n26.1%\\n26.3%\\n49.5%\\n63.1%\\n52.2%\\nPhilPapers\\n13.6%\\n25.3%\\n22.1%\\n46.9%\\n61.9%\\n51.3%\\nPubMed Abstracts\\n14.2%\\n24.6%\\n22.0%\\n48.6%\\n61.6%\\n51.8%\\nStackExchange\\n13.6%\\n27.2%\\n23.6%\\n48.2%\\n64.6%\\n53.6%\\nWikipedia (en)\\n14.4%\\n23.6%\\n25.3%\\n49.8%\\n62.1%\\n51.8%\\nTable 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is\\nassigned to a token i and its following token i+1. We report whether the first chosen expert is the same, or whether\\nthe same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion\\nof repetitions in the case of random assignments is 1\\n8 = 12.5% for “First choice” and 1 −6\\n8\\n5\\n7 ≈46% for “First\\nand second choice”. Repetitions at the first layer are close to random, but are significantly higher at layers 15\\nand 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\\nconsecutive assignments is significantly higher than random for higher layers. This has implications\\nin how one might optimize the model for fast training and inference. For example, cases with high\\nlocality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.\\nConversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of\\nthese same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\\n6\\nConclusion\\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the-\\nart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem-\\nini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each\\ntime step, Mixtral only uses 13B active parameters per token while outperforming the previous best\\nmodel using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod-\\nels publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de-\\nvelopment of new techniques and applications that can benefit a wide range of industries and domains.\\nFigure 8: Text samples where each token is colored with the first expert choice. The selection of experts\\nappears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\\n8\\nAcknowledgements\\nWe thank the CoreWeave and Scaleway teams for technical support as we trained our models. We\\nare grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working\\nalongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\\nReferences\\n[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\\n[2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\\nAlbert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\\nmodel for mathematics. arXiv preprint arXiv:2310.10631, 2023.\\n[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence, pages 7432–7439, 2020.\\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n[5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and\\nLuke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036,\\n2018.\\n[6] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified\\nscaling laws for routed language models. In International Conference on Machine Learning,\\npages 4057–4086. PMLR, 2022.\\n[7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\narXiv preprint arXiv:1905.10044, 2019.\\n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei\\nChang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended\\nlanguage generation. In Proceedings of the 2021 ACM conference on fairness, accountability,\\nand transparency, pages 862–872, 2021.\\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with\\noffloading. arXiv preprint arXiv:2312.17238, 2023.\\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.\\narXiv preprint arXiv:2209.01667, 2022.\\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse\\ntraining with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.\\n[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning. Advances in Neural Information Processing\\nSystems, 34:29335–29347, 2021.\\n9\\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt.\\nMeasuring massive multitask language understanding.\\narXiv preprint\\narXiv:2009.03300, 2020.\\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\\narXiv preprint arXiv:2103.03874, 2021.\\n[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\n[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\\nTriviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehension.\\narXiv preprint\\narXiv:1705.03551, 2017.\\n[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics, pages 453–466, 2019.\\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\\n2018.\\n[23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\\nlength for transformers. arXiv preprint arXiv:2305.16300, 2023.\\n[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-\\nson, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question\\nanswering. arXiv preprint arXiv:2110.08193, 2021.\\n[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\\narXiv preprint arXiv:2305.18290, 2023.\\n[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM, pages 99–106,\\n2021.\\n[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-\\nmonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[29] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\\nChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.\\nChallenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint\\narXiv:2210.09261, 2022.\\n[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937,\\n2018.\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\n[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\\n[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\\n10\\n[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\\nmodels. arXiv preprint arXiv:2304.06364, 2023.\\n[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,\\nQuoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in\\nNeural Information Processing Systems, 35:7103–7114, 2022.\\n11\\n0\\n0.1\\n0.2\\n0.3\\nLayer 0 -- Either choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 0 -- First choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 0 -- Second choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 15 -- Either choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 15 -- First choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 15 -- Second choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 31 -- Either choice\\n0\\n0.1\\n0.2\\n0.3\\nLayer 31 -- First choice\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n0.1\\n0.2\\n0.3\\nLayer 31 -- Second choice\\nExpert ID\\nSelection proportion\\nArXiv\\nDM Mathematics\\nGithub\\nGutenberg\\nPhilPapers\\nPubMed Abstracts\\nStackExchange\\nWikipedia (en)\\nFigure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated\\nby whether the expert was selected as first or second choice, or either. The “Either choice” case is equivalent\\nto Figure 7. The gray dashed vertical line marks 1\\n8, i.e. the proportion expected with uniform sampling.\\n12\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\nFirst choice\\n0\\n10\\n20\\n30\\n0.5\\n0.6\\n0.7\\nFirst or second choice\\nLayer\\nProportion of repeated assignments\\nsource\\nArXiv\\nDM Mathematics\\nGithub\\nGutenberg\\nPhilPapers\\nPubMed Abstracts\\nStackExchange\\nWikipedia (en)\\nFigure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more\\noften than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across\\ndatasets with less repetitions for DM Mathematics.\\n13\\n', 'source_name': 'Mixtral of Experts', 'source_url': 'https://arxiv.org/abs/2401.04088'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mixtral_NOTES.pdf #69\n",
      "{'content': 'Mixtral of Experts (+Mistral 7B) \\nMain Idea: Mixtral is a recent MoE model that is based on the Mistral architecture (Mistral is a \\ndense model). The difference between these models is that each Mixtral layer consists of sparse \\nFFNs, when these are dense in Mistral, with each sparse layer containing 8 experts each and being \\nthe equivalent of a 7B Mistral model. \\n \\nMistral 7B \\nMistral uses grouped-query-attention (GQA) for accelerated inference speed and reduced \\nmemory requirements (allowing larger batch sizes) and sliding window attention (SWA) for \\nhandling longer sequences at a lower computational cost. The goal of Mistral is to provide an \\nopen-source model that beats other existing open-source models of similar size while improving \\non inference speed and memory/computational requirements, with a focus on practical use of \\nthe model and ease of fine-tuning. \\nSliding Window Attention (SWA) \\nIn regular attention, each token in a sequence attends to every other token, resulting in a \\ncomplexity of O(n^2) with respect to the sequence length. In SWA, the tokens attended are \\nlimited by a sliding window, which masks tokens that are farther away from the current token \\nthan a pre-defined distance. This changes the complexity to O(n*w), where w is the maximum \\nnumber of tokens to be attended (maximum window size). \\n- \\nSWA reduces computational complexity and memory usage – the longer the sequences \\nthe bigger the improvement. \\n- \\nSWA, due to the fixed window size, allows for a rolling buffer cache (this increases \\nefficiency). \\n- \\nSWA also allows for pre-fill and chunking for more efficient inference. \\nResults \\n- \\nMistral is compared to Llama 2 7B/13B, Llama 1 34B and Code-Llama 7B. \\n- \\nCompared to Llama 2 7B/13B and Llama 1 34B, Mistral performs significantly better in \\ncomplex reasoning areas (code, math, reasoning) and comes close to Code-Llama 7B in \\ncoding tasks. \\n- \\nOn knowledge tasks, Mistral also tended to perform better but the gap observed was not \\nas significant as in complex reasoning tasks. \\n- \\nInstruction fine-tuning was performed using publicly available data to show the \\nstraightforwardness of fine-tuning on Mistral 7B. \\no This resulted in comparable performance to 13B instruct models. \\n \\nMixtral \\n- \\nMixtral uses top-2 token-choice routing. \\n- \\nMixtral excels at math, code generation and multilingual benchmarks (consistent with \\nMistral). \\n- \\nA Mixtral-Instruct model (performed SFT and DPO) is also provided and surpasses GPT \\n3.5-Turbo. \\n- \\nThe context length of Mixtral is 32k. \\n- \\nThe gating mechanism of Mixtral takes the SoftMax of the top-2 expert scores and \\nweights the expert’s outputs based on these weights.  \\no The final output is then a weighted average of the sum of the two selected experts’ \\noutputs. \\n- \\nMixtral seems to be robust to long-range contexts. \\no Perhaps due to Mistral’s SWA? \\no Experiments showed that up to a context length of 30k tokens, information can \\naccurately be retrieved, and the perplexity of Mixtral decreases with an increase \\nin context length. \\nThe name Mixtral 8-7B might induce the thought of the architecture having 56B total parameters \\n(8*7), but it consists of around 47B parameters due to shared parameters between experts across \\nthe embedding, attention and normalization layers (7B is the full size of each expert if converted \\nto a dense model). Likewise, the inference cost is not the equivalent of running 14B parameters \\n(7*2), but around 13B parameters due to these shared parameters. \\nIn terms of routing analysis, it was shown that experts seem to be selected based on syntax rather \\nthan on specific domains – experts specialize in semantics and syntax, not on tasks. This is logical \\ndue to the token-choice routing. If routing is done on a token granularity, the experts are \\nexpected to specialize on token-level areas. With domain or task-routing (done at a sequence \\nlevel), experts can be expected to specialize in domain/task-level areas. \\n \\nMy takeaways: \\n- \\nThe goal of Mistral 7B is to provide an open-source model with an optimal performance \\nand efficiency balance. \\no Performance meaning quality, efficiency meaning inference speed and \\ncomputational requirements. \\n- \\nSliding Window Attention seems to sacrifice the context length capacity in return of \\nhigher inference speed. The assumption taken for this not to hurt performance seems to \\nbe that the more you move away from a token, the lower the odds of it having meaningful \\ndependencies to the current token. \\no Large context lengths are possible under SWA, but each individual token will not \\nuse the full context length for inference if the input is larger than the maximum \\nwindow size. \\n- \\nPerhaps the idea for Mixtral came after analyzing Mistral’s results? Since Mistral performs \\nsignificantly better on reasoning tasks but the improvement in knowledge tasks is not so \\nbig, it would make sense to try to apply a MoE architecture to this model, with the idea \\nbeing to retain the reasoning abilities while improving knowledge abilities. This makes \\nsense because other studies seem to show that MoE, due to additional model capacity \\nadded, tend to perform very well on knowledge tasks (weakness of Mistral) but the \\nperformance on reasoning and fine-tuning tasks (strength of Mistral) leaves room for \\nimprovement (although MoE was shown to benefit from instruction-tuning in a more \\nsignificant way than dense models). \\n \\n', 'source_name': 'Mixtral of Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Mixtral/Mixtral_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Towards_Understanding_MoE_NOTES.pdf #70\n",
      "{'content': 'Towards Understanding MoE \\n \\nAn MoE layer contains many experts that share the same network architecture and are trained \\nby the same algorithm, with a gating/routing function that routes individual inputs to a few \\nexperts among all the candidates. \\nThe number of experts used for an input can be a hyperparameter choice called top-k (usually 1 \\nor 2). The computation (inference) cost will only be the computation cost for the top-k expert(s) \\nused. \\nIn practice, all experts are initialized with the same weight distribution, optimization \\nconfiguration, and the router is configured to distribute the data evenly between experts \\n(traditionally through random noise and/or an auxiliary load balancing loss). This makes it unclear \\nhow this leads to specialization of each expert, instead of collapsing into a single model. \\nKey findings: \\n- \\nMoE with linear experts cannot be trained to find a good classifier efficiently. An MoE \\nwith non-linear experts trained with gradient descent from random initialization can \\naccomplish this. The gating mechanism, however, can be linear, since it only needs to \\ndifferentiate between input clusters. \\n- \\nThe study shows that adding random noise to the router’s choice in soft routing (before \\nthe discrete choice) helps distribute the data across experts. \\n- \\nFor nonlinear MoE with non-linear expert functions, experts will diverge at the end of the \\nexploration stage. At the end of the exploration stage, an expert will achieve low error in \\na specific cluster, but high error on the other clusters. \\n- \\nThere is a potential load unbalancing issue when training MoE, with the probability of \\neach input being routed to the same few experts being high. This is a self-fulfilling \\nprophecy, as it will lead to more training of these few experts, resulting in a bigger \\nimbalance. Normalized gradient descent can help with this issue, as well as adding a \\npenalty term to the loss function (auxiliary load balancing loss) or random noise to the \\nrouter. \\n- \\nThe advantage of MoE over dense models in terms of performance depends on the task \\nand the cluster structure of the data. \\nMy takeaway(s): \\n- \\nIn MoE, the router specializes in dividing the input space into n parts/clusters (where n is \\nthe number of experts). Each expert then becomes a specialist on a specific cluster of the \\ninput space (as divided by the router).  \\n- \\nThe router’s task can be performed linearly, as it only needs to learn how to divide the \\ninput space into clusters, while the expert’s task is more challenging, benefitting from \\nnon-linearities. \\n- \\nIt is important to employ load balancing strategies to ensure that this clustering is done \\ncorrectly, especially at early stages of training when the clusters are not yet clear. If this \\nis not done, it can lead to generalization (some experts being assigned to large areas of \\nthe input space while others are assigned to too small areas). \\n- \\nThe advantages of MoE will, therefore, depend on the input space of the data – if the data \\ncan be clustered into “specialization” areas, MoE will perform better, otherwise if the task \\nbenefits from a generalized knowledge of the input space, a dense model will outperform \\nMoE. \\n \\n', 'source_name': 'Towards Understanding MoE', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Towards_Understanding_MoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Towards_Understanding_MoE.pdf #71\n",
      "{'content': 'Towards Understanding Mixture of Experts in Deep\\nLearning\\nZixiang Chen∗and Yihe Deng† and Yue Wu‡ and Quanquan Gu§ and Yuanzhi Li¶\\nAbstract\\nThe Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has\\nachieved great success in deep learning. However, the understanding of such architecture remains\\nelusive. In this paper, we formally study how the MoE layer improves the performance of neural\\nnetwork learning and why the mixture model will not collapse into a single model. Our empirical\\nresults suggest that the cluster structure of the underlying problem and the non-linearity of the\\nexpert are pivotal to the success of MoE. To further understand this, we consider a challenging\\nclassiﬁcation problem with intrinsic cluster structures, which is hard to learn using a single\\nexpert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional\\nneural networks (CNNs), we show that the problem can be learned successfully. Furthermore,\\nour theory shows that the router can learn the cluster-center features, which helps divide the\\ninput complex problem into simpler linear classiﬁcation sub-problems that individual experts\\ncan conquer.\\nTo our knowledge, this is the ﬁrst result towards formally understanding the\\nmechanism of the MoE layer for deep learning.\\n1\\nIntroduction\\nThe Mixture-of-Expert (MoE) structure (Jacobs et al., 1991; Jordan and Jacobs, 1994) is a classic\\ndesign that substantially scales up the model capacity and only introduces small computation\\noverhead. In recent years, the MoE layer (Eigen et al., 2013; Shazeer et al., 2017), which is an\\nextension of the MoE model to deep neural networks, has achieved remarkable success in deep\\nlearning. Generally speaking, an MoE layer contains many experts that share the same network\\narchitecture and are trained by the same algorithm, with a gating (or routing) function that routes\\nindividual inputs to a few experts among all the candidates. Through the sparse gating function,\\nthe router in the MoE layer can route each input to the top-K(K ≥2) best experts (Shazeer et al.,\\n2017), or the single (K = 1) best expert (Fedus et al., 2021). This routing scheme only costs the\\ncomputation of K experts for a new input, which enjoys fast inference time.\\n∗Department\\nof\\nComputer\\nScience,\\nUniversity\\nof\\nCalifornia,\\nLos\\nAngeles,\\nCA\\n90095,\\nUSA;\\ne-mail:\\nchenzx19@cs.ucla.edu\\n†Department\\nof\\nComputer\\nScience,\\nUniversity\\nof\\nCalifornia,\\nLos\\nAngeles,\\nCA\\n90095,\\nUSA;\\ne-mail:\\nyihedeng@cs.ucla.edu\\n‡Department of Computer Science,\\nUniversity of California,\\nLos Angeles,\\nCA 90095,\\nUSA; e-mail:\\n:\\nywu@cs.ucla.edu\\n§Department\\nof\\nComputer\\nScience,\\nUniversity\\nof\\nCalifornia,\\nLos\\nAngeles,\\nCA\\n90095,\\nUSA;\\ne-mail:\\nqgu@cs.ucla.edu\\n¶Machine\\nLearning\\nDepartment,\\nCarnegie\\nMellon\\nUniversity,\\nPittsburgh,\\nPA,\\nUSA;\\nemail:\\nyuanzhil@andrew.cmu.edu\\n1\\narXiv:2208.02813v1  [cs.LG]  4 Aug 2022\\nDespite the great empirical success of the MoE layer, the theoretical understanding of such\\narchitecture is still elusive. In practice, all experts have the same structure, initialized from the same\\nweight distribution (Fedus et al., 2021) and are trained with the same optimization conﬁguration.\\nThe router is also initialized to dispatch the data uniformly. It is unclear why the experts can\\ndiverge to diﬀerent functions that are specialized to make predictions for diﬀerent inputs, and why\\nthe router can automatically learn to dispatch data, especially when they are all trained using\\nsimple local search algorithms such as gradient descent. Therefore, we aim to answer the following\\nquestions:\\nWhy do the experts in MoE diversify instead of collapsing into a single model? And how can the\\nrouter learn to dispatch the data to the right expert?\\nIn this paper, in order to answer the above question, we consider the natural “mixture of\\nclassiﬁcation” data distribution with cluster structure and theoretically study the behavior and\\nbeneﬁt of the MoE layer. We focus on the simplest setting of the mixture of linear classiﬁcation,\\nwhere the data distribution has multiple clusters, and each cluster uses separate (linear) feature\\nvectors to represent the labels. In detail, we consider the data generated as a combination of feature\\npatches, cluster patches, and noise patches (See Deﬁnition 3.1 for more details). We study training\\nan MoE layer based on the data generated from the “mixture of classiﬁcation” distribution using\\ngradient descent, where each expert is chosen to be a two-layer CNN. The main contributions of\\nthis paper are summarized as follows:\\n• We ﬁrst prove a negative result (Theorem 4.1) that any single expert, such as two-layer CNNs\\nwith arbitrary activation function, cannot achieve a test accuracy of more than 87.5% on our\\ndata distribution.\\n• Empirically, we found that the mixture of linear experts performs better than the single expert\\nbut is still signiﬁcantly worse than the mixture of non-linear experts. Figure 1 provides such a\\nresult in a special case of our data distribution with four clusters. Although a mixture of linear\\nmodels can represent the labeling function of this data distribution with 100% accuracy, it fails\\nto learn so after training. We can see that the underlying cluster structure cannot be recovered\\nby the mixture of linear experts, and neither the router nor the experts are diversiﬁed enough\\nafter training. In contrast, the mixture of non-linear experts can correctly recover the cluster\\nstructure and diversify.\\n• Motivated by the negative result and the experiment on the toy data, we study a sparsely-gated\\nMoE model with two-layer CNNs trained by gradient descent. We prove that this MoE model\\ncan achieve nearly 100% test accuracy eﬃciently (Theorem 4.2).\\n• Along with the result on the test accuracy, we formally prove that each expert of the sparsely-\\ngated MoE model will be specialized to a speciﬁc portion of the data (i.e., at least one cluster),\\nwhich is determined by the initialization of the weights. In the meantime, the router can learn\\nthe cluster-center features and route the input data to the right experts.\\n• Finally, we also conduct extensive experiments on both synthetic and real datasets to corroborate\\nour theory.\\nNotation. We use lower case letters, lower case bold face letters, and upper case bold face letters\\nto denote scalars, vectors, and matrices respectively. We denote a union of disjoint sets (Ai : i ∈I)\\nby ⊔i∈IAi. For a vector x, we use ∥x∥2 to denote its Euclidean norm. For a matrix W, we use\\n∥W∥F to denote its Frobenius norm. Given two sequences {xn} and {yn}, we denote xn = O(yn) if\\n|xn| ≤C1|yn| for some absolute positive constant C1, xn = Ω(yn) if |xn| ≥C2|yn| for some absolute\\n2\\nInitialization\\nTraining\\nFinished\\nMixture of nonlinear experts\\nMixture of linear experts\\nFigure 1: Visualization of the training of MoE with nonlinear expert and linear expert.\\nDiﬀerent colors denote router’s dispatch to diﬀerent experts. The lines denote the decision boundary\\nof the MoE model. The data points are visualized on 2d space via t-SNE (Van der Maaten and\\nHinton, 2008).\\nThe MoE architecture follows section 3 where nonlinear experts use activation\\nfunction σ(z) = z3. For this visualization, we let the expert number M = 4 and cluster number\\nK = 4. We generate n = 1, 600 data points from the distribution illustrated in Section 3 with\\nα ∈(0.5, 2), β ∈(1, 2), γ ∈(1, 2), and σp = 1. More details of the visualization are discussed in\\nAppendix A.\\npositive constant C2, and xn = Θ(yn) if C3|yn| ≤|xn| ≤C4|yn| for some absolute constants C3, C4 >\\n0. We also use e\\nO(·) to hide logarithmic factors of d in O(·). Additionally, we denote xn = poly(yn)\\nif xn = O(yD\\nn ) for some positive constant D, and xn = polylog(yn) if xn = poly(log(yn)). We also\\ndenote by xn = o(yn) if limn→∞xn/yn = 0. Finally we use [N] to denote the index set {1, . . . , N}.\\n2\\nRelated Work\\nMixture of Experts Model. The mixture of experts model (Jacobs et al., 1991; Jordan and\\nJacobs, 1994) has long been studied in the machine learning community. These MoE models are\\nbased on various base expert models such as support vector machine (Collobert et al., 2002) ,\\nGaussian processes (Tresp, 2001), or hidden Markov models (Jordan et al., 1997). In order to\\nincrease the model capacity to deal with the complex vision and speech data, Eigen et al. (2013)\\nextended the MoE structure to the deep neural networks, and proposed a deep MoE model composed\\nof multiple layers of routers and experts. Shazeer et al. (2017) simpliﬁed the MoE layer by making\\nthe output of the gating function sparse for each example, which greatly improves the training\\nstability and reduces the computational cost. Since then, the MoE layer with diﬀerent base neural\\nnetwork structures (Shazeer et al., 2017; Dauphin et al., 2017; Vaswani et al., 2017) has been\\nproposed and achieved tremendous successes in a variety of language tasks. Very recently, Fedus\\net al. (2021) improved the performance of the MoE layer by routing one example to only a single\\nexpert instead of K experts, which further reduces the routing computation while preserving the\\nmodel quality.\\n3\\nMixture of Linear Regressions/Classiﬁcations.\\nIn this paper, we consider a “mixture of\\nclassiﬁcation” model. This type of models can be dated back to (De Veaux, 1989; Jordan and\\nJacobs, 1994; Faria and Soromenho, 2010) and has been applied to many tasks including object\\nrecognition (Quattoni et al., 2004) human action recognition (Wang and Mori, 2009), and machine\\ntranslation (Liang et al., 2006). In order to learn the unknown parameters for mixture of linear\\nregressions/classiﬁcation model, (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty and Liang,\\n2013; Anandkumar et al., 2014; Li and Liang, 2018) studies the method of moments and tensor\\nfactorization. Another line of work studies speciﬁc algorithms such as Expectation-Maximization\\n(EM) algorithm (Khalili and Chen, 2007; Yi et al., 2014; Balakrishnan et al., 2017; Wang et al.,\\n2015).\\nTheoretical Understanding of Deep Learning. In recent years, great eﬀorts have been made to\\nestablish the theoretical foundation of deep learning. A series of studies have proved the convergence\\n(Jacot et al., 2018; Li and Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019b; Zou et al., 2018)\\nand generalization (Allen-Zhu et al., 2019a; Arora et al., 2019a,b; Cao and Gu, 2019) guarantees\\nin the so-called “neural tangent kernel” (NTK) regime, where the parameters stay close to the\\ninitialization, and the neural network function is approximately linear in its parameters. A recent\\nline of works (Allen-Zhu and Li, 2019; Bai and Lee, 2019; Allen-Zhu and Li, 2020a,b,c; Li et al.,\\n2020; Cao et al., 2022; Zou et al., 2021; Wen and Li, 2021) studied the learning dynamic of neural\\nnetworks beyond the NTK regime. It is worthwhile to mention that our analysis of the MoE model\\nis also beyond the NTK regime.\\n3\\nProblem Setting and Preliminaries\\nWe consider an MoE layer with each expert being a two-layer CNN trained by gradient descent\\n(GD) over n independent training examples {(xi, yi)}n\\ni=1 generated from a data distribution D. In\\nthis section, we will ﬁrst introduce our data model D, and then explain our neural network model\\nand the details of the training algorithm.\\n3.1\\nData distribution\\nWe consider a binary classiﬁcation problem over P-patch inputs, where each patch has d dimensions.\\nIn particular, each labeled data is represented by (x, y), where input x = (x(1), x(2), . . . , x(P)) ∈\\n(Rd)P is a collection of P patches and y ∈{±1} is the data label. We consider data generated from\\nK clusters. Each cluster k ∈[K] has a label signal vector vk and a cluster-center signal vector ck\\nwith ∥vk∥2 = ∥ck∥2 = 1. For simplicity, we assume that all the signals {vk}k∈[K] ∪{ck}k∈[K] are\\northogonal with each other.\\nDeﬁnition 3.1. A data pair (x, y) ∈(Rd)P ×{±1} is generated from the distribution D as follows.\\n• Uniformly draw a pair (k, k′) with k ̸= k′ from {1, . . . , K}.\\n• Generate the label y ∈{±1} uniformly, generate a Rademacher random variable ϵ ∈{±1}.\\n• Independently generate random variables α, β, γ from distribution Dα, Dβ, Dγ. In this paper, we\\nassume there exists absolute constants C1, C2 such that almost surely 0 < C1 ≤α, β, γ ≤C2.\\n• Generate x as a collection of P patches: x = (x(1), x(2), . . . , x(P)) ∈(Rd)P , where\\n– Feature signal. One and only one patch is given by yαvk.\\n– Cluster-center signal. One and only one patch is given by βck.\\n– Feature noise. One and only one patch is given by ϵγvk′.\\n4\\n– Random noise. The rest of the P −3 patches are Gaussian noises that are independently\\ndrawn from N(0, (σ2\\np/d) · Id) where σp is an absolute constant.\\nHow to learn this type of data? Since the positions of signals and noises are not speciﬁed\\nin Deﬁnition 3.1, it is natural to use the CNNs structure that applies the same function to each\\npatch. We point out that the strength of the feature noises γ could be as large as the strength of\\nthe feature signals α. As we will see later in Theorem 4.1, this classiﬁcation problem is hard to\\nlearn with a single expert, such as any two-layer CNNs (any activation function with any number\\nof neurons). However, such a classiﬁcation problem has an intrinsic clustering structure that may\\nbe utilized to achieve better performance. Examples can be divided into K clusters ∪k∈[K]Ωk based\\non the cluster-center signals: an example (x, y) ∈Ωk if and only if at least one patch of x aligns\\nwith ck. It is not diﬃcult to show that the binary classiﬁcation sub-problem over Ωk can be easily\\nsolved by an individual expert. We expect the MoE can learn this data cluster structure from the\\ncluster-center signals.\\nSigniﬁcance of our result. Although this data can be learned by existing works on a mixture of\\nlinear classiﬁers with sophisticated algorithms (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty\\nand Liang, 2013), the focus of our paper is training a mixture of nonlinear neural networks, a\\nmore practical model used in real applications. When an MoE is trained by variants of gradient\\ndescent, we show that the experts automatically learn to specialize on each cluster, while the router\\nautomatically learns to dispatch the data to the experts according to their specialty. Although from\\na representation point of view, it is not hard to see that the concept class can be represented by\\nMoEs, our result is very signiﬁcant as we prove that gradient descent from random initialization\\ncan ﬁnd a good MoE with non-linear experts eﬃciently. To make our results even more compelling,\\nwe empirically show that MoE with linear experts, despite also being able to represent the concept\\nclass, cannot be trained to ﬁnd a good classiﬁer eﬃciently.\\n3.2\\nStructure of the MoE layer\\nAn MoE layer consists of a set of M “expert networks” f1, . . . , fM, and a gating network which is\\ngenerally set to be linear (Shazeer et al., 2017; Fedus et al., 2021). Denote by fm(x; W) the output\\nof the m-th expert network with input x and parameter W.\\nDeﬁne an M-dimensional vector\\nh(x; Θ) = P\\np∈[P] Θ⊤x(p) as the output of the gating network parameterized by Θ = [θ1, . . . , θM] ∈\\nRd×M. The output F of the MoE layer can be written as follows:\\nF(x; Θ, W) = P\\nm∈Txπm(x; Θ)fm(x; W),\\nwhere Tx ⊆[M] is a set of selected indices and πm(x; Θ)’s are route gate values given by\\nπm(x; Θ) =\\nexp(hm(x; Θ))\\nPM\\nm′=1 exp(hm′(x; Θ))\\n, ∀m ∈[M].\\nExpert Model. In practice, one often uses nonlinear neural networks as experts in the MoE layer.\\nIn fact, we found that the non-linearity of the expert is essential for the success of the MoE layer\\n(see Section 6). For m-th expert, we consider a convolution neural network as follows:\\nfm(x; W) = P\\nj∈[J]\\nPP\\np=1σ\\n\\x00⟨wm,j, x(p)⟩\\n\\x01\\n,\\n(3.1)\\n5\\nwhere wm,j ∈Rd is the weight vector of the j-th ﬁlter (i.e., neuron) in the m-th expert, J is\\nthe number of ﬁlters (i.e., neurons). We denote Wm = [wm,1, . . . , wm,J] ∈Rd×J as the weight\\nmatrix of the m-th expert and further let W = {Wm}m∈[M] as the collection of expert weight\\nmatrices. For nonlinear CNN, we consider the cubic activation function σ(z) = z3, which is one of\\nthe simplest nonlinear activation functions (Vecci et al., 1998). We also include the experiment for\\nother activation functions such as RELU in Appendix Table 7.\\nTop-1 Routing Model.\\nA simple choice of the selection set Tx would be the whole experts\\nset Tx = [M] (Jordan and Jacobs, 1994), which is the case for the so-called soft-routing model.\\nHowever, it would be time consuming to use soft-routing in deep learning. In this paper, we consider\\n“switch routing”, which is introduced by Fedus et al. (2021) to make the gating network sparse\\nand save the computation time. For each input x, instead of using all the experts, we only pick\\none expert from [M], i.e., |Tx| = 1. In particular, we choose Tx = argmaxm{hm(x; Θ)}.\\nFigure 2: Illustration of an MoE layer. For each\\ninput x, the router will only select one expert to per-\\nform computations. The choice is based on the output\\nof the gating network (dotted line). The expert layer\\nreturns the output of the selected expert (gray box)\\nmultiplied by the route gate value (softmax of the gat-\\ning function output).\\nAlgorithm 1 Gradient descent with\\nrandom initialization\\nRequire: Number of iterations T, ex-\\npert learning rate η, router learning\\nrate ηr, initialization scale σ0, train-\\ning set S = {(xi, yi)}n\\ni=1.\\n1: Generate each entry of W(0) indepen-\\ndently from N(0, σ2\\n0).\\n2: Initialize each entry of Θ(0) as zero.\\n3: for t = 0, 2, . . . , T −1 do\\n4:\\nGenerate each entry of r(t) inde-\\npendently from Unif[0,1].\\n5:\\nUpdate W(t+1) as in (3.4).\\n6:\\nUpdate Θ(t+1) as in (3.5).\\n7: end for\\n8: return (Θ(T ), W(T )).\\n3.3\\nTraining Algorithm\\nGiven the training data S = {(xi, yi)}n\\ni=1, we train F with gradient descent to minimize the\\nfollowing empirical loss function:\\nL(Θ, W) = 1\\nn\\nPn\\ni=1ℓ\\n\\x00yiF(xi; Θ, W)\\n\\x01\\n,\\n(3.2)\\nwhere ℓis the logistic loss deﬁned as ℓ(z) = log(1 + exp(−z)). We initialize Θ(0) to be zero and\\ninitialize each entry of W(0) by i.i.d N(0, σ2\\n0). Zero initialization of the gating network is widely\\nused in MoE training. As discussed in Shazeer et al. (2017), it can help avoid out-of-memory errors\\nand initialize the network in a state of approximately equal expert load (see (5.1) for the deﬁnition\\nof expert load).\\nInstead of directly using the gradient of empirical loss (3.2) to update weights, we add pertur-\\nbation to the router and use the gradient of the perturbed empirical loss to update the weights.\\nIn particular, the training example xi will be distributed to argmaxm{hm(xi; Θ(t)) + r(t)\\nm,i} instead,\\nwhere {r(t)\\nm,i}m∈[M],i∈[n] are random noises. Adding noise term is a widely used training strategy\\nfor sparsely-gated MoE layer (Shazeer et al., 2017; Fedus et al., 2021), which can encourage explo-\\n6\\nration across the experts and stabilize the MoE training. In this paper, we draw {r(t)\\nm,i}m∈[M],i∈[n]\\nindependently from the uniform distribution Unif[0, 1] and denotes its collection as r(t). Therefore,\\nthe perturbed empirical loss at iteration t can be written as\\nL(t)(Θ(t), W(t)) = 1\\nn\\nPn\\ni=1ℓ\\n\\x00yiπmi,t(xi; Θ(t))fmi,t(xi; W(t))\\n\\x01\\n,\\n(3.3)\\nwhere mi,t = argmaxm{hm(xi; Θ(t)) + r(t)\\nm,i}. Starting from the initialization W(0), the gradient\\ndescent update rule for the experts is\\nW(t+1)\\nm\\n= W(t)\\nm −η · ∇WmL(t)(Θ(t), W(t))/∥∇WmL(t)(Θ(t), W(t))∥F , ∀m ∈[M],\\n(3.4)\\nwhere η > 0 is the expert learning rate. Starting from the initialization Θ(0), the gradient update\\nrule for the gating network is\\nθ(t+1)\\nm\\n= θ(t)\\nm −ηr · ∇θmL(t)(Θ(t), W(t)), ∀m ∈[M],\\n(3.5)\\nwhere ηr > 0 is the router learning rate. In practice, the experts are trained by Adam (?) to make\\nsure they have similar learning speeds. Here we use a normalized gradient which can be viewed as\\na simpler alternative to Adam (Jelassi et al., 2021).\\n4\\nMain Results\\nIn this section, we will present our main results. We ﬁrst provide a negative result for learning with\\na single expert.\\nTheorem 4.1 (Single expert performs poorly). Suppose Dα = Dγ in Deﬁnition 3.1, then any\\nfunction with the form F(x) = PP\\np=1 f(x(p)) will get large test error P(x,y)∼D\\n\\x00yF(x) ≤0\\n\\x01\\n≥1/8.\\nTheorem 4.1 indicates that if the feature noise has the same strength as the feature signal\\ni.e., Dα = Dγ, any two-layer CNNs with the form F(x) = P\\nj∈[J] aj\\nP\\np∈[P] σ(w⊤\\nj x(p) + bj) can’t\\nperform well on the classiﬁcation problem deﬁned in Deﬁnition 3.1 where σ can be any activation\\nfunction. Theorem 4.1 also shows that a simple ensemble of the experts may not improve the\\nperformance because the ensemble of the two-layer CNNs is still in the form of the function deﬁned\\nin Theorem 4.1.\\nAs a comparison, the following theorem gives the learning guarantees for training an MoE layer\\nthat follows the structure deﬁned in Section 3.2 with cubic activation function.\\nTheorem 4.2 (Nonlinear MoE performs well). Suppose the training data size n = Ω(d). Choose\\nexperts number M = Θ(K log K log log d), ﬁlter size J = Θ(log M log log d), initialization scale\\nσ0 ∈[d−1/3, d−0.01], learning rate η = e\\nO(σ0), ηr = Θ(M2)η. Then with probability at least 1−o(1),\\nAlgorithm 1 is able to output (Θ(T), W(T)) within T = e\\nO(η−1) iterations such that the non-linear\\nMoE deﬁned in Section 3.2 satisﬁes\\n• Training error is zero, i.e., yiF(xi; Θ(T), W(T)) > 0, ∀i ∈[n].\\n• Test error is nearly zero, i.e., P(x,y)∼D\\n\\x00yF(x; Θ(T), W(T)) ≤0\\n\\x01\\n= o(1).\\nMore importantly, the experts can be divided into a disjoint union of K non-empty sets [M] =\\n⊔k∈[K]Mk and\\n7\\n• (Each expert is good on one cluster) Each expert m ∈Mk performs good on the cluster Ωk,\\nP(x,y)∼D(yfm(x; W(T)) ≤0|(x, y) ∈Ωk) = o(1).\\n• (Router only distributes example to good expert) With probability at least 1 −o(1), an example\\nx ∈Ωk will be routed to one of the experts in Mk.\\nTheorem 4.2 shows that a non-linear MoE performs well on the classiﬁcation problem in Deﬁ-\\nnition 3.1. In addition, the router will learn the cluster structure and divide the problem into K\\nsimpler sub-problems, each of which is associated with one cluster. In particular, each cluster will\\nbe classiﬁed accurately by a subset of experts. On the other hand, each expert will perform well\\non at least one cluster.\\nFurthermore, together with Theorem 4.1, Theorem 4.2 suggests that there exist problem in-\\nstances in Deﬁnition 3.1 (i.e., Dα = Dγ) such that an MoE provably outperforms a single expert.\\n5\\nOverview of Key Techniques\\nA successful MoE layer needs to ensure that the router can learn the cluster-center features and\\ndivide the complex problem in Deﬁnition 3.1 into simpler linear classiﬁcation sub-problems that\\nindividual experts can conquer. Finding such a gating network is diﬃcult because this problem is\\nhighly non-convex. In the following, we will introduce the main diﬃculties in analyzing the MoE\\nlayer and the corresponding key techniques to overcome those barriers.\\nMain Diﬃculty 1: Discontinuities in Routing. Compared with the traditional soft-routing\\nmodel, the sparse routing model saves computation and greatly reduces the inference time. How-\\never, this form of sparsity also causes discontinuities in routing (Shazeer et al., 2017). In fact, even\\na small perturbation of the gating network outputs h(x; Θ) + δ may change the router behavior\\ndrastically if the second largest gating network output is close to the largest gating network output.\\nKey Technique 1: Stability by Smoothing. We point out that the noise term added to the\\ngating network output ensures a smooth transition between diﬀerent routing behavior, which makes\\nthe router more stable. This is proved in the following lemma.\\nLemma 5.1. Let h, b\\nh ∈RM to be the output of the gating network and {rm}M\\nm=1 to be the noise\\nindependently drawn from Unif[0,1]. Denote p, b\\np ∈RM to be the probability that experts get\\nrouted, i.e., pm = P(argmaxm′∈[M]{hm′ + rm′} = m), b\\npm = P(argmaxm′∈[M]{b\\nhm′ + rm′} = m).\\nThen we have that ∥p −b\\np∥∞≤M2∥h −b\\nh∥∞.\\nLemma 5.1 implies that when the change of the gating network outputs at iteration t and t′\\nis small, i.e., ∥h(x; Θ(t)) −h(x; Θ(t′))∥∞, the router behavior will be similar.\\nSo adding noise\\nprovides a smooth transition from time t to t′. It is also worth noting that Θ is zero initialized. So\\nh(x; Θ(0)) = 0 and thus each expert gets routed with the same probability pm = 1/M by symmetric\\nproperty. Therefore, at the early of the training when ∥h(x; Θ(t)) −h(x; Θ(0))∥∞is small, router\\nwill almost uniformly pick one expert from [M], which helps exploration across experts.\\nMain Diﬃculty 2: No “Real” Expert. At the beginning of the training, the gating network\\nis zero, and the experts are randomly initialized. Thus it is hard for the router to learn the right\\nfeatures because all the experts look the same: they share the same network architecture and are\\ntrained by the same algorithm. The only diﬀerence would be the initialization. Moreover, if the\\nrouter makes a mistake at the beginning of the training, the experts may amplify the mistake\\nbecause the experts will be trained based on mistakenly dispatched data.\\n8\\nKey Technique 2: Experts from Exploration. Motivated by the key technique 1, we intro-\\nduce an exploration stage to the analysis of MoE layer during which the router almost uniformly\\npicks one expert from [M]. This stage starts at t = 0 and ends at T1 = ⌊η−1σ0.5\\n0 ⌋≪T = e\\nO(η−1)\\nand the gating network remains nearly unchanged ∥h(x; Θ(t)) −h(x; Θ(0))∥∞= O(σ1.5\\n0 ).\\nBe-\\ncause the experts are treated almost equally during exploration stage, we can show that the\\nexperts become specialized to some speciﬁc task only based on the initialization.\\nIn particu-\\nlar, the experts set [M] can be divided into K nonempty disjoint sets [M] = ⊔kMk, where\\nMk := {m| argmaxk′∈[K],j∈[J]⟨vk′, w(0)\\nm,j⟩= k}.\\nFor nonlinear MoE with cubic activation func-\\ntion, the following lemma further shows that experts in diﬀerent set Mk will diverge at the end of\\nthe exploration stage.\\nLemma 5.2. Under the same condition as in Theorem 4.2, with probability at least 1 −o(1), the\\nfollowing equations hold for all expert m ∈Mk,\\nP(x,y)∼D\\n\\x00yfm(x; W(T1)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk\\n\\x01\\n= o(1),\\nP(x,y)∼D\\n\\x00yfm(x; W(T1)) ≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk′\\x01\\n= Ω\\n\\x001/K\\n\\x01\\n, ∀k′ ̸= k.\\nLemma 5.2 implies that, at the end of the exploration stage, the expert m ∈Mk can achieve\\nnearly zero test error on the cluster Ωk but high test error on the other clusters Ωk′, k′ ̸= k.\\nMain Diﬃculty 3: Expert Load Imbalance. Given the training data set S = {(xi, yi)}n\\ni=1,\\nthe load of expert m at iterate t is deﬁned as\\nLoad(t)\\nm = P\\ni∈[n]P(mi,t = m),\\n(5.1)\\nwhere P(mi,t = m) is probability that the input xi being routed to expert m at iteration t. Eigen\\net al. (2013) ﬁrst described the load imbalance issues in the training of the MoE layer. The gating\\nnetwork may converge to a state where it always produces large Load(t)\\nm for the same few experts.\\nThis imbalance in expert load is self-reinforcing, as the favored experts are trained more rapidly\\nand thus are selected even more frequently by the router (Shazeer et al., 2017; Fedus et al., 2021).\\nExpert load imbalance issue not only causes memory and performance problems in practice, but\\nalso impedes the theoretical analysis of the expert training.\\nKey Technique 3: Normalized Gradient Descent. Lemma 5.2 shows that the experts will\\ndiverge into ⊔k∈[K]Mk. Normalized gradient descent can help diﬀerent experts in the same Mk\\nbeing trained at the same speed regardless the imbalance load caused by the router. Because the\\nself-reinforcing circle no longer exists, we can prove that the router will treat diﬀerent experts in the\\nsame Mk almost equally and dispatch almost the same amount of data to them (See Section E.2 in\\nAppendix for detail). This Load imbalance issue can be further avoided by adding load balancing\\nloss (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), or advanced MoE layer structure\\nsuch as BASE Layers (Lewis et al., 2021; Dua et al., 2021) and Hash Layers (Roller et al., 2021).\\nRoad Map: Here we provide the road map of the proof of Theorem 4.2 and the full proof is\\npresented in Appendix E. The training process can be decomposed into several stages. The ﬁrst\\nstage is called Exploration stage. During this stage, the experts will diverge into K professional\\ngroups ⊔K\\nk=1Mk = [M]. In particular, we will show that Mk is not empty for all k ∈[K]. Besides,\\nfor all m ∈Mk, fm is a good classiﬁer over Ωk. The second stage is called router learning stage.\\nDuring this stage, the router will learn to dispatch x ∈Ωk to one of the experts in Mk. Finally,\\nwe will give the generalization analysis for the MoEs from the previous two stages.\\n9\\n6\\nExperiments\\nSetting 1:α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3), σp = 1\\nTest accuracy (%)\\nDispatch Entropy\\nSingle (linear)\\n68.71\\nNA\\nSingle (nonlinear)\\n79.48\\nNA\\nMoE (linear)\\n92.99 ± 2.11\\n1.300 ± 0.044\\nMoE (nonlinear)\\n99.46 ± 0.55\\n0.098 ± 0.087\\nSetting 2: α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3), σp = 2\\nTest accuracy (%)\\nDispatch Entropy\\nSingle (linear)\\n60.59\\nNA\\nSingle (nonlinear)\\n72.29\\nNA\\nMoE (linear)\\n88.48 ± 1.96\\n1.294 ± 0.036\\nMoE (nonlinear)\\n98.09 ± 1.27\\n0.171 ± 0.103\\nTable 1: Comparison between MoE (linear) and MoE\\n(nonlinear) in our setting. We report results of top-1 gating\\nwith noise for both linear and nonlinear models.\\nOver ten\\nrandom experiments, we report the average value ± standard\\ndeviation for both test accuracy and dispatch entropy.\\n0\\n100\\n200\\n300\\n400\\n500\\nTraining Epochs\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nEntropy\\nLinear-1\\nNonlinear-1\\nLinear-2\\nNonlinear-2\\nFigure 3: Illustration of router\\ndispatch entropy.\\nWe demon-\\nstrate the change of entropy of\\nMoE during training on the syn-\\nthetic data.\\nMoE (linear)-1 and\\nMoE (nonlinear)-1 refer to Setting\\n1 in Table 1. MoE (linear)-2 and\\nMoE (nonlinear)-2 refer to Setting\\n2 in Table 1.\\n6.1\\nSynthetic-data Experiments\\nDatasets. We generate 16, 000 training examples and 16, 000 test examples from the data distri-\\nbution deﬁned in Deﬁnition 3.1 with cluster number K = 4 , patch number P = 4 and dimension\\nd = 50. We randomly shuﬄe the order of the patches of x after we generate data (x, y). We\\nconsider two parameter settings: 1. α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 3)\\nand σp = 1; 2. α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 3) and σp = 2. Note that\\nTheorem 4.1 shows that when α and γ follow the same distribution, neither single linear expert or\\nsingle nonlinear expert can give good performance. Here we consider a more general and diﬃcult\\nsetting when α and γ are from diﬀerent distributions.\\nModels. We consider the performances of single linear CNN, single nonlinear CNN, linear MoE,\\nand nonlinear MoE. The single nonlinear CNN architecture follows (3.1) with cubic activation\\nfunction, while single linear CNN follows (3.1) with identity activation function. For both linear\\nand nonlinear MoEs, we consider a mixture of 8 experts with each expert being a single linear\\nCNN or a single nonlinear CNN. Finally, we train single models with gradient descent and train\\nthe MoEs with Algorithm 1. We run 10 random experiments and report the average accuracy with\\nstandard deviation.\\nEvaluation. To evaluate how well the router learned the underlying cluster structure of the data,\\nwe deﬁne the entropy of the router’s dispatch as follows. Denote by nk,m the number of data in\\ncluster K that are dispatched to expert m. The total number of data dispatched to expert m is\\nnm = PK\\nk=1 nk,m and the total number of data is n = PK\\nk=1\\nPM\\nm=1 nk,m. The dispatch entropy is\\n10\\nCIFAR-10 (%)\\nCIFAR-10-Rotate (%)\\nCNN\\nSingle\\n80.68 ± 0.45\\n76.78 ± 1.79\\nMoE\\n80.31 ± 0.62\\n79.60 ± 1.25\\nMobileNetV2 Single\\n92.45 ± 0.25\\n85.76 ± 2.91\\nMoE\\n92.23 ± 0.72\\n89.85 ± 2.54\\nResNet18\\nSingle\\n95.51 ± 0.31\\n88.23 ± 0.96\\nMoE\\n95.32 ± 0.68\\n92.60 ± 2.01\\nTable 2: Comparison between MoE and single model on CIFAR-10 and CIFAR-10-Rotate datasets.\\nWe report the average test accuracy over 10 random experiments ± the standard deviation.\\nthen deﬁned as\\nentropy = −PM\\nm=1,nm̸=0\\nnm\\nn\\nPK\\nk=1\\nnk,m\\nnm · log\\n\\x00 nk,m\\nnm\\n\\x01\\n.\\n(6.1)\\nWhen each expert receives the data from at most one cluster, the dispatch entropy will be zero.\\nAnd a uniform dispatch will result in the maximum dispatch entropy.\\nAs shown in Table 1, the linear MoE does not perform as well as the nonlinear MoE in Setting 1,\\nwith around 6% less test accuracy and much higher variance. With stronger random noise (Setting\\n2), the diﬀerence between the nonlinear MoE and linear MoE becomes even more signiﬁcant. We\\nalso observe that the ﬁnal dispatch entropy of nonlinear MoE is nearly zero while that of the\\nlinear MoE is large. In Figure 3, we further demonstrate the change of dispatch entropy during\\nthe training process. The dispatch entropy of nonlinear MoE signiﬁcantly decreases, while that of\\nlinear MoE remains large. Such a phenomenon indicates that the nonlinear MoE can successfully\\nlearn the underlying cluster structure of the data while the linear MoE fails to do so.\\n6.2\\nReal-data Experiments\\nWe further conduct experiments on real image datasets and demonstrate the importance of the\\nclustering data structure to the MoE layer in deep neural networks.\\nDatasets. We consider the CIFAR-10 dataset (Krizhevsky, 2009) and the 10-class classiﬁcation\\ntask. Furthermore, we create a CIFAR-10-Rotate dataset that has a strong underlying cluster\\nstructure that is independent of its labeling function.\\nSpeciﬁcally, we rotate the images by 30\\ndegrees and merge the rotated dataset with the original one. The task is to predict if the image is\\nrotated, which is a binary classiﬁcation problem. We deem that some of the classes in CIFAR-10\\nform underlying clusters in CIFAR-10-Rotate. In Appendix A, we explain in detail how we generate\\nCIFAR-10-Rotate and present some speciﬁc examples.\\nModels. For the MoE, we consider a mixture of 4 experts with a linear gating network. For the\\nexpert/single model architectures, we consider a CNN with 2 convolutional layers (architecture de-\\ntails are illustrated in Appendix A.) For a more thorough evaluation, we also consider expert/single\\nmodels with architecture including MobileNetV2 (Sandler et al., 2018) and ResNet18 (He et al.,\\n2016). The training process of MoE also follows Algorithm 1.\\nThe experiment results are shown in Table 2, where we compare single and mixture models\\nof diﬀerent architectures over CIFAR-10 and CIFAR-10-Rotate datasets.\\nWe observe that the\\nimprovement of MoEs over single models diﬀers largely on the diﬀerent datasets. On CIFAR-10,\\n11\\nthe performance of MoEs is very close to the single models. However, on the CIFAR-10-Rotate\\ndataset, we can observe a signiﬁcant performance improvement from single models to MoEs. Such\\nresults indicate the advantage of MoE over single models depends on the task and the cluster\\nstructure of the data.\\n7\\nConclusion and Future Work\\nIn this work, we formally study the mechanism of the Mixture of Experts (MoE) layer for deep\\nlearning. To our knowledge, we provide the ﬁrst theoretical result toward understanding how the\\nMoE layer works in deep learning. Our empirical evidence reveals that the cluster structure of\\nthe data plays an important role in the success of the MoE layer. Motivated by these empirical\\nobservations, we study a data distribution with cluster structure and show that Mixture-of-Experts\\nprovably improves the test accuracy of a single expert of two-layer CNNs.\\nThere are several important future directions.\\nFirst, our current results are for CNNs.\\nIt\\nis interesting to extend our results to other neural network architectures, such as transformers.\\nSecond, our data distribution is motivated by the classiﬁcation problem of image data. We plan to\\nextend our analysis to other types of data (e.g., natural language data).\\nA\\nExperiment Details\\nA.1\\nVisualization\\nIn the visualization of Figure 1, MoE (linear) and MoE (nonlinear) are trained according to Algo-\\nrithm 1 by normalized gradient descent with learning rate 0.001 and gradient descent with learning\\nrate 0.1. According to Deﬁnition 3.1, we set K = 4, P = 4 and d = 50 and choose α ∈(0.5, 2),\\nβ ∈(1, 2), γ ∈(1, 2) and σp = 1, and generate 3, 200 data examples. We consider mixture of M = 4\\nexperts for both MoE (linear) and MoE (nonlinear). For each expert, we set the number of neu-\\nrons/ﬁlters J = 16. We train MoEs on 1, 600 data examples and visualize classiﬁcation result and\\ndecision boundary on the remaining 1, 600 examples. The data examples are visualized via t-SNE\\n(Van der Maaten and Hinton, 2008). When visualizing the data points and decision boundary on\\nthe 2d space, we increase the magnitude of random noise patch by 3 so that the positive/negative\\nexamples and decision boundaries can be better viewed.\\nA.2\\nSynthetic-data Experiments\\nSynthetic-data experiment setup. For the experiments on synthetic data, we generate the data\\naccording to Deﬁnition 3.1 with K = 4, P = 4 and d = 50. We consider four parameter settings:\\n• α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 3) and σp = 1;\\n• α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 3) and σp = 2;\\n• α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 2) and σp = 1;\\n• α ∼Uniform(0.5, 2), β ∼Uniform(1, 2), γ ∼Uniform(0.5, 2) and σp = 2.\\nWe consider mixture of M = 8 experts for all MoEs and J = 16 neurons/ﬁlters for all experts. For\\nsingle models, we consider J = 128 neurons/ﬁlters. We train MoEs using Algorithm 1. Speciﬁcally,\\n12\\nSetting 1:α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3), σp = 1\\nTest accuracy (%)\\nDispatch Entropy\\nNumber of Filters\\nSingle (linear)\\n68.71\\nNA\\n128\\nSingle (linear)\\n67.63\\nNA\\n512\\nSingle (nonlinear)\\n79.48\\nNA\\n128\\nSingle (nonlinear)\\n78.18\\nNA\\n512\\nMoE (linear)\\n92.99 ± 2.11\\n1.300 ± 0.044\\n128 (16*8)\\nMoE (nonlinear)\\n99.46 ± 0.55\\n0.098 ± 0.087\\n128 (16*8)\\nSetting 2: α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3), σp = 2\\nTest accuracy (%)\\nDispatch Entropy\\nNumber of Filters\\nSingle (linear)\\n60.59\\nNA\\n128\\nSingle (linear)\\n63.04\\nNA\\n512\\nSingle (nonlinear)\\n72.29\\nNA\\n128\\nSingle (nonlinear)\\n52.09\\nNA\\n512\\nMoE (linear)\\n88.48 ± 1.96\\n1.294 ± 0.036\\n128 (16*8)\\nMoE (nonlinear)\\n98.09 ± 1.27\\n0.171 ± 0.103\\n128 (16*8)\\nSetting 3:α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 2), σp = 1\\nTest accuracy (%)\\nDispatch Entropy\\nNumber of Filters\\nSingle (linear)\\n74.81\\nNA\\n128\\nSingle (linear)\\n74.54\\nNA\\n512\\nSingle (nonlinear)\\n72.69\\nNA\\n128\\nSingle (nonlinear)\\n67.78\\nNA\\n512\\nMoE (linear)\\n95.93 ± 1.34\\n1.160 ± 0.100\\n128 (16*8)\\nMoE (nonlinear)\\n99.99 ± 0.02\\n0.008 ± 0.011\\n128 (16*8)\\nSetting 4: α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 2), σp = 2\\nTest accuracy (%)\\nDispatch Entropy\\nNumber of Filters\\nSingle (linear)\\n74.63\\nNA\\n128\\nSingle (linear)\\n72.98\\nNA\\n512\\nSingle (nonlinear)\\n68.60\\nNA\\n128\\nSingle (nonlinear)\\n61.65\\nNA\\n512\\nMoE (linear)\\n93.30 ± 1.48\\n1.160 ± 0.155\\n128 (16*8)\\nMoE (nonlinear)\\n98.92 ± 1.18\\n0.089 ± 0.120\\n128 (16*8)\\nTable 3: Comparison between MoE (linear) and MoE (nonlinear) in our setting.\\nWe\\nreport results of top-1 gating with noise for both linear and nonlinear models. Over ten random\\nexperiments, we report the average value ± standard deviation for both test accuracy and dispatch\\nentropy.\\n13\\nExpert number\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nInitial dispatch\\n1921\\n2032\\n1963\\n1969\\n2075\\n1980\\n2027\\n2033\\nFinal dispatch\\n0\\n3979\\n4009\\n0\\n0\\n3971\\n0\\n4041\\nCluster 1\\n0\\n0\\n0\\n0\\n0\\n3971\\n0\\n0\\nCluster 2\\n0\\n0\\n4009\\n0\\n0\\n0\\n0\\n0\\nCluster 3\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n4041\\nCluster 4\\n0\\n3979\\n0\\n0\\n0\\n0\\n0\\n0\\nTable 4: Dispatch details of MoE (nonlinear) with test accuracy 100%.\\nwe train the experts by normalized gradient descent with learning rate 0.001 and the gating network\\nby gradient descent with learning rate 0.1. We train single linear/nonlinear models by Adam (?)\\nto achieve the best performance, with learning rate 0.01 and weight decay 5e-4 for single nonlinear\\nmodel and learning rate 0.003 and weight decay 5e −4 for single linear model.\\nSynthetic-data experiment results. In Table 3, we present the empirical results of single linear\\nCNN, single nonlinear CNN, linear MoE, and nonlinear MoE under settings 3 and 4, where α and\\nγ follow the same distribution as we assumed in theoretical analysis. Furthermore, we report the\\ntotal number of ﬁlters for both single CNNs and a mixture of CNNs, where the ﬁlter size (equal\\nto 50) is the same for all single models and experts. For linear and nonlinear MoE, there are 16\\nﬁlters for each of the 8 experts, and therefore 128 ﬁlters in total. Note that in the synthetic-data\\nexperiment in the main paper, we let the number of ﬁlters of single models be the same as MoEs\\n(128). Here, we additionally report the performances of single models with 512 ﬁlters, and see\\nif increasing the model size of single models can beat MoE. From Table 3, we observe that: 1.\\nsingle models perform poorly in all settings; 2. linear MoEs do not perform as well as nonlinear\\nMoEs. Speciﬁcally, the ﬁnal dispatch entropy of nonlinear MoEs is nearly zero while the dispatch\\nentropy of linear MoEs is consistently larger under settings 1-4. This indicates that nonlinear MoEs\\nsuccessfully uncover the underlying cluster structure while linear MoEs fail to do so. In addition,\\nwe can see that even larger single models cannot beat linear MoEs or nonlinear MoEs. This is\\nconsistent with Theorem 4.1, where a single model fails under such data distribution regardless of\\nits model size. Notably, by comparing the results in Table 1 and Table 3, we can see that a single\\nnonlinear model suﬀers from overﬁtting as we increase the number of ﬁlters.\\nRouter dispatch examples.\\nWe demonstrate speciﬁc examples of router dispatch for MoE\\n(nonlinear) and MoE (linear). The examples of initial and ﬁnal router dispatch for MoE (nonlinear)\\nare shown in Table 4 and Table 5. Under the dispatch for nonlinear MoE, each expert is given\\neither no data or data that comes from one cluster only. The entropy of such dispatch is thus 0.\\nThe test accuracy of MoE trained under such a dispatch is either 100% or very close to 100%, as\\nthe expert can be easily trained on the data from one cluster only. An example of the ﬁnal dispatch\\nfor MoE (linear) is shown in Table 6, where clusters are not well separated and an expert gets data\\nfrom diﬀerent clusters. The test accuracy under such dispatch is lower (90.61%).\\nMoE during training. We further provide ﬁgures that illustrate the growth of the inner products\\nbetween expert/router weights and feature/center signals during training. Speciﬁcally, since each\\nexpert has multiple neurons, we plot the max absolute value of the inner product over the neurons of\\neach expert. In Figure 4, we demonstrate the training process of MoE (nonlinear), and in Figure 5,\\nwe demonstrate the training process of MoE (linear). The data is the same as setting 1 in Table 1,\\n14\\nExpert number\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nInitial dispatch\\n1978\\n2028\\n2018\\n1968\\n2000\\n2046\\n2000\\n1962\\nFinal dispatch\\n3987\\n4\\n3975\\n6\\n0\\n1308\\n4009\\n2711\\nCluster 1\\n0\\n0\\n3971\\n0\\n0\\n0\\n0\\n0\\nCluster 2\\n0\\n0\\n0\\n0\\n0\\n4\\n4005\\n0\\nCluster 3\\n8\\n4\\n4\\n6\\n0\\n1304\\n4\\n2711\\nCluster 4\\n3979\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nTable 5: Dispatch details of MoE (nonlinear) with test accuracy 99.95%.\\nExpert number\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nInitial dispatch\\n1969\\n2037\\n1983\\n2007\\n1949\\n1905\\n2053\\n2097\\nFinal dispatch\\n136\\n2708\\n6969\\n5311\\n27\\n87\\n4\\n758\\nCluster 1\\n0\\n630\\n1629\\n1298\\n27\\n87\\n4\\n296\\nCluster 2\\n136\\n1107\\n1884\\n651\\n0\\n0\\n0\\n231\\nCluster 3\\n0\\n594\\n1976\\n1471\\n0\\n0\\n0\\n0\\nCluster 4\\n0\\n377\\n1480\\n1891\\n0\\n0\\n0\\n231\\nTable 6: Dispatch details of MoE (linear) with test accuracy 90.61%.\\nwith α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3) and σp = 1. We can observe that, in the top left sub-\\nﬁgure of Figure 4 for MoE (nonlinear), the max inner products between expert weight and feature\\nsignals exhibit a property that each expert picks up one feature signal quickly. Similarly, as shown\\nin the bottom right sub-ﬁgure, the router picks up the corresponding center signal. Meanwhile,\\nthe nonlinear experts almost do not learn center signals and the magnitude of the inner products\\nbetween router weight and feature signals remain small. However, for MoE (linear), as shown in\\nthe top two sub-ﬁgures of Figure 5, an expert does not learn a speciﬁc feature signal, but instead\\nlearns multiple feature and center signals. Moreover, as demonstrated in the bottom sub-ﬁgures\\nof Figure 5, the magnitude of the inner products between router weight and feature signals can be\\neven larger than the inner products between router weight and center signals.\\nVeriﬁcation of Theorem 4.1. In Table 7, we provide the performances of single models with\\ndiﬀerent activation functions under setting 3, where α, γ ∈(1, 2) follow the same distribution. In\\nTable 8, we further report the performances of single models with diﬀerent activation functions\\nunder setting 1 and setting 2. Empirically, even when α and γ do not share the same distribution,\\nsingle models still fail. Note that, for Tables 7 and 8, the numbers of ﬁlters for single models are\\n128.\\nLoad balancing loss. In Table 9, we present the results of linear MoE with load balancing loss\\nand directly compare it with nonlinear MoE without load balancing loss.\\nLoad balancing loss\\nguarantees that the experts receive similar amount of data and prevents MoE from activating only\\none or few experts. However, on the data distribution that we study, load balancing loss is not the\\nkey to the success of MoE: the single experts cannot perform well on the entire data distribution\\nand must diverge to learn diﬀerent labeling functions with respect to each cluster.\\n15\\nInner product between expert weight and feature signal\\nInner product between expert weight and center signal\\nInner product between router weight and feature signal\\nInner product between router weight and center signal\\nFigure 4: Mixture of nonlinear experts. Growth of inner product between expert/router weight\\nand center/feature vector.\\nActivation\\nOptimal Accuracy (%)\\nTest Accuracy (%)\\nLinear\\n87.50%\\n74.81%\\nCubic\\n87.50%\\n72.69%\\nRelu\\n87.50%\\n73.45%\\nCelu\\n87.50%\\n76.91%\\nGelu\\n87.50%\\n74.01%\\nTanh\\n87.50%\\n74.76%\\nTable 7: Veriﬁcation of Theorem 4.1 (single expert performs poorly). Test accuracy of\\nsingle linear/nonlinear models with diﬀerent activation functions. Data is generated according to\\nDeﬁnition 3.1 with α, γ ∈(1, 2), β ∈(1, 2) and σp = 1.\\n16\\nInner product between expert weight and feature signal\\nInner product between expert weight and center signal\\nInner product between router weight and feature signal\\nInner product between router weight and center signal\\nFigure 5: Mixture of linear experts. Growth of inner product between expert/router weight\\nand center/feature vector.\\nActivation\\nSetting 1\\nSetting 2\\nLinear\\n68.71%\\n60.59%\\nCubic\\n79.48%\\n72.29%\\nRelu\\n72.28%\\n80.12%\\nCelu\\n81.75%\\n78.99%\\nGelu\\n79.04%\\n82.01%\\nTanh\\n81.72%\\n81.03%\\nTable 8:\\nSingle expert performs poorly (setting 1&2).\\nTest accuracy of single lin-\\near/nonlinear models with diﬀerent activation functions.\\nData is generated according to Deﬁ-\\nnition 3.1 with α ∈(0.5, 2), β ∈(1, 2), γ ∈(0.5, 3), σp = 1 for setting 1. And we have α ∈(0.5, 2),\\nβ ∈(1, 2), γ ∈(0.5, 3), σp = 1 for setting 2.\\n17\\nLinear MoE with Load Balancing\\nNonlinear MoE without Load Balancing\\nSetting 1\\n93.81 ± 1.02\\n99.46 ± 0.55\\nSetting 2\\n89.20 ± 2.20\\n98.09 ± 1.27\\nSetting 3\\n95.12 ± 0.58\\n99.99 ± 0.02\\nSetting 4\\n92.50 ± 1.55\\n98.92 ± 1.18\\nTable 9: Load balancing loss. We report the results for linear MoE with load balancing loss and\\ncompare them with our previous results on nonlinear MoE without load balancing loss. Over ten\\nrandom experiments, we report the average test accuracy (%) ± standard deviation. Setting 1-4\\nfollows the data distribution introduced above.\\nA.3\\nExperiments on Image Data\\nRotation\\nCrop\\nResize\\nGaussian Blur\\nFigure 6: Examples of the CIFAR-10-Rotate dataset.\\nBoth the original image and the\\nrotated image are processed in the same way, where we crop the image to (24, 24), resize to (32, 32)\\nand apply random Gaussian blur.\\nDatasets. We consider CIFAR-10 (Krizhevsky, 2009) with the 10-class classiﬁcation task, which\\ncontains 50, 000 training examples and 10, 000 testing examples. For CIFAR-10-Rotate, we design\\na binary classiﬁcation task by copying and rotating all images by 30 degree and let the model\\npredict if an image is rotated. In Figure 6, we demonstrate the positive and negative examples\\nof CIFAR-10-Rotate. Speciﬁcally, we crop the rotated images to (24, 24), and resize to (32, 32)\\nfor model architectures that are designed on image size (32, 32). And we further apply random\\nGaussian noise to all images to avoid the models taking advantage of image resolutions.\\nModels. For the simple CNN model, we consider CNN with 2 convolutional layers, both with\\nkernel size 3 and ReLU activation followed by max pooling with size 2 and a fully connected layer.\\nThe number of ﬁlters of each convolutional layer is respectively 64, 128.\\nCIFAR-10 Setup. For real-data experiments on CIFAR-10, we apply the commonly used trans-\\nforms on CIFAR-10 before each forward pass: random horizontal ﬂips and random crops (padding\\nthe images on all sides with 4 pixels and randomly cropping to (32, 32)). And as conventionally,\\n18\\nwe normalize the data by channel. We train the single CNN model with SGD of learning rate 0.01,\\nmomentum 0.9 and weight decay 5e-4. And we train single MobileNetV2 and single ResNet18 with\\nSGD of learning rate 0.1, momentum 0.9 and weight decay 5e-4 to achieve the best performances.\\nWe train MoEs according to Algorithm 1, with normalized gradient descent on the experts and\\nSGD on the gating networks. Speciﬁcally, for MoE (ResNet18) and MoE (MobileNetV2), we use\\nnormalized gradient descent of learning rate 0.1 and SGD of learning rate 1e-4, both with mo-\\nmentum 0.9 and weight decay of 5e-4. For MoE (CNN), we use normalized gradient descent of\\nlearning rate 0.01 and SGD of learning rate 1e-4, both with momentum 0.9 and weight decay of\\n5e-4. We consider top-1 gating with noise and load balancing loss for MoE on both datasets, where\\nthe multiplicative coeﬃcient of load balancing loss is set at 1e-3. All models are trained for 200\\nepochs to achieve convergence.\\nCIFAR-10-Rotate Setup.\\nFor experiments on CIFAR10-Rotate, the data is normalized by\\nchannel as the same as in CIFAR-10 before each forward pass. We train the single CNN, single\\nMobileNetV2 and single ResNet18 by SGD with learning rate 0.01, momentum 0.9 and weight\\ndecay 5e-4 to achieve the best performances. And we train MoEs by Algorithm 1 with normalized\\ngradient descent learning rate 0.01 on the experts and with SGD of learning rate 1e-4 on the gating\\nnetworks, both with momentum 0.9 and weight decay of 5e-4. We consider top-1 gating with noise\\nand load balancing loss for MoE on both datasets, where the multiplicative coeﬃcient for load\\nbalancing loss is set at 1e-3. All models are trained for 50 epochs to achieve convergence.\\nVisualization. In Figure 7, we visualize the latent embedding learned by MoEs (ResNet18) for\\nthe 10-class classiﬁcation task in CIFAR-10 as well as the binary classiﬁcation task in CIFAR-10-\\nRotate. We visualize the data with the same label y to see if cluster structures exist within each\\nclass. For CIFAR-10, we choose y = 1 (”car”), and plot the latent embedding of data with y = 1\\nusing t-SNE on the left subﬁgure, which does not show an salient cluster structure. For CIFAR-\\n10-Rotate, we choose y = 1 (”rotated”) and visualize the data with y = 1 in the middle subﬁgure.\\nHere, we can observe a clear clustering structure even though the class signal is not provided during\\ntraining. We take a step further to investigate what is in each cluster in the right subﬁgure. We\\ncan observe that most of the examples in the “frog” class fall into one cluster, while examples of\\n“ship” class mostly fall into the other cluster.\\ny=1 (car)\\ny=1 (rotated)\\n(frog, ship)\\nFigure 7: Visualization of the latent embedding on CIFAR-10 and CIFAR-10-Rotate with ﬁxed\\nlabel y. The left ﬁgure denotes the visualization of CIFAR-10 when label y is ﬁxed to be 1 (car).\\nThe central ﬁgure represents the visualization of CIFAR-10-Rotate when label y is ﬁxed to be 1\\n(rotated). On the right ﬁgure, red denotes that the data is from the ship class, and blue denotes\\nthat the data is from the frog class.\\n19\\nSingle\\nMoE\\nAccuracy\\n74.13%\\n76.22%\\nTable 10: The test accuracy of the single classiﬁer vs. MoE classiﬁer.\\nExpert 1\\nExpert 2\\nExpert 3\\nExpert 4\\nEnglish\\n1, 374\\n3, 745\\n2, 999\\n31, 882\\nFrench\\n23, 470\\n3, 335\\n13, 182\\n13\\nRussian\\n833\\n9, 405\\n7, 723\\n39\\nTable 11: The ﬁnal router dispatch details with regard to the linguistic source of the test data.\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 8: The distribution of text embedding of the multilingual sentiment analysis dataset. The\\nembedding is generated by the pre-trained BERT multilingual base model and visualized on 2d\\nspace using t-SNE. Each color denotes a linguistic source, including English, French, and Russian.\\nA.4\\nExperiments on Language Data\\nHere we provide a simple example of how MoE would work for multilingual tasks.\\nWe gather\\nmultilingual sentiment analysis data from the source of English (Sentiment140 (Go et al., 2009))\\nwhich is randomly sub-sampled to 200, 000 examples, Russian (RuReviews (Smetanin and Komarov,\\n2019)) which contains 90, 000 examples, and French (Blard, 2020) which contains 200, 000 examples.\\nWe randomly split the dataset into 80% training data and 20% test data. We use a pre-trained\\nBERT multilingual base model (Devlin et al., 2018) to generate text embedding for each text\\nand train 1-layer neural network with cubic activation as the single model. For MoE, we still let\\nM = 4 with each expert sharing the same architecture as the single model. In Figure 8, we show\\nthe visualization of the text embeddings in the 2d space via t-SNE, where each color denotes a\\nlinguistic source, with · representing a positive example and × representing a negative example.\\nData from diﬀerent linguistic sources naturally form diﬀerent clusters. And within each cluster,\\npositive and negative data exist.\\nIn Table 10, we demonstrate the test accuracy of a single classiﬁer and MoE on the multilingual\\nsentiment analysis dataset. And in Table 11, we show the ﬁnal router dispatch details of MoE to\\n20\\neach expert with regard to the linguistic source of the text. Notably, MoE learned to distribute\\nexamples largely according to the original language.\\nB\\nProof of Theorem 4.1\\nBecause we are using CNNs as experts, diﬀerent ordering of the patches won’t aﬀect the value of\\nF(x). So for (x, y) drawn from D in Deﬁnition 3.1, we can assume that the ﬁrst patch x(1) is feature\\nsignal, the second patch x(2) is cluster-center signal, the third patch x(3) is feature noise. The other\\npatches x(p), p ≥4 are random noises. Therefore, we can rewrite x = [αyvk, βck, γϵvk′, ξ], where\\nξ = [ξ4, . . . , ξP ] is a Gaussian matrix of size Rd×(P−3).\\nProof of Theorem 4.1. Conditioned on the event that y = −ϵ, points ([αyvk, βck, −γyvk′, ξ], y),\\n\\x00[−αyvk, βck, γyvk′, ξ], −y\\n\\x01\\n,\\n\\x00[γyvk′, βck′, −αyvk, ξ], y\\n\\x01\\n,\\n\\x00[−γyvk′, βck′, αyvk, ξ], −y\\n\\x01\\nfollow the\\nsame distribution because γ and α follow the same distribution, and y and −y follow the same\\ndistribution. Therefore, we have\\n4P\\n\\x00yF(x) ≤0|ϵ = −y\\n\\x01\\n= E\\n\\x14\\n1(yF([αyvk, βck, −γyvk′, ξ]) ≤0)\\n|\\n{z\\n}\\nI1\\n+ 1(−yF([−αyvk, βck, γyvk′, ξ]) ≤0)\\n|\\n{z\\n}\\nI2\\n+ 1(yF([γyvk′, βck′, −αyvk, ξ]) ≤0)\\n|\\n{z\\n}\\nI3\\n+ 1(−yF([−γyvk′, βck′, αyvk, ξ]) ≤0)\\n\\x15\\n|\\n{z\\n}\\nI4\\n.\\nIt is easy to verify the following fact\\n\\x10\\nyF([αyvk, βck, −γyvk′, ξ])\\n\\x11\\n+\\n\\x10\\n−yF([−αyvk, βck, γyvk′, ξ])\\n\\x11\\n+\\n\\x10\\nyF([γyvk′, βck′, −αyvk, ξ])\\n\\x11\\n+\\n\\x10\\n−yF([−γyvk′, βck′, αyvk, ξ])\\n\\x11\\n=\\n\\x12\\nyf(αyvk) + yf(βck) + yf(−γyvk′) +\\nP\\nX\\np=4\\nyf(ξp)\\n\\x13\\n+\\n\\x12\\n−yf(−αyvk) −yf(βck) −yf(γyvk′) −\\nP\\nX\\np=4\\nyf(ξp)\\n\\x13\\n+\\n\\x12\\nyf(γyvk′) + yf(βck′) + yf(−αyvk) +\\nP\\nX\\np=4\\nyf(ξp)\\n\\x13\\n+\\n\\x12\\n−yf(−γyvk′) −yf(βck′) −yf(αyvk) −\\nP\\nX\\np=4\\nyf(ξp)\\n\\x13\\n= 0.\\nBy pigeonhole principle, at least one of I1, I2, I3, I4 is non-zero. This further implies that 4P\\n\\x00yF(x) ≤\\n0|ϵ = −y\\n\\x01\\n≥1. Applying P(ϵ = −y) = 1/2, we have that\\nP\\n\\x00yF(x) ≤0\\n\\x01\\n≥P\\n\\x00yF(x) ≤0)|ϵ = −y\\n\\x01\\nP(ϵ = −y) ≥1/8,\\n21\\nwhich completes the proof.\\nC\\nSmoothed Router\\nIn this section, we will show that the noise term provides a smooth transition between diﬀerent\\nrouting behavior. All the results in this section is independent from our NN structure and its\\ninitialization. We ﬁrst present a general version of Lemma 5.1 with its proof.\\nLemma C.1 (Extension of Lemma 5.1). Let h, b\\nh ∈RM to be the output of the gating network and\\n{rm}M\\nm=1 to be the noise independently drawn from Dr. Denote p, b\\np ∈RM to be the probability\\nthat experts get routed, i.e., pm = P(argmaxm′∈[M]{hm′ + rm′} = m), b\\npm = P(argmaxm′∈[M]{b\\nhm′ +\\nrm′} = m). Suppose the probability density function of Dr is bounded by κ, Then we have that\\n∥p −b\\np∥∞≤(κM2) · ∥h −b\\nh∥∞.\\nProof. Given random variable {rm}M\\nm=1, let us ﬁrst consider the event that argmaxm{hm + rm} ̸=\\nargmaxm{b\\nhm + rm}. Let m1 = argmaxm{hm + rm} and m2 = argmaxm{b\\nhm + rm}, then we have\\nthat\\nhm1 + rm1 ≥hm2 + rm2,b\\nhm2 + rm2 ≥b\\nhm1 + rm1,\\nwhich implies that\\nb\\nhm2 −b\\nhm1 ≥rm1 −rm2 ≥hm2 −hm1.\\n(C.1)\\nDeﬁne C(m1, m2) = (b\\nhm2 −b\\nhm1 + hm2 −hm1)/2, then (C.1) implies that\\n|rm1 −rm2 −C(m1, m2)| ≤|b\\nhm2 −b\\nhm1 −hm2 + hm1|/2 ≤∥b\\nh −h∥∞.\\n(C.2)\\nTherefore, we have that,\\nP(argmax\\nm\\n{hm + rm} ̸= argmax\\nm\\n{b\\nhm + rm})\\n≤P(∃m1 ̸= m2 ∈[M], s.t. |rm1 −rm2 −C(m1, m2)| ≤∥b\\nh −h∥∞)\\n≤\\nX\\nm1<m2\\nP\\n\\x00|rm1 −rm2 −C(m1, m2)| ≤∥b\\nh −h∥∞\\n\\x01\\n=\\nX\\nm1<m2\\nE\\nh\\nP\\n\\x00rm2 + C(m1, m2) −∥b\\nh −h∥∞≤rm1 ≤rm2 + C(m1, m2) + ∥b\\nh −h∥∞\\n\\x01\\x0c\\n\\x0c\\n\\x0crm2\\ni\\n≤(κM2) · ∥b\\nh −h∥∞,\\nwhere the ﬁrst inequality is by (C.2), the second inequality is by union bound and the last inequality\\nis due to the fact that the probability density function of rm1 is bounded by κ. Then we have that\\nfor i ∈[M],\\n|pi −b\\npi| ≤\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0cE\\n\\x14\\n1\\n\\x00argmax\\nm\\n{b\\nhm + rm} = i\\n\\x01\\n−1\\n\\x00argmax\\nm\\n{hm + rm} = i\\n\\x01\\x15\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n≤E\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c 1\\n\\x00argmax\\nm\\n{b\\nhm + rm} = i\\n\\x01\\n−1\\n\\x00argmax\\nm\\n{hm + rm} = i\\n\\x01\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n22\\n≤P\\n\\x00argmax\\nm\\n{b\\nhm + rm} ̸= argmax\\nm\\n{hm + rm}\\n\\x01\\n≤(κM2) · ∥b\\nh −h∥∞,\\nwhich completes the proof.\\nRemark C.2. A widely used choice of Dr in Lemma C.1 is uniform noise Unif[a, b], in which\\ncase the density function can be upper bounded by 1/(b −a). Another widely used choice of Dr is\\nGaussian noise N(0, σ2\\nr), in which case the density function can be upper bounded by 1/(σr\\n√\\n2π).\\nIncrease the range of uniform noise or increase the variance of the Gaussian noise will result in\\na smaller density function upper bound and a smoother behavior of routing. In our paper, we\\nconsider unif[0,1] for simplicity, in which case the the density function can be upper bounded by 1\\n(κ = 1).\\nThe following Lemma shows that when two gate network outputs are close, the router will\\ndistribute the examples to those corresponding experts with nearly the same probability.\\nLemma C.3. Let h ∈RM be the output of the gating network and {rm}M\\nm=1 be the noise in-\\ndependently drawn from Unif[0,1].\\nDenote the probability that experts get routed by p, i.e.,\\npm = P(argmaxm′{hm′ + rm′} = m). Then we have that\\n|pm −pm′| ≤M2|hm −hm′|.\\nProof. Construct b\\nh as copy of h and permute its m, m′-th element. Denote the corresponding\\nprobability vector as b\\np. Then it is obviously that |pm−pm′| = ∥p−b\\np∥∞and |hm−hm′| = ∥b\\nh−h∥∞.\\nApplying Lemma 5.1 completes the proof.\\nThe following lemma shows that the router won’t route examples to the experts with small\\ngating network outputs, which saves computation and improves the performance.\\nLemma C.4. Suppose the noise {rm}M\\nm=1 are independently drawn from Unif[0,1] and hm(x; Θ) ≤\\nmaxm′ hm′(x; Θ) −1, example x will not get routed to expert m.\\nProof. Because hm(x; Θ) ≤maxm′ hm′(x; Θ) −1 implies that for any Uniform noise {rm′}m′∈[M]\\nwe have that\\nhm(x; Θ) + rm ≤max\\nm′ hm′(x; Θ) ≤max\\nm′ {hm′(x; Θ) + rm′},\\nwhere the ﬁrst inequality is by rm ≤1, the second inequality is by rm′ ≥0, ∀m′ ∈[M].\\nD\\nInitialization of the Model\\nBefore we look into the detailed proof of Theorem 4.2, let us ﬁrst discuss some basic properties of\\nthe data distribution and our MoE model. For simplicity of notation, we simplify (xi, yi) ∈Ωk as\\ni ∈Ωk.\\nTraining Data Set Property. Because we are using CNNs as experts, diﬀerent ordering of the\\npatches won’t aﬀect the value of F(x). So for (x, y) drawn from D in Deﬁnition 3.1, we can assume\\nthat the ﬁrst patch x(1) is feature signal, the second patch x(2) is cluster-center signal, the third\\npatch x(3) is feature noise. The other patches x(p), p ≥4 are random noises. Therefore, we can\\n23\\nrewrite x = [αyvk, βck, γϵvk′, ξ], where ξ = [ξ4, . . . , ξP ] is a Gaussian matrix of size Rd×(P−3).\\nAccording to the type of the feature noise, we further divide Ωk into Ωk = ∪Ωk,k′ based on the\\nfeature noise, i.e. x ∈Ωk,k′ if x = [αyvk, βck, γϵvk′, ξ]. To better characterize the router training,\\nwe need to break down Ωk,k′ into Ω+\\nk,k′ and Ω−\\nk,k′. Denote by Ω+\\nk,k′ the set that {yi = ϵi|i ∈Ωk,k′},\\nby Ω−\\nk,k′ the set that {yi = −ϵi|i ∈Ωk,k′}.\\nLemma D.1. With probability at least 1 −δ, the following properties hold for all k ∈[K],\\nX\\ni∈Ωk\\nyiβ3\\ni = e\\nO(√n),\\nX\\ni∈Ωk\\nα3\\ni = E[α3] · n/K + e\\nO(√n),\\nX\\ni∈Ωk\\nyiϵiγ3\\ni = e\\nO(√n),\\n(D.1)\\nX\\ni∈Ω+\\nk,k′\\nyiαi = e\\nO(√n),\\nX\\ni∈Ω−\\nk,k′\\nyiαi = e\\nO(√n),\\nX\\ni∈Ω+\\nk,k′\\nϵiγi = e\\nO(√n),\\n(D.2)\\nX\\ni∈Ω−\\nk,k′\\nϵiγi = e\\nO(√n),\\nX\\ni∈Ωk\\nβi = E[β] · n/K + e\\nO(√n).\\n(D.3)\\nProof. Fix k ∈[K], by Hoeﬀding’s inequality we have that with probability at least 1 −δ/8K,\\nX\\ni∈Ωk\\nyiβ3\\ni =\\nn\\nX\\ni=1\\nyiβ3\\ni 1\\n\\x00(xi, yi) ∈Ωk\\n\\x01\\n= e\\nO(√n),\\nwhere the last equality is by the fact that the expectation of yβ3 1\\n\\x00(x, y) ∈Ωk\\n\\x01\\nis zero. Fix k ∈[K],\\nby Hoeﬀding’s inequality we have that with probability at least 1 −δ/8K,\\nX\\ni∈Ωk\\nα3\\ni =\\nn\\nX\\ni=1\\nα3\\ni 1\\n\\x00(xi, yi) ∈Ωk\\n\\x01\\n= nE[α3]\\nK\\n+ e\\nO(√n),\\nwhere the last equality is by the fact that the expectation of α3 1\\n\\x00(x, y) ∈Ωk\\n\\x01\\nis E[α3]/K. Fix\\nk ∈[K], by Hoeﬀding’s inequality we have that with probability at least 1 −δ/8K,\\nX\\ni∈Ωk\\nyiϵiγ3\\ni =\\nn\\nX\\ni=1\\nyiϵiγ3\\ni 1\\n\\x00(xi, yi) ∈Ωk\\n\\x01\\n= e\\nO(√n),\\nwhere the last equality is by the fact that the expectation of yϵγ3 1\\n\\x00(x, y) ∈Ωk\\n\\x01\\nis zero. Now we\\nhave proved the bounds in (D.1). We can get other bounds in (D.2) and (D.3) similarly. Applying\\nunion bound over [K] completes the proof.\\nLemma D.2. Suppose that d = Ω(log(4nP/δ)), with probability at least 1 −δ, the following\\ninequalities hold for all i ∈[n], k ∈[K], p ≥4,\\n• ∥ξi,p∥2 = O(1),\\n• ⟨vk, ξi,p⟩≤e\\nO(d−1/2), ⟨ck, ξi,p⟩≤e\\nO(d−1/2), ⟨ξi,p, ξi′,p′⟩≤e\\nO(d−1/2), ∀(i′, p′) ̸= (i, p).\\n24\\nProof of Lemma D.2. By Bernstein’s inequality, with probability at least 1 −δ/(2nP) we have\\n\\x0c\\n\\x0c∥ξi,p∥2\\n2 −σ2\\np\\n\\x0c\\n\\x0c ≤O(σ2\\np\\np\\nd−1 log(4nP/δ)).\\nTherefore, as long as d = Ω(log(4nP/δ)), we have ∥ξi,p∥2\\n2 ≤2. Moreover, clearly ⟨ξi,p, ξi′,p′⟩has\\nmean zero, ∀(i, p) ̸= (i′, p′). Then by Bernstein’s inequality, with probability at least 1−δ/(6n2P 2)\\nwe have\\n|⟨ξi,p, ξi′,p′⟩| ≤2σ2\\np\\np\\nd−1 log(12n2P 2/δ).\\nSimilarly, ⟨vk, ξi,p⟩and ⟨ck, ξi,p⟩have mean zero. Then by Bernstein’s inequality, with probability\\nat least 1 −δ/(3nPK) we have\\n|⟨ξi,p, vk⟩| ≤2σp\\np\\nd−1 log(6nPK/δ), |⟨ξi,p, ck⟩| ≤2σp\\np\\nd−1 log(6nPK/δ).\\nApplying a union bound completes the proof.\\nMoE Initialization Property.\\nWe divide the experts into K sets based on the initialization.\\nDeﬁnition D.3. Fix expert m ∈[M], denote (k∗\\nm, j∗\\nm) = argmaxj,k⟨vk, w(0)\\nm,j⟩. Fix cluster k ∈[K],\\ndenote the profession experts set as Mk = {m|k∗\\nm = k}.\\nLemma D.4. For M ≥Θ(K log(K/δ)), J ≥Θ(log(M/δ)), the following inequalities hold with\\nprobability at least 1 −δ.\\n• max(j,k)̸=(j∗\\nm,k∗\\nm)⟨w(0)\\nm,j, vk⟩≤\\n\\x001 −δ/\\n\\x003MJ2K2)\\n\\x01\\n⟨w(0)\\nm,j∗\\nm, vk∗\\nm⟩for all m ∈[M]\\n• ⟨w(0)\\nm,j∗\\nm, vk∗\\nm⟩≥0.01σ0 for all m ∈[M].\\n• |Mk| ≥1 for all k ∈[K].\\nProof. Recall that wm,j ∼N(0, σ2\\n0Id). Notice that signals v1, . . . , vK are orthogonal. Given ﬁxed\\nm ∈[M], we have that {⟨w(0)\\nm,j, vk⟩|j ∈[J], k ∈[K]} are independent and individually draw from\\nN(0, σ2\\n0) we have that\\nP(⟨w(0)\\nm,j, vk⟩< 0.01σ0) < 0.9.\\nTherefore, we have that\\nP(max\\nj,k ⟨w(0)\\nm,j, vk⟩< 0.01σ0) < 0.9KJ.\\nTherefore, as long as J ≥Θ(K−1 log(M/δ)), ﬁx m ∈[M] we can guarantee that with probability\\nat least 1 −δ/(3M),\\nmax\\nj,k ⟨w(0)\\nm,j, vk⟩> 0.01σ0.\\nTake G = δ/(3MJ2K2), by Lemma F.1 we have that with probability at least 1 −δ/(3M),\\nmax\\n(j,k)̸=(j∗\\nm,k∗\\nm)⟨w(0)\\nm,j, vk⟩≤(1 −G)⟨w(0)\\nm,j∗\\nm, vk∗\\nm⟩.\\n25\\nBy the symmetric property, we have that for all k ∈[K], m ∈[M],\\nP(k = k∗\\nm) = K−1.\\nTherefore, the probability that |Mk| at least include one element is as follows,\\nP(|Mk| ≥1) ≥1 −(1 −K−1)M.\\nBy union bound we get that\\nP(|Mk| ≥1, ∀k) ≥1 −K(1 −K−1)M ≥1 −K exp(−M/K) ≥1 −δ/3,\\nwhere the last inequality is by condition M ≥K log(3K/δ). Therefore, with probability at least\\n1 −δ/3, |Mk| ≥1, ∀k.\\nApplying Union bound, we have that with probability at least 1 −δ,\\nmax\\n(j,k)̸=(j∗\\nm,k∗\\nm)⟨w(0)\\nm,j, vk⟩≤\\n\\x001 −δ/\\n\\x003MJ2K2)\\n\\x01\\n⟨w(0)\\nm,j∗\\nm, vk∗\\nm⟩,\\n⟨w(0)\\nm,j∗\\nm, vk∗\\nm⟩≥0.01σ0, ∀m ∈[M],\\n|Mk| ≥1, ∀k ∈[K].\\nLemma D.5. Suppose the conclusions in Lemma D.2 hold, then with probability at least 1−δ we\\nhave that |⟨w(0)\\nm,j, v⟩| ≤e\\nO(σ0) for all v ∈{vk}k∈[K] ∪{ck}k∈[K] ∪{ξi,p}i∈[n],p∈[P−3], m ∈[M], j ∈[J].\\nProof. Fix v ∈{vk}k∈[K]∪{ck}k∈[K]∪{ξi,p}i∈[n],p∈[P−3], m ∈[M], j ∈[J], we have that ⟨w(0)\\nm,j, v⟩∼\\nN(0, σ2\\n0∥v∥2\\n2) and ∥v∥2 = O(1). Therefore, with probability at least 1 −δ/(nPMJ) we have that\\n|⟨w(0)\\nm,j, v⟩| ≤e\\nO(σ0). Applying union bound completes the proof.\\nE\\nProof of Theorem 4.2\\nIn this section we always assume that the conditions in Theorem 4.2 holds. It is easy to show that\\nall the conclusions in this section D hold with probability at least 1 −O(1/ log d). The results in\\nthis section hold when all the conclusions in Section D hold. For simplicity of notation, we simplify\\n(xi, yi) ∈Ωk,k′ as i ∈Ωk,k′, and ℓ′(yiπmi,t(xi; Θ(t))fmi,t(xi; W(t))) as ℓ′\\ni,t.\\nRecall that at iteration t, data xi is routed to the expert mi,t. Here mi,t should be interpreted\\nas a random variable. The gradient of MoE model at iteration t can thus be computed as follows\\n∇θmL(t) = 1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπmi,t(xi; Θ(t))(1 −πmi,t(xi; Θ(t)))yifmi,t(xi; W(t))x(p)\\ni\\n−1\\nn\\nX\\ni,p\\n1(mi,t ̸= m)ℓ′\\ni,tπmi,t(xi; Θ(t))πm(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n= 1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n26\\n−1\\nn\\nX\\ni,p\\nℓ′\\ni,tπmi,t(xi; Θ(t))πm(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni ,\\n(E.1)\\n∇wm,jL(t) = 1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)x(p)\\ni .\\n(E.2)\\nFollowing lemma shows implicit regularity in the gating network training.\\nLemma E.1. For all t ≥0, we have that PM\\nm=1 ∇θmL(t) = 0 and thus P\\nm θ(t)\\nm = P\\nm θ(0)\\nm . In\\nparticular, when Θ is zero initialized, then P\\nm θ(t)\\nm = 0\\nProof. We ﬁrst write out the gradient of θm for all m ∈[M],\\n∇θmL(t) = 1\\nn\\nX\\ni∈[n],p∈[P]\\n1(mi,t = m)ℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n−1\\nn\\nX\\ni∈[n],p∈[P]\\nℓ′\\ni,tπmi,t(xi; Θ(t))πm(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni .\\nTake summation from m = 1 to m = M, then we have\\nM\\nX\\nm=1\\n∇θmL(t) = 1\\nn\\nX\\ni∈[n],p∈[P]\\nℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n−1\\nn\\nX\\ni∈[n],p∈[P]\\nℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi, W(t))x(p)\\ni\\n= 0.\\nNotice that the gradient at iteration t in (E.1) and (E.2) is depend on the random variable mi,t,\\nthe following lemma shows that it can be approximated by its expectation.\\nLemma E.2. With probability at least 1 −1/d, for all the vector v ∈{vk}k∈[K] ∪{ck}k∈[K],\\nm ∈[M], j ∈[J], we have the following equations hold |⟨∇θmL(t), v⟩−E[⟨∇θmL(t), v⟩]| =\\ne\\nO(n−1/2(σ0 + ηt)3), |⟨∇wm,jL(t), v⟩−E[⟨∇wm,jL(t), v⟩]| = e\\nO(n−1/2(σ0 + ηt)2), for all t ≤d100.\\nHere E[⟨∇wm,jL(t), v⟩] and E[⟨∇θmL(t), v⟩] can be computed as follows,\\nE[⟨∇θmL(t), v⟩] = 1\\nn\\nX\\ni,p\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yifm(xi; W(t))⟨x(p)\\ni , v⟩\\n−1\\nn\\nX\\ni,p,m′\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))yifm′(xi; W(t))⟨x(p)\\ni , v⟩\\nE[⟨∇wm,jL(t), v⟩] = 1\\nn\\nX\\ni,p\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)⟨x(p)\\ni , v⟩.\\nProof. Because we are using normalized gradient descent, ∥w(t)\\nm,j −w(0)\\nm,j∥2 ≤O(ηt) and thus by\\n27\\nLemma D.5 we have |⟨w(t)\\nm,j, x(p)\\ni ⟩| ≤e\\nO(σ0 + ηt). Therefore,\\n⟨∇wm,jL(t), v⟩= 1\\nn\\nX\\ni\\nX\\np\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)⟨x(p)\\ni , v⟩\\n|\\n{z\\n}\\nAi\\n,\\nwhere Ai are independent random variables with |Ai| ≤e\\nO\\n\\x00(σ0 + ηt)2\\x01\\n.\\nApplying Hoeﬀding’s\\ninequality gives that with probability at least 1 −1/(4d101MJK) we have that |⟨∇wm,jL(t), v⟩−\\nE[⟨∇wm,jL(t), v⟩]| = e\\nO(n−1/2(σ0 +ηt)2). Applying union bound gives that with probability at least\\n1 −1/(2d), |⟨∇wm,jL(t), v⟩−E[⟨∇wm,jL(t), v⟩]| = e\\nO(n−1/2(σ0 + ηt)2), ∀m ∈[M], j ∈[J], t ≤d100.\\nSimilarly, we can prove |⟨∇θmL(t), v⟩−E[⟨∇θmL(t), v⟩]| = e\\nO(n−1/2(σ0 + ηt)3).\\nE.1\\nExploration Stage\\nDenote T1 = ⌊η−1σ0.5\\n0 ⌋. The ﬁrst stage ends when t = T1. During the ﬁrst stage training, we can\\nprove that the neural network parameter maintains the following property.\\nLemma E.3. For all t ≤T1, we have the following properties hold,\\n• ⟨w(t)\\nm,j, vk⟩= O(σ0.5\\n0 ), ⟨w(t)\\nm,j, ck⟩= O(σ0.5\\n0 ), ⟨w(t)\\nm,j, ξi,p⟩= e\\nO(σ0.5\\n0 ),\\n• fm(xi; W(t)) = e\\nO(σ1.5\\n0 ),\\n• |ℓ′\\ni,t −1/2| ≤e\\nO(σ1.5\\n0 ),\\n• ∥θ(t)\\nm ∥2 ≤e\\nO(σ1.5\\n0 ),\\n• ∥h(xi; Θ(t))∥∞= e\\nO(σ1.5\\n0 ), πm(xi; Θ(t)) = M−1 + e\\nO(σ1.5\\n0 ),\\nfor all m ∈[M], k ∈[k], i ∈[n], p ≥4.\\nProof. The ﬁrst property is obvious since ∥w(t)\\nm,j −w(0)\\nm,j∥2 ≤O(ηT1) = O(σ0.5\\n0 ) and thus\\n|fm(xi; W(t))| ≤\\nX\\np∈[P]\\nX\\nj∈[J]\\n|σ(⟨w(t)\\nm,j, x(p)\\ni ⟩)| = e\\nO(σ1.5\\n0 ).\\nThen we show that the loss derivative is close to 1/2 during this stage.\\nLet s = yiπmi,t(xi; Θ(t))fmi,t(xi, W(t)), then we have that |s| = e\\nO(σ1.5\\n0 ) and\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0cℓ′\\ni,t −1\\n2\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c =\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n1\\nes + 1 −1/2\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n(i)\\n≤|s| = e\\nO(σ1.5\\n0 ),\\nwhere (i) can be proved by considering |s| ≤1 and |s| > 1.\\nNow we prove the fourth bullet in Lemma E.3. Because |fm| = e\\nO(σ1.5\\n0 ), we can upper bound\\nthe gradient of the gating network by\\n∥∇θmL(t)∥2 =\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n28\\n−1\\nn\\nX\\ni,p\\nℓ′\\ni,tπmi,t(xi; Θ(t))πm(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n.\\n= e\\nO(σ1.5\\n0 ),\\nwhere the last inequality is due to |ℓ′\\ni,t| ≤1, πm, πmi,t ∈[0, 1] and ∥x(p)\\ni ∥2 = O(1). This further\\nimplies that\\n∥θ(t)\\nm ∥2 = ∥θ(t)\\nm −θ(0)\\nm ∥2 ≤e\\nO(σ1.5\\n0 tηr) = e\\nO(σ1.5\\n0 ),\\nwhere the last inequality is by ηr = Θ(M2)η.\\nThe proof of ∥h(xi; Θ(t))∥∞≤O(σ1.5\\n0 ) and\\nπm(xi; Θ(t)) = M−1 + O(σ1.5\\n0 ) are straight forward given ∥θ(t)\\nm ∥2 = e\\nO(σ1.5\\n0 ).\\nWe will ﬁrst investigate the property of the router.\\nLemma E.4. maxm∈[M] |P(mi,t = m) −1/M| = e\\nO(σ1.5\\n0 ) for all t ≤T1, i ∈[n] and m ∈[M].\\nProof. By Lemma E.3 we have that ∥h(xi; Θ(t))∥∞≤e\\nO(σ1.5\\n0 ). Lemma 5.1 further implies that\\nmax\\nm∈[M] |P(mi,t = m) −1/M| = e\\nO(σ1.5\\n0 ).\\nLemma E.5. We have following gradient update rules hold for the experts,\\n⟨∇wm,jL(t), vk⟩= −E[α3] + e\\nO(d−0.005)\\n2KM2\\nσ′(⟨w(t)\\nm,j, vk⟩) + e\\nO(σ2.5\\n0 ),\\n⟨∇wm,jL(t), ck⟩= e\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ck⟩) + e\\nO(σ2.5\\n0 ),\\n⟨∇wm,jL(t), ξi,p⟩= e\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ξi,p⟩) + e\\nO(σ2.5\\n0 )\\nfor all t ≤T1, j ∈[J], k ∈[K], m ∈[M], p ≥4. Besides, we have the following gradient norm upper\\nbound holds\\n∥∇wm,jL(t)∥2 ≤\\nX\\nk∈[K]\\nE[α3] + e\\nO(d−0.005)\\n2KM2\\nσ′(⟨w(t)\\nm,j, vk⟩) +\\nX\\nk∈[K]\\ne\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ck⟩)\\n+\\nX\\ni∈[n],p≥4\\ne\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ξi,p⟩) + e\\nO(σ2.5\\n0 )\\nfor all t ≤T1, j ∈[J], m ∈[M].\\nProof. The experts gradient can be computed as follows,\\n∇wm,jL(t) = 1\\nn\\nX\\ni∈[n],p∈[P]\\n1(mi,t = m)ℓ′\\ni,tfm(xi; W(t))πm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)x(p)\\ni .\\nWe ﬁrst compute the inner product ⟨∇wm,jL(t), ck⟩. By Lemma E.2, we have that |⟨∇wm,jL(t), ck⟩−\\n29\\nE[⟨∇wm,jL(t), ck⟩]| = e\\nO(n−1/2σ0) ≤e\\nO(σ2.5\\n0 ).\\nE[⟨∇wm,jL(t), ck⟩] = −1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, ck⟩)yiβ3\\ni ∥ck∥2\\n2\\n−1\\nn\\nX\\ni∈[n],p≥4\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, ξi,p⟩)yi⟨ck, ξi,p⟩\\n=\\n\\x14\\n−\\n1\\n2nM\\nX\\ni∈Ωk\\nyiβ3\\ni P(mi,t = m) + e\\nO(σ1.5\\n0 )\\n\\x15\\nσ′(⟨w(t)\\nm,j, ck⟩) + e\\nO(σ2.5\\n0 )\\n= e\\nO(n−1/2 + σ1.5\\n0 )σ′(⟨w(t)\\nm,j, ck⟩) + e\\nO(σ2.5\\n0 )\\n= e\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ck⟩) + e\\nO(σ2.5\\n0 )\\nwhere the second equality is due to Lemma E.3 and D.2, the third equality is due to Lemma E.4,\\nthe last equality is by the choice of n and σ0. Next we compute the inner product ⟨∇wm,jL, vk⟩.\\nBy Lemma E.2, we have that |⟨∇wm,jL(t), vk⟩−E[⟨∇wm,jL(t), vk⟩]| = e\\nO(n−1/2σ0) ≤e\\nO(σ2.5\\n0 ).\\nE[⟨∇wm,jL(t), vk⟩] = −1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, vk⟩)α3\\ni ∥vk∥2\\n2\\n−1\\nn\\nX\\nk′̸=k\\nX\\ni∈Ωk′,k\\nP(mi,t = m)ℓ′\\ni,tπm(xi; θ(t))σ′(⟨w(t)\\nm,j, vk⟩)γ3\\ni yiϵi∥vk∥2\\n2\\n−1\\nn\\nX\\ni∈[n],p≥4\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, ξi,p⟩)yi⟨vk, ξi,p⟩\\n=\\n\\x14\\n−\\n1\\n2nM\\nX\\ni∈Ωk\\nP(mi,t = m)α3\\ni −\\n1\\n2nM\\nX\\ni∈Ωk′,k\\nP(mi,t = m)γ3\\ni yiϵi + O(σ1.5\\n0 )\\n\\x15\\n·\\nσ′(⟨w(t)\\nm,j, ck⟩) + e\\nO(σ2.5\\n0 )\\n=\\n\\x00E[α3] + e\\nO(n−1/2 + σ1.5\\n0 )\\n\\x01\\nσ′(⟨w(t)\\nm,j, vk⟩) + e\\nO(σ2.5\\n0 )\\n=\\n\\x12 E[α3]\\n2KM2 + e\\nO(d−0.005)\\n\\x13\\nσ′(⟨w(t)\\nm,j, vk⟩) + e\\nO(σ2.5\\n0 )\\nwhere the second equality is due to Lemma E.3 and D.2, the third equality is due to Lemma E.4,\\nthe last equality is by the choice of n and σ0. Finally we compute the inner product ⟨∇wm,jL, ξi,p⟩\\nas follows\\n⟨∇wm,jL(t), ξi,p⟩= −1\\nn 1(mi,t = m)ℓ′\\ni,tπm(xi; θ(t))σ′(⟨w(t)\\nm,j, ξi,p⟩)∥ξi,p∥2\\n2 + e\\nO(σ0d−1/2)\\n= e\\nO\\n\\x12∥ξi,p∥2\\n2\\nn\\n\\x13\\nσ′(⟨w(t)\\nm,j, ξi,p⟩) + e\\nO(σ0d−1/2)\\n= e\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ξi,p⟩) + e\\nO(σ2.5\\n0 ),\\nwhere the ﬁrst equality is due to Lemma D.2, second equality is due to |ℓ′\\ni,t| ≤1, πm ∈[0, 1] and\\nthe third equality is due to Lemma D.2 and our choice of n, σ0. Based on previous results, let B\\n30\\nbe the projection matrix on the linear space spanned by {vk}k∈[K] ∪{ck}k∈[K]. We can verify that\\n∥∇wm,jL(t)∥2 ≤∥B∇wm,jL(t)∥2 + ∥(I −B)∇wm,jL(t)∥2\\n≤\\nX\\nk∈[K]\\nE[α3] + e\\nO(d−0.005)\\n2KM2\\nσ′(⟨w(t)\\nm,j, vk⟩) +\\nX\\nk∈[K]\\ne\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ck⟩)\\n+\\nX\\ni∈[n],p≥4\\ne\\nO(d−0.005)σ′(⟨w(t)\\nm,j, ξi,p⟩) + e\\nO(σ2.5\\n0 ).\\nBecause we use normalized gradient descent, all the experts get trained at the same speed.\\nFollowing lemma shows that expert m will focus on the signal vk∗\\nm.\\nLemma E.6. For all m ∈[M] and t ≤T1, we have following inequalities hold,\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩= O(σ0.5\\n0 ),\\n⟨w(t)\\nm,j, vk⟩= e\\nO(σ0), ∀(j, k) ̸= (j∗\\nm, k∗\\nm),\\n⟨w(t)\\nm,j, ck⟩= e\\nO(σ0), ∀j ∈[J], k ∈[K],\\n⟨w(t)\\nm,j, ξi,p⟩= e\\nO(σ0), ∀j ∈[J], i ∈[n], p ≥4.\\nProof. For t ≤T1, the update rule of every expert could be written as,\\n⟨w(t+1)\\nm,j , vk⟩= ⟨w(t)\\nm,j, vk⟩+\\nη\\n∥∇WmL(t)∥F\\n\\x143E[α3] + e\\nO(d−0.005)\\n2KM 2\\n⟨w(t)\\nm,j, vk⟩2 + e\\nO(σ2.5\\n0 )\\n\\x15\\n,\\n⟨w(t+1)\\nm,j , ξi,p⟩= ⟨w(t)\\nm,j, ξi,p⟩+\\nη\\n∥∇WmL(t)∥F\\n\\x02 e\\nO(d−0.005)⟨w(t)\\nm,j, ξi,p⟩2 + e\\nO(σ2.5\\n0 )\\n\\x03\\n,\\n⟨w(t+1)\\nm,j , ck⟩= ⟨w(t)\\nm,j, ck⟩+\\nη\\n∥∇WmL(t)∥F\\n\\x02 e\\nO(d−0.005)⟨w(t)\\nm,j, ck⟩2 + e\\nO(σ2.5\\n0 )\\n\\x03\\n.\\n(E.3)\\nFor t ≤T1, we have that ⟨w(t)\\nm,j, vk∗\\nm⟩≤O(σ0.5\\n0 ). By comparing the update rule of ⟨w(t)\\nm,j, vk∗\\nm⟩\\nand other inner product presented in (E.3) , We can prove that ⟨w(t)\\nm,j, vk∗\\nm⟩will grow to σ0.5\\n0\\nwhile\\nother inner product still remain nearly unchanged.\\nComparison with ⟨w(t)\\nm,j, vk⟩. Consider k ̸= k∗\\nm. We want to get an upper bound of ⟨w(t)\\nm,j, vk⟩,\\nso without loss of generality we can assume ⟨w(t)\\nm,j, vk⟩= Ω(σ0). Since σ0 ≤d−0.01, we have that\\n⟨w(t)\\nm,j, vk⟩2 + e\\nO(σ2.5\\n0 ) = (1 + e\\nO(d−0.005))⟨w(t)\\nm,j, vk⟩2. Therefore, we have that\\n⟨w(t+1)\\nm,j , vk∗\\nm⟩= ⟨w(t)\\nm,j, vk∗\\nm⟩+\\nη\\n∥∇WmL(t)∥F\\n3E[α3] + e\\nO(d−0.005)\\n2KM2\\n⟨w(t)\\nm,j, vk∗\\nm⟩2,\\n(E.4)\\n⟨w(t+1)\\nm,j , vk⟩= ⟨w(t)\\nm,j, vk⟩+\\nη\\n∥∇WmL(t)∥F\\n3E[α3] + e\\nO(d−0.005)\\n2KM2\\n⟨w(t)\\nm,j, vk⟩2.\\n(E.5)\\nApplying Lemma F.2 by choosing Ct = (3E[α3] + e\\nO(d−0.005))/(2KM 2∥∇WmL(t)∥F ), S = 1 +\\ne\\nO(d−0.005), G = 1/(3 log(d)M2) and verifying ⟨w(0)\\nm , vk∗\\nm⟩≥S(1 + G−1)⟨w(0)\\nm , vk⟩(events in Sec-\\ntion D hold), we have that ⟨w(t)\\nm,j, vk⟩≤O(G−1σ0) = e\\nO(σ0).\\n31\\nComparison with ⟨w(t)\\nm,j, ck⟩.We want to get an upper bound of ⟨w(t)\\nm,j, ck⟩, so without loss of\\ngenerality we can assume ⟨w(t)\\nm,j, vk⟩= Ω(σ0). Because σ0 ≤d−0.01, one can easily show that\\n⟨w(t+1)\\nm,j , vk∗\\nm⟩= ⟨w(t)\\nm,j, vk∗\\nm⟩+\\nη\\n∥∇WmL(t)∥F\\n3E[α3] + e\\nO(d−0.005)\\n2KM2\\n⟨w(t)\\nm,j, vk∗\\nm⟩2,\\n⟨w(t+1)\\nm,j , ck⟩≤⟨w(t)\\nm,j, ck⟩+\\nη\\n∥∇WmL(t)∥F\\ne\\nO(d−0.01)⟨w(t)\\nm,j, ck⟩2.\\nAgain, applying Lemma F.2 by choosing Ct = (3E[α3] + e\\nO(d−0.005))/(2KM 2∥∇WmL(t)∥F ), S =\\ne\\nO(d−0.01), G = 2 and verifying ⟨w(0)\\nm , vk∗\\nm⟩≥S(1 + G−1)⟨w(0)\\nm , ck⟩(events in Section D hold), we\\nhave that ⟨w(t), vk⟩≤O(G−1σ0) = e\\nO(σ0).\\nComparison with ⟨w(t)\\nm,j, ξi,p⟩. The proof is exact the same as the one with ck.\\nDenote the iteration T (m) as the ﬁrst time that ∥∇WmL(t)∥F ≥σ1.8\\n0 . Then Following lemma\\ngives an upper bound of T (m) for all m ∈M.\\nLemma E.7. For all m ∈[M], we have that T (m) = e\\nO(η−1σ0.8\\n0 ) and thus T (m) < 0.01T1. Besides,\\nfor all Tm < t ≤T1 we have that\\n⟨∇wm,j∗\\nmL(t), vk∗\\nm⟩≥(1 −σ0.1\\n0 )∥∇WmL(t)∥F .\\nProof. Let projection matrix B = vk∗\\nmv⊤\\nk∗\\nm ∈Rd×d, then we can divide the gradient into two\\northogonal part\\n∥∇wm,j∗\\nmL(t)∥2 = ∥B∇wm,j∗\\nmL(t) + (I −B)∇wm,j∗\\nmL(t)∥2\\n≤∥B∇wm,j∗\\nmL(t)∥2 + ∥(I −B)∇wm,j∗\\nmL(t)∥2\\nRecall that\\n∇wm,j∗\\nmL(t) = 1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j∗\\nm, x(p)\\ni ⟩)x(p)\\ni ,\\nSo we have that\\n∥(I −B)∇wm,j∗\\nmL(t)∥2 =\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j∗\\nm, x(p)\\ni ⟩)(I −B)x(p)\\ni\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n≤1\\nn\\nX\\ni,p\\n\\r\\n\\r\\n\\r\\n\\rσ′(⟨w(t)\\nm,j∗\\nm, x(p)\\ni ⟩)(I −B)x(p)\\ni\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n≤e\\nO(σ2\\n0),\\nwhere the ﬁrst inequality is by |ℓ′\\ni,t| ≤1, πm ∈[0, 1] and the second equality is because\\n1. when x(p)\\ni\\nalign with vk∗\\nm, (I −B)x(p)\\ni\\n= 0.\\n2. when x(p)\\ni\\ndoesn’t align with vk∗\\nm, ⟨w(t)\\nm,j∗\\nm, x(p)\\ni ⟩= e\\nO(σ0).\\n32\\nTherefore, we have that\\n∥∇wm,j∗\\nmL(t)∥2 ≤∥B∇wm,j∗\\nmL(t)∥2 + e\\nO(σ2\\n0) = ⟨∇wm,j∗\\nmL(t), vk∗\\nm⟩+ e\\nO(σ2\\n0).\\nWe next compute the gradient of the neuron wm,j, j ̸= j∗\\nm,\\n∥∇wm,jL(t)∥2 =\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)x(p)\\ni\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n= e\\nO(σ2\\n0),\\n(E.6)\\nwhere the inequality is by ⟨w(t)\\nm,j, x(p)\\ni ⟩= e\\nO(σ0), ∀j ̸= j∗\\nm which is due to Lemma E.6. Now we can\\nupper bound the gradient norm,\\n∥∇WmL(t)∥F ≤\\nX\\nj∈[J]\\n∥∇wm,jL(t)∥2 ≤∥∇wm,j∗\\nmL(t)∥2 + e\\nO(σ2\\n0).\\n(E.7)\\nWhen ∥∇WmL(t)∥F ≥σ1.8\\n0 , it is obviously that\\n⟨∇wm,j∗\\nmL, vk∗\\nm⟩≥∥∇wm,j∗\\nmL(t)∥2 −e\\nO(σ2\\n0) ≥∥∇WmL(t)∥F −e\\nO(σ2\\n0) ≥(1 −σ0.1\\n0 )∥∇WmL(t)∥F ,\\nwhere the ﬁrst inequality is by (E.6) and the second inequality is by (E.7). Now let us give an\\nupper bound for T (m). During the period t ≤T (m), ∥∇WmL(t)∥F < σ1.8\\n0 . On the one hand, by\\nLemma E.5 we have that\\n∥∇WmL(t)∥2 ≥−⟨∇wm,jL(t), vk∗\\nm⟩= 3E[α3] −e\\nO(d−0.005)\\n2KM2\\n[⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩]2 −e\\nO(σ2.5\\n0 )\\nwhich implies that the inner product ⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≤e\\nO(σ0.9\\n0 ). On the other hand, by Lemma E.6\\nwe have that\\n⟨w(t+1)\\nm,j∗\\nm, vk∗\\nm⟩≥⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩+\\nη\\n∥∇WmL(t)∥F\\nΘ(\\n1\\nKM2 )⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩2\\n≥⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩+ Θ\\n\\x10\\nη\\nKM2σ1.8\\n0\\n\\x11\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩2\\n≥⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩+ Θ\\n\\x10\\nη\\nKM2σ0.8\\n0\\n\\x11\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩,\\nwhere last inequality is by ⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≥0.1σ0.\\nTherefore, we have that the inner product\\n⟨w(t)\\nm,j, vk∗\\nm⟩grows exponentially and will reach e\\nO(σ0.9\\n0 ) within e\\nO(η−1σ0.8\\n0 ) iterations.\\nRecall that T1 = ⌊η−1σ0.5\\n0 ⌋, following Lemma shows that the expert m ∈[M] only learns one\\nfeature during the ﬁrst stage,\\nLemma E.8. For all t ≤T1, m ∈[M], we have that\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩= O(σ0.5\\n0 ),\\n⟨w(t)\\nm,j, vk⟩= e\\nO(σ0), ∀(j, k) ̸= (j∗\\nm, k∗\\nm),\\n⟨w(t)\\nm,j, ck⟩= e\\nO(σ0), ∀j ∈[J], k ∈[K],\\n33\\n⟨w(t)\\nm,j, ξi,p⟩= e\\nO(σ0), ∀j ∈[J], i ∈[n], p ≥4.\\nBesides ⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≥(1 −σ0.1\\n0 )ηt, for all t ≥T1/2.\\nProof. By Lemma E.7, we have T (m) = e\\nO(η−1σ0.8\\n0 ) < σ0.2\\n0\\n· T1. Notice that ⟨∇wm,j∗\\nmL(t), vk∗⟩≥\\n(1 −σ0.1\\n0 )∥∇WmL(t)∥F , for all Tm ≤t ≤T1. Therefore, we have that\\n⟨w(t+1)\\nm,j∗\\nm, vk∗\\nm⟩≥⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩+ (1 −σ0.1\\n0 )η, ∀Tm ≤t ≤T1,\\nwhich implies ⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≥(1 −O(σ0.1\\n0 ))ηt, ∀t ≥T1/2. Finally, applying Lemma E.6 completes\\nthe proof.\\nE.2\\nRouter Learning Stage\\nDenote T2 = ⌊η−1M−2⌋, The second stage ends when t = T2. Given x = [αyvk, βck, γϵvk′, ξ],\\nwe denote by ¯\\nx = [0, βck, 0, . . . , 0] the one only keeps cluster-center signal and denote by b\\nx =\\n[αyvk, 0, γϵvk′, 0] the one that only keeps feature signal and feature noise.\\nFor all T1 ≤t ≤T2, we will show that the router only focuses on the cluster-center signals and\\nthe experts only focus on the feature signals, i.e., we will prove that |fm(xi; W(t)) −fm(b\\nxi; W(t))|\\nand ∥h(xi; Θ(t)) −h(¯\\nxi, Θ(t))∥∞are small.\\nIn particular, We claim that for all T1 ≤t ≤T2,\\nfollowing proposition holds.\\nProposition E.9. For all T1 ≤t ≤T2, following inequalities hold,\\n|fm(xi; W(t)) −fm(b\\nxi; W(t))| ≤O(d−0.001), ∀m ∈[M], i ∈[n],\\n(E.8)\\n∥h(xi; Θ(t)) −h(¯\\nxi; Θ(t))∥∞≤O(d−0.001), ∀i ∈[n],\\n(E.9)\\nP(mi,t = m), πm(xi; Θ(t)) = Ω(1/M), ∀m ∈[M], i ∈Ωk∗\\nm.\\n(E.10)\\nProposition E.9 implies that expert will only focus on the label signal and router will only focus\\non the cluster-center signal. We will prove Proposition E.9 by induction. Before we move into the\\ndetailed proof of Proposition E.9, we will ﬁrst prove some important lemmas.\\nLemma E.10. For all T1 ≤t ≤T2, the neural network parameter maintains following property.\\n• |fm(xi; W(t))| = O(1), ∀m ∈[M],\\n• πmi,t(xi; Θ(t)) = Ω(1/M), ∀i ∈[n].\\nProof. Because we use normalized gradient descent, the ﬁrst bullet would be quite straight forward.\\n|fm(xi, W(t))| =\\nX\\nj∈[J]\\nX\\np∈[P]\\nσ(⟨w(t)\\nm,j, x(p)\\ni ⟩)\\n(i)\\n= O(1),\\nwhere (i) is by ∥w(t)\\nm,j −w(0)\\nm,j∥2 = O(ηT2) = O(M−2) and x(p)\\ni\\n= O(1).\\n34\\nNow we prove the second bullet. By Lemma C.4, we have that hmi,t(x; Θ) ≥maxm hm(x; Θ)−1,\\nwhich implies that\\nπmi,t(xi; Θ(t)) =\\nexp(hmi,t(xi; Θ(t)))\\nP\\nm exp(hm(x; Θ(t))) ≥\\nexp(hmi,t(xi; Θ(t)))\\nM maxm exp(hm(x; Θ(t))) ≥\\n1\\neM .\\nLemma E.11. Denote δΘ = maxi ∥h(¯\\nxi; Θ) −h(xi; Θ)∥∞and let the random variable ¯\\nmi,t be\\nexpert that get routed if we use the gating network output h(¯\\nxi; Θ(t)) instead. Then we have\\nfollowing inequalities,\\n|πm(xi; Θ) −πm(¯\\nxi; Θ)| = O(δΘ), ∀m ∈[M], i ∈[n], .\\n(E.11)\\n|P(mi,t = m) −P( ¯\\nmi,t = m)| = O(M2δΘ), ∀m ∈[M], i ∈[n].\\n(E.12)\\nProof. By deﬁnition of δΘ, we have that ∥h(xi; Θ(t)) −h(¯\\nxi; Θ(t))∥∞≤δΘ.\\nThen applying\\nLemma 5.1 gives |P(mi,t = m) −P( ¯\\nmk,t = m)| = e\\nO(δΘ), ∀m ∈[M], i ∈[n], which completes\\nthe proof for (E.12).\\nNext we prove (E.11), which needs more eﬀort. For all i ∈[n], we have\\nπm(xi; Θ) =\\nπm(¯\\nxi; Θ) exp(hm(xi; Θ) −hm(¯\\nxi; Θ))\\nP\\nm′ πm′(¯\\nxi; Θ) exp(hm′(xi; Θ) −hm′(¯\\nxi; Θ)).\\nLet δm′ = exp(hm′(xi; Θ) −hm′(¯\\nxi; Θ)) = 1 + O(δΘ). Then for suﬃciently small δΘ, we have that\\nδm′ ≥0.5 . Then we can further compute\\n|πm(xi; Θ(t)) −πm(¯\\nxi; Θ)| = πm(¯\\nxi; Θ)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nδm\\nP\\nm′ πm′(¯\\nxi; Θ)δm′ −1\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n= πm(¯\\nxi; Θ)| P\\nm′ πm′(¯\\nxi; Θ)(δm′ −δm)|\\nP\\nm′ πm′(¯\\nxi; Θ)δm′\\n≤πm(¯\\nxi; Θ)\\nP\\nm′ πm′(¯\\nxi; Θ)|δm′ −δm|\\nP\\nm′ πm′(¯\\nxi; Θ)δm′\\n≤O(δΘ),\\nwhere the last inequality is by |δm′ −δm| ≤O(δΘ), πm(¯\\nxi; Θ) ≤1 and P\\nm′ πm′(¯\\nxi; Θ)δm′ ≥\\n[P\\nm′ πm′(¯\\nxi; Θ)]/2 = 0.5.\\nFollowing Lemma implies that the pattern learned by experts during the ﬁrst stage won’t change\\nin the second stage.\\nLemma E.12. Suppose (E.8), (E.9), (E.10) hold for all t ∈[T1, T] ⊆[T1, T2 −1], then we have\\nfollowing inequalities hold for all t ∈[T1, T + 1],\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≥(1 −O(σ0.1\\n0 ))ηt,\\n⟨w(t)\\nm,j, vk⟩= e\\nO(σ0), ∀(j, k) ̸= (j∗\\nm, k∗\\nm),\\n⟨w(t)\\nm,j, ck⟩= e\\nO(σ0), ∀j ∈[J], k ∈[K],\\n⟨w(t)\\nm,j, ξi,p⟩= e\\nO(σ0), ∀j ∈[J], k ∈[K], i ∈[n], p ≥4.\\n35\\nProof. Most of the proof exactly follows the proof in the ﬁrst stage, so we only list some key steps\\nhere. Recall that\\n∇wm,jL(t) = 1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiσ′(⟨w(t)\\nm,j, x(p)\\ni ⟩)x(p)\\ni .\\nIn the proof of Lemma E.5, we do Taylor expansion at the zero point. Now we will do Taylor\\nexpansion at fm(b\\nxi; W) and π(¯\\nxi; Θ) as follows,\\n|πm(xi; Θ(t))fm(xi; W(t)) −πm(¯\\nxi; Θ(t))fm(b\\nxi; W(t))|\\n≤|πm(¯\\nxi; Θ(t))[fm(xi; W(t)) −fm(b\\nxi; W(t))]| + |[πm(xi; Θ(t)) −πm(¯\\nxi; Θ(t))]fm(xi; W(t))|\\n≤|fm(xi; W(t)) −fm(b\\nxi; W(t))| + O(|πm(xi; Θ(t)) −πm(¯\\nxi; Θ(t))|)\\n≤O(d−0.001),\\nwhere the ﬁrst inequality is by triangle inequality, the second inequality is by πm(¯\\nxi; Θ(t)) ≤1 and\\n|fm(xi; W(t))| = O(1) in Lemma E.10, the third inequality is by (E.8), (E.9) and (E.11).\\nThen follow the proof of Lemma E.5, we have that\\nE[⟨∇wm,jL(t), vk∗\\nm⟩] = −1\\nn\\nX\\ni∈Ωk∗\\nm\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, vk∗\\nm⟩)α3\\ni ∥vk∗\\nm∥2\\n2\\n−1\\nn\\nX\\ni∈Ωk′,k∗\\nm\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, vk∗\\nm⟩)γ3\\ni yiϵi∥vk∗\\nm∥2\\n2\\n−1\\nn\\nX\\ni,p\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))σ′(⟨w(t)\\nm,j, ξi,p⟩)yi⟨vk∗\\nm, ξi,p⟩\\n=\\n\\x14\\n−e\\nΘ\\n\\x10 1\\nn\\n\\x11 X\\ni∈Ωk∗\\nm\\nP(mi,t = m)α3\\ni −e\\nΘ\\n\\x10 1\\nn\\n\\x11\\nX\\ni∈Ωk′,k∗\\nm\\nP(mi,t = m)γ3\\ni yiϵi\\n+ O(d−0.001)\\n\\x15\\n· σ′(⟨w(t)\\nm,j, vk∗\\nm⟩) + e\\nO(d−1/2)\\n(i)\\n= −e\\nΘ(1)σ′(⟨w(t)\\nm,j, vk∗\\nm⟩),\\nwhere (i) is due to (E.10): P(mi,t = m) ≥Θ(1/M), ∀i ∈Ωk∗\\nm, m ∈[M]. Again follow Lemma E.5\\nand Lemma E.6, we further have that\\n⟨∇wm,jL(t), vk⟩= −e\\nΘ(1)[⟨w(t)\\nm,j, vk⟩]2,\\n⟨∇wm,jL(t), ck⟩= e\\nO(1)[⟨w(t)\\nm,j, ck⟩]2,\\n⟨∇wm,jL(t), ξi,p⟩= e\\nO(1)[⟨w(t)\\nm,j, ξi,p⟩]2.\\nThus for all T1 ≤t ≤T, the update rule of every expert could be written as,\\n⟨w(t+1)\\nm,j , vk∗\\nm⟩= ⟨w(t)\\nm,j, vk∗\\nm⟩+ e\\nΘ(1)\\nη\\n∥∇WmL(t)∥F\\n⟨w(t)\\nm,j, vk∗\\nm⟩2\\n⟨w(t+1)\\nm,j , vk⟩= ⟨w(t)\\nm,j, vk⟩+ e\\nO(1)\\nη\\n∥∇WmL(t)∥F\\n⟨w(t)\\nm,j, vk⟩2\\n36\\n⟨w(t+1), ξi,p⟩= ⟨w(t), ξi,p⟩+ e\\nO(1)\\nη\\n∥∇WmL(t)∥F\\n⟨w(t), ξi,p⟩2\\n⟨w(t+1)\\nm,j , ck⟩= ⟨w(t)\\nm,j, ck⟩+ e\\nO(1)\\nη\\n∥∇WmL(t)∥F\\n⟨w(t)\\nm,j, ck⟩2.\\nBy the ﬁrst stage of training we have that ⟨w(T1)\\nm,j , vk∗\\nm⟩= Θ(σ0.5\\n0 ), while others remains e\\nO(σ0).\\nThen we can use Lemma F.2, by choosing S = e\\nΘ(1) and G = 2, then we have that\\n⟨w(t)\\nm,j, vk∗\\nm⟩= O(1).\\n⟨w(t)\\nm,j, vk⟩= e\\nO(σ0), ∀k ̸= k∗\\nm.\\n⟨w(t)\\nm,j, ck⟩= e\\nO(σ0).\\n⟨w(t), ξi,p⟩= e\\nO(σ0).\\nThen following Lemma E.7 and E.8, we can prove that for all T1 ≤t ≤T + 1, m ∈[M],\\n⟨w(t)\\nm,j∗\\nm, vk∗\\nm⟩≥(1 −O(σ0.1\\n0 ))ηt,\\n⟨w(t)\\nm,j, vk⟩= e\\nO(σ0), ∀(j, k) ̸= (j∗\\nm, k∗\\nm),\\n⟨w(t)\\nm,j, ck⟩= e\\nO(σ0), ∀j ∈[J], k ∈[K],\\n⟨w(t)\\nm,j, ξi,p⟩= e\\nO(σ0), ∀j ∈[J], i ∈[n], p ≥4.\\nBy the result of expert training we have following results\\nLemma E.13. Suppose (E.8), (E.9), (E.10) hold for all t ∈[T1, T] ⊆[T1, T2 −1], then we have\\nthat |fm(xi; W(t)) −fm(b\\nxi; W(t))| = e\\nO(σ3\\n0) for all m ∈[M] and i ∈[n], t ∈[T1, T + 1]. Besides,\\nyifm(b\\nxi; W(t)) =\\nX\\nj∈[J]\\nh\\nα3\\ni σ(⟨w(t)\\nm,j, vk⟩) + γ3\\ni σ(⟨w(t)\\nm,j, vk′⟩)\\ni\\n, ∀i ∈Ω+\\nk,k′, m ∈[M],\\nyifm(b\\nxi; W(t)) =\\nX\\nj∈[J]\\nh\\nα3\\ni σ(⟨w(t)\\nm,j, vk⟩) −γ3\\ni σ(⟨w(t)\\nm,j, vk′⟩)\\ni\\n, ∀i ∈Ω−\\nk,k′, m ∈[M].\\nProof. For all i ∈Ωk, we have that\\n\\x0c\\n\\x0cfm(xi; W(t)) −fm(b\\nxi; W(t))\\n\\x0c\\n\\x0c ≤\\n\\x0c\\n\\x0c X\\nj∈[J]\\nσ(⟨w(t)\\nm,j, ck⟩)\\n\\x0c\\n\\x0c +\\n\\x0c\\n\\x0c\\nX\\nj∈[J],p≥4\\nσ(⟨w(t)\\nm,j, ξi,p⟩)\\n\\x0c\\n\\x0c\\n≤O(J) · max\\nk,j σ(⟨w(t)\\nm,j, ck⟩) + O(J) · max\\ni,j,p |σ(⟨w(t)\\nm,j, ξi,p⟩)|\\n= e\\nO(σ3\\n0),\\nwhere the ﬁrst inequality is by triangle inequality and the last equality is by Lemma E.12.\\nNext we will show that router only focus on the cluster-center signal rather than the label signal\\nduring the router training.\\n37\\nLemma E.14. Suppose (E.8), (E.9), (E.10) hold for all t ∈[T1, T] ⊆[T1, T2 −1], then we have\\nthat ∥h(¯\\nxi, Θ(t)) −h(xi; Θ(t))∥∞= e\\nO(d−0.005) hold for all i ∈[n] and t ∈[T1, T + 1]. Besides, we\\nhave that maxm,k |⟨θ(t)\\nm , vk⟩|, maxm,i,p |⟨θ(t)\\nm , ξi,p⟩| = e\\nO(d−0.005) for all t ∈[T1, T + 1].\\nProof. Recall the deﬁnition of δΘ in Lemma E.11, we need to show that δΘ(t) = e\\nO(d−0.005) for all\\nt ∈[T1, T + 1]. We ﬁrst prove following router parameter update rules,\\n⟨∇θmL(t), vk⟩= O(δΘ(t)K2) + e\\nO(d−0.005), ⟨∇θmL(t), ξi,p⟩= e\\nO(d−0.005),\\n(E.13)\\nfor all T1 ≤t ≤T, m ∈[M], k ∈[K], i ∈[n] and p ≥4.\\nConsider the inner product of the router gradient and the feature vector and we have\\nE[\\n\\n∇θmL(t), vk\\n\\x0b\\n]\\n= 1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tyiπm(xi; Θ(t))fm(xi; W(t))yiαi\\n|\\n{z\\n}\\nI1\\n+ 1\\nn\\nX\\ni∈Ωk′,k\\nP(mi,t = m)ℓ′\\ni,tyiπm(xi; Θ(t))fm(xi; W(t))ϵiγi\\n|\\n{z\\n}\\nI2\\n−1\\nn\\nX\\ni∈Ωk,m′∈[M]\\nP(mi,t = m′)ℓ′\\ni,tyiπm′(xi; Θ(t))πm(xi; Θ(t))fm′(xi, W(t))yiαi\\n|\\n{z\\n}\\nI3\\n−1\\nn\\nX\\ni∈Ωk′,k,m′∈[M]\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))yiπm(xi; Θ(t))fm′(xi, W(t))ϵiγi\\n|\\n{z\\n}\\nI4\\n+ 1\\nn\\nX\\ni∈[n],p≥4\\nP(mi,t = m)ℓ′\\ni,tyiπm(xi; Θ(t))fm(xi; W(t))⟨x(p)\\ni , vk⟩\\n|\\n{z\\n}\\nI5\\n−1\\nn\\nX\\ni∈[n],p≥4,m′∈[M]\\nP(mi,t = m′)ℓ′\\ni,tyiπm′(xi; Θ(t))πm(xi; Θ(t))fm′(xi; W(t))⟨x(p)\\ni , vk⟩\\n|\\n{z\\n}\\nI6\\n. (E.14)\\nDenote yiπm(¯\\nxi; Θ(t))fm(b\\nxi; W(t)), ∀i ∈Ω+\\nk,k′ by ¯\\nF +\\nk,k′. We next show that the output of the MoE\\nmultiplied by label: yiπm(xi; Θ(t))fm(xi; W), ∀i ∈Ω+\\nk,k′ can be approximated by ¯\\nF +\\nk,k′.\\n|πm(xi; Θ(t))fm(xi; W(t)) −πm(¯\\nxi; Θ(t))fm(b\\nxi; W(t))|\\n≤|[πm(xi; Θ(t)) −πm(¯\\nxi; Θ(t))]fm(xi; W(t))| + |πm(¯\\nxi; Θ(t))[fm(xi; W(t)) −fm(b\\nxi; W(t))]|\\n≤O(|πm(xi; Θ(t)) −πm(¯\\nxi; Θ(t))|) + |fm(xi; W(t)) −fm(b\\nxi; W(t))|\\n≤O(δΘ(t)) + e\\nO(σ3\\n0),\\nwhere the ﬁrst inequality is by triangle inequality, the second inequality is by πm(¯\\nxi; Θ(t)) ≤1 and\\n|fm(xi; W(t))| = O(1) in Lemma E.10, the third inequality is by (E.11) and Lemma E.13.\\n38\\nSimilarly, denote yiπm(¯\\nxi; Θ(t))fm(b\\nxi; W(t)), i ∈Ω−\\nk,k′ by ¯\\nF −\\nk,k′ and we can show that value\\nyiπm(xi; Θ(t))fm(xi; W(t)), ∀i ∈Ω−\\nk,k′ can be approximated by ¯\\nF −\\nk,k′.\\nNow we can bound I1 as\\nfollows,\\nI1 =\\nX\\nk′̸=k\\nℓ′( ¯\\nFk,k′+) ¯\\nF +\\nk,k′\\nn\\nX\\ni∈Ω+\\nk,k′\\n\\x02\\nP(mi,t = m)yiαi + O(δΘ(t))\\n\\x03\\n+ e\\nO(σ3\\n0)\\n+\\nX\\nk′̸=k\\nℓ′( ¯\\nFk,k′−) ¯\\nF −\\nk,k′\\nn\\nX\\ni∈Ω−\\nk,k′\\n\\x02\\nP(mi,t = m)yiαi + O(δΘ(t))\\n\\x03\\n+ e\\nO(σ3\\n0)\\n(i)\\n=\\nX\\nk′̸=k\\nℓ′( ¯\\nFk,k′+) ¯\\nF +\\nk,k′\\nn\\nX\\ni∈Ω+\\nk,k′\\n\\x02\\nP( ¯\\nmi,t = m)yiαi + O(M2δΘ(t))\\n\\x03\\n+ e\\nO(σ3\\n0)\\n+\\nX\\nk′̸=k\\nℓ′( ¯\\nFk,k′−) ¯\\nF −\\nk,k′\\nn\\nX\\ni∈Ω−\\nk,k′\\n\\x02\\nP( ¯\\nmi,t = m)yiαi + O(M2δΘ(t))\\n\\x03\\n+ e\\nO(σ3\\n0)\\n(ii)\\n= O(M2δΘ(t)) + e\\nO(n−1/2 + σ3\\n0)\\n= O(M2δΘ(t)) + e\\nO(d−0.005)\\nwhere (i) is due to (E.12) and (ii) is by P\\ni∈Ω+\\nk,k′ yiα = e\\nO(√n) and P\\ni∈Ω−\\nk,k′ yiα = e\\nO(√n) in\\nLemma D.1. Similarly we can prove that I2, I3, I4 = O(M2δΘ(t)) + e\\nO(d−0.005). Since ⟨x(p)\\ni , vi⟩=\\ne\\nO(d−1/2), ∀p ≥4, πm, πmi,t ≤1 and fmi,t = O(1), we can upper bound I5, I6 by e\\nO(d−1/2). Plugging\\nthose bounds into the gradient computation (E.14) gives\\nE[\\n\\n∇θmL(t), vk\\n\\x0b\\n] = O(M2δΘ(t)) + e\\nO(d−0.005).\\nWe ﬁnally consider the alignment between router gradient and noise\\n\\n∇θmL(t), ξi′,p′\\x0b\\n= 1\\nn\\nX\\ni∈[n],p≥4\\n1(mi,t = m)ℓ′\\ni,tyiπmi,t(xi; Θ(t))fmi,t(xi; W(t))⟨x(p)\\ni , ξi′,p′⟩\\n−1\\nn\\nX\\ni∈[n],p≥4\\nℓ′\\ni,tyiπmi,t(xi; Θ(t))πm(xi; Θ(t))fmi,t(xi; W(t))⟨x(p)\\ni , ξi′,p′⟩.\\n(i)\\n= e\\nO\\n\\x12 1\\nn\\n\\x13\\n+ e\\nO(d−1/2)\\n(ii)\\n= e\\nO(d−1/2),\\nwhere the (i) is by considering the cases (i′, p′) = ξi,p and ξi′,p′ ̸= ξi,p respectively and (ii) is due to\\nour choice of n. Now, we have completed the proof of (E.13).\\nPlugging the gradient estimation (E.13) in to the gradient update rule for the gating network\\n(3.5) gives\\nmax\\nm,k |⟨θ(t+1)\\nm\\n, vk⟩| ≤max\\nm,k |⟨θ(t)\\nm , vk⟩| + O(ηrM2δΘ(t)) + e\\nO(ηrd−0.005)\\n(E.15)\\nmax\\nm,i,p |⟨θ(t+1)\\nm\\n, ξi,p⟩| ≤max\\nm,i,p |⟨θ(t)\\nm , ξi,p⟩| + e\\nO(ηrd−0.005)\\n(E.16)\\n39\\nCombining (E.15) and (E.16), we have that there exist C1 = O(M2) and C2 = e\\nO(d−0.005) such\\nthat δΘ(t+1) ≤δΘ(t) + C1ηrδΘ(t) + C2ηr. Therefore, we have that\\nδΘ(t+1) + C−1\\n1 C2 ≤(1 + C1ηr)[δΘ(t) + C−1\\n1 C2]\\n≤exp(C1ηr)[δΘ(t) + C−1\\n1 C2],\\nwhere the last inequality is due to exp(z) ≥1 + z for all z ∈R. Then we further have that\\nδΘ(t) ≤exp(C1ηrt)[δΘ(0) + C−1\\n1 C2] ≤exp(C1ηrη−1M−2)[δΘ(0) + C−1\\n1 C2] = e\\nO(d−0.005),\\nwhere the last equality is by ηr = Θ(M2)η.\\nDeﬁne ∆Θ := maxk∈[K] maxm,m′∈Mk max(xi,yi)∈Ωk |hm(xi; Θ) −hm′(xi; Θ)|, which measures\\nthe bias of the router towards diﬀerent experts in the same Mk. Following Lemma shows that the\\nrouter will treats professional experts equally when ∆θ is small.\\nLemma E.15. For all t ≥0, we have that following inequality holds,\\nmax\\nk∈[K]\\nmax\\nm,m′∈Mk\\nmax\\n(xi,yi)∈Ωk\\n|πm′(xi; Θ(t)) −πm(xi; Θ(t))| ≤2∆Θ(t),\\nmax\\nk∈[K]\\nmax\\nm,m′∈Mk\\nmax\\n(xi,yi)∈Ωk\\n|P(mi,t = m) −P(mi,t = m′)| = O(M2)∆Θ(t).\\nProof. By Lemma C.3, we directly have that\\n|P(mi,t = m) −P(mi,t = m′)| ≤O(M2)|hm(xi; Θ(t)) −hm′(xi; Θ(t))|.\\nThen, we prove that\\n|πm′(xi; Θ) −πm(xi; Θ)| ≤2|hm(xi; Θ(t)) −hm′(xi; Θ(t))|.\\n(E.17)\\nWhen |hm(xi; Θ(t)) −hm′(xi; Θ(t))| ≥1, it is obvious that (E.17) is true. When |hm(xi; Θ(t)) −\\nhm′(xi; Θ(t))| ≤1 we have that\\n|πm′(xi; Θ) −πm(xi; Θ)| =\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nexp(hm(xi; Θ(t))) −exp(hm′(xi; Θ(t)))\\nP\\nm′′ exp\\n\\x00hm′′(xi; Θ(t))\\n\\x01\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n=\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nexp(hm′(xi; Θ(t)))\\nP\\nm′′ exp\\n\\x00hm′′(xi; Θ(t))\\n\\x01\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c · | exp(hm(xi; Θ(t)) −hm′(xi; Θ(t))) −1|\\n≤2|hm(xi; Θ(t)) −hm′(xi; Θ(t))|,\\nwhich completes the proof of (E.17).\\nNotice that the gating network is initialized to be zero, so we have ∆Θ = 0 at initialization.\\nWe can further show that ∆Θ = O\\n\\x001/poly(d)\\n\\x01\\nduring the training up to time T = e\\nO(η−1).\\nLemma E.16. Suppose (E.8), (E.9), (E.10) hold for all t ∈[T1, T] ⊆[T1, T2 −1], then we have\\nthat ∆Θ(t) ≤e\\nO(d−0.001) holds for all t ∈[T1, T + 1].\\nProof. One of the key observation is the similarity of the m-th and the m′-th expert in the same\\nexpert class Mk. Lemma E.12 implies that maxi∈Ωk |fm(xi, W(t)) −fm′(xi, W(t))| = e\\nO(σ0.1\\n0 ) ≤\\ne\\nO(d−0.001).\\n40\\nAnother key observe is that, we only need to focus on the k−th cluster-center signal. Lemma E.14\\nimplies that,\\n∆Θ(t) = max\\nk∈[K]\\nmax\\nm,m′∈Mk\\nmax\\n(xi,yi)∈Ωk\\n|hm(xi; Θ) −hm′(xi; Θ(t))|\\n≤max\\nk∈[K]\\nmax\\nm,m′∈Mk\\nmax\\n(xi,yi)∈Ωk\\n|hm(¯\\nxi; Θ(t)) −hm′(¯\\nxi; Θ(t))| + 2δΘ(t)\\n= max\\nk∈[K]\\nmax\\nm,m′∈Mk\\n|⟨θm −θm′, βick⟩| + 2δΘ(t)\\n≤C2 max\\nk∈[K]\\nmax\\nm,m′∈Mk\\n|⟨θm −θm′, ck⟩| + 2δΘ(t),\\nwhere the ﬁrst inequality is by Lemma E.14 and the second inequality is by βi ≤C2. We now prove\\nthat following gradient diﬀerence is small\\n\\n∇θmL(t) −∇θm′L(t), ck\\n\\x0b\\n(i)\\n= 1\\nn\\nX\\ni∈[n]\\nX\\np∈[P]\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yifm(xi; W(t))⟨x(p)\\ni , ck⟩\\n−1\\nn\\nX\\ni∈[n]\\nX\\np∈[P]\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))yifm′(xi; W(t))⟨x(p)\\ni , ck⟩\\n+ 1\\nn\\nX\\ni∈Ωk\\nX\\np∈[P]\\nX\\nm′′∈[M]\\n[πm′(xi; Θ(t)) −πm(xi; Θ(t))]P(mi,t = m′′)ℓ′\\ni,tπm′′(xi; Θ(t))·\\nyifm′′(xi, W)⟨x(p)\\ni , ck⟩+ e\\nO(d−0.001)\\n= O\\n\\x10 1\\nn\\n\\x11 X\\ni∈Ωk\\n[P(mi,t = m′) −P(mi,t = m)]|ℓ′\\ni,tπm(xi; Θ)βiyifm(xi; W(t))| + e\\nO(d−0.001)\\n+ O(1) max\\ni∈Ωk |πm′(xi; Θ(t)) −πm(xi; Θ(t))| + O(1) max\\ni∈Ωk |fm(xi, W(t)) −fm′(xi, W(t))|\\n= O(1)|P(mi,t = m′) −P(mi,t = m)]| + O(1) max\\ni∈Ωk |πm′(xi; Θ(t)) −πm(xi; Θ(t))|\\n+ O(1) max\\ni∈Ωk |fm(xi, W(t)) −fm′(xi, W(t))| + e\\nO(d−0.001)\\n(ii)\\n= O(M2∆Θ(t)) + e\\nO(d−0.001),\\nwhere the (i) is by Lemma E.2 and (ii) is by Lemma E.15.\\nIt further implies that ∆Θ(t+1) ≤\\nO(ηrM2)∆Θ(t) + e\\nO(ηrd−0.001). Following previous proof of δΘ, we have that ∆Θ(T +1) = e\\nO(d−0.001).\\nTogether with the key technique 1, we can infer that each expert m ∈Mk will get nearly the\\nsame load as other experts in Mk. Since ∆Θ keeps increasing during the training, it cannot be\\nbounded if we allow the total number of iterations goes to inﬁnity in Algorithm 1. This is the\\nreason that we require early stopping in Theorem 4.2, which we believe can be waived by adding\\nload balancing loss (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), or advanced MoE\\nlayer structure such as BASE Layers (Lewis et al., 2021; Dua et al., 2021) and Hash Layers (Roller\\net al., 2021).\\nLemma E.17. Suppose (E.8), (E.9), (E.10) hold for all t ∈[T1, T] ⊆[T1, T2 −1], then for m /\\n∈Mk\\n41\\nand t ∈[T1, T] , if ⟨θ(t)\\nm , ck⟩≥maxm′⟨θ(t)\\nm′, ck⟩−1 we have that\\n⟨∇θmL(t), ck⟩≥Ω\\n\\x12 η3t3\\nKM3\\n\\x13\\n+ e\\nO(d−0.005).\\nProof. The expectation of the inner product ⟨∇θmL(t), ck⟩can be computed as follows,\\nE[⟨∇θmL(t), ck⟩] = 1\\nn\\nX\\ni,p\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yifm(xi; W(t))⟨x(p)\\ni , ck⟩\\n−1\\nn\\nX\\ni,p,m′\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))yifm′(xi, W(t))⟨x(p)\\ni , ck⟩\\n(i)\\n= 1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))βiyifm(xi; W(t)) + e\\nO(d−0.005)\\n−1\\nn\\nX\\ni∈Ωk\\nX\\nm′∈[M]\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))βiyifm′(xi, W).\\n(E.18)\\nwhere (i) is due to |⟨ξi,p, ck⟩| = e\\nO(d−0.5).\\nWe can rewrite the inner product (E.18) as follows,\\nE[\\n\\n∇θmL(t), ck\\n\\x0b\\n] = 1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))βiyifm(xi; W(t)) + e\\nO(d−0.005)\\n−1\\nn\\nX\\ni∈Ωk\\nX\\nm′∈[M]\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))βiyifm′(xi, W)\\n= 1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiβifm(xi; W(t))\\n|\\n{z\\n}\\nI1\\n+ e\\nO(d−0.005)\\n−1\\nn\\nX\\ni∈Ωk,m′∈Mk\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))βiyifm′(xi, W(t))\\n|\\n{z\\n}\\nI2\\n(E.19)\\n−1\\nn\\nX\\ni∈Ωk,m′ /\\n∈Mk\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))βiyifm′(xi, W(t))\\n|\\n{z\\n}\\nI3\\n.\\n(E.20)\\nTo calculate I1, I2, I3, let’s ﬁrst lower bound I2. We now consider the case that m ̸∈Mk, m′ ∈Mk.\\nBecause ⟨θ(t)\\nm , ck⟩≥maxm′⟨θ(t)\\nm , ck⟩−1, we can easily prove that πm(xi; Θ(t)) = Ω(1/M), ∀i ∈Ωk.\\nThen we have that\\nI2 = −1\\nn\\nX\\ni∈Ωk,m′∈Mk\\nP(mi,t = m′)ℓ′\\ni,tπm′(xi; Θ(t))πm(xi; Θ(t))βiyifm′(xi, W(t))\\n42\\n≥Ω\\n\\x10 η3t3\\nnM3\\n\\x11\\nX\\ni∈Ωk,m′∈Mk\\nβi\\n≥Ω\\n\\x10 η3t3\\nKM3\\n\\x11\\n,\\nwhere the ﬁrst inequality is by πm′(xi; Θ(t)) = Ω(1/M), P(mi,t = m′) ≥Θ(1/M), ∀i ∈Ωk∗\\nm, m ∈\\n[M], yifm′(xi; W(t)) = η3t3(1 −O(σ0.1\\n0 )) and ℓ′ = −Θ(1) for all i ∈Ωk, m′ ∈Mk due to\\nProposition E.9 and Lemma E.12, and the last inequality is by |Mk| ≥1 in Lemma D.4 and\\nP\\ni∈Ωk βi = Ω(n/K) in Lemma D.1.\\nThen we consider the case that m, m′ ̸∈Mk. Applying Taylor expansion of ℓ′\\ni,t = 1/2+O(Jη3t3)\\ngives\\n1\\nn\\nX\\ni∈Ωk\\nP(mi,t = m)ℓ′\\ni,tπm(xi; Θ(t))yiβifm(xi; W(t))\\n= 1\\n2n\\nX\\ni∈Ωk\\nP(mi,t = m)πm(xi; Θ(t))yiβifm(xi; W(t)) + O\\n\\x00J2η6t6\\x01\\n= 1\\n2n\\nX\\nk′\\nX\\ni∈Ω+\\nk,k′\\nP(mi,t = m)πm(xi; Θ(t))yiβifm(xi; W(t)) + O\\n\\x00J2η6t6\\x01\\n+ 1\\n2n\\nX\\nk′\\nX\\ni∈Ω−\\nk,k′\\nP(mi,t = m)πm(xi; Θ(t))yiβifm(xi; W(t))\\n= O(J2η6t6) + e\\nO(d−0.005).\\n(E.21)\\nwhere the last inequality is by the technique we have used before in Lemma E.16. By (E.21), we\\ncan get upper bound |I1|, |I3| by O(J2η6t6) + e\\nO(d−0.005).\\nPlugging the bound of I1, I2, I3 into (E.20) gives,\\n\\n∇θmL(t), ck\\n\\x0b\\n≥Ω\\n\\x12 η3t3\\nKM3\\n\\x13\\n+ O(J2η6t6) + e\\nO(d−0.005)\\n≤Ω\\n\\x12 η3t3\\nKM3\\n\\x13\\n+ e\\nO(d−0.005),\\nwhere the last inequality is by t ≤T2 = ⌊η−1M−2⌋.\\nNow we can claim that Proposition E.9 is true and we summarize the results as follow lemma.\\nLemma E.18. For all T1 ≤t ≤T2, we have Proposition E.9 holds.\\nBesides, we have that\\n⟨θ(T2)\\nm\\n, ck⟩≤maxm′∈[M]⟨θ(T2)\\nm′ , ck⟩−Ω(K−1M−9) for all m /\\n∈Mk..\\nProof. We will ﬁrst use induction to prove Proposition E.9. It is worth noting that proposition E.9\\nis true at the beginning of the second stage t = T1.\\nSuppose (E.8), (E.9), (E.10) hold for all\\nt ∈[T1, T] ⊆[T1, T2 −1], we next verify that they also hold for t ∈[T1, T + 1]. Lemma E.13\\nshows that (E.8) holds for t ∈[T1, T + 1]. Lemma E.14 further shows that (E.8) holds for t ∈\\n[T1, T + 1]. Therefore, we only need to verify whether (E.10) holds for t ∈[T1, T + 1]. Therefore,\\nfor each pair i ∈Ωk, m ∈Mk, we need to estimate the gap between expert m and the expert\\nwith best performance hm(xi; Θ(t)) −maxm′ hm′(xi; Θ(t)). By Lemma E.17 and Lemma E.14, we\\n43\\ncan induce that hm(xi; Θ(t)) is small therefore cannot be the largest one. Thus hm(xi; Θ(t)) −\\nmaxm′ hm′(xi; Θ(t)) = hm(xi; Θ(t)) −maxm′ hm′(xi; Θ(t)) ≤∆Θ(t) ≤e\\nO(d−0.001).\\nTherefore, by\\nLemma C.3 we have (E.10) holds. Now we have veriﬁed that (E.10) also holds for t ∈[T1, T + 1],\\nwhich completes the induction for Lemma E.9.\\nFinally, we carefully characterize the value of ⟨θ(t)\\nm , ck⟩, for ηrη−1 = Θ(M2) and m /\\n∈Mk. If\\n⟨θ(t)\\nm , ck⟩≥maxm′⟨θ(t)\\nm′, ck⟩−1, by Lemma E.17 we have that\\n⟨θ(t+1)\\nm\\n, ck⟩≤⟨θ(t)\\nm , ck⟩−Θ\\n\\x12ηrη3t3\\nKM3\\n\\x13\\n+ e\\nO(ηrd−0.005) ≤0.\\n(E.22)\\nIf there exists t ≤T2−1 such that ⟨θ(t+1)\\nm\\n, ck⟩≤maxm′⟨θ(t)\\nm′, ck⟩−1, clearly we have that ⟨θ(T2)\\nm\\n, ck⟩≤\\n−Ω(K−1M−9) since ⟨θ(t)\\nm , ck⟩will keep decreasing as long as ⟨θ(t+1)\\nm\\n, ck⟩≥−1 and our step size\\nηr = Θ(M2)η is small enough. If ⟨θ(t+1)\\nm\\n, ck⟩≥maxm′⟨θ(t)\\nm′, ck⟩−1 holds for all t ≤T2 −1, take\\ntelescope sum of (E.22) from t = 0 to t = T2 −1 gives that\\n⟨θ(T2)\\nm\\n, ck⟩≤⟨θ(0)\\nm , ck⟩−\\nT2−1\\nX\\ns=0\\nΘ\\n\\x12ηrη3s3\\nKM3\\n\\x13\\n+ e\\nO(d−0.005)\\n(i)\\n= −\\nT2−1\\nX\\ns=0\\nΘ\\n\\x12ηrη3s3\\nKM 3\\n\\x13\\n+ e\\nO(d−0.005)\\n(ii)\\n= −Θ\\n\\x12ηrη3T 4\\n2\\nKM3\\n\\x13\\n+ e\\nO(d−0.005)\\n≤−Ω(K−1M−9),\\nwhere the (i) is by θ(0)\\nm = 0 and (ii) is by Pn−1\\ni=0 i3 = n2(n −1)2/4 and the last inequality is due to\\nT2 = ⌊η−1M−2⌋and ηr = Θ(M2)η. Now we have proved that ⟨θ(T2)\\nm\\n, ck⟩≤−Ω(K−1M−9) for all\\nm /\\n∈Mk. Finally, by Lemma E.1 we have that\\nmax\\nm′∈[M]⟨θ(T2)\\nm′ , ck⟩≥1\\nm\\nX\\nm′∈[M]\\n⟨θ(T2)\\nm′ , ck⟩= 0.\\nTherefore, we have that ⟨θ(T2)\\nm\\n, ck⟩≤−Ω(K−1M−9) ≤maxm′∈[M]⟨θ(T2)\\nm′ , ck⟩−Ω(K−1M−9), which\\ncompletes the proof.\\nE.3\\nGeneralization Results\\nIn this section, we will present the detailed proof of Lemma 5.2 and Theorem 4.2 based on analysis\\nin the previous stages.\\nProof of Lemma 5.2. We consider the m-th expert in the MoE layer, suppose that m ∈Mk. Then if\\nwe draw a new sample (x, y) ∈Ωk. Without loss of generality, we assume x = [αyvk, βck, γϵvk′, ξ].\\nBy Lemma E.8, we have already get the bound for inner product between weights and feature\\nsignal, cluster-center signal and feature noise. However, we need to recalculate the bound of the\\ninner product between weights and random noises because we have fresh random noises i.i.d drawn\\nfrom N(0, (σ2\\np/d) · Id). Notice that we use normalized gradient descent for expert with step size η,\\n44\\nso we have that\\n∥w(T1)\\nm,j −w(0)\\nm,j∥2 ≤ηT1 = O(σ0.5\\n0 ).\\nTherefore, by triangle inequality we have that ∥w(T1)\\nm,j ∥2 ≤∥w(0)\\nm,j∥2 + O(σ0.5\\n0 ) ≤e\\nO(σ0\\n√\\nd). Because\\nthe inner product ⟨w(t)\\nm,j, ξp⟩follows the distribution N(0, (σ2\\np/d) · ∥w(T1)\\nm,j ∥2\\n2), we have that with\\nprobability at least 1 −1/(dPMJ),\\n|⟨w(T1)\\nm,j , ξp⟩| = O(σpd−1/2∥w(t)\\nm,j∥2 log(dPMJ)) ≤e\\nO(σ0).\\nApplying Union bound for m ∈[M], j ∈[J], p ≥4 gives that, with probability at least 1 −1/d,\\n|⟨w(T1)\\nm,j , ξp⟩| = e\\nO(σ0), ∀m ∈[M], j ∈[J], p ≥4.\\n(E.23)\\nNow under the event that (E.23) holds, we have that\\nyfm(x, W(t)) = y\\nX\\nj∈[J]\\nX\\np∈[P]\\nσ(⟨wm,j, x(p)⟩)\\n= yσ(⟨wm,j∗\\nm, αyvk⟩) + y\\nX\\n(j,p)̸=(j∗\\nm,1)\\nσ(⟨wm,j, x(p)⟩)\\n≥C3\\n1(1 −σ0.1\\n0 )3σ1.5\\n0\\n−e\\nO(σ3\\n0)\\n≥Ω(σ1.5\\n0 ),\\nwhere the ﬁrst inequality is due to (E.3). Because (E.23) holds holds with probability at least\\n1 −1/d, so we have prove that\\nP(x,y)∼D\\n\\x00yfm(x; W(T1)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk\\n\\x01\\n≤1/d.\\nOn the other hand, if we draw a new sample (x, y) ∈Ωk′, k′ ̸= k. Then we consider the special\\nset Ω−\\nk′,k ⊆Ωk′ where feature noise is vk and the sign of the feature noise ϵ is not equal to the label\\ny. Without loss of generality, we assume it as x = [αyvk′, βck′, −γyvk, ξ]. Then under the event\\nthat (E.23) holds, we have that\\nyfm(x, W(t)) = y\\nX\\nj∈[J]\\nX\\np∈[P]\\nσ(⟨wm,j, x(p)⟩)\\n= yσ(⟨wm,j∗\\nm, −γyvk⟩) + y\\nX\\n(j,p)̸=(j∗\\nm,3)\\nσ(⟨wm,j, x(p)⟩)\\n≤−C3\\n1(1 −σ0.1\\n0 )3σ1.5\\n0\\n+ e\\nO(σ3\\n0)\\n≤−Ω(σ1.5\\n0 ),\\nwhere the ﬁrst inequality is due to (E.3). Because (E.23) holds holds with probability at least\\n1 −1/d, so we have prove that\\nP(x,y)∼D\\n\\x00yfm(x; W(T1)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ω−\\nk′,k\\n\\x01\\n≥1 −1/d.\\n45\\nThen we further have that\\nP(x,y)∼D\\n\\x00yfm(x; W(T1)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk′\\x01\\n≥P(x,y)∼D\\n\\x00yfm(x; W(T1)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ω−\\nk′,k\\n\\x01\\n· P(x,y)∼D\\n\\x00(x, y) ∈Ω−\\nk′,k\\n\\x0c\\n\\x0c(x, y) ∈Ωk′\\x01\\n≥Ω(1/K),\\nwhich completes the proof.\\nProof of Theorem 4.2. We will give the prove for T = T2, i.e., at the end of the second stage.\\nTest Error is small. We ﬁrst prove the following result for the experts. For all expert m ∈Mk,\\nwe have that\\nP(x,y)∼D\\n\\x00yfm(x; W(T)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk\\n\\x01\\n= o(1).\\n(E.24)\\nThe proof of is similar to the proof of Lemma 5.2. We consider the m-th expert in the MoE layer,\\nsuppose that m ∈Mk. Then if we draw a new sample (x, y) ∈Ωk. Without loss of generality, we\\nassume x = [αyvk, βck, γϵvk′, ξ]. By Lemma E.8, we have already get the bound for inner product\\nbetween weights and feature signal, cluster-center signal and feature noise. However, we need to\\nrecalculate the bound of the inner product between weights and random noises because we have\\nfresh random noises i.i.d drawn from N(0, (σ2\\np/d) · Id). Notice that we use normalized gradient\\ndescent with step size η, so we have that\\n∥w(T)\\nm,j −w(0)\\nm,j∥2 ≤ηT = e\\nO(1).\\nTherefore, by triangle inequality we have that ∥w(T)\\nm,j∥2 ≤∥w(0)\\nm,j∥2 + e\\nO(1) ≤e\\nO(σ0\\n√\\nd). Because the\\ninner product ⟨w(t)\\nm,j, ξp⟩follows the distribution N(0, (σ2\\np/d) · ∥w(T)\\nm,j∥2\\n2), with probability at least\\n1 −1/(dPMJ) we have that ,\\n|⟨w(T)\\nm,j, ξp⟩| = O(σpd−1/2∥w(t)\\nm,j∥2 log(dPMJ)) ≤e\\nO(σ0).\\nApplying Union bound for m ∈[M], j ∈[J], p ≥4 gives that, with probability at least 1 −1/d,\\n|⟨w(T)\\nm,j, ξp⟩| = e\\nO(σ0), ∀m ∈[M], j ∈[J], p ≥4.\\n(E.25)\\nNow, under the event that (E.25) holds, we have that\\nyfm(x, W(T)) = y\\nX\\nj∈[J]\\nX\\np∈[P]\\nσ(⟨w(T)\\nm,j, x(p)⟩)\\n= yσ(⟨w(T)\\nm,j∗\\nm, αyvk⟩) + y\\nX\\n(j,p)̸=(j∗\\nm,1)\\nσ(⟨w(T)\\nm,j, x(p)⟩)\\n≥C3\\n1(1 −σ0.1\\n0 )3M−4 −e\\nO(σ3\\n0)\\n= e\\nΩ(1),\\nwhere the ﬁrst inequality is by Lemma E.12. Because (E.25) holds with probability at least 1−1/d,\\n46\\nso we have prove that\\nP(x,y)∼D\\n\\x00yfm(x; W(T)\\x01\\n≤0\\n\\x0c\\n\\x0c(x, y) ∈Ωk\\n\\x01\\n≤1/d.\\nWe then prove that, with probability at least 1 −o(1), an example x ∈Ωk will be routed to\\none of the experts in Mk. For x = [αyvk, βck, γϵvk′, ξ], we need to check that hm(x; Θ(T)) <\\nmaxm′ hm′(x; Θ(T)), ∀m ̸∈Mk. By Lemma E.18, we know that ⟨θ(T)\\nm , ck⟩≤maxm′⟨θ(T)\\nm′ , ck⟩−\\n−Ω(K−1M−9). Further by Lemma E.14, we have that maxm,k |⟨θ(T)\\nm , vk⟩| = O(d−0.001). Again to\\ncalculate test error, we need to give an upper bound ⟨θ(T)\\nm , ξp⟩, where ξp is a fresh noise drawn from\\nN(0, (σ2\\np/d) · Id). We can upper bound the gradient of the gating network by\\n∥∇θmL(t)∥2 =\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\nn\\nX\\ni,p\\n1(mi,t = m)ℓ′\\ni,tπmi,t(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n−1\\nn\\nX\\ni,p\\nℓ′\\ni,tπmi,t(xi; Θ(t))πm(xi; Θ(t))yifmi,t(xi; W(t))x(p)\\ni\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n.\\n= e\\nO(1),\\nwhere the last inequality is due to |ℓ′\\ni,t| ≤1, πm, πmi,t ∈[0, 1] and ∥x(p)\\ni ∥2 = O(1). This further\\nimplies that\\n∥θ(T)\\nm ∥2 = ∥θ(T)\\nm −θ(0)\\nm ∥2 ≤e\\nO(tηr) ≤e\\nO(η−1ηr) = e\\nO(1),\\nwhere the last inequality is by ηr = Θ(M2)η. Because the inner product ⟨θ(T)\\nm , ξp⟩follows the\\ndistribution N(0, (σ2\\np/d) · ∥θ(T)\\nm ∥2\\n2), we have that with probability at least 1 −1/(dPM),\\n|⟨θ(T)\\nm , ξp⟩| = O(σpd−1/2∥θ(T)\\nm ∥2 log(dPM)) ≤e\\nO(d−1/2).\\nApplying Union bound for m ∈[M], p ≥4 gives that, with probability at least 1 −1/d,\\n|⟨θ(T)\\nm , ξp⟩| = e\\nO(d−1/2), ∀m ∈[M], p ≥4.\\n(E.26)\\nNow, under the event that (E.26) holds, we have that\\nhm(x; Θ(T)) −max\\nm′ hm′(x; Θ(T))\\n≤⟨θ(T)\\nm , ck⟩−max\\nm′ ⟨θ(T)\\nm′ , ck⟩+ 4 max\\nm,k |⟨θ(T)\\nm , vk⟩| + 4P max\\nm,p |⟨θ(T)\\nm , ξp⟩|\\n≤−Ω(K−1M−9) + e\\nO(d−0.001)\\n< 0.\\nBecause (E.26) holds holds with probability at least 1−1/d, so we have prove that with probability\\nat least 1 −1/d, an example x ∈Ωk will be routed to one of the experts in Mk.\\nTraining Error is zero. The prove for training error is much easier, because we no longer need\\nto deal with the fresh noises and we no longer need to use high probability bound for those inner\\nproducts with fresh noises. That’s the reason we can get exactly zero training error. We ﬁrst prove\\n47\\nthe following result for the experts. For all expert m ∈Mk, we have that\\nyifm(xi; W(T)\\x01\\n≤0, ∀i ∈Ωk.\\nWithout loss of generality, we assume that the feature patch appears in x(1)\\ni . By Lemma E.12,\\nwe have that for all i ∈Ωk\\nyifm(xi, W(T)) = yi\\nX\\nj∈[J]\\nX\\np∈[P]\\nσ(⟨w(T)\\nm,j, x(p)\\ni ⟩)\\n= yiσ(⟨w(T)\\nm,j∗\\nm, αyivk⟩) + yi\\nX\\n(j,p)̸=(j∗\\nm,1)\\nσ(⟨w(T)\\nm,j, x(p)⟩)\\n≥C3\\n1(1 −σ0.1\\n0 )3M−4 −e\\nO(σ3\\n0)\\n> 0,\\nwhere the ﬁrst inequality is Lemma E.12. We then prove that, and example (xi, yi) ∈Ωwill be\\nrouted to one of the experts in Mk. Suppose the m-th expert is not in Mk. We only need to check\\nthe value of hm(xi; Θ(T)) < maxm′ hm′(xi; Θ(T)), which is straight forward by Lemma E.18 and\\nLemma E.14.\\nF\\nAuxiliary Lemmas\\nLemma F.1. Let {am}M\\nm=1 are the random variable i.i.d. drawn from N(0, 1). Deﬁne the non-\\nincreasing sequence of {am}M\\nm=1 as a(1) ≥. . . ≥a(M). Then we have that\\nP(a(2) ≥(1 −G)a(1)) ≤GM2\\nProof. Let Ψ be the CDF of N(0, 1) and let ρ be the PDF of N(0, σ2\\n0). Then we have that,\\nP(a(2) ≥(1 −G)a(1))\\n=\\nZ\\na(1)≥...≥a(M) 1(a(2) ≥(1 −G)a(1))M!Πmρ(a(m))da\\n=\\nZ\\na(1)≥a(2) 1(a(2) ≥(1 −G)a(1))M(M −1)ρ(a(1))ρ(a(2))Ψ(a(2))M−2da(1)da(2)\\n≤\\nZ\\na(1)≥a(2) 1(a(2) ≥(1 −G)a(1))M(M −1)ρ(a(1))\\n1\\n√\\n2πda(1)da(2)\\n=\\nZ\\na(1)≥0\\nGM(M −1)\\n√\\n2π\\na(1)ρ(a(1))da(1)\\n≤GM2.\\nFor normalized gradient descent we have following lemma,\\n48\\nLemma F.2 (Lemma C.19 Allen-Zhu and Li 2020c). Let {xt, yt}t=1,.. be two positive sequences\\nthat satisfy\\nxt+1 ≥xt + η · Ctx2\\nt\\nyt+1 ≤yt + Sη · Cty2\\nt ,\\nand |xt+1 −xt|2 + |yt+1 −yt|2 ≤η2. Suppose x0, y0 = o(1), x0 ≥y0S(1 + G),\\nη ≤min{\\nG2x0\\nlog(A/x0),\\nG2y0\\nlog(1/G)}.\\nThen we have for all A > x0, let Tx be the ﬁrst iteration such that xt ≥A, then we have yTx ≤\\nO(y0G−1).\\nProof. We only need to replace O(ηAq−1) in the proof of Lemma C.19 by O(η), because we use\\nnormalized gradient descent, i.e, Ctx2\\nt ≤1. For completeness, we present the whole poof here.\\nfor all g = 0, 1, 2, . . . ,, let Tg be the ﬁrst iteration such that xt ≥(1+δ)gx0, let b be the smallest\\ninteger such that (1+δ)bx0 ≥A. For simplicity of notation, we replace xt with A whenever xt ≥A.\\nThen by the deﬁnition of Tg, we have that\\nX\\nt∈[Tg,Tg+1)\\nηCt[(1 + δ)gx0]2 ≤xTg+1 −xTg ≤δ(1 + δ)gx0 + O(η),\\nwhere the last inequality holds because we are using normalized gradient descent, i.e., maxt |xt+1 −\\nxt| ≤η. This implies that\\nX\\nt∈[Tg,Tg+1)\\nηCt ≤\\nδ\\n(1 + δ)g\\n1\\nx0\\n+ O(η)\\nx2\\n0\\n.\\nRecall that b is the smallest integer such that (1 + δ)bx0 ≥A, so we can calculate\\nX\\nt≥0,xt≤A\\nηCt ≤\\n\\x14 b−1\\nX\\ng=0\\nδ\\n(1 + δ)g\\n1\\nx0\\n\\x15\\n+ O(η)\\nx2\\n0\\nb = 1 + δ\\nx0\\n+ O(η)b\\nx2\\n0\\n≤1 + δ\\nx0\\n+ O(η) log(A/x0)\\nx2\\n0 log(1 + δ)\\nLet Tx be the ﬁrst iteration t in which xt ≥A. Then we have that\\nTx\\nX\\nt=0\\nηCt ≤1 + δ\\nx0\\n+ O(η) log(A/x0)\\nδx2\\n0\\n.\\n(F.1)\\nOn the other hand, let A′ = G−1y0 and b’ be the smallest integer such that (1 + δ)b′x0 ≥A′. For\\nsimplicity of notation, we replace yt with A′ when yt ≥A′. Then let T ′\\ng be the ﬁrst iteration such\\nthat yt ≥(1 + δ)gy0, then we have that\\nX\\nt∈[T ′\\ng,T ′\\ng+1)\\nηSCt[(1 + δ)g+1x0](q−1) ≥yT ′\\ng+1 −yT ′\\ng ≥δ(1 + δ)gy0 −O(η).\\n49\\nTherefore, we have that\\nX\\nt∈[T ′\\ng,T ′\\ng+1)\\nSηCt ≥\\nδ\\n(1 + δ)g(1 + δ)2\\n1\\ny0\\n−O(η)\\ny2\\n0\\n.\\nRecall that b′ is the smallest integer such that (1 + δ)b′y0 ≥A′. wo we have that\\nX\\nt≥0,xt≤A\\nηSCt ≥\\nb′−2\\nX\\ng=0\\nδ\\n(1 + δ)g(1 + δ)2\\n1\\ny0\\n−O(η)b′\\ny2\\n0\\nLet Ty be the ﬁrst iteration t in which yt ≥A′, so we can calculate\\nTy\\nX\\nt=0\\nηSCt ≥1 −O(δ + G)\\ny0\\n−O(η) log(A′/y0)\\ny2\\n0δ\\n.\\n(F.2)\\nCompare (F.1) and (F.2). Choosing δ = G and η ≤min{\\nG2x0\\nlog(A/x0),\\nG2y0\\nlog(1/G)}, together with x0 ≥\\ny0S(1 + G)\\nReferences\\nAllen-Zhu, Z. and Li, Y. (2019). What can ResNet learn eﬃciently, going beyond kernels? In\\nAdvances in Neural Information Processing Systems.\\nAllen-Zhu, Z. and Li, Y. (2020a). Backward feature correction: How deep learning performs\\ndeep learning. arXiv preprint arXiv:2001.04413 .\\nAllen-Zhu, Z. and Li, Y. (2020b). Feature puriﬁcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 .\\nAllen-Zhu, Z. and Li, Y. (2020c). Towards understanding ensemble, knowledge distillation and\\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816 .\\nAllen-Zhu, Z., Li, Y. and Liang, Y. (2019a). Learning and generalization in overparameterized\\nneural networks, going beyond two layers. In Advances in Neural Information Processing Systems.\\nAllen-Zhu, Z., Li, Y. and Song, Z. (2019b). A convergence theory for deep learning via over-\\nparameterization. In International Conference on Machine Learning.\\nAnandkumar, A., Ge, R., Hsu, D., Kakade, S. M. and Telgarsky, M. (2014).\\nTensor\\ndecompositions for learning latent variable models. Journal of machine learning research 15\\n2773–2832.\\nAnandkumar, A., Hsu, D. and Kakade, S. M. (2012). A method of moments for mixture\\nmodels and hidden markov models. In Conference on Learning Theory. JMLR Workshop and\\nConference Proceedings.\\n50\\nArora, S., Du, S., Hu, W., Li, Z. and Wang, R. (2019a). Fine-grained analysis of optimization\\nand generalization for overparameterized two-layer neural networks. In International Conference\\non Machine Learning.\\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. and Wang, R. (2019b). On exact\\ncomputation with an inﬁnitely wide neural net. In Advances in Neural Information Processing\\nSystems.\\nBai, Y. and Lee, J. D. (2019). Beyond linearization: On quadratic and higher-order approxima-\\ntion of wide neural networks. arXiv preprint arXiv:1910.01619 .\\nBalakrishnan, S., Wainwright, M. J. and Yu, B. (2017). Statistical guarantees for the em\\nalgorithm: From population to sample-based analysis. The Annals of Statistics 45 77–120.\\nBlard, T. (2020). French sentiment analysis with bert. https://github.com/TheophileBlard/\\nfrench-sentiment-analysis-with-bert.\\nCao, Y., Chen, Z., Belkin, M. and Gu, Q. (2022). Benign overﬁtting in two-layer convolutional\\nneural networks. arXiv preprint arXiv:2202.06526 .\\nCao, Y. and Gu, Q. (2019). Generalization bounds of stochastic gradient descent for wide and\\ndeep neural networks. In Advances in Neural Information Processing Systems.\\nChaganty, A. T. and Liang, P. (2013).\\nSpectral experts for estimating mixtures of linear\\nregressions. In International Conference on Machine Learning. PMLR.\\nCollobert, R., Bengio, S. and Bengio, Y. (2002). A parallel mixture of svms for very large\\nscale problems. Neural computation 14 1105–1114.\\nDauphin, Y. N., Fan, A., Auli, M. and Grangier, D. (2017). Language modeling with gated\\nconvolutional networks. In International conference on machine learning. PMLR.\\nDe Veaux, R. D. (1989). Mixtures of linear regressions. Computational Statistics & Data Analysis\\n8 227–245.\\nDevlin, J., Chang, M., Lee, K. and Toutanova, K. (2018).\\nBERT: pre-training of deep\\nbidirectional transformers for language understanding. CoRR abs/1810.04805.\\nDu, S. S., Zhai, X., Poczos, B. and Singh, A. (2019). Gradient descent provably optimizes\\nover-parameterized neural networks. In International Conference on Learning Representations.\\nDua, D., Bhosale, S., Goswami, V., Cross, J., Lewis, M. and Fan, A. (2021). Tricks for\\ntraining sparse translation models. arXiv preprint arXiv:2110.08246 .\\nEigen, D., Ranzato, M. and Sutskever, I. (2013). Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314 .\\nFaria, S. and Soromenho, G. (2010). Fitting mixtures of linear regressions. Journal of Statistical\\nComputation and Simulation 80 201–225.\\nFedus, W., Zoph, B. and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter\\nmodels with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961 .\\n51\\nGo, A., Bhayani, R. and Huang, L. (2009).\\nTwitter sentiment classiﬁcation using distant\\nsupervision. CS224N project report, Stanford 1 2009.\\nHe, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image recognition.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\nHsu, D. J., Kakade, S. M. and Liang, P. S. (2012). Identiﬁability and unmixing of latent parse\\ntrees. Advances in neural information processing systems 25.\\nJacobs, R. A., Jordan, M. I., Nowlan, S. J. and Hinton, G. E. (1991). Adaptive mixtures\\nof local experts. Neural computation 3 79–87.\\nJacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and\\ngeneralization in neural networks. In Advances in neural information processing systems.\\nJelassi, S., Mensch, A., Gidel, G. and Li, Y. (2021). Adam is no better than normalized sgd:\\nDissecting how adaptivity improves gan performance .\\nJordan, M. I., Ghahramani, Z. and Saul, L. K. (1997). Hidden markov decision trees. Advances\\nin neural information processing systems 501–507.\\nJordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the em algorithm.\\nNeural computation 6 181–214.\\nKhalili, A. and Chen, J. (2007). Variable selection in ﬁnite mixture of regression models. Journal\\nof the american Statistical association 102 1025–1038.\\nKrizhevsky, A. (2009). Learning multiple layers of features from tiny images. Tech. rep.\\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N. and Zettlemoyer, L. (2021). Base layers:\\nSimplifying training of large, sparse models. In International Conference on Machine Learning.\\nPMLR.\\nLi, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems.\\nLi, Y., Ma, T. and Zhang, H. R. (2020). Learning over-parametrized two-layer neural networks\\nbeyond ntk. In Conference on learning theory. PMLR.\\nLiang, P., Bouchard-Cˆ\\not´\\ne, A., Klein, D. and Taskar, B. (2006). An end-to-end discrim-\\ninative approach to machine translation. In Proceedings of the 21st International Conference\\non Computational Linguistics and 44th Annual Meeting of the Association for Computational\\nLinguistics.\\nQuattoni, A., Collins, M. and Darrell, T. (2004).\\nConditional random ﬁelds for object\\nrecognition. Advances in neural information processing systems 17.\\nRoller, S., Sukhbaatar, S., Weston, J. et al. (2021). Hash layers for large sparse models.\\nAdvances in Neural Information Processing Systems 34 17555–17566.\\n52\\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A. and Chen, L.-C. (2018). Mobilenetv2:\\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J.\\n(2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv\\npreprint arXiv:1701.06538 .\\nSmetanin, S. and Komarov, M. (2019). Sentiment analysis of product reviews in russian using\\nconvolutional neural networks. In 2019 IEEE 21st conference on business informatics (CBI),\\nvol. 1. IEEE.\\nTresp, V. (2001). Mixtures of gaussian processes. Advances in neural information processing\\nsystems 654–660.\\nVan der Maaten, L. and Hinton, G. (2008). Visualizing data using t-sne. Journal of machine\\nlearning research 9.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\\n \\nL. and Polosukhin, I. (2017). Attention is all you need. In Advances in neural information\\nprocessing systems.\\nVecci, L., Piazza, F. and Uncini, A. (1998). Learning and approximation capabilities of adaptive\\nspline activation function neural networks. Neural Networks 11 259–270.\\nWang, Y. and Mori, G. (2009). Max-margin hidden conditional random ﬁelds for human action\\nrecognition. In 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE.\\nWang, Z., Gu, Q., Ning, Y. and Liu, H. (2015). High dimensional em algorithm: Statistical\\noptimization and asymptotic normality. Advances in neural information processing systems 28.\\nWen, Z. and Li, Y. (2021). Toward understanding the feature learning process of self-supervised\\ncontrastive learning. In International Conference on Machine Learning. PMLR.\\nYi, X., Caramanis, C. and Sanghavi, S. (2014). Alternating minimization for mixed linear\\nregression. In International Conference on Machine Learning. PMLR.\\nZou, D., Cao, Y., Li, Y. and Gu, Q. (2021).\\nUnderstanding the generalization of adam in\\nlearning neural networks with proper regularization. arXiv preprint arXiv:2108.11371 .\\nZou, D., Cao, Y., Zhou, D. and Gu, Q. (2018). Stochastic gradient descent optimizes over-\\nparameterized deep relu networks. arXiv preprint arXiv:1811.08888 .\\n53\\n', 'source_name': 'Towards Understanding MoE', 'source_url': 'https://arxiv.org/abs/2208.02813'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mixture of Experts Explained_HF_NOTES.pdf #72\n",
      "{'content': 'MoE articles \\nThe original MoE had 3 components: \\n- \\nExperts, specialized models which are either regressors or classifiers. \\n- \\nManager (router), gate mechanism (like a softmax, for example) which decides in which \\narea(s) of the input space each expert is trustworthy. \\n- \\nProbabilistic model, which combines the expert and the manager. It joins the experts’ \\nGaussian distributions (outputs) together based on the probability given by the manager. \\nY = summation of pi (probability given to expert I by the manager) * yi (output of the \\nexpert), for all experts. \\nThis forms a fully differentiable dense ensemble of all experts with no inference speedup, as no \\nexpert computation is discarded. \\nLarge dense neural networks are not efficient scaling in terms of training costs. Conditional \\ncomputation models (sparse models) can provide advantages, but have their downfalls, such as \\nthe computational limitations of training such models (GPUs and TPUs are optimized for large \\nmatrix-matrix multiplication). \\n“Sparsely-Gated MoE Layer” tries to propose a solution to MoE’s computation issues. When \\ntraining an MoE model the deep learning way, the input is passed through the router the same \\nway as the original MoE method, however, the router only sends the input signal through to the \\ntop-k selected experts (a discrete choice, not fully differentiable), and uses the scores given by \\nthe router as weights of each expert’s output on the final output. The final output is then a \\ncombination of the top-k experts’ outputs weighted by their respective router score. \\nThis deep learning approach has numerous potential problems: \\n- \\nIf one expert gets ahead and generalizes well fast, the router might send most of the data \\nto this expert, overfitting and undertraining others while not specializing on anything. \\nTherefore, training between experts needs to be somewhat uniform. \\no Common approaches to fix this are adding random noise to the router’s \\nprobabilities (scores given to experts) in order to create some randomness in the \\nselection of experts’ process, especially in early stages of training (although we \\ndon’t want this to be fully random, since it will prevent specialization) to ensure \\nthat worse performing experts are still randomly picked for updates; adding a \\npenalty term for uneven router choice to the loss function so the router has \\nmotivation to distribute its picks in a more uniform manner. This means the loss \\nwould look like: loss = cross-entropy loss + auxiliary loss, where auxiliary loss \\nrepresents the penalty term for uneven distribution. \\nThis sparse approach is promising in some ways as it provides computational efficiency for \\ninference (only the selected expert weights are a part of the computation). So given 8 experts of \\n100M parameters each and a dense model of 800M parameters, a forward pass on the MoE \\nmodel using k=2 would only trigger 2*100M=200M parameters, while the dense model would \\nalways activate all 800M parameters (in reality, shared parameters should be accounted as well \\nin MoE, but this is not mentioned here for simplicity). In theory, the quality of these 2 models \\nshould be roughly the same since they both have the same number of total parameters available \\n(800M). \\nOn another hand, due to the need to balance loads through the router function, MoE can be a \\nbit slower to train. That is, the random noise and auxiliary loss to help with router uniformity \\nbetween experts can slow down training due to data being sent and updated on suboptimal \\nplaces. Due to its parameter efficiency, MoE has the potential to provide significant speed ups on \\ntraining steps, but due to challenges such as load balancing and communication costs incurred \\nby MoE, the cost of each step tends to be larger, so each training step takes longer. Therefore, \\nwhen comparing training speed-ups between sparse and dense models, it is important to \\nconsider both training steps and training time. \\n \\n', 'source_name': 'HuggingFace MoE Article', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Mixture%20of%20Experts%20Explained_HF_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sparsely_Gated_MoE_NOTES.pdf #73\n",
      "{'content': 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer \\n \\nMain Idea: \\n- \\nTo propose a way to improve model capacity, training time and model quality through a \\nconditional computation approach that alternates between dense LSTM and MoE blocks. \\nApproach: \\n- \\nIntroduces a new neural network component (a new block/layer) which consists of: \\no n experts, each a feed-forward neural network \\no a trainable gating network, which selects a sparse combination of these experts \\nto process each input token given. \\n- \\nThe gating network presented is an improvement over the standard approach, which \\ntrains a weight matrix to give score to an input x and pass that to a softmax (gating output \\n𝐺(𝑥) = 𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑊\\n𝑔∗𝑥). The gating mechanism proposed is called noisy top-k routing, \\nwhich adds noise and sparsity: \\no Gaussian noise is added before taking the softmax to help with load balancing \\nbetween experts during training. \\n▪ 𝐻(𝑥) = (𝑊\\n𝑔∗𝑥) + 𝑆𝑡𝑎𝑛𝑑𝑎𝑟𝑑𝑁𝑜𝑟𝑚𝑎𝑙() ∗𝑆𝑜𝑓𝑡𝑃𝑙𝑢𝑠((𝑊\\n𝑛𝑜𝑖𝑠𝑒∗𝑥)𝑖) \\no Sparsity is added by taking only the top k scores given by the gating mechanism. \\n▪ 𝐺(𝑥) = 𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝐾𝑒𝑒𝑝𝑇𝑜𝑝𝐾(𝐻(𝑥), 𝑘) \\n▪ If not in the top k, H(x) becomes -inf so it is not considered in the final \\noutput. \\n- \\nTo balance expert utilization, an auxiliary term to the loss is added, which works by being \\ncomputed at a batch level. \\no For each expert and the training batch X, take the expert’s importance in the \\nbatch: \\n▪ 𝐼𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒(𝑋) = ∑\\n𝐺(𝑥)\\n𝑥 ∈𝑋\\n \\n▪ Importance(X)e = sum of all the expert’s G(x) for the batch \\no The term Limportance is added to the loss (which will be computed at the batch \\nlevel) to encourage all experts to have equal importance: \\n▪ 𝐿𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒(𝑋) = 𝑊\\n𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒∗𝑉(𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒(𝑋) \\n▪ 𝑊\\n𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒 is a hand-tuned scaling factor and V is the coefficient of \\nvariation squared. \\n- \\nThe final network consists of alternating LSTM blocks with these new MoE blocks. \\nMy takeaways: \\n- \\nThis approach means that for the first time MoE was used as a network component and \\nnot as the network itself, providing a method to integrate it with dense layers. \\n- \\nIntroduced top-k routing. \\n- \\nExperiments showed that experts tend to become specialized on syntax and semantics, \\nwhich is an important follow-up to the findings of the “Learning Factored \\nRepresentations…” paper which hinted that different experts specialize in different \\nclusters of the data. \\n- \\nThis paper also provides advancements in load balancing, crafting an auxiliary loss term \\nfor load balancing that seems much more effective than the previous method of pausing \\nthe training of highly utilized experts. \\n \\n', 'source_name': 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Sparsely_Gated_MoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sparsely_Gated_MoE.pdf #74\n",
      "{'content': 'Under review as a conference paper at ICLR 2017\\nOUTRAGEOUSLY LARGE NEURAL NETWORKS:\\nTHE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER\\nNoam Shazeer1, Azalia Mirhoseini∗\\n†\\n1, Krzysztof Maziarz∗2, Andy Davis1, Quoc Le1, Geoffrey\\nHinton1 and Jeff Dean1\\n1Google Brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com\\n2Jagiellonian University, Cracow, krzysztof.maziarz@student.uj.edu.pl\\nABSTRACT\\nThe capacity of a neural network to absorb information is limited by its number of\\nparameters. Conditional computation, where parts of the network are active on a\\nper-example basis, has been proposed in theory as a way of dramatically increas-\\ning model capacity without a proportional increase in computation. In practice,\\nhowever, there are signiﬁcant algorithmic and performance challenges. In this\\nwork, we address these challenges and ﬁnally realize the promise of conditional\\ncomputation, achieving greater than 1000x improvements in model capacity with\\nonly minor losses in computational efﬁciency on modern GPU clusters. We in-\\ntroduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to\\nthousands of feed-forward sub-networks. A trainable gating network determines\\na sparse combination of these experts to use for each example. We apply the MoE\\nto the tasks of language modeling and machine translation, where model capacity\\nis critical for absorbing the vast quantities of knowledge available in the training\\ncorpora. We present model architectures in which a MoE with up to 137 billion\\nparameters is applied convolutionally between stacked LSTM layers. On large\\nlanguage modeling and machine translation benchmarks, these models achieve\\nsigniﬁcantly better results than state-of-the-art at lower computational cost.\\n1\\nINTRODUCTION AND RELATED WORK\\n1.1\\nCONDITIONAL COMPUTATION\\nExploiting scale in both training data and model size has been central to the success of deep learn-\\ning. When datasets are sufﬁciently large, increasing the capacity (number of parameters) of neural\\nnetworks can give much better prediction accuracy. This has been shown in domains such as text\\n(Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), images\\n(Krizhevsky et al., 2012; Le et al., 2012), and audio (Hinton et al., 2012; Amodei et al., 2015). For\\ntypical deep learning models, where the entire model is activated for every example, this leads to\\na roughly quadratic blow-up in training costs, as both the model size and the number of training\\nexamples increase. Unfortunately, the advances in computing power and distributed computation\\nfall short of meeting such demand.\\nVarious forms of conditional computation have been proposed as a way to increase model capacity\\nwithout a proportional increase in computational costs (Davis & Arel, 2013; Bengio et al., 2013;\\nEigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi\\net al., 2015). In these schemes, large parts of a network are active or inactive on a per-example\\nbasis. The gating decisions may be binary or sparse and continuous, stochastic or deterministic.\\nVarious forms of reinforcement learning and back-propagation are proposed for trarining the gating\\ndecisions.\\n∗Equally major contributors\\n†Work done as a member of the Google Brain Residency program (g.co/brainresidency)\\n1\\narXiv:1701.06538v1  [cs.LG]  23 Jan 2017\\nUnder review as a conference paper at ICLR 2017\\nFigure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In this\\ncase, the sparse gating function selects two experts to perform computations. Their outputs are\\nmodulated by the outputs of the gating network.\\nWhile these ideas are promising in theory, no work to date has yet demonstrated massive improve-\\nments in model capacity, training time, or model quality. We blame this on a combination of the\\nfollowing challenges:\\n• Modern computing devices, especially GPUs, are much faster at arithmetic than at branch-\\ning. Most of the works above recognize this and propose turning on/off large chunks of the\\nnetwork with each gating decision.\\n• Large batch sizes are critical for performance, as they amortize the costs of parameter trans-\\nfers and updates. Conditional computation reduces the batch sizes for the conditionally\\nactive chunks of the network.\\n• Network bandwidth can be a bottleneck. A cluster of GPUs may have computational power\\nthousands of times greater than the aggregate inter-device network bandwidth. To be com-\\nputationally efﬁcient, the relative computational versus network demands of an algorithm\\nmust exceed this ratio. Embedding layers, which can be seen as a form of conditional com-\\nputation, are handicapped by this very problem. Since the embeddings generally need to\\nbe sent across the network, the number of (example, parameter) interactions is limited by\\nnetwork bandwidth instead of computational capacity.\\n• Depending on the scheme, loss terms may be necessary to achieve the desired level of\\nsparsity per-chunk and/or per example. Bengio et al. (2015) use three such terms. These\\nissues can affect both model quality and load-balancing.\\n• Model capacity is most critical for very large data sets. The existing literature on condi-\\ntional computation deals with relatively small image recognition data sets consisting of up\\nto 600,000 images. It is hard to imagine that the labels of these images provide a sufﬁcient\\nsignal to adequately train a model with millions, let alone billions of parameters.\\nIn this work, we for the ﬁrst time address all of the above challenges and ﬁnally realize the promise\\nof conditional computation. We obtain greater than 1000x improvements in model capacity with\\nonly minor losses in computational efﬁciency and signiﬁcantly advance the state-of-the-art results\\non public language modeling and translation data sets.\\n1.2\\nOUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER\\nOur approach to conditional computation is to introduce a new type of general purpose neural net-\\nwork component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a num-\\nber of experts, each a simple feed-forward neural network, and a trainable gating network which\\nselects a sparse combination of the experts to process each input (see Figure 1). All parts of the\\nnetwork are trained jointly by back-propagation.\\n2\\nUnder review as a conference paper at ICLR 2017\\nWhile the introduced technique is generic, in this paper we focus on language modeling and machine\\ntranslation tasks, which are known to beneﬁt from very large models. In particular, we apply a MoE\\nconvolutionally between stacked LSTM layers (Hochreiter & Schmidhuber, 1997), as in Figure 1.\\nThe MoE is called once for each position in the text, selecting a potentially different combination\\nof experts at each position. The different experts tend to become highly specialized based on syntax\\nand semantics (see Appendix E Table 9). On both language modeling and machine translation\\nbenchmarks, we improve on best published results at a fraction of the computational cost.\\n1.3\\nRELATED WORK ON MIXTURES OF EXPERTS\\nSince its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994),\\nthe mixture-of-experts approach has been the subject of much research. Different types of expert\\narchitectures hae been proposed such as SVMs (Collobert et al., 2002), Gaussian Processes (Tresp,\\n2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009),\\nand deep networks. Other work has focused on different expert conﬁgurations such as a hierarchical\\nstructure (Yao et al., 2009), inﬁnite numbers of experts (Rasmussen & Ghahramani, 2002), and\\nadding experts sequentially (Aljundi et al., 2016). Garmash & Monz (2016) suggest an ensemble\\nmodel in the format of mixture of experts for machine translation. The gating network is trained on\\na pre-trained ensemble NMT model.\\nThe works above concern top-level mixtures of experts. The mixture of experts is the whole model.\\nEigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as\\nparts of a deep model. It is intuitive that the latter approach is more powerful, since complex prob-\\nlems may contain many sub-problems each requiring different experts. They also allude in their\\nconclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational\\ncomputation.\\nOur work builds on this use of MoEs as a general purpose neural network component. While Eigen\\net al. (2013) uses two stacked MoEs allowing for two sets of gating decisions, our convolutional\\napplication of the MoE allows for different gating decisions at each position in the text. We also\\nrealize sparse gating and demonstrate its use as a practical way to massively increase model capacity.\\n2\\nTHE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER\\nThe Mixture-of-Experts (MoE) layer consists of a set of n “expert networks\" E1, · · · , En, and a\\n“gating network\" G whose output is a sparse n-dimensional vector. Figure 1 shows an overview\\nof the MoE module. The experts are themselves neural networks, each with their own parameters.\\nAlthough in principle we only require that the experts accept the same sized inputs and produce the\\nsame-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where\\nthe models are feed-forward networks with identical architectures, but with separate parameters.\\nLet us denote by G(x) and Ei(x) the output of the gating network and the output of the i-th expert\\nnetwork for a given input x. The output y of the MoE module can be written as follows:\\ny =\\nn\\nX\\ni=1\\nG(x)iEi(x)\\n(1)\\nWe save computation based on the sparsity of the output of G(x). Wherever G(x)i = 0, we need not\\ncompute Ei(x). In our experiments, we have up to thousands of experts, but only need to evaluate\\na handful of them for every example. If the number of experts is very large, we can reduce the\\nbranching factor by using a two-level hierarchical MoE. In a hierarchical MoE, a primary gating\\nnetwork chooses a sparse weighted combination of “experts\", each of which is itself a secondary\\nmixture-of-experts with its own gating network. In the following we focus on ordinary MoEs. We\\nprovide more details on hierarchical MoEs in Appendix B.\\nOur implementation is related to other models of conditional computation. A MoE whose experts are\\nsimple weight matrices is similar to the parameterized weight matrix proposed in (Cho & Bengio,\\n2014). A MoE whose experts have one hidden layer is similar to the block-wise dropout described\\nin (Bengio et al., 2015), where the dropped-out layer is sandwiched between fully-activated layers.\\n3\\nUnder review as a conference paper at ICLR 2017\\n2.1\\nGATING NETWORK\\nSoftmax Gating:\\nA simple choice of non-sparse gating function (Jordan & Jacobs, 1994) is to\\nmultiply the input by a trainable weight matrix Wg and then apply the Softmax function.\\nGσ(x) = Softmax(x · Wg)\\n(2)\\nNoisy Top-K Gating:\\nWe add two components to the Softmax gating network: sparsity and noise.\\nBefore taking the softmax function, we add tunable Gaussian noise, then keep only the top k values,\\nsetting the rest to −∞(which causes the corresponding gate values to equal 0). The sparsity serves\\nto save computation, as described above. While this form of sparsity creates some theoretically\\nscary discontinuities in the output of gating function, we have not yet observed this to be a problem\\nin practice. The noise term helps with load balancing, as will be discussed in Appendix A. The\\namount of noise per component is controlled by a second trainable weight matrix Wnoise.\\nG(x) = Softmax(KeepTopK(H(x), k))\\n(3)\\nH(x)i = (x · Wg)i + StandardNormal() · Softplus((x · Wnoise)i)\\n(4)\\nKeepTopK(v, k)i =\\n\\x1avi\\nif vi is in the top k elements of v.\\n−∞\\notherwise.\\n(5)\\nTraining the Gating Network\\nWe train the gating network by simple back-propagation, along\\nwith the rest of the model. If we choose k > 1, the gate values for the top k experts have nonzero\\nderivatives with respect to the weights of the gating network. This type of occasionally-sensitive\\nbehavior is described in (Bengio et al., 2013) with respect to noisy rectiﬁers. Gradients also back-\\npropagate through the gating network to its inputs. Our method differs here from (Bengio et al.,\\n2015) who use boolean gates and a REINFORCE-style approach to train the gating network.\\n3\\nADDRESSING PERFORMANCE CHALLENGES\\n3.1\\nTHE SHRINKING BATCH PROBLEM\\nOn modern CPUs and GPUs, large batch sizes are necessary for computational efﬁciency, so as\\nto amortize the overhead of parameter loads and updates. If the gating network chooses k out of\\nn experts for each example, then for a batch of b examples, each expert receives a much smaller\\nbatch of approximately kb\\nn ≪b examples. This causes a naive MoE implementation to become\\nvery inefﬁcient as the number of experts increases. The solution to this shrinking batch problem is\\nto make the original batch size as large as possible. However, batch size tends to be limited by the\\nmemory necessary to store activations between the forwards and backwards passes. We propose the\\nfollowing techniques for increasing the batch size:\\nMixing Data Parallelism and Model Parallelism:\\nIn a conventional distributed training setting,\\nmultiple copies of the model on different devices asynchronously process distinct batches of data,\\nand parameters are synchronized through a set of parameter servers. In our technique, these different\\nbatches run synchronously so that they can be combined for the MoE layer. We distribute the\\nstandard layers of the model and the gating network according to conventional data-parallel schemes,\\nbut keep only one shared copy of each expert. Each expert in the MoE layer receives a combined\\nbatch consisting of the relevant examples from all of the data-parallel input batches. The same set\\nof devices function as data-parallel replicas (for the standard layers and the gating networks) and\\nas model-parallel shards (each hosting a subset of the experts). If the model is distributed over d\\ndevices, and each device processes a batch of size b, each expert receives a batch of approximately\\nkbd\\nn examples. Thus, we achieve a factor of d improvement in expert batch size.\\nIn the case of a hierarchical MoE (Section B), the primary gating network employs data parallelism,\\nand the secondary MoEs employ model parallelism. Each secondary MoE resides on one device.\\n4\\nUnder review as a conference paper at ICLR 2017\\nThis technique allows us to increase the number of experts (and hence the number of parameters) by\\nproportionally increasing the number of devices in the training cluster. The total batch size increases,\\nkeeping the batch size per expert constant. The memory and bandwidth requirements per device also\\nremain constant, as do the step times, as does the amount of time necessary to process a number of\\ntraining examples equal to the number of parameters in the model. It is our goal to train a trillion-\\nparameter model on a trillion-word corpus. We have not scaled our systems this far as of the writing\\nof this paper, but it should be possible by adding more hardware.\\nTaking Advantage of Convolutionality:\\nIn our language models, we apply the same MoE to each\\ntime step of the previous layer. If we wait for the previous layer to ﬁnish, we can apply the MoE\\nto all the time steps together as one big batch. Doing so increases the size of the input batch to the\\nMoE layer by a factor of the number of unrolled time steps.\\nIncreasing Batch Size for a Recurrent MoE:\\nWe suspect that even more powerful models may\\ninvolve applying a MoE recurrently. For example, the weight matrices of a LSTM or other RNN\\ncould be replaced by a MoE. Sadly, such models break the convolutional trick from the last para-\\ngraph, since the input to the MoE at one timestep depends on the output of the MoE at the previous\\ntimestep. Gruslys et al. (2016) describe a technique for drastically reducing the number of stored\\nactivations in an unrolled RNN, at the cost of recomputing forward activations. This would allow\\nfor a large increase in batch size.\\n3.2\\nNETWORK BANDWIDTH\\nAnother major performance concern in distributed computing is network bandwidth. Since the ex-\\nperts are stationary (see above) and the number of gating parameters is small, most of the communi-\\ncation involves sending the inputs and outputs of the experts across the network. To maintain com-\\nputational efﬁciency, the ratio of an expert’s computation to the size of its input and output must ex-\\nceed the ratio of computational to network capacity of the computing device. For GPUs, this may be\\nthousands to one. In our experiments, we use experts with one hidden layer containing thousands of\\nRELU-activated units. Since the weight matrices in the expert have sizes input_size×hidden_size\\nand hidden_size × output_size, the ratio of computation to input and output is equal to the size of\\nthe hidden layer. Conveniently, we can increase computational efﬁciency simply by using a larger\\nhidden layer, or more hidden layers.\\n4\\nBALANCING EXPERT UTILIZATION\\nWe have observed that the gating network tends to converge to a state where it always produces\\nlarge weights for the same few experts. This imbalance is self-reinforcing, as the favored experts\\nare trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013)\\ndescribe the same phenomenon, and use a hard constraint at the beginning of training to avoid this\\nlocal minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each\\ngate.1\\nWe take a soft constraint approach. We deﬁne the importance of an expert relative to a batch of\\ntraining examples to be the batchwise sum of the gate values for that expert. We deﬁne an additional\\nloss Limportance, which is added to the overall loss function for the model. This loss is equal to\\nthe square of the coefﬁcient of variation of the set of importance values, multiplied by a hand-tuned\\nscaling factor wimportance. This additional loss encourages all experts to have equal importance.\\nImportance(X) =\\nX\\nx∈X\\nG(x)\\n(6)\\nLimportance(X) = wimportance · CV (Importance(X))2\\n(7)\\n1Bengio et al. (2015) also include two additional losses. One controls per-example sparsity, which we do\\nnot need since it is enforced by the ﬁxed value of k. A third loss encourages diversity of gate values. In our\\nexperiments, we ﬁnd that the gate values naturally diversify as the experts specialize (in a virtuous cycle), and\\nwe do not need to enforce diversity of gate values.\\n5\\nUnder review as a conference paper at ICLR 2017\\nWhile this loss function can ensure equal importance, experts may still receive very different num-\\nbers of examples. For example, one expert may receive a few examples with large weights, and\\nanother may receive many examples with small weights. This can cause memory and performance\\nproblems on distributed hardware. To solve this problem, we introduce a second loss function,\\nLload , which ensures balanced loads. Appendix A contains the deﬁnition of this function, along\\nwith experimental results.\\n5\\nEXPERIMENTS\\n5.1\\n1 BILLION WORD LANGUAGE MODELING BENCHMARK\\nDataset:\\nThis dataset, introduced by (Chelba et al., 2013) consists of shufﬂed unique sentences\\nfrom news articles, totaling approximately 829 million words, with a vocabulary of 793,471 words.\\nPrevious State-of-the-Art:\\nThe best previously published results (Jozefowicz et al., 2016) use\\nmodels consisting of one or more stacked Long Short-Term Memory (LSTM) layers (Hochreiter\\n& Schmidhuber, 1997; Gers et al., 2000). The number of parameters in the LSTM layers of these\\nmodels vary from 2 million to 151 million. Quality increases greatly with parameter count, as do\\ncomputational costs. Results for these models form the top line of Figure 2-right.\\nMoE Models:\\nOur models consist of two stacked LSTM layers with a MoE layer between them\\n(see Figure 1). We vary the sizes of the layers and the number of experts. For full details on model\\narchitecture, training regimen, additional baselines and results, see Appendix C.\\nLow Computation, Varied Capacity:\\nTo investigate the effects of adding capacity, we trained\\na series of MoE models all with roughly equal computational costs: about 8 million multiply-and-\\nadds per training example per timestep in the forwards pass, excluding the softmax layer. We call\\nthis metric (ops/timestep). We trained models with ﬂat MoEs containing 4, 32, and 256 experts, and\\nmodels with hierarchical MoEs containing 256, 1024, and 4096 experts. Each expert had about 1\\nmillion parameters. For all the MoE layers, 4 experts were active per input.\\nThe results of these models are shown in Figure 2-left. The model with 4 always-active experts\\nperformed (unsurprisingly) similarly to the computationally-matched baseline models, while the\\nlargest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set.\\nFigure 2: Model comparison on 1-Billion-Word Language-Modeling Benchmark. On the left, we\\nplot test perplexity as a function of model capacity for models with similar computational budgets\\nof approximately 8-million-ops-per-timestep. On the right, we plot test perplexity as a function of\\ncomputational budget. The top line represents the LSTM models from (Jozefowicz et al., 2016).\\nThe bottom line represents 4-billion parameter MoE models with different computational budgets.\\nVaried Computation, High Capacity:\\nIn addition to the largest model from the previous section,\\nwe trained two more MoE models with similarly high capacity (4 billion parameters), but higher\\ncomputation budgets. These models had larger LSTMs, and fewer but larger and experts. Details\\n6\\nUnder review as a conference paper at ICLR 2017\\nTable 1: Summary of high-capacity MoE-augmented models with varying computational budgets,\\nvs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.\\nTest\\nTest\\n#Parameters\\nops/timestep\\nTraining\\nTFLOPS\\nPerplexity\\nPerplexity\\nexcluding embedding\\nTime\\n/GPU\\n10 epochs 100 epochs\\nand softmax layers\\n10 epochs\\nBest Published Results\\n34.7\\n30.6\\n151 million\\n151 million\\n59 hours, 32 k40s\\n1.09\\nLow-Budget MoE Model\\n34.1\\n4303 million\\n8.9 million\\n15 hours, 16 k40s\\n0.74\\nMedium-Budget MoE Model\\n31.3\\n4313 million\\n33.8 million\\n17 hours, 32 k40s\\n1.22\\nHigh-Budget MoE Model\\n28.0\\n4371 million\\n142.7 million 47 hours, 32 k40s\\n1.56\\ncan be found in Appendix C.2. Results of these three models form the bottom line of Figure 2-right.\\nTable 1 compares the results of these models to the best previously-published result on this dataset .\\nEven the fastest of these models beats the best published result (when controlling for the number of\\ntraining epochs), despite requiring only 6% of the computation.\\nComputational Efﬁciency:\\nWe trained our models using TensorFlow (Abadi et al., 2016) on clus-\\nters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational efﬁ-\\nciency in TFLOPS/GPU by dividing the number of ﬂoating point operations required to process\\none training batch by the observed step time and the number of GPUs in the cluster. The operation\\ncounts used here are higher than the ones we report in our ops/timestep numbers in that we include\\nthe backwards pass, we include the importance-sampling-based training of the softmax layer, and\\nwe count a multiply-and-add as two separate operations. For all of our MoE models, the ﬂoating\\npoint operations involved in the experts represent between 37% and 46% of the total.\\nFor our baseline models wtih no MoE, observed computational efﬁciency ranged from 1.07-1.29\\nTFLOPS/GPU. For our low-computation MoE models, computation efﬁciency ranged from 0.74-\\n0.90 TFLOPS/GPU, except for the 4-expert model which did not make full use of the available\\nparallelism. Our highest-computation MoE model was more efﬁcient at 1.56 TFLOPS/GPU, likely\\ndue to the larger matrices. These numbers represent a signiﬁcant fraction of the theoretical maximum\\nof 4.29 TFLOPS/GPU claimed by NVIDIA. Detailed results are in Appendix C, Table 7.\\n5.2\\n100 BILLION WORD GOOGLE NEWS CORPUS\\nFigure 3: Language modeling on a 100 billion word corpus. Models have similar computational\\nbudgets (8 million ops/timestep).\\nOn the 1-billion-word corpus, adding additional capacity seems to produce diminishing returns as\\nthe number of parameters in the MoE layer exceeds 1 billion, as can be seen in Figure 2-left. We\\nhypothesized that for a larger training set, even higher capacities would produce signiﬁcant quality\\nimprovements.\\nWe constructed a similar training set consisting of shufﬂed unique sentences from Google’s internal\\nnews corpus, totalling roughly 100 billion words. Similarly to the previous section, we tested a\\nseries of models with similar computational costs of about 8 million ops/timestep. In addition to a\\nbaseline LSTM model, we trained models augmented with MoE layers containing 32, 256, 1024,\\n7\\nUnder review as a conference paper at ICLR 2017\\n4096, 16384, 65536, and 131072 experts. This corresponds to up to 137 billion parameters in the\\nMoE layer. Details on architecture, training, and results are given in Appendix D.\\nResults:\\nFigure 3 shows test perplexity as a function of capacity after training on 10 billion words\\n(top line) and 100 billion words (bottom line). When training over the full 100 billion words, test\\nperplexity improves signiﬁcantly up to 65536 experts (68 billion parameters), dropping 39% lower\\nthan the computationally matched baseline, but degrades at 131072 experts, possibly a result of too\\nmuch sparsity. The widening gap between the two lines demonstrates (unsurprisingly) that increased\\nmodel capacity helps more on larger training sets.\\nEven at 65536 experts (99.994% layer sparsity), computational efﬁciency for the model stays at a\\nrespectable 0.72 TFLOPS/GPU.\\n5.3\\nMACHINE TRANSLATION (SINGLE LANGUAGE PAIR)\\nModel Architecture:\\nOur model was a modiﬁed version of the GNMT model described in (Wu\\net al., 2016). To reduce computation, we decreased the number of LSTM layers in the encoder\\nand decoder from 9 and 8 to 3 and 2 respectively. We inserted MoE layers in both the encoder\\n(between layers 2 and 3) and the decoder (between layers 1 and 2). Each MoE layer contained up\\nto 2048 experts each with about two million parameters, adding a total of about 8 billion parameters\\nto the models. Further details on model architecture, testing procedure and results can be found in\\nAppendix E.\\nDatasets:\\nWe benchmarked our method on the WMT’14 En→Fr and En→De corpora, whose\\ntraining sets have 36M sentence pairs and 5M sentence pairs, respectively. The experimental proto-\\ncols were also similar to those in (Wu et al., 2016): newstest2014 was used as the test set to compare\\nagainst previous work (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), while the combina-\\ntion of newstest2012 and newstest2013 was used as the development set. We also tested the same\\nmodel on a Google’s Production English to French data.\\nTable 2: Results on WMT’14 En→Fr newstest2014 (bold values represent best results).\\nModel\\nTest\\nTest\\nops/timenstep\\nTotal\\nTraining\\nPerplexity BLEU\\n#Parameters\\nTime\\nMoE with 2048 Experts\\n2.69\\n40.35\\n85M\\n8.7B\\n3 days/64 k40s\\nMoE with 2048 Experts (longer training)\\n2.63\\n40.56\\n85M\\n8.7B\\n6 days/64 k40s\\nGNMT (Wu et al., 2016)\\n2.79\\n39.22\\n214M\\n278M\\n6 days/96 k80s\\nGNMT+RL (Wu et al., 2016)\\n2.96\\n39.92\\n214M\\n278M\\n6 days/96 k80s\\nPBMT (Durrani et al., 2014)\\n37.0\\nLSTM (6-layer) (Luong et al., 2015b)\\n31.5\\nLSTM (6-layer+PosUnk) (Luong et al., 2015b)\\n33.1\\nDeepAtt (Zhou et al., 2016)\\n37.7\\nDeepAtt+PosUnk (Zhou et al., 2016)\\n39.2\\nTable 3: Results on WMT’14 En →De newstest2014 (bold values represent best results).\\nModel\\nTest\\nTest\\nops/timestep\\nTotal\\nTraining\\nPerplexity\\nBLEU\\n#Parameters\\nTime\\nMoE with 2048 Experts\\n4.64\\n26.03\\n85M\\n8.7B\\n1 day/64 k40s\\nGNMT (Wu et al., 2016)\\n5.25\\n24.91\\n214M\\n278M\\n1 day/96 k80s\\nGNMT +RL (Wu et al., 2016)\\n8.08\\n24.66\\n214M\\n278M\\n1 day/96 k80s\\nPBMT (Durrani et al., 2014)\\n20.7\\nDeepAtt (Zhou et al., 2016)\\n20.6\\nTable 4: Results on the Google Production En→Fr dataset (bold values represent best results).\\nModel\\nEval\\nEval\\nTest\\nTest\\nops/timestep\\nTotal\\nTraining\\nPerplexity\\nBLEU\\nPerplexity\\nBLEU\\n#Parameters\\nTime\\nMoE with 2048 Experts\\n2.60\\n37.27\\n2.69\\n36.57\\n85M\\n8.7B\\n1 day/64 k40s\\nGNMT (Wu et al., 2016)\\n2.78\\n35.80\\n2.87\\n35.56\\n214M\\n278M\\n6 days/96 k80s\\n8\\nUnder review as a conference paper at ICLR 2017\\nResults:\\nTables 2, 3, and 4 show the results of our largest models, compared with published\\nresults. Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT’14 En→Fr and\\nEn→De benchmarks. As our models did not use RL reﬁnement, these results constitute signiﬁcant\\ngains of 1.34 and 1.12 BLEU score on top of the strong baselines in (Wu et al., 2016). The perplexity\\nscores are also better.2 On the Google Production dataset, our model achieved 1.01 higher test BLEU\\nscore even after training for only one sixth of the time.\\n5.4\\nMULTILINGUAL MACHINE TRANSLATION\\nDataset:\\n(Johnson et al., 2016) train a single GNMT (Wu et al., 2016) model on a very large com-\\nbined dataset of twelve language pairs. Results are somewhat worse than those for 12 separately\\ntrained single-pair GNMT models. This is not surprising, given that the twelve models have 12\\ntimes the capacity and twelve times the aggregate training of the one model. We repeat this ex-\\nperiment with a single MoE-augmented model. See Appendix E for details on model architecture.\\nWe train our model on the same dataset as (Johnson et al., 2016) and process the same number of\\ntraining examples (about 3 billion sentence pairs). Our training time was shorter due to the lower\\ncomputational budget of our model.\\nResults:\\nResults for the single-pair GNMT models, the multilingual GNMT model and the mul-\\ntilingual MoE model are given in Table 5. The MoE model achieves 19% lower perplexity on the\\ndev set than the multilingual GNMT model. On BLEU score, the MoE model signiﬁcantly beats\\nthe multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even\\nbeats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English\\n→Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number\\nof real examples were highly oversampled in the training corpus.\\nTable 5: Multilingual Machine Translation (bold values represent best results).\\nGNMT-Mono\\nGNMT-Multi\\nMoE-Multi\\nMoE-Multi vs.\\nGNMT-Multi\\nParameters 278M / model\\n278M\\n8.7B\\nops/timestep\\n212M\\n212M\\n102M\\ntraining time, hardware\\nvarious\\n21 days, 96 k20s 12 days, 64 k40s\\nPerplexity (dev)\\n4.14\\n3.35\\n-19%\\nFrench →English Test BLEU\\n36.47\\n34.40\\n37.46\\n+3.06\\nGerman →English Test BLEU\\n31.77\\n31.17\\n34.80\\n+3.63\\nJapanese →English Test BLEU\\n23.41\\n21.62\\n25.91\\n+4.29\\nKorean →English Test BLEU\\n25.42\\n22.87\\n28.71\\n+5.84\\nPortuguese →English Test BLEU\\n44.40\\n42.53\\n46.13\\n+3.60\\nSpanish →English Test BLEU\\n38.00\\n36.04\\n39.39\\n+3.35\\nEnglish →French Test BLEU\\n35.37\\n34.00\\n36.59\\n+2.59\\nEnglish →German Test BLEU\\n26.43\\n23.15\\n24.53\\n+1.38\\nEnglish →Japanese Test BLEU\\n23.66\\n21.10\\n22.78\\n+1.68\\nEnglish →Korean Test BLEU\\n19.75\\n18.41\\n16.62\\n-1.79\\nEnglish →Portuguese Test BLEU\\n38.40\\n37.35\\n37.90\\n+0.55\\nEnglish →Spanish Test BLEU\\n34.50\\n34.25\\n36.21\\n+1.96\\n6\\nCONCLUSION\\nThis work is the ﬁrst to demonstrate major wins from conditional computation in deep networks.\\nWe carefully identiﬁed the design considerations and challenges of conditional computing and ad-\\ndressed them with a combination of algorithmic and engineering solutions. While we focused on\\ntext, conditional computation may help in other domains as well, provided sufﬁciently large train-\\ning sets. We look forward to seeing many novel implementations and applications of conditional\\ncomputation in the years to come.\\nACKNOWLEDGMENTS\\nWe would like to thank all of the members of the Google Brain and Google Translate teams who\\nhelped us with this project, in particular Zhifeng Chen, Yonghui Wu, and Melvin Johnson. Thanks\\nalso to our anonymous ICLR reviewers for the helpful suggestions on making this paper better.\\n2Reported perplexities relative to the tokenization used by both our models and GNMT.\\n9\\nUnder review as a conference paper at ICLR 2017\\nREFERENCES\\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gre-\\ngory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Good-\\nfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Józefowicz, Lukasz\\nKaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Gor-\\ndon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\\nTalwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas, Oriol Vinyals,\\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow:\\nLarge-scale machine learning on heterogeneous distributed systems.\\nCoRR, abs/1603.04467,\\n2016. URL http://arxiv.org/abs/1603.04467.\\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\\nnetwork of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\\n06194.\\nA. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, and A. Courville. Dynamic Capac-\\nity Networks. ArXiv e-prints, November 2015.\\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jing-\\ndong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi\\nFan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin,\\nSharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh,\\nDavid Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yo-\\ngatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english\\nand mandarin. arXiv preprint arXiv:1512.02595, 2015.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\\nYoshua Bengio, Nicholas Léonard, and Aaron Courville.\\nEstimating or propagating gradients\\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\\narXiv preprint arXiv:1312.3005, 2013.\\nK. Cho and Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional\\nComputation in Deep Learning. ArXiv e-prints, June 2014.\\nRonan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large\\nscale problems. Neural Computing, 2002.\\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation\\nin deep neural networks. arXiv preprint arXiv:1312.4461, 2013.\\nMarc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In ICML, 2015.\\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization, 2010.\\nNadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based\\nmachine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical\\nMachine Translation, 2014.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\nEkaterina Garmash and Christof Monz. Ensemble learning for multi-source neural machine transla-\\ntion. In staff.science.uva.nl/c.monz, 2016.\\n10\\nUnder review as a conference paper at ICLR 2017\\nFelix A. Gers, Jürgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Continual pre-\\ndiction with lstm. Neural Computation, 2000.\\nAudrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efﬁcient\\nbackpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/\\nabs/1606.03401.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. IEEE Conference on Computer Vision and Pattern Recognition, 2015.\\nGeoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks\\nfor acoustic modeling in speech recognition: The shared views of four research groups. IEEE\\nSignal Processing Magazine, 2012.\\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 1997.\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\\nof local experts. Neural Computing, 1991.\\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\\nThorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s multilingual neural machine translation system: Enabling zero-shot translation.\\nCoRR, abs/1611.04558, 2016. URL http://arxiv.org/abs/1611.04558.\\nMichael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computing, 1994.\\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nReinhard Kneser and Hermann. Ney. Improved backingoff for m-gram language modeling., 1995.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nQuoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado,\\nJeffrey Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised\\nlearning. In ICML, 2012.\\nPatrick Gallinari Ludovic Denoyer.\\nDeep sequential neural network.\\narXiv preprint\\narXiv:1410.0510, 2014.\\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\\nbased neural machine translation. EMNLP, 2015a.\\nMinh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing\\nthe rare word problem in neural machine translation. ACL, 2015b.\\nCarl Edward Rasmussen and Zoubin Ghahramani. Inﬁnite mixtures of Gaussian process experts.\\nNIPS, 2002.\\nHasim Sak, Andrew W Senior, and Françoise Beaufays. Long short-term memory recurrent neural\\nnetwork architectures for large scale acoustic modeling. In INTERSPEECH, pp. 338–342, 2014.\\nMike Schuster and Kaisuke Nakajima. Japanese and Korean voice search. ICASSP, 2012.\\nBabak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR,\\n2009.\\n11\\nUnder review as a conference paper at ICLR 2017\\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.\\nIn NIPS, 2014.\\nLucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS, 2015.\\nVolker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-\\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,\\nKeith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,\\nAlex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural\\nmachine translation system: Bridging the gap between human and machine translation. arXiv\\npreprint arXiv:1609.08144, 2016.\\nBangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical mixture of classiﬁcation\\nexperts uncovers interactions between brain regions. In NIPS. 2009.\\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\\narXiv preprint arXiv:1409.2329, 2014.\\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward\\nconnections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.\\n12\\nUnder review as a conference paper at ICLR 2017\\nAPPENDICES\\nA\\nLOAD-BALANCING LOSS\\nAs discussed in section 4, for load-balancing purposes, we want to deﬁne an additional loss function\\nto encourage experts to receive roughly equal numbers of training examples. Unfortunately, the\\nnumber of examples received by an expert is a discrete quantity, so it can not be used in back-\\npropagation. Instead, we deﬁne a smooth estimator Load(X) of the number of examples assigned to\\neach expert for a batch X of inputs. The smoothness allows us to back-propagate gradients through\\nthe estimator. This is the purpose of the noise term in the gating function. We deﬁne P(x, i) as the\\nprobability that G(x)i is nonzero, given a new random choice of noise on element i, but keeping\\nthe already-sampled choices of noise on the other elements. To compute P(x, i), we note that the\\nG(x)i is nonzero if and only if H(x)i is greater than the kth-greatest element of H(x) excluding\\nitself. The probability works out to be:\\nP(x, i) = Pr\\n\\x10\\n(x · Wg)i + StandardNormal() · Softplus((x · Wnoise)i)\\n> kth_excluding(H(x), k, i)\\n\\x11\\n(8)\\nWhere kth_excluding(v, k, i) means the kth highest component of v, excluding component i. Sim-\\nplifying, we get:\\nP(x, i) = Φ\\n\\x10(x · Wg)i −kth_excluding(H(x), k, i)\\nSoftplus((x · Wnoise)i)\\n\\x11\\n(9)\\nWhere Φ is the CDF of the standard normal distribution.\\nLoad(X)i =\\nX\\nx∈X\\nP(x, i)\\n(10)\\nWe can now deﬁne the load loss to be the square of the coefﬁcient of variation of the load vector,\\nmultiplied by a hand-tuned scaling factor wload.\\nLload(X) = wload · CV (Load(X))2\\n(11)\\nInitial Load Imbalance:\\nTo avoid out-of-memory errors, we need to initialize the network in a\\nstate of approximately equal expert load (since the soft constraints need some time to work). To\\naccomplish this, we initialize the matrices Wg and Wnoise to all zeros, which yields no signal and\\nsome noise.\\nExperiments:\\nWe trained a set of models with identical architecture (the MoE-256 model de-\\nscribed in Appendix C), using different values of wimportance and wload. We trained each model for\\n10 epochs, then measured perplexity on the test set. We also measured the coefﬁcients of variation\\nin Importance and Load, as well as ratio of the load on the most overloaded expert to the average\\nload. This last value is signiﬁcant for load balancing purposes on distributed hardware. All of these\\nmetrics were averaged over several training batches.\\nTable 6: Experiments with different combinations of losses.\\nwimportance wload Test Perplexity CV (Importance(X)) CV (Load(X))\\nmax(Load(X))\\nmean(Load(X))\\n0.0\\n0.0\\n39.8\\n3.04\\n3.01\\n17.80\\n0.2\\n0.0\\n35.6\\n0.06\\n0.17\\n1.47\\n0.0\\n0.2\\n35.7\\n0.22\\n0.04\\n1.15\\n0.1\\n0.1\\n35.6\\n0.06\\n0.05\\n1.14\\n0.01\\n0.01\\n35.7\\n0.48\\n0.11\\n1.37\\n1.0\\n1.0\\n35.7\\n0.03\\n0.02\\n1.07\\n13\\nUnder review as a conference paper at ICLR 2017\\nResults:\\nResults are reported in Table 6. All the combinations containing at least one the two\\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\\nvalues of wload had lower loads on the most overloaded expert.\\nB\\nHIERACHICAL MIXTURE OF EXPERTS\\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\\nbination of “experts\", each of which is itself a secondary mixture-of-experts with its own gating\\nnetwork.3 If the hierarchical MoE consists of a groups of b experts each, we denote the primary gat-\\ning network by Gprimary, the secondary gating networks by (G1, G2..Ga), and the expert networks\\nby (E0,0, E0,1..Ea,b). The output of the MoE is given by:\\nyH =\\na\\nX\\ni=1\\nb\\nX\\nj=1\\nGprimary(x)i · Gi(x)j · Ei,j(x)\\n(12)\\nOur metrics of expert utilization change to the following:\\nImportanceH(X)i,j =\\nX\\nx∈X\\nGprimary(x)i · Gi(x)j\\n(13)\\nLoadH(X)i,j = Loadprimary(X)i · Loadi(X(i))j\\n|X(i)|\\n(14)\\nLoadprimary and Loadi deonte the Load functions for the primary gating network and ith sec-\\nondary gating network respectively. X(i) denotes the subset of X for which Gprimary(x)i > 0.\\nIt would seem simpler to let LoadH(X)i,j = Loadi(Xi)j , but this would not have a gradient with\\nrespect to the primary gating network, so we use the formulation above.\\nC\\n1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS\\nC.1\\n8-MILLION-OPERATIONS-PER-TIMESTEP MODELS\\nModel Architecture:\\nOur model consists of ﬁve layers: a word embedding layer, a recurrent\\nLong Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), a\\nMoE layer, a second LSTM layer, and a softmax layer. The dimensionality of the embedding layer,\\nthe number of units in each LSTM layer, and the input and output dimensionality of the MoE layer\\nare all equal to 512. For every layer other than the softmax, we apply drouput (Zaremba et al.,\\n2014) to the layer output, dropping each activation with probability DropProb, otherwise dividing\\nby (1 −DropProb). After dropout, the output of the previous layer is added to the layer output.\\nThis residual connection encourages gradient ﬂow (He et al., 2015).\\nMoE Layer Architecture:\\nEach expert in the MoE layer is a feed forward network with one\\nReLU-activated hidden layer of size 1024 and an output layer of size 512. Thus, each expert contains\\n[512 ∗1024] + [1024 ∗512] = 1M parameters. The output of the MoE layer is passed through a\\nsigmoid function before dropout. We varied the number of experts between models, using ordinary\\nMoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts.\\nWe call the resulting models MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h and MoE-4096-\\nh. For the hierarchical MoE layers, the ﬁrst level branching factor was 16, corresponding to the\\nnumber of GPUs in our cluster. We use Noisy-Top-K Gating (see Section 2.1) with k = 4 for the\\nordinary MoE layers and k = 2 at each level of the hierarchical MoE layers. Thus, each example is\\nprocessed by exactly 4 experts for a total of 4M ops/timestep. The two LSTM layers contribute 2M\\nops/timestep each for the desired total of 8M.\\n3 We have not found the need for deeper hierarchies.\\n14\\nUnder review as a conference paper at ICLR 2017\\nComputationally-Matched Baselines:\\nThe MoE-4 model does not employ sparsity, since all 4\\nexperts are always used. In addition, we trained four more computationally-matched baseline models\\nwith no sparsity:\\n• MoE-1-Wide: The MoE layer consists of a single \"expert\" containing one ReLU-activated\\nhidden layer of size 4096.\\n• MoE-1-Deep: The MoE layer consists of a single \"expert\" containing four ReLU-activated\\nhidden layers, each with size 1024.\\n• 4xLSTM-512: We replace the MoE layer with two additional 512-unit LSTM layers.\\n• LSTM-2048-512: The model contains one 2048-unit LSTM layer (and no MoE). The out-\\nput of the LSTM is projected down to 512 dimensions (Sak et al., 2014). The next timestep\\nof the LSTM receives the projected output. This is identical to one of the models published\\nin (Jozefowicz et al., 2016). We re-ran it to account for differences in training regimen, and\\nobtained results very similar to the published ones.\\nTraining:\\nThe models were trained on a cluster of 16 K40 GPUs using the synchronous method\\ndescribed in Section 3. Each batch consisted of a set of sentences totaling roughly 300,000 words. In\\nthe interest of time, we limited training to 10 epochs, (27,000 steps). Training took 12-16 hours for\\nall models, except for MoE-4, which took 18 hours (since all the expert computation was performed\\non only 4 of 16 GPUs). We used the Adam optimizer (Kingma & Ba, 2015). The base learning\\nrate was increased linearly for the ﬁrst 1000 training steps, and decreased after that so as to be\\nproportional to the inverse square root of the step number. The Softmax output layer was trained\\nefﬁciently using importance sampling similarly to the models in (Jozefowicz et al., 2016). For each\\nmodel, we performed a hyper-parmeter search to ﬁnd the best dropout probability, in increments of\\n0.1.\\nTo ensure balanced expert utilization we set wimportance = 0.1 and wload = 0.1, as described in\\nSection 4 and Appendix A.\\nResults:\\nWe evaluate our model using perplexity on the holdout dataset, used by (Chelba et al.,\\n2013; Jozefowicz et al., 2016). We follow the standard procedure and sum over all the words in-\\ncluding the end of sentence symbol. Results are reported in Table 7. For each model, we report\\nthe test perplexity, the computational budget, the parameter counts, the value of DropProb, and the\\ncomputational efﬁciency.\\nTable 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked\\nwith * are from (Jozefowicz et al., 2016).\\nModel\\nTest\\nTest\\nops/timestep #Params excluding\\nTotal\\nDrop-\\nTFLOPS\\nPerplexity Perplexity\\n(millions)\\nembed. & softmax\\n#Params\\nProb\\nper GPU\\n10 epochs\\n(ﬁnal)\\n(millions)\\n(billions)\\n(observed)\\nKneser-Ney 5-gram*\\n67.6\\n0.00001\\n1.8\\nLSTM-512-512*\\n54.1\\n2.4\\n2.4\\n0.8\\n0.1\\nLSTM-1024-512*\\n48.2\\n4.7\\n4.7\\n0.8\\n0.1\\nLSTM-2048-512*\\n45.0\\n43.7\\n9.4\\n9.4\\n0.8\\n0.1\\n0.61\\nLSTM-2048-512\\n44.7\\n9.4\\n9.4\\n0.8\\n0.1\\n1.21\\n4xLSTM-512\\n46.0\\n8.4\\n8.4\\n0.8\\n0.1\\n1.07\\nMoE-1-Wide\\n46.1\\n8.4\\n8.4\\n0.8\\n0.1\\n1.29\\nMoE-1-Deep\\n45.7\\n8.4\\n8.4\\n0.8\\n0.1\\n1.29\\nMoE-4\\n45.0\\n8.4\\n8.4\\n0.8\\n0.1\\n0.52\\nMoE-32\\n39.7\\n8.4\\n37.8\\n0.9\\n0.1\\n0.87\\nMoE-256\\n35.7\\n8.6\\n272.9\\n1.1\\n0.1\\n0.81\\nMoE-256-h\\n36.0\\n8.4\\n272.9\\n1.1\\n0.1\\n0.89\\nMoE-1024-h\\n34.6\\n8.5\\n1079.0\\n1.9\\n0.2\\n0.90\\nMoE-4096-h\\n34.1\\n8.9\\n4303.4\\n5.1\\n0.2\\n0.74\\n2xLSTM-8192-1024*\\n34.7\\n30.6\\n151.0\\n151.0\\n1.8\\n0.25\\n1.09\\nMoE-34M\\n31.3\\n33.8\\n4313.9\\n6.0\\n0.3\\n1.22\\nMoE-143M\\n28.0\\n142.7\\n4371.1\\n6.0\\n0.4\\n1.56\\n15\\nUnder review as a conference paper at ICLR 2017\\nC.2\\nMORE EXPENSIVE MODELS\\nWe ran two additional models (MoE-34M and MoE-143M) to investigate the effects of adding more\\ncomputation in the presence of a large MoE layer. These models have computation budgets of 34M\\nand 143M ops/timestep. Similar to the models above, these models use a MoE layer between two\\nLSTM layers. The dimensionality of the embedding layer, and the input and output dimensionality\\nof the MoE layer are set to 1024 instead of 512. For MoE-34M, the LSTM layers have 1024 units.\\nFor MoE-143M, the LSTM layers have 4096 units and an output projection of size 1024 (Sak et al.,\\n2014). MoE-34M uses a hierarchical MoE layer with 1024 experts, each with a hidden layer of size\\n2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size\\n8192. Both models have 4B parameters in the MoE layers. We searched for the best DropProb for\\neach model, and trained each model for 10 epochs.\\nThe two models achieved test perplexity of 31.3 and 28.0 respectively, showing that even in the\\npresence of a large MoE, more computation is still useful. Results are reported at the bottom of\\nTable 7. The larger of the two models has a similar computational budget to the best published\\nmodel from the literature, and training times are similar. Comparing after 10 epochs, our model has\\na lower test perplexity by 18%.\\nD\\n100 BILLION WORD GOOGLE NEWS CORPUS - EXPERIMENTAL DETAILS\\nModel Architecture:\\nThe models are similar in structure to the 8-million-operations-per-timestep\\nmodels described in the previous section. We vary the number of experts between models, using\\nan ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384,\\n65536 and 131072 experts. For the hierarchical MoE layers, the ﬁrst level branching factors are 32,\\n32, 64, 128, 256 and 256, respectively.\\nTraining:\\nModels are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models,\\nwhich are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the param-\\neters. For all models, training batch sizes are approximately 2.5 million words. Models are trained\\nonce-through over about 100 billion words.\\nWe implement several memory optimizations in order to ﬁt up to 1 billion parameters per GPU.\\nFirst, we do not store the activations of the hidden layers of the experts, but instead recompute them\\non the backwards pass. Secondly, we modify the optimizer on the expert parameters to require less\\nauxiliary storage:\\nThe Adam optimizer (Kingma & Ba, 2015) keeps ﬁrst and second moment estimates of the per-\\nparameter gradients. This triples the required memory. To avoid keeping a ﬁrst-moment estimator,\\nwe set β1 = 0. To reduce the size of the second moment estimator, we replace it with a factored\\napproximation. For a matrix of parameters, instead of maintaining a full matrix of second-moment\\nestimators, we maintain vectors of row-wise and column-wise averages of that matrix. At each step,\\nthe matrix of estimators is taken to be the outer product of those two vectors divided by the mean of\\neither one. This technique could similarly be applied to Adagrad (Duchi et al., 2010).\\nTable 8: Model comparison on 100 Billion Word Google News Dataset\\nModel\\nTest\\nTest\\nops/timestep #Params excluding\\nTotal\\nTFLOPS\\nPerplexity Perplexity\\n(millions)\\nembed. & softmax\\n#Params\\nper GPU\\n.1 epochs\\n1 epoch\\n(millions)\\n(billions) (observed)\\nKneser-Ney 5-gram\\n67.1\\n45.3\\n0.00001\\n76.0\\n4xLSTM-512\\n54.5\\n47.0\\n8.4\\n8.4\\n0.1\\n1.23\\nMoE-32\\n48.5\\n40.4\\n8.4\\n37.8\\n0.1\\n0.83\\nMoE-256-h\\n42.8\\n35.3\\n8.4\\n272.9\\n0.4\\n1.11\\nMoE-1024-h\\n40.3\\n32.7\\n8.5\\n1079.0\\n1.2\\n1.14\\nMoE-4096-h\\n38.9\\n30.9\\n8.6\\n4303.4\\n4.4\\n1.07\\nMoE-16384-h\\n38.2\\n29.7\\n8.8\\n17201.0\\n17.3\\n0.96\\nMoE-65536-h\\n38.2\\n28.9\\n9.2\\n68791.0\\n68.9\\n0.72\\nMoE-131072-h\\n39.8\\n29.2\\n9.7\\n137577.6\\n137.7\\n0.30\\nResults:\\nWe evaluate our model using perplexity on a holdout dataset. Results are reported in\\nTable 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE\\n16\\nUnder review as a conference paper at ICLR 2017\\nmodel than for the baseline model. It is notable that the measured computational efﬁciency of\\nthe largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely\\na result of the fact that, for purposes of comparison to the other models, we did not increase the\\ntraining batch size proportionally to the number of GPUs. For comparison, we include results for\\na computationally matched baseline model consisting of 4 LSTMs, and for an unpruned 5-gram\\nmodel with Kneser-Ney smoothing (Kneser & Ney, 1995).4\\nE\\nMACHINE TRANSLATION - EXPERIMENTAL DETAILS\\nModel Architecture for Single Language Pair MoE Models:\\nOur model is a modiﬁed version\\nof the GNMT model described in (Wu et al., 2016). To reduce computation, we decrease the number\\nof LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We insert MoE\\nlayers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). We use\\nan attention mechanism between the encoder and decoder, with the ﬁrst decoder LSTM receiving\\noutput from and providing input for the attention 5. All of the layers in our model have input and\\noutput dimensionality of 512. Our LSTM layers have 2048 hidden units, with a 512-dimensional\\noutput projection. We add residual connections around all LSTM and MoE layers to encourage\\ngradient ﬂow (He et al., 2015). Similar to GNMT, to effectively deal with rare words, we used sub-\\nword units (also known as “wordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our\\nsystem.\\nWe use a shared source and target vocabulary of 32K wordpieces. We also used the same beam\\nsearch technique as proposed in (Wu et al., 2016).\\nWe train models with different numbers of experts in the MoE layers. In addition to a baseline\\nmodel with no MoE layers, we train models with ﬂat MoE layers containing 32 experts, and models\\nwith hierarchical MoE layers containing 512 and 2048 experts. The ﬂat MoE layers use k = 4 and\\nthe hierarchical MoE models use k = 2 at each level of the gating network. Thus, each input is\\nprocessed by exactly 4 experts in each MoE layer. Each expert in the MoE layer is a feed forward\\nnetwork with one hidden layer of size 2048 and ReLU activation. Thus, each expert contains [512 ∗\\n2048] + [2048 ∗512] = 2M parameters. The output of the MoE layer is passed through a sigmoid\\nfunction. We use the strictly-balanced gating function described in Appendix F.\\nModel Architecture for Multilingual MoE Model:\\nWe used the same model architecture as\\nfor the single-language-pair models, with the following exceptions: We used noisy-top-k gating as\\ndescribed in Section 2.1, not the scheme from Appendix F. The MoE layers in the encoder and\\ndecoder are non-hierarchical MoEs with n = 512 experts, and k = 2. Each expert has a larger\\nhidden layer of size 8192. This doubles the amount of computation in the MoE layers, raising the\\ncomputational budget of the entire model from 85M to 102M ops/timestep.\\nTraining:\\nWe trained our networks using the Adam optimizer (Kingma & Ba, 2015). The base\\nlearning rate was increased linearly for the ﬁrst 2000 training steps, held constant for an additional\\n8000 steps, and decreased after that so as to be proportional to the inverse square root of the step\\nnumber. For the single-language-pair models, similarly to (Wu et al., 2016), we applied dropout\\n(Zaremba et al., 2014) to the output of all embedding, LSTM and MoE layers, using DropProb =\\n0.4. Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each\\ntraining batch consisted of a set of sentence pairs containing roughly 16000 words per GPU.\\nTo ensure balanced expert utilization we set wimportance = 0.01 and wload = 0.01, as described in\\nSection 4 and Appendix A.\\nMetrics:\\nWe evaluated our models using the perplexity and the standard BLEU score metric. We\\nreported tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public\\nimplementation of Moses (on Github), which was also used in (Luong et al., 2015a).\\n4While the original size of the corpus was 130 billion words, the neural models were trained for a maximum\\nof 100 billion words. The reported Kneser-Ney 5-gram models were trained over 13 billion and 130 billion\\nwords respectively, giving them a slight advantage over the other reported results.\\n5For performance reasons, we use a slightly different attention function from the one described in (Wu et al.,\\n2016) - See Appendix G\\n17\\nUnder review as a conference paper at ICLR 2017\\nResults:\\nTables 2, 3 and 4 in Section 5.3 show comparisons of our results to other published\\nmethods. Figure 4 shows test perplexity as a function of number of words in the (training data’s)\\nsource sentences processed for models with different numbers of experts. As can be seen from the\\nFigure, as we increased the number of experts to approach 2048, the test perplexity of our model\\ncontinued to improve.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nNumber of source words processed\\n1e9\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\nPerplexity\\n#Experts=0\\n#Experts=32\\n#Experts=512\\n#Experts=2048\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nNumber of source words processed\\n1e10\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nPerplexity\\n#Experts=0\\n#Experts=32\\n#Experts=512\\n#Experts=2048\\nFigure 4: Perplexity on WMT’14 En→Fr (left) and Google Production En→Fr (right) datasets as\\na function of number of words processed. The large differences between models at the beginning\\nof training are due to different batch sizes. All models incur the same computational budget (85M\\nops/timestep) except the one with no experts.\\nWe found that the experts indeed become highly specialized by syntax and/or semantics, as can be\\nseen in Table 9. For example, one expert is used when the indeﬁnite article “a\" introduces the direct\\nobject in a verb phrase indicating importance or leadership.\\nTable 9: Contexts corresponding to a few of the 2048 experts in the MoE layer in the encoder portion\\nof the WMT’14 En→Fr translation model. For each expert i, we sort the inputs in a training batch\\nin decreasing order of G(x)i, and show the words surrounding the corresponding positions in the\\ninput sentences.\\nExpert 381\\nExpert 752\\nExpert 2004\\n... with researchers , ...\\n... plays a core ...\\n... with rapidly growing ...\\n... to innovation .\\n... plays a critical ...\\n... under static conditions ...\\n... tics researchers .\\n... provides a legislative ...\\n... to swift ly ...\\n... the generation of ...\\n... play a leading ...\\n... to dras tically ...\\n... technology innovations is ...\\n... assume a leadership ...\\n... the rapid and ...\\n... technological innovations , ...\\n... plays a central ...\\n... the fast est ...\\n... support innovation throughout ...\\n... taken a leading ...\\n... the Quick Method ...\\n... role innovation will ...\\n... established a reconciliation ...\\n... rec urrent ) ...\\n... research scienti st ...\\n... played a vital ...\\n... provides quick access ...\\n... promoting innovation where ...\\n... have a central ...\\n... of volatile organic ...\\n...\\n...\\n...\\nF\\nSTRICTLY BALANCED GATING\\nDue to some peculiarities in our infrastructure which have since been ﬁxed, at the time we ran some\\nof the machine translation experiments, our models ran faster if every expert received exactly the\\nsame batch size. To accommodate this, we used a different gating function which we describe below.\\nRecall that we deﬁne the softmax gating function to be:\\nGσ(x) = Softmax(x · Wg)\\n(15)\\nSparse Gating (alternate formulation):\\nTo obtain a sparse gating vector, we multiply Gσ(x)\\ncomponent-wise with a sparse mask M(Gσ(x)) and normalize the output. The mask itself is a\\nfunction of Gσ(x) and speciﬁes which experts are assigned to each input example:\\n18\\nUnder review as a conference paper at ICLR 2017\\nG(x)i =\\nGσ(x)iM(Gσ(x))i\\nPn\\nj=1 Gσ(x)jM(Gσ(x))j\\n(16)\\nTop-K Mask:\\nTo implement top-k gating in this formulation, we would let M(v) = TopK(v, k),\\nwhere:\\nTopK(v, k)i =\\n\\x1a1\\nif vi is in the top k elements of v.\\n0\\notherwise.\\n(17)\\nBatchwise Mask:\\nTo force each expert to receive the exact same number of examples, we intro-\\nduce an alternative mask function, Mbatchwise(X, m), which operates over batches of input vectors.\\nInstead of keeping the top k values per example, we keep the top m values per expert across the\\ntraining batch, where m = k|X|\\nn , so that each example is sent to an average of k experts.\\nMbatchwise(X, m)j,i =\\n\\x1a1\\nif Xj,i is in the top m values for to expert i\\n0\\notherwise\\n(18)\\nAs our experiments suggest and also observed in (Ioffe & Szegedy, 2015), using a batchwise func-\\ntion during training (such as Mbatchwise) requires modiﬁcations to the inference when we may not\\nhave a large batch of examples. Our solution to this is to train a vector T of per-expert threshold\\nvalues to approximate the effects of the batchwise mask. We use the following mask at inference\\ntime:\\nMthreshold(x, T)i =\\n\\x1a1\\nif xi > Ti\\n0\\notherwise\\n(19)\\nTo learn the threshold values, we apply an additional loss at training time which is minimized when\\nthe batchwise mask and the threshold mask are identical.\\nLbatchwise(X, T, m) =\\n|X|\\nX\\nj=1\\nn\\nX\\ni=1\\n(Mthreshold(x, T)i −Mbatchwise(X, m)j,i)(Xj,i −Ti)\\n(20)\\nG\\nATTENTION FUNCTION\\nThe attention mechanism described in GNMT (Wu et al., 2016) involves a learned “Attention Func-\\ntion\" A(xi, yj) which takes a “source vector\" xi and a “target vector\" yj, and must be computed for\\nevery source time step i and target time step j. In GNMT, the attention function is implemented as\\na feed forward neural network with a hidden layer of size n. It can be expressed as:\\nAGNMT (xi, yj) =\\nn\\nX\\nd=1\\nVdtanh((xiU)d + (yjW)d)\\n(21)\\nWhere U and W are trainable weight matrices and V is a trainable weight vector.\\nFor performance reasons, in our models, we used a slightly different attention function:\\nA(xi, yj) =\\nn\\nX\\nd=1\\nVdtanh((xiU)d)tanh((yjW)d)\\n(22)\\nWith our attention function, we can simultaneously compute the attention function on multiple\\nsource time steps and multiple target time steps using optimized matrix multiplications. We found\\nlittle difference in quality between the two functions.\\n19\\n', 'source_name': 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer', 'source_url': 'https://arxiv.org/abs/1701.06538'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original_MoE.pdf #75\n",
      "{'content': '', 'source_name': 'Adaptive Mixture of Local Experts', 'source_url': 'https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Learning_Factored_Representations_NOTES.pdf #76\n",
      "{'content': 'Learning Factored Representations in a Deep Mixture-of-Experts \\n \\nMain Idea: \\n- \\nTo apply stacked layers of mixture-of-experts, so to have multiple sets of (gating, experts). \\nThis allows multiple combinations of experts to be called while keeping a modest model \\nsize. \\nThe problem they are trying to solve for is that deep neural networks are expensive to compute \\nat inference time since all the neurons are used. \\nThe solution proposed is to implement stacked MoE layers, where multiple expert combinations \\nare possible, and the gating mechanism ensures only useful neurons for that input are used \\n(experts on the specific input space). This gives better computational efficiency at inference, \\nallowing for a model that is both large and efficient. \\n \\nApproach: \\n- \\nThe input is first passed through the first MoE layer (represented by z1): \\no 𝑧1 = ∑\\n𝑔𝑖1(𝑥)\\n𝑁\\n𝑖=1\\n∙𝑓\\n𝑖1(𝑥), where 𝑔𝑖1(𝑥) \\nand \\n𝑓\\n𝑖1(𝑥)represent the gating \\nprobability and expert output for expert i at layer 1, respectively. \\n▪ both the gating mechanism and the expert function use a non-linearity \\n(ReLU) \\no The outputs of the first layer (z1) are then passed as an input to the next MoE layer \\nz2, which replaces x with z1. \\no z2 is then passed through a final layer (f3) and a softmax is applied (in the context \\nof classification) \\n▪ 𝐹(𝑥) = 𝑧3 = 𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑓\\n3(𝑧2)) \\nThe network is trained with SGD with a caveat to help balance the training through the experts: \\n- \\nThe mean of all experts’ total assignment is compared to each expert’s running total \\nassignment. If an expert is found to have a running total assignment significantly higher \\nthan the mean, its training is paused temporarily to allow for the training of other experts. \\n- \\nThis strategy is found to mostly be useful in early stages of training, where the experts \\nhave not yet specialized significantly on a part of the input space. After some training, the \\nexperts are expected to have some specialization, and thus this constraint can be lifted. \\nThis paper makes use of conditional computation, although the details about this are not shown \\nin-depth. \\nResults: \\n- \\nThe stacked MoE layer showed promising results, as it came close to fully dense networks \\nin terms of performance while having significant inference pros due to conditional \\ncomputation. \\n- \\nExperiments in specific tasks also showed that different experts indeed did specialize in \\ndifferent clusters of the data. \\nMy takeaways: \\n- \\nThis paper is revolutionary in terms of the idea presented in terms of stacking MoE layers \\nin a deep neural network and trying to find a way to balance the load between experts. \\n- \\nIntroduces the idea that MoE can have improved performance when stacked, paving the \\nway for adding this as a modular component that can be added to other architectures. \\n- \\nThis strategy is still not sparse (top-k), but it opens the field to the idea that a top-k \\nstrategy is possible as a future line of research. \\n \\n', 'source_name': 'Learning Factorized Representations in a Deep Mixture-of-Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mixture of Experts Explained_HF.pdf #77\n",
      "{'content': '1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n1/24\\nBack to blog\\nPublished December 11, 2023\\nUpdate on GitHub\\nosanseviero\\nOmar Sanseviero\\nlewtun\\nLewis Tunstall\\nphilschmid\\nPhilipp Schmid\\nsmangrul\\nSourab Mangrulkar\\nybelkada\\nYounes Belkada\\npcuenq\\nPedro Cuenca\\nWith the release of Mixtral 8x7B (announcement, model card), a class of transformer has become\\nthe hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog\\npost, we take a look at the building blocks of MoEs, how they’re trained, and the tradeoffs to\\nconsider when serving them for inference.\\nLet’s dive in!\\nWhat is a Mixture of Experts?\\nA Brief History of MoEs\\nWhat is Sparsity?\\nLoad Balancing tokens for MoEs\\nMoEs and Transformers\\nSwitch Transformers\\nStabilizing training with router Z-loss\\nSearch models, datasets, users...\\nMixture of Experts Explained\\nTable of Contents\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n2/24\\nWhat does an expert learn?\\nHow does scaling the number of experts impact pretraining?\\nFine-tuning MoEs\\nWhen to use sparse MoEs vs dense models?\\nMaking MoEs go brrr\\nExpert Parallelism\\nCapacity Factor and Communication costs\\nServing Techniques\\nEfficient Training\\nOpen Source MoEs\\nExciting directions of work\\nSome resources\\nMoEs:\\nAre pretrained much faster vs. dense models\\nHave faster inference compared to a model with the same number of parameters\\nRequire high VRAM as all experts are loaded in memory\\nFace many challenges in fine-tuning, but recent work with MoE instruction-tuning is\\npromising\\nLet’s dive in!\\nTL;DR\\nWhat is a Mixture of Experts (MoE)?\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n3/24\\nThe scale of a model is one of the most important axes for better model quality. Given a fixed\\ncomputing budget, training a larger model for fewer steps is better than training a smaller model\\nfor more steps.\\nMixture of Experts enable models to be pretrained with far less compute, which means you can\\ndramatically scale up the model or dataset size with the same compute budget as a dense model.\\nIn particular, a MoE model should achieve the same quality as its dense counterpart much faster\\nduring pretraining.\\nSo, what exactly is a MoE? In the context of transformer models, a MoE consists of two main\\nelements:\\nSparse MoE layers are used instead of dense feed-forward network (FFN) layers. MoE layers\\nhave a certain number of “experts” (e.g. 8), where each expert is a neural network. In\\npractice, the experts are FFNs, but they can also be more complex networks or even a MoE\\nitself, leading to hierarchical MoEs!\\nA gate network or router, that determines which tokens are sent to which expert. For\\nexample, in the image below, the token “More” is sent to the second expert, and the token\\n\"Parameters” is sent to the first network. As we’ll explore later, we can send a token to more\\nthan one expert. How to route a token to an expert is one of the big decisions when working\\nwith MoEs - the router is composed of learned parameters and is pretrained at the same time\\nas the rest of the network.\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n4/24\\nMoE layer from the [Switch Transformers paper](https://arxiv.org/abs/2101.03961)\\nSo, to recap, in MoEs we replace every FFN layer of the transformer model with an MoE layer,\\nwhich is composed of a gate network and a certain number of experts.\\nAlthough MoEs provide benefits like efficient pretraining and faster inference compared to dense\\nmodels, they also come with challenges:\\nTraining: MoEs enable significantly more compute-efficient pretraining, but they’ve\\nhistorically struggled to generalize during fine-tuning, leading to overfitting.\\nInference: Although a MoE might have many parameters, only some of them are used during\\ninference. This leads to much faster inference compared to a dense model with the same\\nnumber of parameters. However, all parameters need to be loaded in RAM, so memory\\nrequirements are high. For example, given a MoE like Mixtral 8x7B, we’ll need to have\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n5/24\\nenough VRAM to hold a dense 47B parameter model. Why 47B parameters and not 8 x 7B =\\n56B? That’s because in MoE models, only the FFN layers are treated as individual experts, and\\nthe rest of the model parameters are shared. At the same time, assuming just two experts are\\nbeing used per token, the inference speed (FLOPs) is like using a 12B model (as opposed to a\\n14B model), because it computes 2x7B matrix multiplications, but with some layers shared\\n(more on this soon).\\nNow that we have a rough idea of what a MoE is, let’s take a look at the research developments\\nthat led to their invention.\\nThe roots of MoEs come from the 1991 paper Adaptive Mixture of Local Experts. The idea, akin to\\nensemble methods, was to have a supervised procedure for a system composed of separate\\nnetworks, each handling a different subset of the training cases. Each separate network, or expert,\\nspecializes in a different region of the input space. How is the expert chosen? A gating network\\ndetermines the weights for each expert. During training, both the expert and the gating are\\ntrained.\\nBetween 2010-2015, two different research areas contributed to later MoE advancement:\\nExperts as components: In the traditional MoE setup, the whole system comprises a gating\\nnetwork and multiple experts. MoEs as the whole model have been explored in SVMs,\\nGaussian Processes, and other methods. The work by Eigen, Ranzato, and Ilya explored MoEs\\nas components of deeper networks. This allows having MoEs as layers in a multilayer\\nnetwork, making it possible for the model to be both large and efficient simultaneously.\\nConditional Computation: Traditional networks process all input data through every layer.\\nIn this period, Yoshua Bengio researched approaches to dynamically activate or deactivate\\ncomponents based on the input token.\\nThese works led to exploring a mixture of experts in the context of NLP. Concretely, Shazeer et al.\\n(2017, with “et al.” including Geoffrey Hinton and Jeff Dean, Google’s Chuck Norris) scaled this\\nA Brief History of MoEs\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n6/24\\nidea to a 137B LSTM (the de-facto NLP architecture back then, created by Schmidhuber) by\\nintroducing sparsity, allowing to keep very fast inference even at high scale. This work focused on\\ntranslation but faced many challenges, such as high communication costs and training\\ninstabilities.\\nMoE layer from the Outrageously Large Neural Network paper\\nMoEs have allowed training multi-trillion parameter models, such as the open-sourced 1.6T\\nparameters Switch Transformers, among others. MoEs have also been explored in Computer\\nVision, but this blog post will focus on the NLP domain.\\nSparsity uses the idea of conditional computation. While in dense models all the parameters are\\nused for all the inputs, sparsity allows us to only run some parts of the whole system.\\nLet’s dive deeper into Shazeer\\'s exploration of MoEs for translation. The idea of conditional\\ncomputation (parts of the network are active on a per-example basis) allows one to scale the size\\nof the model without increasing the computation, and hence, this led to thousands of experts\\nbeing used in each MoE layer.\\nWhat is Sparsity?\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n7/24\\nThis setup introduces some challenges. For example, although large batch sizes are usually better\\nfor performance, batch sizes in MOEs are effectively reduced as data flows through the active\\nexperts. For example, if our batched input consists of 10 tokens, five tokens might end in one\\nexpert, and the other five tokens might end in five different experts, leading to uneven batch\\nsizes and underutilization. The Making MoEs go brrr section below will discuss other challenges\\nand solutions.\\nHow can we solve this? A learned gating network (G) decides which experts (E) to send a part of\\nthe input:\\nIn this setup, all experts are run for all inputs - it’s a weighted multiplication. But, what happens if\\nG is 0? If that’s the case, there’s no need to compute the respective expert operations and hence we\\nsave compute. What’s a typical gating function? In the most traditional setup, we just use a simple\\nnetwork with a softmax function. The network will learn which expert to send the input.\\nShazeer’s work also explored other gating mechanisms, such as Noisy Top-k Gating. This gating\\napproach introduces some (tunable) noise and then keeps the top k values. That is:\\n1. We add some noise\\n2. We only pick the top k\\n3. We apply the softmax.\\ny =\\n\\u200bG(x)\\n\\u200bE\\n\\u200b(x)\\ni=1\\n∑\\nn\\ni\\ni\\nG\\n\\u200b(x) =\\nσ\\nSoftmax(x ⋅W\\n\\u200b)\\ng\\nH(x)\\n\\u200b =\\ni\\n(x ⋅W\\n\\u200b)\\n\\u200b +\\ng i\\nStandardNormal() ⋅Softplus((x ⋅W\\n\\u200b)\\n\\u200b)\\nnoise i\\nKeepTopK(v, k)\\n\\u200b =\\ni\\n\\u200b\\n\\u200b\\n{v\\n\\u200b\\ni\\n−∞\\nif\\xa0v\\n\\u200b\\xa0is\\xa0in\\xa0the\\xa0top\\xa0k\\xa0elements\\xa0of\\xa0v,\\ni\\notherwise.\\nG(x) = Softmax(KeepTopK(H(x), k))\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n8/24\\nThis sparsity introduces some interesting properties. By using a low enough k (e.g. one or two), we\\ncan train and run inference much faster than if many experts were activated. Why not just select\\nthe top expert? The initial conjecture was that routing to more than one expert was needed to\\nhave the gate learn how to route to different experts, so at least two experts had to be picked. The\\nSwitch Transformers section revisits this decision.\\nWhy do we add noise? That’s for load balancing!\\nAs discussed before, if all our tokens are sent to just a few popular experts, that will make training\\ninefficient. In a normal MoE training, the gating network converges to mostly activate the same\\nfew experts. This self-reinforces as favored experts are trained quicker and hence selected more.\\nTo mitigate this, an auxiliary loss is added to encourage giving all experts equal importance. This\\nloss ensures that all experts receive a roughly equal number of training examples. The following\\nsections will also explore the concept of expert capacity, which introduces a threshold of how\\nmany tokens can be processed by an expert. In transformers, the auxiliary loss is exposed via the\\naux_loss parameter.\\nTransformers are a very clear case that scaling up the number of parameters improves the\\nperformance, so it’s not surprising that Google explored this with GShard, which explores scaling\\nup transformers beyond 600 billion parameters.\\nGShard replaces every other FFN layer with an MoE layer using top-2 gating in both the encoder\\nand the decoder. The next image shows how this looks like for the encoder part. This setup is quite\\nbeneficial for large-scale computing: when we scale to multiple devices, the MoE layer is shared\\nacross devices while all the other layers are replicated. This is further discussed in the “Making\\nMoEs go brrr” section.\\nLoad balancing tokens for MoEs\\nMoEs and Transformers\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n9/24\\nMoE Transformer Encoder from the GShard Paper\\nTo maintain a balanced load and efficiency at scale, the GShard authors introduced a couple of\\nchanges in addition to an auxiliary loss similar to the one discussed in the previous section:\\nRandom routing: in a top-2 setup, we always pick the top expert, but the second expert is\\npicked with probability proportional to its weight.\\nExpert capacity: we can set a threshold of how many tokens can be processed by one expert.\\nIf both experts are at capacity, the token is considered overflowed, and it’s sent to the next\\nlayer via residual connections (or dropped entirely in other projects). This concept will\\nbecome one of the most important concepts for MoEs. Why is expert capacity needed? Since\\nall tensor shapes are statically determined at compilation time, but we cannot know how\\nmany tokens will go to each expert ahead of time, we need to fix the capacity factor.\\nThe GShard paper has contributions by expressing parallel computation patterns that work well\\nfor MoEs, but discussing that is outside the scope of this blog post.\\nNote: when we run inference, only some experts will be triggered. At the same time, there are\\nshared computations, such as self-attention, which is applied for all tokens. That’s why when we\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n10/24\\ntalk of a 47B model of 8 experts, we can run with the compute of a 12B dense model. If we use\\ntop-2, 14B parameters would be used. But given that the attention operations are shared (among\\nothers), the actual number of used parameters is 12B.\\nAlthough MoEs showed a lot of promise, they struggle with training and fine-tuning instabilities.\\nSwitch Transformers is a very exciting work that deep dives into these topics. The authors even\\nreleased a 1.6 trillion parameters MoE on Hugging Face with 2048 experts, which you can run\\nwith transformers. Switch Transformers achieved a 4x pre-train speed-up over T5-XXL.\\nSwitch Transformer Layer of the Switch Transformer paper\\nJust as in GShard, the authors replaced the FFN layers with a MoE layer. The Switch Transformers\\npaper proposes a Switch Transformer layer that receives two inputs (two different tokens) and has\\nfour experts.\\nContrary to the initial idea of using at least two experts, Switch Transformers uses a simplified\\nsingle-expert strategy. The effects of this approach are:\\nSwitch Transformers\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n11/24\\nThe router computation is reduced\\nThe batch size of each expert can be at least halved\\nCommunication costs are reduced\\nQuality is preserved\\nSwitch Transformers also explores the concept of expert capacity.\\nThe capacity suggested above evenly divides the number of tokens in the batch across the number\\nof experts. If we use a capacity factor greater than 1, we provide a buffer for when tokens are not\\nperfectly balanced. Increasing the capacity will lead to more expensive inter-device\\ncommunication, so it’s a trade-off to keep in mind. In particular, Switch Transformers perform\\nwell at low capacity factors (1-1.25)\\nSwitch Transformer authors also revisit and simplify the load balancing loss mentioned in the\\nsections. For each Switch layer, the auxiliary loss is added to the total model loss during training.\\nThis loss encourages uniform routing and can be weighted using a hyperparameter.\\nThe authors also experiment with selective precision, such as training the experts with bfloat16\\nwhile using full precision for the rest of the computations. Lower precision reduces\\ncommunication costs between processors, computation costs, and memory for storing tensors. The\\ninitial experiments, in which both the experts and the gate networks were trained in bfloat16,\\nyielded more unstable training. This was, in particular, due to the router computation: as the\\nrouter has an exponentiation function, having higher precision is important. To mitigate the\\ninstabilities, full precision was used for the routing as well.\\nExpert\\xa0Capacity =\\n\\u200b\\n×\\n(number\\xa0of\\xa0experts\\ntokens\\xa0per\\xa0batch )\\ncapacity\\xa0factor\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n12/24\\nUsing selective precision does not degrade quality and enables faster models\\nThis notebook showcases fine-tuning Switch Transformers for summarization, but we suggest first\\nreviewing the fine-tuning section.\\nSwitch Transformers uses an encoder-decoder setup in which they did a MoE counterpart of T5.\\nThe GLaM paper explores pushing up the scale of these models by training a model matching GPT-\\n3 quality using 1/3 of the energy (yes, thanks to the lower amount of computing needed to train a\\nMoE, they can reduce the carbon footprint by up to an order of magnitude). The authors focused\\non decoder-only models and few-shot and one-shot evaluation rather than fine-tuning. They used\\nTop-2 routing and much larger capacity factors. In addition, they explored the capacity factor as a\\nmetric one can change during training and evaluation depending on how much computing one\\nwants to use.\\nThe balancing loss previously discussed can lead to instability issues. We can use many methods to\\nstabilize sparse models at the expense of quality. For example, introducing dropout improves\\nstability but leads to loss of model quality. On the other hand, adding more multiplicative\\ncomponents improves quality but decreases stability.\\nRouter z-loss, introduced in ST-MoE, significantly improves training stability without quality\\ndegradation by penalizing large logits entering the gating network. Since this loss encourages\\nabsolute magnitude of values to be smaller, roundoff errors are reduced, which can be quite\\nimpactful for exponential functions such as the gating. We recommend reviewing the paper for\\ndetails.\\nThe ST-MoE authors observed that encoder experts specialize in a group of tokens or shallow\\nconcepts. For example, we might end with a punctuation expert, a proper noun expert, etc. On the\\nStabilizing training with router Z-loss\\nWhat does an expert learn?\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n13/24\\nother hand, the decoder experts have less specialization. The authors also trained in a multilingual\\nsetup. Although one could imagine each expert specializing in a language, the opposite happens:\\ndue to token routing and load balancing, there is no single expert specialized in any given\\nlanguage.\\nTable from the ST-MoE paper showing which token groups were sent to which expert.\\nHow does scaling the number of experts impact pretraining?\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n14/24\\nMore experts lead to improved sample efficiency and faster speedup, but these are diminishing\\ngains (especially after 256 or 512), and more VRAM will be needed for inference. The properties\\nstudied in Switch Transformers at large scale were consistent at small scale, even with 2, 4, or 8\\nexperts per layer.\\n“Mixtral is supported with version 4.36.0 of transformers. You can install it with pip install\\n\"transformers==4.36.0 --upgrade”\\nThe overfitting dynamics are very different between dense and sparse models. Sparse models are\\nmore prone to overfitting, so we can explore higher regularization (e.g. dropout) within the\\nexperts themselves (e.g. we can have one dropout rate for the dense layers and another, higher,\\ndropout for the sparse layers).\\nOne question is whether to use the auxiliary loss for fine-tuning. The ST-MoE authors\\nexperimented with turning off the auxiliary loss, and the quality was not significantly impacted,\\neven when up to 11% of the tokens were dropped. Token dropping might be a form of\\nregularization that helps prevent overfitting.\\nSwitch Transformers observed that at a fixed pretrain perplexity, the sparse model does worse\\nthan the dense counterpart in downstream tasks, especially on reasoning-heavy tasks such as\\nSuperGLUE. On the other hand, for knowledge-heavy tasks such as TriviaQA, the sparse model\\nperforms disproportionately well. The authors also observed that a fewer number of experts\\nhelped at fine-tuning. Another observation that confirmed the generalization issue is that the\\nmodel did worse in smaller tasks but did well in larger tasks.\\nFine-tuning MoEs\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n15/24\\nIn the small task (left), we can see clear overfitting as the sparse model does much worse in the validation set. In\\nthe larger task (right), the MoE performs well. This image is from the ST-MoE paper.\\nOne could experiment with freezing all non-expert weights. That is, we\\'ll only update the MoE\\nlayers. This leads to a huge performance drop. We could try the opposite: freezing only the\\nparameters in MoE layers, which worked almost as well as updating all parameters. This can help\\nspeed up and reduce memory for fine-tuning. This can be somewhat counter-intuitive as 80% of\\nthe parameters are in the MoE layers (in the ST-MoE project). Their hypothesis for that\\narchitecture is that, as expert layers only occur every 1/4 layers, and each token sees at most two\\nexperts per layer, updating the MoE parameters affects much fewer layers than updating other\\nparameters.\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n16/24\\nBy only freezing the MoE layers, we can speed up the training while preserving the quality. This image is from\\nthe ST-MoE paper.\\nOne last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning\\nhyperparameter setups - e.g., sparse models tend to benefit more from smaller batch sizes and\\nhigher learning rates.\\nSparse models fine-tuned quality improves with higher learning rates and smaller batch sizes. This image is from\\nthe ST-MoE paper.\\nAt this point, you might be a bit sad that people have struggled to fine-tune MoEs. Excitingly, a\\nrecent paper, MoEs Meets Instruction Tuning (July 2023), performs experiments doing:\\nSingle task fine-tuning\\nMulti-task instruction-tuning\\nMulti-task instruction-tuning followed by single-task fine-tuning\\nWhen the authors fine-tuned the MoE and the T5 equivalent, the T5 equivalent was better. When\\nthe authors fine-tuned the Flan T5 (T5 instruct equivalent) MoE, the MoE performed significantly\\nbetter. Not only this, the improvement of the Flan-MoE over the MoE was larger than Flan T5 over\\nT5, indicating that MoEs might benefit much more from instruction tuning than dense models.\\nMoEs benefit more from a higher number of tasks. Unlike the previous discussion suggesting to\\nturn off the auxiliary loss function, the loss actually prevents overfitting.\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n17/24\\nSparse models benefit more from instruct-tuning compared to dense models. This image is from the MoEs Meets\\nInstruction Tuning paper\\nExperts are useful for high throughput scenarios with many machines. Given a fixed compute\\nbudget for pretraining, a sparse model will be more optimal. For low throughput scenarios with\\nlittle VRAM, a dense model will be better.\\nNote: one cannot directly compare the number of parameters between sparse and dense models,\\nas both represent significantly different things.\\nThe initial MoE work presented MoE layers as a branching setup, leading to slow computation as\\nGPUs are not designed for it and leading to network bandwidth becoming a bottleneck as the\\ndevices need to send info to others. This section will discuss some existing work to make\\npretraining and inference with these models more practical. MoEs go brrrrr.\\nWhen to use sparse MoEs vs dense models?\\nMaking MoEs go brrr\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n18/24\\nLet’s do a brief review of parallelism:\\nData parallelism: the same weights are replicated across all cores, and the data is partitioned\\nacross cores.\\nModel parallelism: the model is partitioned across cores, and the data is replicated across\\ncores.\\nModel and data parallelism: we can partition the model and the data across cores. Note that\\ndifferent cores process different batches of data.\\nExpert parallelism: experts are placed on different workers. If combined with data\\nparallelism, each core has a different expert and the data is partitioned across all cores\\nWith expert parallelism, experts are placed on different workers, and each worker takes a different\\nbatch of training samples. For non-MoE layers, expert parallelism behaves the same as data\\nparallelism. For MoE layers, tokens in the sequence are sent to workers where the desired experts\\nreside.\\nParallelism\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n19/24\\nIllustration from the Switch Transformers paper showing how data and models are split over cores with different\\nparallelism techniques.\\nIncreasing the capacity factor (CF) increases the quality but increases communication costs and\\nmemory of activations. If all-to-all communications are slow, using a smaller capacity factor is\\nbetter. A good starting point is using top-2 routing with 1.25 capacity factor and having one expert\\nper core. During evaluation, the capacity factor can be changed to reduce compute.\\n“You can deploy mistralai/Mixtral-8x7B-Instruct-v0.1 to Inference Endpoints. ”\\nA big downside of MoEs is the large number of parameters. For local use cases, one might want to\\nuse a smaller model. Let\\'s quickly discuss a few techniques that can help with serving:\\nThe Switch Transformers authors did early distillation experiments. By distilling a MoE back\\nto its dense counterpart, they could keep 30-40% of the sparsity gains. Distillation, hence,\\nprovides the benefits of faster pretaining and using a smaller model in production.\\nRecent approaches modify the routing to route full sentences or tasks to an expert, permitting\\nextracting sub-networks for serving.\\nAggregation of Experts (MoE): this technique merges the weights of the experts, hence\\nreducing the number of parameters at inference time.\\nFasterMoE (March 2022) analyzes the performance of MoEs in highly efficient distributed systems\\nand analyzes the theoretical limit of different parallelism strategies, as well as techniques to skew\\nexpert popularity, fine-grained schedules of communication that reduce latency, and an adjusted\\ntopology-aware gate that picks experts based on the lowest latency, leading to a 17x speedup.\\nCapacity Factor and communication costs\\nServing techniques\\nMore on efficient training\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n20/24\\nMegablocks (Nov 2022) explores efficient sparse pretraining by providing new GPU kernels that\\ncan handle the dynamism present in MoEs. Their proposal never drops tokens and maps efficiently\\nto modern hardware, leading to significant speedups. What’s the trick? Traditional MoEs use\\nbatched matrix multiplication, which assumes all experts have the same shape and the same\\nnumber of tokens. In contrast, Megablocks expresses MoE layers as block-sparse operations that\\ncan accommodate imbalanced assignment.\\nBlock-sparse matrix multiplication for differently sized experts and number of tokens (from [MegaBlocks]\\n(https://arxiv.org/abs/2211.15841)).\\nThere are nowadays several open source projects to train MoEs:\\nMegablocks: https://github.com/stanford-futuredata/megablocks\\nFairseq: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm\\nOpenMoE: https://github.com/XueFuzhao/OpenMoE\\nIn the realm of released open access MoEs, you can check:\\nSwitch Transformers (Google): Collection of T5-based MoEs going from 8 to 2048 experts.\\nThe largest model has 1.6 trillion parameters.\\nNLLB MoE (Meta): A MoE variant of the NLLB translation model.\\nOpenMoE: A community effort that has released Llama-based MoEs.\\nOpen Source MoEs\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n21/24\\nMixtral 8x7B (Mistral): A high-quality MoE that outperforms Llama 2 70B and has much\\nfaster inference. A instruct-tuned model is also released. Read more about it in the\\nannouncement blog post.\\nFurther experiments on distilling a sparse MoE back to a dense model with less parameters but\\nsimilar number of parameters.\\nAnother area will be quantization of MoEs. QMoE (Oct. 2023) is a good step in this direction by\\nquantizing the MoEs to less than 1 bit per parameter, hence compressing the 1.6T Switch\\nTransformer which uses 3.2TB accelerator to just 160GB.\\nSo, TL;DR, some interesting areas to explore:\\nDistilling Mixtral into a dense model\\nExplore model merging techniques of the experts and their impact in inference time\\nPerform extreme quantization techniques of Mixtral\\nAdaptive Mixture of Local Experts (1991)\\nLearning Factored Representations in a Deep Mixture of Experts (2013)\\nOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)\\nGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun\\n2020)\\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021)\\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\\n(Jan 2022)\\nExciting directions of work\\nSome resources\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n22/24\\nST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)\\nFasterMoE: modeling and optimizing training of large-scale dynamic pre-trained\\nmodels(April 2022)\\nMegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022)\\nMixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language\\nModels (May 2023)\\nMixtral-8x7B-v0.1, Mixtral-8x7B-Instruct-v0.1.\\nCitation\\n@misc {sanseviero2023moe,\\n    author       = { Omar Sanseviero and\\n                     Lewis Tunstall and\\n                     Philipp Schmid and\\n                     Sourab Mangrulkar and\\n                     Younes Belkada and\\n                     Pedro Cuenca\\n                   },\\n    title        = { Mixture of Experts Explained },\\n    year         = 2023,\\n    url          = { https://huggingface.co/blog/moe },\\n    publisher    = { Hugging Face Blog }\\n}\\nSanseviero, et al., \"Mixture of Experts Explained\", Hugging Face Blog, 2023.\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n23/24\\nMore articles from our Blog\\nCompany\\nTOS\\nPrivacy\\nAbout\\nJobs\\nWebsite\\nModels\\nDatasets\\nSpaces\\nWelcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\nBy\\xa0lewtun\\nDecember 11, 2023\\n1/12/24, 9:50 AM\\nMixture of Experts Explained\\nhttps://huggingface.co/blog/moe\\n24/24\\nPricing\\nDocs\\n© Hugging Face\\n', 'source_name': 'HuggingFace MoE Article', 'source_url': 'https://huggingface.co/blog/moe'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "OpenMoE.pdf #78\n",
      "{'content': 'OpenMoE: An Early Effort on Open\\nMixture-of-Experts Language Models\\nFuzhao Xue1†\\nZian Zheng1\\nYao Fu2\\nJinjie Ni1\\nZangwei Zheng1\\nWangchunshu Zhou3\\nYang You1\\n1National University of Singapore\\n2University of Edinburgh\\n3ETH Zurich\\nAbstract\\nTo help the open-source community have a better understanding of Mixture-of-\\nExperts (MoE) based large language models (LLMs), we train and release Open-\\nMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs,\\nranging from 650M to 34B parameters and trained on up to over 1T tokens. Our\\ninvestigation confirms that MoE-based LLMs can offer a more favorable cost-\\neffectiveness trade-off than dense LLMs, highlighting the potential effectiveness\\nfor future LLM development.\\nOne more important contribution of this study is an in-depth analysis of the routing\\nmechanisms within our OpenMoE models, leading to three significant findings:\\nContext-Independent Specialization, Early Routing Learning, and Drop-towards-\\nthe-End. We discovered that routing decisions in MoE models are predominantly\\nbased on token IDs, with minimal context relevance. The token-to-expert assign-\\nments are determined early in the pre-training phase and remain largely unchanged.\\nThis imperfect routing can result in performance degradation, particularly in se-\\nquential tasks like multi-turn conversations, where tokens appearing later in a\\nsequence are more likely to be dropped. Finally, we rethink our design based on\\nthe above-mentioned observations and analysis. To facilitate future MoE LLM\\ndevelopment, we propose potential strategies for mitigating the issues we found\\nand further improving off-the-shelf MoE LLM designs.3\\n†Email: f.xue@u.nus.edu\\n3https://github.com/XueFuzhao/OpenMoE\\nPreprint.\\narXiv:2402.01739v1  [cs.CL]  29 Jan 2024\\n1\\nIntroduction\\nLarge Language Model (LLM) has exhibited remarkable performance on various NLP tasks [27,\\n35], and has even become a part of our daily lives through chatbot applications such as ChatGPT,\\nBard, and Copilot. However, LLMs are computationally expensive in both training and inference. As\\nLLMs become increasingly prevalent, enhancing their performance without proportionally increasing\\ncomputational resources is a critical challenge. In response to this challenge, Fedus et al. [15] and\\nRiquelme et al. [36] proposed the Mixture-of-Experts (MoE) to scale up the trainable parameters of\\nthe transformer with little additional computation overhead. Recent advancements in MoE-based\\nlanguage models, such as GLaM [14] and ST-MoE [59] have demonstrated superior performance in\\nvarious tasks. However, before the release of OpenMoE, there were few open-sourced MoE language\\nmodels trained with trillion-level diverse datasets.\\nIn this work, we set forth three primary goals: (1) To offer a first-attempt solution in detail for training\\na decoder-only MoE model within the existing framework of training LLMs. (2) To perform an\\nin-depth analysis of the MoE routing mechanisms, thereby providing the research community with\\ndeeper insights into the behaviors and potential limitations of MoE-based LLMs. (3) To pave the way\\nfor future MoE LLM development. Through this early endeavor, we aim to stimulate and accelerate\\nthe growth of the open-source MoE community.\\nReleasing OpenMoE. First, we release OpenMoE, a series of open-sourced MoE-based LLMs,\\nincluding: (1) OpenMoE-Base/16E: a small model with 0.65B parameters for debugging purposes.\\n16E means 16 experts per MoE layer; (2) OpenMoE-8B/32E: this variant features 8B parameters\\nin total, activating around 2B parameters per token in Transformer blocks, and is pre-trained on\\nover 1 trillion tokens; (3) OpenMoE-8B/32E-Chat, a chat version of OpenMoE-8B/32E, fine-tuned\\nwith a 100K subset of the WildChat [2] dataset; (4) OpenMoE-34B/32E: a larger scale model,\\nactivating 6B parameters per token in Transformer blocks and trained with 200B tokens, serving as\\na testament to the scalability of our approach. Detailed configuration can be found in Appendix B\\nOur OpenMoE-8B/32E models achieved comparable performance with OpenLLaMA-3B [18] and\\nTinyLLaMA-1.1B [54], two dense open LLMs used higher training cost. Notably, On the MT-\\nBench [56], OpenMoE-8B/32E-Chat outperformed the two dense LLMs significantly on the single-\\nturn conversation. In addition, we release 5 intermediate checkpoints of OpenMoE-8B/32E, each\\ntrained with 200B more tokens than the previous one, to support and encourage future research.\\nSection 2 and 3 will discuss the design, training details, and evaluation results of OpenMoE.\\nExploring Advanced Training Strategies. As part of our research endeavor, we are committed\\nto exploring more advanced Techniques in LLM training: (1) Different from the common practice\\nof training models on in-house or text-dominated open-sourced data, we train OpenMoE with a\\nsubstantial proportion of code, constituting up to 52.25% during the early stages of pre-training; (2)\\nMoving beyond the conventional next-token prediction training objective, we investigate UL2 training\\nobjective [45], motivated by its proven effectiveness in previous work [1] and its good alignment\\nwith coding data [5]. We acknowledge that the performance of our model, while acceptable, does\\nnot significantly exceed our expectations, which may be attributed to some sub-optimal design\\nchoices. Nevertheless, we believe that this exploratory work offers substantial value to the open-\\nsource community, particularly in assessing the potential and effectiveness of these under-explored\\ntechniques.\\nStudying MoE Routing In-depth. While MoE is effective, there remains a lack of study on why\\nMoE performs well. From a high level, MoE introduces more trainable parameters than its dense\\ncounterpart. To keep the FLOPs fixed when scaling the number of parameters, MoE applies a routing\\nlayer that sparsely and adaptively assigns each token to a few experts. This process of sparse expert\\nselection is crucial to MoE’s functionality. Unfortunately, despite existing pieces of literature briefly\\nvisualizing the routing decison [26, 31, 36, 40, 59], we still don’t have a clear understanding of\\nhow the router works and how the routing decision impacts the results in MoE models, especially\\nfor the post-ChatGPT LLMs trained on a mixture of datasets from diverse domains. In this work,\\nwe study this problem based on various taxonomies, including domain, language, task, and token.\\nOur key findings are as follows: (1) Context-independent Specialization: MoE tends to simply\\ncluster tokens based on similar token-level semantics, implying that, regardless of context, a certain\\ntoken is more likely to be routed to a certain expert; (2) Early Routing Learning: Token ID routing\\nspecialization is established early in pre-training and remains largely fixed, resulting in tokens being\\nconsistently processed by the same experts throughout the training; (3) Drop-towards-the-End:\\n2\\nSince each expert has a fixed max capacity, tokens appearing later in the sequence face a higher risk\\nof being dropped if the expert is already at capacity. This issue is more severe in instruction-tuning\\ndatasets. These datasets often exhibit a domain gap compared to the pre-training data, meaning\\nthat the balanced token assignment strategies established and solidified during early pre-training\\nmay not be samely effective in instruction-tuning scenarios. This is concerning as instruction data\\nplays an important role in deploying LLMs to real-world applications. Section 4 discusses the above\\nphenomenons in detail.\\nRethinking Our Mistakes and Proposing Potential Solutions. In retrospect, our project encoun-\\ntered several mistakes and made sub-optimal decisions (e.g., aggressive data mixture), as detailed\\nin Section 5. As an early open-source effort, we believe that sharing these experiences and insights\\nis crucial, perhaps even more important than solely focusing on successful strategies. Based on\\nour empirical findings during training and subsequent visualization analysis (Section 4), we have\\ndeveloped a set of potential solutions. We sincerely hope these insights can help the community\\ndevelop better models in the future.\\nThe structure of this paper mirrors the lifecycle of the OpenMoE project, encompassing all its phases.\\nThis includes the initial design (Section 2), training and evaluation (Section 3, in-depth analysis\\n(Section 4), and a rethinking of the OpenMoE project (Section 5).\\n2\\nDesigning OpenMoE\\nFirst, we introduce our initialized design of OpenMoE models regarding the pre-training data, model\\narchitecture, training objective, and supervised fine-tuning data.\\n2.1\\nPre-training Dataset: More Code than Usual\\nModern LLMs are usually trained by a combination of datasets from various domains, i.e., data\\nmixture [7, 9, 20, 34, 47]. Except for the LLMs customized towards coding (e.g., StarCoder [28],\\nCodeLLaMA [38]), most existing models’ pre-training data is dominated by text data. For instance,\\nthe sampling rate of the GitHub dataset is only 4.5% for LLaMA [47]. However, we argue that the\\ncode data is highly important for two reasons. First, the code data is likely to improve the ability of\\ncomplex reasoning with chain-of-thought [16]. More importantly, different from natural language,\\nwhich is sometimes blurry and easy to misunderstand, code is always precise. This enables code to\\nbe a more efficient language for machines to convey information concisely without misunderstanding\\nbetween different (embodied) AI agents, and as a result, code has great potential to dominate LLM\\ncommunications in real-life applications. Therefore, we design a more code-dominated pre-training\\ndata mixture. As shown in Table 1, we extracted 50% of data from the RedPajama [11] and 50% of\\ndata from the duplication version of The Stack [24]. Our experimental results show that the version I\\ndata mixture might be a bit aggressive in its code proportion. We fix these issues at the later stage of\\npre-training, please see the following Section 3.2 for details.\\nTable 1: Three versions of OpenMoE pre-training data mixture.\\nVersion I\\nVersion II\\nVersion III\\nModel\\nOpenMoE-Base, OpenMoE-8B/32E\\nOpenMoE-34B/32E\\nPeriod\\nbefore 780B tokens →after 780B tokens\\nfrom start to end\\nDataset\\nSampling Ratio\\nRedPajama\\n50.0%\\n83.5%\\n67.5%\\nC4\\n7.50%\\n15.0%\\n15.0%\\nWikipedia\\n2.25%\\n6.50%\\n4.50%\\nStackexchange\\n1.00%\\n2.50%\\n1.00%\\nArXiv\\n1.25%\\n4.50%\\n4.50%\\nBooks\\n2.25%\\n6.50%\\n4.50%\\nGitHub\\n2.25%\\n5.00%\\n5.00%\\nCommoncrawl\\n33.5%\\n43.5%\\n33.0%\\nWikipedia-en\\n0.00%\\n6.50%\\n2.50%\\nThe Stack Dedup\\n50.0%\\n10.0%\\n30.0%\\n3\\n2.2\\nModel Architecture: Decoder-only ST-MoE\\nTokenizer. We applied umT5 [10] tokenizer with 256K vocab size for two reasons: (1) umT5\\ntokenizer with a large multi-lingual vocab supports low-resource language better than the tokenizers\\nusing a small vocab (e.g., LLaMA tokenizer with 32K vocab); (2) comparing to some old tokenizers,\\nsuch as BERT [23] and T5 [35] tokenizer, umT5 tokenizer has byte fallback feature to support\\nout-of-vocab tokens better.\\nToken-choice Routing. We generally follow ST-MoE [59] for our model architecture and routing\\ndesign to ensure training stability, which is extremely important when training larger models. Given\\nE trainable experts and input representation x ∈RD, the output of MoE model can be formulated as:\\nMoE(x) =\\nE\\nX\\ni=1\\ng(x)iei(x),\\n(1)\\nwhere ei(·) is a non-linear transformation RD →RD of the ith expert, and g(·)i is the ith element of\\nthe output of the trainable router g(·), a non-linear mapping RD →RE. Usually, both e(·) and g(·)\\nare parameterized by neural networks. Please note each expert is an FFN layer instead of a complete\\nTransformer model in most MoE-based Transformer models, including ours.\\nTop-2 Selection. According to the formulation above, when g(·) is a sparse vector, only part of the\\nexperts would be activated and updated by back-propagation during training. We set the gating layer\\nas a top-K selection as:\\ng(x) = TopK(softmax(f(x))),\\n(2)\\nwhere f(·) is routing linear transformation RD →RE. When K ≪E, most elements of g(x) would\\nbe zero so that sparse conditional computation is achieved. We set K = 2 following Zoph et al. [59].\\nResidual MoE. Each vanilla Transformer block can be written as:\\nx′ = LayerNormatt\\ni (x),\\nx = MHA(x′) + x,\\nx′′ = LayerNormffn\\ni (x),\\nx = FFN(x′′) + x,\\n(3)\\nIn OpenMoE, for each MoE-based Transformer block, we use one residual MoE layer to ensure that\\none fixed FFN layer is always activated for every token. That is:\\nx′ = LayerNormatt\\ni (x),\\nx = MHA(x′) + x,\\nx′′ = LayerNormffn\\ni (x),\\nx = MoE(x′′) + FFN(x′′) + x,\\n(4)\\nNote we use MoE-based Transformer blocks in an interleaved manner instead of placing MoE in\\nevery Transformer block. In our setting, we use MoE every 4 layers in OpenMoE-Base/16E and\\nOpenMoE 34B/32E and use MoE every 6 layers for OpenMoE-8B/32E. This setting is inspired by\\nthe findings in ViT-MoE [36], i.e., using MoE every layer introduces more computational overhead\\nduring routing, and then induces a worse cost-effective trade-off than interleaved MoE usage.\\nLoad Balance Loss and Router Z-loss. ST-MoE [59] follows Shazeer et al. [40], using MoE load\\nbalance loss to ensure a balanced number of tokens assigned to different experts so that MoE models\\ncan achieve better parallelism. For each routing operation, given E experts and N batches with\\nB = NL tokens, the following auxiliary loss is added to the total model loss during training:\\nLb = E ·\\nE\\nX\\ni=1\\nmi · Pi,\\n(5)\\nwhere m is a vector, Pi is softmax(f(x)). i denotes the expert ID. The ith element is the fraction of\\ntokens dispatched to expert i:\\nmi = 1\\nB\\nB\\nX\\nj=1\\nh(xj)i,\\n(6)\\n4\\nTable 2: UL2’s mixture-of-denoisers configuration, µ is average span length and r is the mask ratio.\\nTraining Objective\\nPercentage\\nPrefixLM, r=0.5\\n50%\\nSpanCorrupt\\nµ=3, r=0.15\\n10%\\nµ=8, r=0.15\\n10%\\nµ=3, r=0.5\\n10%\\nµ=8, r=0.5\\n10%\\nµ=64, r=0.5\\n10%\\nwhere h(·) is an index vector selected by TopK in Eq. 2. h(xj)i is the ith element of h(xj). It is\\nnoticeable that, different from g(x)i in Eq. 2, mi and h(xj)i are non-differentiable. However, a\\ndifferentiable loss function is required to optimize MoE in an end-to-end fashion, so we use the\\nrouting score softmax(f(x)) in Eq. 2 (i.e., Pi in Eq. 5)to make the routing decision differentiable\\nand then learnable.\\nIn addition to the load balance loss, Zoph et al. [59] proposed router z-loss for more stable MoE\\ntraining:\\nLz(x) = 1\\nB\\nB\\nX\\ni=1\\n\\uf8eb\\n\\uf8edlog\\nE\\nX\\nj=1\\nex(i)\\nj\\n\\uf8f6\\n\\uf8f8\\n2\\n(7)\\nThis router z-loss can penalize large logits input to the gating network and encourage the absolute\\nmagnitude of numbers to be small so that it can reduce the round-off errors in MoE layers. Please\\nrefer to ST-MoE paper [59] for a detailed explanation.\\nTaken together, our final training loss can be written as:\\nL = LCE + Lb + Lz\\n(8)\\nwhere LCE is the cross-entropy loss in language model pre-training.\\n2.3\\nTraining Objective: UL2 and CasualLM\\nInstead of adopting vanilla casual language modeling (CasualLM) directly, we explore UL2 [44], a\\nmore diverse language model pre-training objective combining span corruption (SpanCorrupt) and\\nprefix language modeling (PrefixLM) [35]. It is noteworthy that the SpanCorrupt in UL2 is more\\ndiverse than the vanilla SpanCorrupt because it mixes various span lengths and corruption rates. We\\nhave two reasons to explore UL2 in OpenMoE. First, UL2 has shown promising results in PaLM-2 [1].\\nMore importantly, the aggressive token masking is very similar to the code completion task in the\\nreal world, such as Copilot. Bavarian et al. [5] also found that the similar filling-in-the-middle (FiM)\\nobjective can model the code better than the vanilla training objective. Since we used more code in\\nour pre-training data mixture, adapting UL2 that covers FiM is a more reasonable choice intuitively.\\nOur detailed UL2 training objective configuration is shown in Table 2. We use only 20% low mask\\nratio (r=0.15) because there are fewer output tokens during training, which may slow down the\\nlearning. We also use more PrefixLM than the default UL2 setting because we think the zero-shot and\\nin-context learning ability enhanced by PrefixLM training is important. We faced some difficulties\\nwhen training with UL2 in OpenMoE, which will be discussed in Section 3.2.\\n2.4\\nSupervised Fine-tuning\\nAlthough alignment is not the focus of this OpenMoE project, we still conduct supervised fine-tuning\\n(SFT) with a subset of the open-sourced WildChat dataset [2] to enhance the instruction following\\nability and study the behavior of the MoE model before and after SFT. We only pick the instruction-\\nresponse pairs from GPT-4 in WildChat because of the lack of computation resources at the late stage\\nof OpenMoE development. The subset includes 58K conversations and each conversation includes\\n1.8 turns on average.\\n5\\nTable 3: Ablation study with OpenMoE-Base/16E on zero-shot TriviaQA [22].\\nMethod\\nEM\\nF1\\nOpenMoE\\n1.4\\n4.5\\nw/o MoE\\n0.1\\n0.3\\nw/o UL2 (PrefixLM only)\\n0.0\\n0.0\\nw/o Code data\\n0.7\\n1.1\\nw/ LLaMA tokenizer\\n2.2\\n5.7\\n0\\n1\\n2\\n3\\n4\\n5\\nStep\\n1e5\\n1\\n2\\n3\\n4\\n5\\n6\\nLoss\\nC4\\nCommonCrawl\\nBooks\\nWikipedia\\nStackExchange\\nArXiv\\nTheStack\\nGitHub\\n(a) Comparison of the validation loss on different\\npre-training datasets.\\n0\\n1\\n2\\n3\\n4\\n5\\nStep\\n1e5\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAccuracy\\nC4\\nCommonCrawl\\nBooks\\nWikipedia\\nStackExchange\\nArXiv\\nTheStack\\nGitHub\\n(b) Comparison of validation accuracy on different\\npre-training datasets.\\nFigure 1: Comparison of the validation loss and accuracy on different pre-training datasets. We can\\nobserve that models are easier to achieve higher accuracy and lower loss on code data.\\n2.5\\nOther Designs\\nFollowing recent LLMs, we adopt RoPE [43] for position embedding and SwiGLU [39] for activation\\nfunction for FFNs in both dense and MoE Transformer blocks. More detailed model configuration\\nand training hyperparameters for OpenMoE models can be found in Appendix B. We applied data\\nparallelism, tensor parallelism [41, 50], and expert parallelism [25] for training models at scale. We\\ntrain OpenMoE models on Google Cloud TPU with 64 to 512 v3 chips depending on the availability.\\n3\\nTraining OpenMoE\\n3.1\\nAblation Study\\nAs an initial evaluation of our design decisions, we conducted an ablation study using the OpenMoE-\\nBase/16E model. It’s important to note that while these results provide early insights, we cannot be\\ncertain of their generalizability to larger models, primarily due to computational resource constraints\\nthat preclude larger-scale ablations.\\nOur findings indicate that several elements — the MoE approach, the UL2 training objective, and the\\nincreased emphasis on code data — all contribute positively to the base version’s performance in\\nzero-shot TriviaQA tasks. The model using LLaMA tokenizer [47] outperforms the one with umT5\\ntokenizer. This outcome is considered acceptable, even though a larger vocabulary size might slightly\\nimpair performance. We believe that supporting low-resource languages is crucial, as foundational\\nmodels should be accessible and beneficial to a diverse global audience. After this sanctity check, we\\nproceed to scale OpenMoE up to OpenMoE-8B/32E.\\nWe also conduct an ablation study to compare the progress of learning the data from different domains.\\nAs shown in Figure 1, we can observe that models are easier to achieve higher accuracy and lower loss\\non code data. On Github, although our model is small, it can still achieve over 80% token prediction\\naccuracy. We infer that this is because of the long-tail token distribution in code data. For instance, a\\nlarge number of tokens in code are “\\\\n” and “\\\\t”, which are relatively easier to predict.\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\nStep\\n1e5\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nAccuracy\\nOpenMoE-Base/16E\\nOpenMoE-8B/32E\\nOpenMoE-34B/32E\\nFigure 2: Token prediction accuracy of OpenMoE models. OpenMoE-8B/32E uses UL2 before 390K\\nsteps and falls back to CasualLM after 790K steps. OpenMoE-34B/32E uses UL2 until 50B tokens.\\n3.2\\nTraining Progress\\nUL2 Saturation During training, we found that, although UL2 can help the model to learn faster at\\nthe early stage of training, it is easier to saturate at the later training stage of OpenMoE-8B/32E. As\\nshown in Figure 2, if we zoom in, we can find that OpenMoE-8B/32E improves very slowly from 35K\\nto 39K steps. We suggest that this may be because, although UL2 is more diverse, the SpanCorrupt is\\nstill relatively easy compared to CasualLM. Therefore, we fall back to CasualLM after 390K steps\\n(780B) tokens. In addition, since code data aligns better with UL2 and our initial code data mixture\\nis relatively aggressive, we also decreased our code data sampling ratio to 15%. The Second version\\ndata mixture is reported in Table 1.\\nObviously, in Figure 2, after 780B tokens, there is a significant drop in the token prediction accuracy\\nafter 390K steps for OpenMoE-8B/32E. This is caused by the more difficult CasualLM objective\\nand less easy code data. Note that, although we encountered a saturation issue at the later stage\\nof OpenMoE-8B/32E training, we think such an easy-to-hard curriculum may be helpful for LLM\\ntraining. Therefore, we still adapted UL2 for 25K steps (50B tokens) in OpenMoE-34B/32E. We used\\na relatively moderate code-heavy data mixture in OpenMoE-34B/32E. As shown in Table 1, we utilize\\n35% of code data in total. Due to the computation resource limitation, we train OpenMoE-34B/32E\\nwith only 200B tokens to verify its scalability. We leave training a large-scale OpenMoE with more\\ntokens as future work if possible.\\n3.3\\nEvaluation on Benchmarks\\n3.3.1\\nRaw Model Evaluation\\nBefore all, we highlight that we did not hack the benchmarks at all and the pre-training is purely\\non the open-sourced datasets mentioned above. Since our model is relatively small in terms of\\ntraining budget, we mainly evaluate the raw model on established but not that hard benchmarks,\\ni.e., TriviaQA [22], HumanEval [8], WMT16-En-Ro [6], BigBench-Lite (24 tasks) [4], and a subset\\nof the lm-evaluation-harness collection [17] with 13 tasks. For popular but relatively challenging\\nbenchmarks like 5-shot MMLU [19], our OpenMoE-8B/32E achieves around 26.2% accuracy, which\\nmeans the model is almost randomly guessing from the four options. We mainly compare with the\\nopen-sourced models with more training cost, i.e., TinyLLaMA-1.1B [54] and OpenLLaMA-3B [18].\\nOn BigBench-Lite, we also compare with GPT-3 [7], Big-G [4] and Big-G-Sparse [4]. Big-G and\\nBig-G-Sparse are two sets of Google in-house Transformer models evaluated on BigBench-Lite, and\\nBig-G-Sparse models are MoE-based Transformers.\\nWe first report our results on Commonsense QA (TriviaQA), Coding (HumanEval), and Low-resource\\nMachine Translation (WMT16 En-Ro). We think these three benchmarks are meaningful for us\\nbecause (1) Commonsense is to check whether OpenMoE can memorize more commonsense given\\nits efficient parameter scaling advantage; (2) Coding is important because of its prevalent use cases in\\nsolving coding-related user prompts, LLM as agents, and embodied AI; (3) Low-resource Machine\\nTranslation is important because we want to share the benefits of foundation models to everyone on\\n7\\nTable 4: Results on TriviaQA (Exact Match). We also report the number of training tokens from\\nWikipedia because the commonsense questions in TriviaQA have a relatively close relation with\\nWikipedia data.\\nModel\\nAct. Params\\nTotal Tokens\\nText Tokens\\nWiki Tokens\\nTriviaQA\\nTinyLLaMA-1.1B\\n0.9B\\n3.0T\\n2.1T\\n75B\\n11.2\\nOpenLLaMA-3B\\n2.9B\\n1.0T\\n991B\\n24B\\n29.7\\nOpenMoE-8B/32E\\n2.1B\\n1.1T\\n644B\\n58B\\n32.7\\nOpenMoE-34B/32E\\n6.4B\\n0.2T\\n130B\\n14B\\n31.3\\nTable 5: Results on HumanEval (Pass@1). We also report the number of training tokens from the\\ncode domain (The Stack and GitHub data).\\nModel\\nAct. Params\\nTotal Tokens\\nCode Tokens\\nHumanEval\\nTinyLLaMA-1.1B\\n0.9B\\n3.0T\\n900B\\n9.1\\nOpenLLaMA-3B\\n2.9B\\n1.0T\\n59B\\n0\\nOpenMoE-8B/32E\\n2.1B\\n1.1T\\n456B\\n9.8\\nOpenMoE-34B/32E\\n6.4B\\n0.2T\\n70B\\n10.3\\nearth. As shown in Table 4, OpenMoE-8B/32E outperforms baselines clearly with less training cost\\n(Activated parameter×Total Training tokens). Also, please note that TinyLLaMA-1.1B performs\\nsignificantly worse than other models on TriviaQA although it has a comparable training cost with\\nOpenLLaMA-3B. Therefore, this highlights the importance of the number of parameters to keeping\\nknowledge in LLMs, which also indicates the significance of using MoE.\\nIn Table 5, OpenMoE models achieve better performance than baselines. OpenMoE-34B/32E\\nonly used 70B code data, while it still performs relatively well on HumanEval, which shows the\\nscalability of OpenMoE, although we don’t have enough computation resources to train it until the\\nend. OpenLLaMA-3B struggles on HumanEval because consecutive whitespaces are treated as one,\\ncontradicting the Python syntax[32].\\nTable 6 shows our results on WMT16 En-Ro translation task. Note that our model did not include\\nmuch multi-lingual data intentionally. Most multi-lingual data should be from the multi-lingual\\nversion of Wikipedia in the RedPajama, which is also used in TinyLLaMA-1.1B and OpenLLaMA-3B.\\nHowever, OpenMoE models still show better results than baselines, which potentially highlights the\\nimportance of umT5 tokenizer.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nRelative Cost\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nBigBench-Lite (3-shot)\\nBIG-G-42M\\nBIG-G-1B\\nBIG-G-2B\\nBIG-G-4B\\nBIG-G-8B\\nBIG-G-27B\\nBIG-G-Sparse-4B\\nBIG-G-Sparse-7B\\nBIG-G-Sparse-17B\\nBIG-G-Sparse-25B\\nBIG-G-Sparse-60B\\nGPT-3-35M\\nGPT-3-76M\\nGPT-3-1B\\nGPT-3-3B\\nGPT-3-6B\\nGPT-3-13B\\nOpenMoE-8B\\nModel\\nBIG-G\\nBIG-G-Sparse\\nGPT-3\\nOpenMoE\\nFigure 3: Results on BigBench-Lite. The relative cost is computed based on multiplying activated\\nparameters in the Transformer and the number of training tokens. The size of the color dots denotes\\nthe number of activated parameters, and the size of the shadow denotes the number of total parameters\\nfor MoE models.\\n8\\nTable 6: Results on WMT16 En-Ro (BLEU score). We also report the number of explicit multi-lingual\\ntokens in the pre-training dataset, i.e., the multi-lingual version of Wikipedia from the RedPajama\\ndataset.\\nModel\\nAct. Params\\nTotal Tokens\\nMulti-lingual Tokens\\nWMT16 En-Ro\\nTinyLLaMA-1.1B\\n0.9B\\n3.0T\\n75B\\n2.6\\nOpenLLaMA-3B\\n2.9B\\n1.0T\\n24B\\n1.9\\nOpenMoE-8B/32E\\n2.1B\\n1.1T\\n38B\\n3.1\\nOpenMoE-34B/32E\\n6.4B\\n0.2T\\n9B\\n3.4\\nTable 7: Evaluate OpenMoE-8B/32E on lm-evaluation-harness. The results of OpenLLaMA are from\\nits homepage, which only provides two effective digits.\\nDataset\\nTinyLLaMA-1.1B\\nOpenLLaMA-3B\\nOpenMoE-8B/32E\\nANLI-R1\\n34.2\\n33.0\\n32.7\\nANLI-R2\\n32.4\\n36.0\\n33.2\\nANLI-R3\\n35.1\\n38.0\\n33.9\\nHellaSwag\\n59.2\\n52.0\\n45.5\\nWinoGrande\\n59.1\\n63.0\\n60.3\\nPIQA\\n73.3\\n77.0\\n74.2\\nARC-Easy\\n55.2\\n68.0\\n64.1\\nARC-Challenge\\n30.1\\n34.0\\n30.3\\nBoolq\\n57.8\\n66.0\\n61.2\\nTruthfulQA\\n37.6\\n35.0\\n36.0\\nOpenbookQA\\n21.8\\n26.0\\n24.6\\nRTE\\n51.9\\n55.0\\n53.4\\nWiC\\n50.1\\n50.0\\n49.8\\nAverage\\n45.9\\n48.7\\n46.1\\nIn Figure 3, the relative cost is computed based on multiplying activated parameters (Act. Params)\\nin Transformer blocks and the number of training tokens. The size of the color dots denotes the\\nnumber of activated parameters, and the size of the shadow denotes the number of total parameters\\nfor MoE models. We can observe that OpenMoE achieved a better cost-effectiveness trade-off on\\nBigBench-Lite, in terms of both training and inference cost.\\nWe also evaluate OpenMoE on the 13 tasks from the LM-Evaluation-Harness collection. As shown in\\nTable 7, both OpenMoE and TinyLLaMA performed worse than OpenLLaMA. However, the scores\\nachieved by OpenMOE are acceptable. We suggest that the initial high sampling rate on the code\\ndata may harm the results on these text-dominated benchmarks, which is one of the issues we will\\ndiscuss in Section 5.\\n3.3.2\\nChat Model Evaluation\\nWe further evaluate our model on MTBench, an established ChatBot benchmark that is able to\\nexamine models comprehensively. We report both single-turn and multi-turn results in Figure 4a and\\nTable 8. We can observe that OpenMoE outperforms baselines by a large margin on the single-turn\\nresults, especially on coding tasks. However, OpenMoE’s performance drops more on the second\\nturn, which results in worse multi-turn results in Figure 4b. We found that this probably be caused by\\nthe token drop of a long sequence. Please see the following Section 4 for a detailed analysis.\\n4\\nAnalyzing OpenMoE\\nWe generally think MoE is an effective way to scale parameters up with a fixed computation budget.\\nHowever, we have little idea about what the experts in MoE specialize in. In this section, we conduct\\nan in-depth analysis of OpenMoE in multiple aspects to study the routing behavior.\\n9\\nWriting\\nRoleplay\\nReasoning\\nMath\\nCoding\\nExtraction\\nSTEM\\nHumanities\\nGPT-J-6B\\nTinyLLaMA-1.1B\\nOpenLLaMA-3B\\nOpenMoE-8B/32E\\n(a) Single-turn results.\\nWriting\\nRoleplay\\nReasoning\\nMath\\nCoding\\nExtraction\\nSTEM\\nHumanities\\nGPT-J-6B\\nTinyLLaMA-1.1B\\nOpenLLaMA-3B\\nOpenMoE-8B/32E\\n(b) Multi-turn results.\\nFigure 4: Evaluate OpenMoE on MTBench.\\nTable 8: Average scores on MT-Bench.\\nModel\\nMT-Bench 1st Turn\\nMT-Bench 2nd Turn\\nMT-Bench Avg\\nGPT-J-6B (0.4T)\\n2.51\\n2.35\\n2.43\\nTinyLLaMA-1.1B (3T)\\n4.08\\n2.54\\n3.31\\nOpenLLaMA-3B (1T)\\n4.36\\n3.62\\n3.99\\nOpenMoE-8B/32E (1.1T)\\n4.69\\n3.26\\n3.98\\n4.1\\nWhat are the Experts Specializing in?\\nDoes MoE specialize in domain level? We first visualize the routing decision of the tokens from\\ndifferent subsets in the RedPajama dataset. Note that all visualization results are from the third MoE\\nlayer by default because we did not observe significant differences across layers. We can observe\\nthat the tokens from different subsets (i.e., domains) are uniformed distributed on the plot. That is,\\nalthough E21 slightly prefers code tokens and E10 like books a little, most experts in MoE are not\\nspecialized based on the domains.\\nDoes MoE specialize in language level? We move forward toward finer-grain data to check whether\\nMoE specializes in different coding languages and natural languages. In Figure 6, we compare 4\\ndifferent coding languages, i.e., Assembly, Blitzmax, Java, and Python. Similar to the domain level,\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\narxiv\\nbook\\nc4\\ncc\\ngithub\\nstackexchange\\nwikipedia\\nFigure 5: Visualization of the routing decision on the RedPajama dataset. Ei denotes the ratio of\\ntokens routed to ith expert.\\n10\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\nassembly\\nblitzmax\\njava\\npython\\nFigure 6: Visualization of the routing decision on TheStack dataset. Ei denotes the ratio of tokens\\nrouted to ith expert.\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\nar\\nde\\nes\\nfr\\nhe\\nit\\nja\\nko\\nnl\\nru\\nzh-cn\\nzh-tw\\nFigure 7: Visualization of the routing decision on TED-Parallel-Corpus including 12 languages,\\ni.e., ar (Arabic), de (German), es (Spanish), fr (French), he (Hebrew), it (Italian), ja (Japanese), ko\\n(Korean), nl (Dutch), ru (Russian), zh-cn (Chinese Simplified), zh-tw (Chinese, Traditional), Ei\\ndenotes the ratio of tokens routed to the ith expert.\\neven for Assembly and Blitzmax, i.e., two low-resource languages compared with Java and Python,\\nthey still did not exhibit significant expert specialization.\\nWe further study the expert specialization on different natural languages. We adopted a multi-lingual\\nparallel corpus, i.e., TED-Parallel-Corpus 4 as the platform. In Figure 7, we found that there is a\\nrelatively clear specialization among different experts. For instance, zh-cn (Chinese, Simplified) and\\nzh-tw (Chinese, Traditional) both have a strong preference for E5 and E16; ja (Japanese), and ko\\n(Korean) both prefer E14.\\nDoes MoE specialize in task level? Based on the findings above, finer-grained data has clearer\\nexpert specialization observation. We then visualize the routing decision on MT-Bench conversation\\ndata in Figure 8. We can see a similar specialization as above, especially for the math data. We\\nsuggest that the main reason is that the math tasks include more special tokens than other tasks.\\nDoes MoE specialize in Position ID? Routers in MoE make decisions based on the token represen-\\ntations. The token representations are from token embeddings and position embeddings. We thus\\n4https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\\n11\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\nwriting\\nroleplay\\nreasoning\\nmath\\ncoding\\nextraction\\nstem\\nhumanities\\nFigure 8: Visualization of the routing decision on MT-Bench. We adopt the conversation history\\nwhen evaluating OpenMoE MT-Bench as the visualization data source. Ei denotes the ratio of tokens\\nrouted to the ith expert.\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\nPosition 256\\nPosition 512\\nPosition 768\\nPosition 1024\\nPosition 1280\\nPosition 1536\\nPosition 1792\\n(a) Uniform sampled token IDs.\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\nPosition 256\\nPosition 257\\nPosition 258\\nPosition 259\\n(b) Consecutive token IDs.\\nFigure 9: Visualization of the routing decision at different Position IDs. Ei denotes the ratio of tokens\\nrouted to the ith expert.\\nvisualize the routing decisions on different positions in Figure 9a and Figure 9b. We can observe:(1)\\nthere are indeed some specializations in different Position IDs; (2) consecutive positions prefer similar\\nexperts, such as the E10 and E19 in Figure 9b.\\nDoes MoE specialize in Token ID? Since we are using the umT5 tokenizer, tokens from different\\nlanguages usually have different token IDs. Therefore, we further study whether the router in MoE\\nmainly makes its decisions based on the Token ID. We visualize the routing decisions of a few\\nrepresentative tokens in Figure 10. All these tokens show a very strong specialization on only a\\nfew experts. This is a very interesting finding because the tokens with the same Token ID have\\nvery diverse contexts in different sentences. For instance, the token “ed” can be the suffix of many\\ndifferent words, e.g., “preferred”, and “led”. The token “an” can also be part of “an apple” or\\n\"another\". However, all these tokens have very strong specialization on only a few fixed experts.\\nThat means, MoE simply routes based on the Token ID instead of high-level semantics. We name\\nthis observation as Context-independent Specialization in the following sections. To verify that\\nthe Context-independent Specialization also exists for other Token IDs, we plot the routing decision\\nstandard deviation in Appendix E.\\n12\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\n.\\n,\\n1\\n2\\na\\nan\\ned\\nFigure 10: Visualization of the routing decision at different Token IDs. Ei denotes the ratio of tokens\\nrouted to the ith expert.\\nTable 9: Top Tokens selected by each expert.\\nExpert ID\\nTop Tokens\\n0\\n\\\\n , ‘ , ’ , s , - , $ , y , _ ,\\n, 2\\n1\\n\\\\n , 1 ,\\n, 2 , \\\\\\\\ , S , . , - , C , {\\n21\\n, , and ,\\n, . , \\\\n , = , \\\\t , the ,\\n, n\\n30\\n} , ed , d , have , ing , , , has , s , \" , had\\n31\\nto , can , s , of , ing , will , not , e , ed , would\\n4.2\\nToken Specialization Study\\nAre experts clustering similar tokens? As we discussed above, the tokens with the same Token\\nID are always routed to the same expert no matter what the context is, i.e., Context-independent\\nSpecialization. We thus investigate whether the experts prefer the Token IDs corresponding to the\\ntokens with similar low-level semantics. We list the top 10 favorite tokens for each expert in Table 9.\\nWe can observe that similar tokens are clustered in experts. For instance, “can”. “will”, and “would”\\nare all in expert 31. “have”. “has”, and “had” are all included in expert 30. This visualization can also\\nexplain many observations above. An example is that, in most figures above, we can find most coding\\nand math data prefer expert 21. Here it reveals the real reason. Expert 21 has a strong preference for\\n“=”, “and”, and “\\\\n”, which appear more frequently in math and code.\\nWhen did the model learn the specialization? According to the Context-independent Specialization\\nobserved above, the model is not learning how to route based on high-level semantics. Therefore,\\nwe raise another question, when did the model learn and fix the routing decision for the tokens?\\nWe compare the routing decisions of different OpenMoE intermediate checkpoints in Figure 11a\\nand Figure 11b. We can see that the expert preferences are almost totally overlapped for different\\ncheckpoints, which means that the model has started to fix its routing at the very early stage of\\ntraining. Even if we change the training data mixture (from 52.25% code to 20% code) and training\\nobjective (from UL2 to CasualLM), the routing decision is still fixed. We infer that the reason is that,\\nwhen the token is usually assigned to one specific expert, the loss would increase a lot if the token is\\nsent to another unseen expert, which pushes the model to assign the token back to the original expert.\\nTherefore, the routing probably has been learned at the warmup stage or so, and kept throughout the\\nwhole following training stage.\\n4.3\\nToken Drop During Routing\\nDrop-towards-the-End In MoE models, we usually set a pre-defined max capacity C for every\\nexpert to ensure a balanced workload, which means each expert cannot process more than C tokens.\\n13\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\n200B training tokens\\n400B training tokens\\n600B training tokens\\n800B training tokens\\n1.0T training tokens\\n(a) Token “ed” routing decision of different intermedi-\\nate checkpoints.\\nE_0\\nE_1\\nE_2\\nE_3\\nE_4\\nE_5\\nE_6\\nE_7\\nE_8\\nE_9\\nE_10\\nE_11\\nE_12\\nE_13\\nE_14\\nE_15\\nE_16\\nE_17\\nE_18\\nE_19\\nE_20\\nE_21\\nE_22\\nE_23 E_24 E_25\\nE_26\\nE_27\\nE_28\\nE_29\\nE_30\\nE_31\\n200B training tokens\\n400B training tokens\\n600B training tokens\\n800B training tokens\\n1.0T training tokens\\n(b) Token “an” routing decision of different intermedi-\\nate checkpoints.\\nFigure 11: Visualization of token IDs’ routing decision of different intermediate checkpoints. Ei\\ndenotes the ratio of tokens routed to the ith\\n0\\n500\\n1000\\n1500\\n2000\\nPosition ID\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nT\\noken Drop Ratio (%)\\nRedpajama\\nTheStack\\nMultilingual TED\\nMTBench\\n(a) Different datasets.\\n0\\n500\\n1000\\n1500\\n2000\\nPosition ID\\n0\\n10\\n20\\n30\\n40\\n50\\nT\\noken Drop Ratio (%)\\nRaw Model\\nChat Model\\n(b) Before and after supervised fine-tuning.\\nFigure 12: Comparing the ratio of tokens dropped at different position IDs.\\nThis can ensure the throughput when training and deploying the MoE model with expert parallelism,\\ni.e., distributing different experts to different GPUs. However, this will also introduce an issue, the\\nlater tokens would be dropped if the previous tokens have filled the expert. In decoder-only MoE\\narchitecture, due to the auto-regressive nature, the later tokens in a sequence may be dropped more.\\nFor instance, if one expert prefers “\\\\n” token, and a sequence starts with many “\\\\n”s and also has a\\nlot of “\\\\n’s in the following output generated, the expert would be filled with “\\\\n” tokens quickly and\\nall other tokens appeared later, which should be assigned to this expert, would be dropped. To verify\\nthis, we visualize the ratio of tokens dropped at different position IDs. As shown in Figure 12a, the\\ngeneral pre-training datasets, e.g., RedPajama and TheStack achieved balanced token assignment,\\nonly having a small proportion of tokens dropped, even for the Position ID after 1500. However,\\nfor multi-lingual and instruction-following datasets, a large ratio of tokens is dropped. We suggest\\nthe reason is, as we discussed above, the routing decision is fixed at the early stage of training and\\ndoes not change anymore, so the load balance is also achieved based on the pre-training dataset. The\\ninstruction following data can be seen as a type of out-of-domain (OOD) data of the MoE router,\\nwhich would induce an unbalanced token assignment so that many tokens appearing later would be\\ndropped.\\nCan supervised fine-tuning with instruction-following data alleviate this Drop-towards-the-End\\nissue? Since the Drop-towards-the-End issue is mainly caused by the OOD data, it is natural to\\nthink and study whether it is possible to convert the instruction-following data to in-domain data\\nby tuning MoE with the instruction dataset. Therefore, we compare the models before and after\\n14\\nTable 10: Compare umT5 tokenizer and LLaMA tokenizer on the subsets extracted from different\\ndatasets. Vocab used denotes the number of token IDs activated when tokenizing the whole subset.\\nThe umT5/LLaMA means, when tokenizing the same subset, the ratio of the number of tokens\\ngenerated by umT5 and LLaMA.\\nDataset\\nSubset\\nLLaMA Tokenizer\\numT5 Tokenizer\\numT5/LLaMA\\n#Tokens\\nVocab Used\\n#Tokens\\nVocab Used\\nRedPajama\\narxiv\\n125,339\\n8,327\\n131,059\\n8,762\\n1.046\\nbook\\n137,972\\n11,603\\n131,072\\n15,202\\n0.950\\nc4\\n28,592\\n5,439\\n26,428\\n5,554\\n0.924\\ncc\\n78,450\\n8,738\\n73,403\\n9,927\\n0.936\\ngithub\\n54,707\\n4,769\\n59,732\\n4,539\\n1.092\\nstackexchange\\n40,659\\n4,714\\n43,195\\n4,317\\n1.062\\nwikipedia\\n37,406\\n7,179\\n30,555\\n8,748\\n0.817\\nTheStack\\nassembly\\n49,143\\n3,066\\n50,738\\n3,130\\n1.032\\nblitzmax\\n78,259\\n4,200\\n80,658\\n4,209\\n1.031\\njava\\n64,236\\n4,229\\n69,902\\n3,905\\n1.088\\npython\\n66,243\\n5,095\\n70,795\\n4,799\\n1.069\\nMTBench\\nwriting\\n6,062\\n1,700\\n5,786\\n1,535\\n0.954\\nroleplay\\n4,309\\n1,291\\n4,076\\n1,172\\n0.946\\nreasoning\\n2,369\\n478\\n2,309\\n429\\n0.975\\nmath\\n5,163\\n290\\n5,154\\n282\\n0.998\\ncoding\\n4,955\\n651\\n5,256\\n631\\n1.061\\nextraction\\n7,058\\n1,376\\n6,817\\n1,234\\n0.966\\nstem\\n4,783\\n1,151\\n4,527\\n1,039\\n0.946\\nhumanities\\n6,398\\n1,451\\n5,946\\n1,320\\n0.929\\nMulti-lingual\\nar\\n256,952\\n187\\n88,406\\n8,037\\n0.344\\nTED\\nde\\n103,270\\n4,880\\n80,593\\n8,470\\n0.780\\nes\\n101,212\\n4,745\\n78,713\\n8,519\\n0.778\\nfr\\n115,057\\n5,156\\n95,978\\n8,164\\n0.834\\nhe\\n242,446\\n239\\n86,891\\n4,074\\n0.358\\nit\\n109,591\\n4,593\\n84,201\\n8,833\\n0.768\\nja\\n144,825\\n931\\n63,491\\n6,860\\n0.438\\nko\\n257,107\\n596\\n106,770\\n2,736\\n0.415\\nnl\\n102,703\\n4,234\\n75,084\\n7,540\\n0.731\\nru\\n107,144\\n2,502\\n74,445\\n9,658\\n0.695\\nzh-cn\\n149,581\\n1,058\\n88,107\\n3,611\\n0.589\\nzh-tw\\n173,415\\n1,107\\n93,693\\n3,619\\n0.540\\nsupervised fine-tuning in Figure 12b. We can see the models do not have a significant difference in\\nthe Drop-towards-the-End issue. This matches well with our insight above, i.e., the routing behavior\\nlearned and fixed at the very early stage of LLM pre-training.\\n5\\nRethinking OpenMoE\\nWorking on this project is a long journey for authors. We indeed made some mistakes during design\\nand development, but we also achieved some new insights in the analysis. We thus write down\\neverything we found without any reservation in this paper to help future practitioners. Then, in this\\nsection, we discuss how to train a better model in the future, which are the most important takeaways\\nof our work.\\nHow much code shall we use? To be honest, we do not have a very precise answer. Conducting\\nan ablation study is extremely expensive because of the cost of pre-training LLM at scale. The\\nconclusion may also strongly depend on the model size and data quality. However, according to\\nour observation, over 50% code looks too aggressive which may harm the abilities on text tasks,\\nbut considering the importance of writing code, we suggest using around 30% code as we used in\\nOpenMoE-34B/32E.\\n15\\nTokenizer Selection Our large tokenizer vocabulary introduces computation overhead at the last\\noutput layer after Transformer blocks. Although this overhead would become relatively small after\\nscaling the Transformer model up, it is still valuable to make the tokenizer selection smarter. We\\nconduct a quantitative analysis of the tokenizer with the datasets we used in Section 4. As shown\\nin Table 10, umT5 tokenizer is indeed much better than LLaMA tokenizer on the multi-lingual\\ndataset, especially on the low-resource language. It is also slightly better than LLaMA on the\\ninstruction-following data. However, it did not match well with our expectation that it could save\\nmore tokens for the code data. In addition, we observe that the token usage in both tokenizers is\\nextremely long-tail distributed, which indicates that there is a large room to improve the tokenizer\\nand following algorithms. As we know, learning from long-tailed data is hard [55]. Since we only\\nhave a little multi-lingual data in our pre-training data mixture, the computation cost of predicting the\\nlogits of those low-resource tokens is wasted. Based on our sub-optimal choice, we also need a solid\\ntokenizer benchmark, which would help people evaluate tokenizers systematically. And we can then\\npick the best tokenizer before training the model.\\nMore Efficient MoE Architecture According to our observation, MoE routing is almost context-\\nindependent (i.e., Context-independent Specialization), we suggest that we can (1) remove the\\ntrainable router after warmup stage; (2) adopt parallel Transformer layer [9, 48] computing FFN layer\\nbased on the input directly instead of using the output of attention layer; (3) overlapping the attention\\nlayer computation and MoE layer all-to-all communication. (1) and (3) will improve the hardware\\nutilization and (2) can enable (3) without performance drop when scaling up [9].\\nMix instruction-following data during pre-training warm-up to control load balance and\\nalleviate Drop-towards-the-End. According to our results on multi-turn MT-Bench, it is very\\nimportant to alleviate the Drop-towards-the-End issue. To this end, the key is to make the MoE\\nachieve load balance on instruction-following data. Again, since the MoE learns and fixes the routing\\nbehavior at the early stage of pre-training, a straightforward solution is mixing the instruction-tuning\\ndata into the pre-training corpus during warm-up. This data mixing is not to align the model to\\nlearn how to follow instructions. Instead, we hope the model achieves the balanced token routing on\\ninstruction-tuning data, which paves the way to our final usage case of LLMs.\\n6\\nConclusion\\nIn this work, we explore how to train MoE for open-sourced communities. We achieved posi-\\ntive results that verified the effectiveness of MoE-based LLM in the post-ChatGPT stage. We\\ndisclosed all details, and our model is fully reproducible with the open-sourced code and data.\\nMore importantly, we conducted an in-depth analysis on our MoE-based LLM and found important\\n“Context-independent Specialization” “Early Routing Learning” and “Drop-towards-the-End”. We\\nalso rethink the mistakes we made and propose possible solutions for future developers. We sincerely\\nhope this work can help the open-source community have a better understanding of MoE models. All\\nthe best!\\nReferences\\n[1]\\nR. Anil et al., “Palm 2 technical report,” arXiv preprint arXiv:2305.10403, 2023.\\n[2]\\nAnonymous, “(inthe)wildchat: 570k chatGPT interaction logs in the wild,” in The Twelfth\\nInternational Conference on Learning Representations, 2024. [Online]. Available: https:\\n//openreview.net/forum?id=Bl8u7ZRlbM.\\n[3]\\nM. Artetxe et al., “Efficient large scale language modeling with mixtures of experts,” arXiv\\npreprint arXiv:2112.10684, 2021.\\n[4]\\nB.-b. authors, “Beyond the imitation game: Quantifying and extrapolating the capabilities\\nof language models,” Transactions on Machine Learning Research, 2023, ISSN: 2835-8856.\\n[Online]. Available: https://openreview.net/forum?id=uyTL5Bvosj.\\n[5]\\nM. Bavarian et al., “Efficient training of language models to fill in the middle,” arXiv preprint\\narXiv:2207.14255, 2022.\\n[6]\\nO. r. Bojar et al., “Findings of the 2016 conference on machine translation,” in Proceedings of\\nthe First Conference on Machine Translation, Berlin, Germany: Association for Computational\\nLinguistics, Aug. 2016, pp. 131–198. [Online]. Available: http://www.aclweb.org/\\nanthology/W/W16/W16-2301.\\n16\\n[7]\\nT. B. Brown et al., “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165,\\n2020.\\n[8]\\nM. Chen et al., “Evaluating large language models trained on code,” 2021. arXiv: 2107.03374\\n[cs.LG].\\n[9]\\nA. Chowdhery et al., “Palm: Scaling language modeling with pathways,” arXiv preprint\\narXiv:2204.02311, 2022.\\n[10]\\nH. W. Chung et al., “Unimax: Fairer and more effective language sampling for large-scale\\nmultilingual pretraining,” arXiv preprint arXiv:2304.09151, 2023.\\n[11]\\nT. Computer, Redpajama: An open source recipe to reproduce llama training dataset, 2023.\\n[Online]. Available: https://github.com/togethercomputer/RedPajama-Data.\\n[12]\\nD. Dai et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts\\nlanguage models,” arXiv preprint arXiv:2401.06066, 2024.\\n[13]\\nA. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for image recognition at\\nscale,” arXiv preprint arXiv:2010.11929, 2020.\\n[14]\\nN. Du et al., “Glam: Efficient scaling of language models with mixture-of-experts,” in Interna-\\ntional Conference on Machine Learning, PMLR, 2022, pp. 5547–5569.\\n[15]\\nW. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models\\nwith simple and efficient sparsity,” J. Mach. Learn. Res, vol. 23, pp. 1–40, 2021.\\n[16]\\nH. Fu Yao; Peng and T. Khot, “How does gpt obtain its ability? tracing emergent abili-\\nties of language models to their sources,” Yao Fu’s Notion, Dec. 2022. [Online]. Avail-\\nable: https : / / yaofu . notion . site / How - does - GPT - Obtain - its - Ability -\\nTracing - Emergent - Abilities - of - Language - Models - to - their - Sources -\\nb9a57ac0fcf74f30a1ab9e3e36fa1dc1.\\n[17]\\nL. Gao et al., A framework for few-shot language model evaluation, version v0.4.0, Dec. 2023.\\nDOI: 10.5281/zenodo.10256836. [Online]. Available: https://zenodo.org/records/\\n10256836.\\n[18]\\nX. Geng and H. Liu, Openllama: An open reproduction of llama, May 2023. [Online]. Available:\\nhttps://github.com/openlm-research/open_llama.\\n[19]\\nD. Hendrycks et al., “Measuring massive multitask language understanding,” arXiv preprint\\narXiv:2009.03300, 2020.\\n[20]\\nJ. Hoffmann et al., “Training compute-optimal large language models,” arXiv preprint\\narXiv:2203.15556, 2022.\\n[21]\\nA. Q. Jiang et al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024.\\n[22]\\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA: A large scale distantly supervised\\nchallenge dataset for reading comprehension,” in Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y.\\nKan, Eds., Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–\\n1611. DOI: 10.18653/v1/P17-1147. [Online]. Available: https://aclanthology.org/\\nP17-1147.\\n[23]\\nJ. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep bidirectional trans-\\nformers for language understanding,” in Proceedings of naacL-HLT, vol. 1, 2019, p. 2.\\n[24]\\nD. Kocetkov et al., “The stack: 3 tb of permissively licensed source code,” Preprint, 2022.\\n[25]\\nD. Lepikhin et al., “Gshard: Scaling giant models with conditional computation and automatic\\nsharding,” arXiv preprint arXiv:2006.16668, 2020.\\n[26]\\nM. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, “Base layers: Simplifying\\ntraining of large, sparse models,” in International Conference on Machine Learning, PMLR,\\n2021, pp. 6265–6274.\\n[27]\\nJ. Li, Z. Zhang, and H. Zhao, “Self-prompting large language models for open-domain qa,”\\narXiv preprint arXiv:2212.08635, 2022.\\n[28]\\nR. Li et al., “Starcoder: May the source be with you!” arXiv preprint arXiv:2305.06161, 2023.\\n[29]\\nY. Liu et al., “Roberta: A robustly optimized bert pretraining approach,” arXiv preprint\\narXiv:1907.11692, 2019.\\n[30]\\nY. Lou, F. Xue, Z. Zheng, and Y. You, “Cross-token modeling with conditional computation,”\\narXiv preprint arXiv:2109.02008, 2021.\\n17\\n[31]\\nB. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, “Multimodal contrastive\\nlearning with limoe: The language-image mixture of experts,” Advances in Neural Information\\nProcessing Systems, vol. 35, pp. 9564–9576, 2022.\\n[32]\\nE. Nijkamp et al., “Xgen-7b technical report,” arXiv preprint arXiv:2309.03450, 2023.\\n[33]\\nJ. Puigcerver, C. Riquelme, B. Mustafa, and N. Houlsby, “From sparse to soft mixtures of\\nexperts,” arXiv preprint arXiv:2308.00951, 2023.\\n[34]\\nJ. W. Rae et al., “Scaling language models: Methods, analysis & insights from training gopher,”\\narXiv preprint arXiv:2112.11446, 2021.\\n[35]\\nC. Raffel et al., “Exploring the limits of transfer learning with a unified text-to-text transformer,”\\nJournal of Machine Learning Research, vol. 21, no. 140, pp. 1–67, 2020. [Online]. Available:\\nhttp://jmlr.org/papers/v21/20-074.html.\\n[36]\\nC. Riquelme et al., “Scaling vision with sparse mixture of experts,” Advances in Neural\\nInformation Processing Systems, vol. 34, pp. 8583–8595, 2021.\\n[37]\\nS. Roller, S. Sukhbaatar, J. Weston, et al., “Hash layers for large sparse models,” Advances in\\nNeural Information Processing Systems, vol. 34, pp. 17 555–17 566, 2021.\\n[38]\\nB. Roziere et al., “Code llama: Open foundation models for code,” arXiv preprint\\narXiv:2308.12950, 2023.\\n[39]\\nN. Shazeer, “Glu variants improve transformer,” arXiv preprint arXiv:2002.05202, 2020.\\n[40]\\nN. Shazeer et al., “Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer,” arXiv preprint arXiv:1701.06538, 2017.\\n[41]\\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, “Megatron-lm:\\nTraining multi-billion parameter language models using model parallelism,” arXiv preprint\\narXiv:1909.08053, 2019.\\n[42]\\nL. Soldaini et al., “Dolma: An Open Corpus of Three Trillion Tokens for Language Model\\nPretraining Research,” arXiv preprint, 2023.\\n[43]\\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, “Roformer: Enhanced transformer with\\nrotary position embedding,” Neurocomputing, vol. 568, p. 127 063, 2024.\\n[44]\\nY. Tay et al., “Ul2: Unifying language learning paradigms,” in The Eleventh International\\nConference on Learning Representations, 2022.\\n[45]\\nY. Tay et al., “Unifying language learning paradigms,” arXiv preprint arXiv:2205.05131, 2022.\\n[46]\\nL.-M. Team, Llama-moe: Building mixture-of-experts from llama with continual pre-training,\\nDec. 2023. [Online]. Available: https://github.com/pjlab-sys4nlp/llama-moe.\\n[47]\\nH. Touvron et al., “Llama: Open and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971, 2023.\\n[48]\\nB. Wang and A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language\\nModel, https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\\n[49]\\nG. Wenzek et al., “CCNet: Extracting high quality monolingual datasets from web crawl data,”\\nEnglish, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N.\\nCalzolari et al., Eds., Marseille, France: European Language Resources Association, May 2020,\\npp. 4003–4012, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.\\norg/2020.lrec-1.494.\\n[50]\\nY. Xu et al., “Gspmd: General and scalable parallelization for ml computation graphs,” arXiv\\npreprint arXiv:2105.04663, 2021.\\n[51]\\nF. Xue, X. He, X. Ren, Y. Lou, and Y. You, “One student knows all experts know: From sparse\\nto dense,” arXiv preprint arXiv:2201.10890, 2022.\\n[52]\\nF. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, “Go wider instead of deeper,” in Proceedings\\nof the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8779–8787.\\n[53]\\nP. Yu et al., “Efficient language modeling with sparse all-mlp,” arXiv preprint\\narXiv:2203.06850, 2022.\\n[54]\\nP. Zhang, G. Zeng, T. Wang, and W. Lu, Tinyllama: An open-source small language model,\\n2024. arXiv: 2401.02385 [cs.CL].\\n[55]\\nY. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, “Deep long-tailed learning: A survey,” IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, 2023.\\n[56]\\nL. Zheng et al., “Judging llm-as-a-judge with mt-bench and chatbot arena,” arXiv preprint\\narXiv:2306.05685, 2023.\\n18\\n[57]\\nY. Zhou et al., “Brainformers: Trading simplicity for efficiency,” in International Conference\\non Machine Learning, PMLR, 2023, pp. 42 531–42 542.\\n[58]\\nY. Zhou et al., “Mixture-of-experts with expert choice routing,” Advances in Neural Informa-\\ntion Processing Systems, vol. 35, pp. 7103–7114, 2022.\\n[59]\\nB. Zoph et al., “St-moe: Designing stable and transferable sparse expert models,” URL\\nhttps://arxiv. org/abs/2202.08906, 2022.\\n19\\nAppendix\\nA\\nFrequent Asked Questions\\nWe list the potentially frequently asked questions and the point-to-point5 answers as follows:\\nA.1\\nWhy not show the token specialization of the checkpoints at the warmup stage?\\nWe did not expect that the routing would be learned and fixed so early. During training, due to limited\\nstorage quota, we only keep the checkpoints every 200B tokens.\\nA.2\\nWhy not compare with advanced open MoE models like Mixtral and DeepSeek-MoE?\\nFirst, our model was announced and released over 4 months earlier than Mistral and even more\\nthan DeepSeek-MoE. Second, different from models used in-house training data, our model is fully\\ntransparent. We also disclose all details and code to ensure everyone can train a comparable OpenMoE\\nmodel from scratch.\\nA.3\\nWhy not use MoE upcycling?\\nMoE is more efficient in training instead of inference, because of better parallelism induced by large\\nbatch size. Building MoE on top of dense LLMs is a smart and faster way to get an MoE model,\\nbut not a more efficient way from a long-term view. Instead, maybe distilling MoE into a dense\\nmodel [51] would be helpful if there is little performance drop.\\nA.4\\nWhy not use AdamW optimizer and Cosine Learning Rate Schedule?\\nWe applied Adafactor optimizer and Inverse Square Root learning rate schedule following ST-\\nMoE [59]. We tried AdamW Optimizer but found that would introduce unstable issues (i.e., NAN\\nloss) frequently, which may introduce a significant amount of hyper-parameter sweep. Considering\\nthe limited computational resources we have, we decide to simply follow the well-studied learning\\nrate schedule from ST-MoE [59].\\nA.5\\nWhy not use better and larger datasets?\\nWhen launching this project in 2023 May, there were only a few available open-source pre-training\\ndatasets. However, the scale and quality of open-sourced pre-training datasets are getting better. For\\ninstance, Soldaini et al. [42] released 3T tokens with careful cleaning. Computer [11] also released a\\nhuge dataset with 30T tokens in total. We believe training on the future better data will improve the\\nLLM performance generally by a large margin.\\n20\\nB\\nHyper-parameters\\nTable 11: Model Configurations. H is the hidden size. “Layout” means the way of using the MoE\\nlayer. For instance, “Every 4” means we use one MoE layer for every 4 transformer blocks. HFFN is\\nthe FFN intermediate size. NHead and HHead are the number of attention heads and attention head\\ndimensions. L is the number of layers. #Param is the total parameters. #ActParam is the number of\\nparameters we used to process each token in Transformer blocks. #ActParam w/ E is the sum of the\\n#ActParam and the number of parameters in the token embedding layer.\\nModel\\nLayout\\nH\\nHFFN\\nNHead\\nHHead\\nL\\n#Param\\n#ActParam w/ E\\n#ActParam\\nOpenMoE-Base/16E\\nEvery 4\\n768\\n3072\\n12\\n64\\n12\\n650M\\n339M\\n142M\\nOpenMoE-8B/32E\\nEvery 6\\n2048\\n8192\\n24\\n128\\n24\\n8.7B\\n2.6B\\n2.1B\\nOpenMoE-34B/32E\\nEvery 4\\n3072\\n12288\\n24\\n128\\n32\\n34B\\n6.8B\\n6.0B\\nTinyLLaMA\\n-\\n2048\\n5632\\n32\\n64\\n22\\n1.0B\\n1.0B\\n0.9B\\nOpenLLaMA-3B\\n-\\n3200\\n8640\\n32\\n64\\n26\\n3.0B\\n3.0B\\n2.9B\\nLLaMA-7B\\n-\\n4096\\n11008\\n32\\n128\\n32\\n6.6B\\n6.4B\\n6.4B\\nFor OpenMoE-8B/32E, we set the head dimension as 128 instead of 64, which may be too large\\nfor a model using 2B activated Transformer parameters. We suggest that using 64 may induce a\\nbetter cost-effectiveness trade-off than ours. For the number of parameters in the table above, since\\nmost parameters in Transformer blocks are from attention layer and FFN layer, we only account the\\ntrainable parameters from these two for simplicity.\\nTable 12: OpenMoE training hyper-parameters.\\nBase/16E\\n8B/32E\\n34B/32E\\nOptimizer\\nAdafactor\\nBatch Size\\n128\\n2048\\n2048\\nTraining Steps\\n500K\\n500K\\n100K\\nPeak Learning Rate\\n0.01\\nLearning Rate Schedule\\nInverse Square Root Decay\\nWarmup Steps\\n10K\\nSequence Length\\n2048\\nLoad Balance Loss Weight\\n0.01\\nZ-Loss Weight\\n0.001\\nRouter Z-Loss Weight\\n0.0001\\nDifferent from existing LLMs trained with AdamW, we used Adafactor, a more memory-efficient\\noptimizer. Although it performs slightly worse than AdamW with the same training steps, the memory\\nefficiency enables us to use less model parallelism and more data parallelism. In this case, using\\nAdafactor makes our training cheaper than using AdamW to train the same model on the same\\ndata. However, we highlight that the margin of this gap is unclear because it highly depends on the\\nhardware and model size. For our infrastructure, i.e., TPUv3, this gap should be relatively larger due\\nto the limited on-chip memory (16 GB per core).\\n21\\nC\\nRelated Work\\nC.1\\nBefore OpenMoE\\nMoE is not new. One representative early effort is, Shazeer et al. [40] embed the MoE layer\\ninto a recurrent language model. Due to the scalability of Transformer architecture, GShard [25]\\nintegrates MoE into Transformer layer and uses expert parallelism to train MoE-based Transformer\\nat scale. Switch Transformer [15] is the earliest open-source MoE-based LM to our best knowledge,\\nwhich used encoder-decoder architecture and trained with C4 [35] dataset. Due to the success of\\nSwitch Transformer on large-scale pre-training, MoE got more attention, and more advanced routing\\nalgorithms were invented. For instance, BASE Layers [26] formulates token-to-expert allocation as a\\nlinear assignment problem, allowing an optimal assignment in which each expert receives an equal\\nnumber of tokens. Roller et al. [37] simply modifies the feedforward layer to hash to different sets of\\nweights depending on the current token and achieves promising results compared to learning-based\\nrouting. Different Token-based routing above, Zhou et al. [58] propose to let experts select their\\nfavorite tokens, i.e., Expert-Choice Routing. Expert-choice Routing achieves more balanced token\\nassignment and better cost-effectiveness trade-off.\\nBeyond the routing algorithm, there is also some work focusing on scaling MoE efficiently. Artetxe\\net al. [3] trained their MoE models mainly on the datasets used in RoBERTa [29] and CC100 [49]\\n(112B tokens in total). GaLM [14] further scale decoder-only MoE model with an in-house high-\\nquality dataset with 1.6T tokens. Brainformer [57] proposes an evolutionary search to discover MoE\\nattributes, e.g., the best way to interleave layers and layer capacities, when to fuse layers, and when\\nto specialize layers with MoE modules and show its effectiveness at different scales.\\nIn addition to language modeling, Vision Transformer (ViT) [13] can also be enhanced by MoE\\narchitecture. ViT-MoE [36] verifies the scalability of MoE on ViT models. WideNet [52] shares\\nMoE-based Transformer blocks with individual layer normalization to achieve better parameter\\nefficiency. SoftMoE [33] further improves the routing algorithm by applying soft token selection,\\nwhich not only keeps the efficiency but also stabilizes the routing gradient. There are also some efforts\\ndevoted to include MoE into non-Transformer architecture, e.g., Sparse-MLP [30] for computer\\nvision and s-MoE for language modeling [53].\\nC.2\\nAfter OpenMoE\\nTable 13: Open-sourced MoE LLMs timeline. We use the model release date as the key to sort the\\nopen-souced MoE LLMs. Dataset Size is the number of tokens in the pre-training dataset, i.e., the\\nnumber of tokens for one epoch. LLaMA-MoE is continued pre-trained on off-the-shelf LLaMA\\nfamily models. We account its continue training dataset only.\\nModel Name\\nDataset Size\\nReproducible\\nRelease Date\\nSwitch Transformer [15]\\n156B\\nYes\\nFeb 2021\\nMeta-MoE [3]\\n112B\\nYes\\nDec 2021\\nOpenMoE (Ours)\\n1.1T\\nYes\\nAug 2023\\nMixtral of Experts [21]\\nUnknown\\nNo\\nDec 2023\\nLLaMA-MoE [46]\\n200B\\nYes\\nDec 2023\\nDeepSeek-MoE [12]\\n2T\\nNo\\nJan 2024\\nWe released our model and implementation much earlier than writing this report. As shown in\\nTable 13, after our release, there are some partially open-sourced models released, e.g., Mixtral [21]\\nand Deepseek-MoE [12]. As we known, these models are significantly better in terms of final results.\\nHowever, since these models are trained with in-house data, we have no idea about how things\\nhappened. We believe, although our results are not that amazing, the fully open-sourced nature and\\nthe in-depth analysis are both meaningful for the community.\\n22\\nD\\nBigBench-Lite Results\\nTable 14: Detailed BigBench-Lite results. Note that BIG-G-sparse 8B is an MoE model with 60B\\nparameters in total.\\nModel\\nBIG-G 8B\\nBIG-G-sparse 8B\\nGPT-3 6B\\nOpenMoE-8B\\nauto_debugging\\n0.0\\n0.0\\n0.0\\n17.65\\nbbq_lite_json\\n58.63\\n46.13\\n49.85\\n42.67\\ncode_line_description\\n4.66\\n2.44\\n20.18\\n2.44\\nconceptual_combinations\\n−2.16\\n1.07\\n−3.36\\n0.81\\nconlang_translation\\n31.38\\n33.25\\n37.92\\n36.93\\nemoji_movie\\n3.75\\n7.5\\n−5.0\\n3.75\\nformal_fallacies_syllogisms_negation\\n0.78\\n−0.39\\n−0.8\\n−0.56\\nhindu_knowledge\\n12.44\\n8.63\\n19.29\\n16.24\\nknown_unknowns\\n−34.78\\n−4.35\\n−8.7\\n−13.04\\nlanguage_identification\\n1.39\\n−0.33\\n1.66\\n1.77\\nlinguistics_puzzles\\n0.0\\n0.0\\n0.0\\n0.05\\nlogic_grid_puzzle\\n−2.45\\n0.01\\n−0.28\\n0.89\\nlogical_deduction\\n1.38\\n4.2\\n1.05\\n0.09\\nmisconceptions_russian\\n−34.69\\n−38.78\\n−34.69\\n−38.78\\nnovel_concepts\\n10.16\\n14.06\\n17.97\\n6.25\\noperators\\n10.48\\n16.67\\n20.0\\n20.48\\nparsinlu_reading_comprehension\\n0.0\\n0.0\\n0.0\\n11.97\\nplay_dialog_same_or_different\\n12.5\\n4.69\\n−3.8\\n1.1\\nrepeat_copy_logic\\n0.0\\n6.25\\n0.0\\n3.12\\nstrange_stories\\n−7.15\\n−4.77\\n9.54\\n14.52\\nstrategyqa\\n7.23\\n8.4\\n−3.8\\n3.36\\nsymbol_interpretation\\n6.06\\n0.13\\n4.17\\n2.65\\nvitaminc_fact_verification\\n6.25\\n1.27\\n−3.2\\n21.34\\nwinowhy\\n4.69\\n5.27\\n11.6\\n10.14\\nAverage\\n3.77\\n4.63\\n5.40\\n6.93\\n23\\nE\\nRouting Decision Standard Deviation\\n0\\n500\\n1000\\n1500\\n2000\\nPosition ID\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\nStd Value\\nRouting Decision Std\\nUniform Distribution Std\\nAverage Routing Decision Std\\nFigure 13: The routing decision standard deviation at different position IDs.\\nT\\noken ID\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\nValue\\nRouting Decision Std\\nUniform Distribution Std\\nAverage Routing Decision Std\\nFigure 14: The routing decision standard deviation at different token IDs. We only take the token IDs\\nwith over 128 tokens, because the extremely low-resourced tokens always have large routing decision\\nstandard deviation. The token IDs never appeared also have variance at all.\\nIn Figure 13 and 14, we can clearly see that the token IDs have a larger standard deviation on routing\\ndecisions than position IDs. Also, most token IDs have a relatively large standard deviation, which\\nmeans most of the token IDs have Context-independent Routing.\\n24\\nF\\nTop Token Selection by Experts\\nTable 15: Top Tokens selected by each expert.\\nExpert ID\\nTop Tokens\\n0\\n“\\\\n’, “ ‘ ”, “ ’ ”, “s”, “-”, “$”, “y”, “_”, “ ”, “2”\\n1\\n“\\\\n’, “1”, “ ”, “2”, “’\\\\\\\\”, “S”, “.”, “-”, “C”, “{”\\n2\\n“in”, “.”, “2”, “1”, “0”, “\\\\n’, “ ”, “3”, “_”, “4”\\n3\\n“s”, “)”, “a”, “\\\\n’, “which”, “es”, “);”, “}”, “\\\\\\\\”, “e”\\n4\\n“\\\\n’, “.”, “0”, “the”, “,“, “_”, “that”, “1”, “as”, “ˆ\\n’’\\n5\\n“ ”, “\\\\n’, “s”, “2”, “a”, “on”, “ter”, “*”, “\\\\\\\\”, “all”\\n6\\n“the”, “,”, “.”, “a”, “to”, “of”, “ ”, “s”, “de”, “\\\\n’\\n7\\n“,“, “and”, “\\\\n’, “:”, “_”, “ ”, “0”, “on”, “at”, “{”\\n8\\n“(”, “.”, “that”, “s”, “ ”, “,“, “C”, “which”, “of”, “G”\\n9\\n“(”, “this”, “2”, “\\\\n’, “\\\\\\\\”, “ ”, “3”, “also”, “I”, “1”, “,“\\n10\\n“\\\\n’, “.”, “and”, “\\\\r”, “).”, “;”, “\\\\t”, “:”, “?”, “The”\\n11\\n“to”, “1”, “the”, “2”, “0”, “s”, “for”, “t”, “3”, “\\\\n’\\n12\\n“the”, “,“, “$’, “to”, “in”, “?”, “as”, “that”, “In”, “who”\\n13\\n“in”, “/”, “0”, “\\\\n’, “with”, “-”, “ ”, “{”, “of”, “2”\\n14\\n“is”, “.”, “are”, “be”, “was”, “s”, “\\\\n’, “,“, “has”, “not”\\n15\\n“of”, “\\\\n’, “_”, “s”, “ ”, “.”, “S”, “the”, “for”, “\\\\\\\\”\\n16\\n“cite”, “,“, “\\\\n’, “.”, “{”, “s”, \"’\", “ing”, “data”, “\\\\\\\\$”, “\\\\t”\\n17\\n“the”, “.”, “\\\\n’, “The”, “0”, “1”, “as”, “of”, “5”, “2”\\n18\\n“-”, “{”, “for”, “(”, “_”, “ ”, “$”, “(”, “\\\\n’, “}”\\n19\\n“ ”, “and”, “in”, “to”, “,“, “of”, “or”, “\\\\n’, “by”, “$”\\n20\\n“\\\\n’, “the”, “$”, “a”, “0”, “}”, “this”, “1”, “s”, “9”, “ ”\\n21\\n“,“, “and”, “ ”, “.”, “\\\\n’, “=”, “\\\\t”, “the”, “ ”, “n”\\n22\\n“the”, “\\\\n’, “)”, “,“, “his”, “their”, “s”, ’\"’, “,“, “i”\\n23\\n“.”, “\\\\n’, “,“, “*”, “<pad>”, “Cha”, “i”, “!”, “our”, “/”\\n24\\n“a”, “with”, ’}”, “in”, “)”’, “:”, “an”, “1”, “\\\\n’, “at”\\n25\\n“\\\\\\\\”, “the”, “.”, “of”, “er”, “, ”, “s”, “ter”, “book”, “model”\\n26\\n“\\\\n’, “, ”, “.”, “a”, “<pad>”, “s”, “de”, “al”, “-”\\n27\\n“the”, \"’\", “I”, “The”, “, ”, “it”, “we”, “he”, “a”, “x”\\n28\\n“, ”, “ly”, “{”, “_{”, “new”, “-”, “ed”, “more”, “\\\\n’, “d”\\n29\\n“, ”, “.”, “of”, “;”, “by”, “,:”, “\\\\n’, “to”, “from”, “(”,\\n30\\n“}”, “ed”, “d”, “have”, “ing”, “, ”, “has”, “s”, ’\"’, “had”\\n31\\n“to”, “can”, “s”, “of”, “ing”, “will”, “not”, “e”, “ed”, “would”\\n25\\n', 'source_name': 'OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models', 'source_url': 'https://arxiv.org/abs/2402.01739'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MoESurvey.pdf #79\n",
      "{'content': 'A REVIEW OF SPARSE EXPERT MODELS IN\\nDEEP LEARNING\\nWilliam Fedus∗\\nGoogle Brain\\nJeff Dean\\nGoogle Research\\nBarret Zoph∗\\nGoogle Brain\\nABSTRACT\\nSparse expert models are a thirty-year old concept re-emerging as a popular ar-\\nchitecture in deep learning. This class of architecture encompasses Mixture-of-\\nExperts, Switch Transformers, Routing Networks, BASE layers, and others, all\\nwith the unifying idea that each example is acted on by a subset of the parameters.\\nBy doing so, the degree of sparsity decouples the parameter count from the com-\\npute per example allowing for extremely large, but efﬁcient models. The resulting\\nmodels have demonstrated signiﬁcant improvements across diverse domains such\\nas natural language processing, computer vision, and speech recognition. We re-\\nview the concept of sparse expert models, provide a basic description of the com-\\nmon algorithms, contextualize the advances in the deep learning era, and conclude\\nby highlighting areas for future work.\\n1\\nINTRODUCTION\\nRemarkable advances in machine learning – especially in natural language – have been achieved\\nby increasing the computational budget, training data, and model size. Notable milestone language\\nmodels include GPT-2 (Radford et al., 2018), BERT (Devlin et al., 2018), T5 (Raffel et al., 2019),\\nGPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), and PaLM\\n(Chowdhery et al., 2022). However, state-of-the-art models now require thousands of specialized,\\ninterconnected accelerators for weeks or months at a time. These models are therefore expensive\\nto produce and incur high energy costs (Patterson et al., 2021). Therefore, as the scale of machine\\nlearning systems has increased, the ﬁeld has sought more efﬁcient training and serving paradigms.\\nSparse expert models have risen as a promising solution.\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nSelf-Attention\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\nAdd + Normalize\\nSelf-Attention\\nAdd + Normalize\\nSparse FFN Layer\\ny\\nx\\n\"The\"\\n\"Dog\"\\nAdd + Normalize\\nSelf-Attention\\nAdd + Normalize\\nFFN Layer\\ny\\nx\\nDense Model\\nSparse Model\\nFFN\\nAdd + Normalize\\nFFN\\nSelf-Attention\\nAdd + Normalize\\nx1\\nx2\\ny1\\n\"The\"\\n\"Dog\"\\ny2\\nFigure 1: Comparing a dense and sparse expert Transformer. A dense model (left) sends both\\ninput tokens to the same feed-forward network parameters (FFN). A sparse expert model (right)\\nroutes each input token independently among its four experts (FFN1 · · · FFN4). In this diagram,\\neach model uses a similar amount of computation, but the sparse model has more unique parameters.\\nNote while this ﬁgure showcases a speciﬁc and common approach of sparse feed-forward network\\nlayers in a Transformer (Vaswani et al., 2017), the technique is more general.\\nSparse expert models, of which, Mixture-of-Experts (MoE) is the most popular variant, are neural\\nnetworks where a set of the parameters are partitioned into “experts”, each with a unique weight.\\n∗Equal contribution. Correspondence to {liam.fedus,barretzoph}@gmail.com.\\n1\\narXiv:2209.01667v1  [cs.LG]  4 Sep 2022\\nDuring training and inference, the models route input examples to speciﬁc expert(s) weights. As a\\nresult, each example only interacts with a subset of the network parameters, contrasting the usual\\napproach where the entire network is used for each input. Because only a fraction of the experts are\\nused for each example, the amount of computation may remain small relative to the total model size.\\nMany modern sparse expert models draw inspiration from Shazeer et al. (2017), which trained the\\nlargest model at the time and achieved state-of-the-art language modeling and translation results.\\nSparse expert models have further surged in popularity when combined with Transformer language\\nmodels (Lepikhin et al., 2020; Fedus et al., 2021). And while most work has been in natural language\\nprocessing, they have also been successfully used in a variety of domains including computer vision\\n(Puigcerver et al., 2020), speech recognition (You et al., 2021) and multi-modal learning (Mustafa\\net al., 2022). Recent work by Clark et al. (2022) rigorously studied the scaling properties of sparse\\nexpert models across different model sizes and number of experts. Further, state-of-the-art results on\\nmany benchmarks are currently held by sparse expert models such as ST-MoE (Zoph et al., 2022).\\nThe ﬁeld is evolving quickly with research and engineering advances increasing our understanding\\nand improving empirical results.\\nWe narrow our survey to sparse expert models in the era of deep learning (heuristically 2012-\\nonward), recounting recent advances and discussing promising future avenues. For a comprehensive\\nreview of the history of Mixture-of-Experts, predating the recent deep learning advances, we refer\\nreaders to the survey, “Twenty Years of Mixture-of-Experts” (Yuksel et al., 2012). Further, sparse\\nexpert models may be regarded as a special class of adaptive computation models which are sur-\\nveyed in Xu and McAuley (2022). Finally, Tay et al. (2020) surveys a broader set of methods aimed\\nat increasing the computational efﬁciency of Transformers, of which, sparse expert models are one\\npromising approach.\\n2\\nSPARSE EXPERT MODELS\\nThe concept of MoE in machine learning dates back at least three decades to the work of Jacobs et al.\\n(1991); Jordan and Jacobs (1994). In early concepts, the experts deﬁned an entire neural network\\nand the MoE was similar to ensemble methods.\\n2.1\\nIN DEEP LEARNING\\nEigen et al. (2013) proposed architectures that used stacked layers of Mixture-of-Experts on jittered\\nMNIST (LeCun et al., 1998). This work used a continuous mixture of the experts’ outputs (soft\\nselection) rather than restricting to the top subset of experts at each layer (hard selection) – limiting\\nits practicality1. This work, however, set the stage for later efﬁcient implementations which relied on\\nthe idea of MoE as a component of a neural network. The ﬁrst large-scale success of this approach\\nin deep learning came from Shazeer et al. (2017). This work inserted an MoE layer between two\\nLSTM layers (Hochreiter and Schmidhuber, 1997) where the output from the lower layer’s LSTM\\nwas sent for computation within the MoE. The resulting sparse model was state-of-the-art in machine\\ntranslation, though the largest variant with 131,072 experts and 137B parameters generalized worse\\nthan smaller variants. Despite this success, however, follow-on research was relatively dormant\\nwith greater emphasis on directly studying the Transformer (Vaswani et al., 2017). This changed\\nwith the release of GShard (Lepikhin et al., 2020) and Switch Transformers (Fedus et al., 2021) –\\nboth of which replaced the feed-forward layers in Transformers with expert layers. However, while\\nthe experts-as-a-layer approach has become the dominant paradigm, more recent works revisit the\\nconcept of experts as fully independent models (Gururangan et al., 2021; Li et al., 2022). This\\nconfers a beneﬁt of modularity and composability; Li et al. (2022) shows custom networks can be\\nconstructed by composing them of expert language models trained on speciﬁc domains.\\nFigure 2 illustrates the original top-k routing mechanism proposed in Shazeer et al. (2017) which\\nwas foundational to many follow-on works. New advances to the routing algorithm are described\\nin Section 4. Choosing experts based on the input usually entails a discrete selection (i.e. which\\nexpert to use), which complicates backpropagation algorithms relying on differentiability. As a\\nsolution, Shazeer et al. (2017) proposed a top-k routing function which takes as an input a token\\n1The full computational cost is incurred with soft selection even if the expert was not necessary (i.e. an\\nexceedingly small routing weight).\\n2\\n-0.3\\n0.5\\n1.2\\n-1.6\\n-0.6\\n1.3\\n0.1\\n-1.1\\n0.7\\n0.8\\n-0.2\\n1.5\\n-0.1\\n-0.4\\n-1.1\\n0.2\\n1.3\\n-0.7\\n2.3\\n-1.1\\n0.1\\n1.7\\n0.9\\n0.4\\nExpert 1\\nExpert 2\\nExpert 5\\nExpert 4\\nExpert 3\\nRouter Weights\\nToken \\nRepresentations\\n-0.3\\n0.5\\n1.2\\n0.2\\n1.3\\n-0.7\\nDot Product\\n3.13\\n0.51\\n2.25\\n-1.32\\n-2.81\\n0.14\\n-0.25\\n2.61\\n1.97\\n-0.68\\n0.74\\n1.58\\n0.02\\n0.1\\n-0.41\\nRouter Scores\\n0.67\\n0.05\\n0.27\\n0.01\\n0.00\\n0.05\\n0.03\\n0.59\\n0.31\\n0.02\\n0.22\\n0.5\\n0.1\\n0.11\\n0.07\\nNormalized\\nRouter Scores\\nExpert Weights\\nT1\\nT2\\nT3\\nE1\\nE2\\nE3\\nE4\\nE5\\nT1\\nT2\\nT3\\nE1\\nE2\\nE3\\nE4\\nE5\\nFigure 2: Schematic of top-k routing. We visualize an example of the top-k token routing scheme\\nover ﬁve experts and three input tokens. Each expert and token is color-coded and the router weights\\n(Wr) have a representation for each expert (color matched). To determine the routing, the router\\nweight performs a dot product with each token embedding (x) to produce the router scores (h(x)).\\nThese scores are then normalized to sum to one (p(x)).\\nrepresentation x and then routes it to the top-k experts out of the set {Ei}N\\ni=1 of N experts. The\\nrouter has a trainable variable Wr which computes the logits h(x) = Wr · x which are normalized\\nvia a softmax distribution over the N experts. The gate-value for expert i is given by,\\npi(x) =\\neh(x)i\\nPN\\nj eh(x)j .\\n(1)\\nWe denote the set of selected top-k expert indices as T . The output computation of the layer is the\\nlinearly weighted combination of each expert’s computation on the token by the gate value,\\ny =\\nX\\ni∈T\\npi(x)Ei(x).\\n(2)\\nWe note that in contrast to Eigen et al. (2013), this selection is only over the top-k experts and is\\nthus more computationally efﬁcient.\\n2.2\\nON MODERN HARDWARE\\nModern sparse expert models have been co-designed with the distributed systems used to train the\\nlargest neural networks. These are a special case of sparse neural networks (Gale et al., 2019;\\nDettmers and Zettlemoyer, 2019; Evci et al., 2020) which are similar in that they only use a subset\\nof parameters, but differ because they have potentially irregular sparsity patterns. And while generic\\nsparse neural networks (with irregular sparsity patterns) reduce overall theoretical FLOPs, these are\\noften not efﬁciently supported on current hardware which specialize in linear algebra operations on\\ncontiguous (regular) blocks of memory. Sparse expert models, on the other hand, activate entire\\nblocks of parameters (i.e. entire matrices), and thus easily translate theoretical FLOPs savings to\\npractical time savings on modern hardware (Fedus et al., 2021; Rajbhandari et al., 2022).\\nThe largest neural networks (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) now far\\nexceed the memory capacity of a single accelerator and therefore tensors (e.g. weights, activations,\\noptimizer variables) are sharded using various parallelism strategies. Three common approaches\\n3\\ninclude data parallelism (model weights replicated, but data sharded), tensor model-parallelism\\n(Shazeer et al., 2018) (data and weight tensors are split across devices), and pipeline parallelism\\n(Harlap et al., 2018; Huang et al., 2019) (entire layers or groups of layers are split across devices).\\nMixture-of-Experts ﬁt naturally with these parallelism schemes. Experts reside on different accel-\\nerators and the input data is dynamically dispatched to and fetched from them. Early architectures\\noften employed many, small experts that would ﬁt within an individual accelerator (Lepikhin et al.,\\n2020), but later works designed larger experts that must be split across accelerators (Fedus et al.,\\n2021; Du et al., 2021) and required additional optimizations for communication efﬁciency (Shazeer\\net al., 2018; Roberts et al., 2022; Rajbhandari et al., 2022).\\nDynamic routing on distributed systems incurs additional communication overhead beyond standard\\nTransformer models. Dispatching the inputs to the experts is often implemented as an all2all\\ncommunication primitive, where each accelerator communicates data to all other accelerators.2 The\\ncapacity factor directly impacts the communication cost by modulating the expert batch size (Lep-\\nikhin et al., 2020) to be CF · (B/E), where CF is the capacity factor, B is the total tokens per\\nbatch and E is the number of experts. Larger capacity factor values can improve quality, but at the\\nexpense of increased communication, memory and compute costs. Efﬁcient implementations of the\\nall2all primitive, along with changes to the routing algorithm (e.g. reduced capacity factor),\\nalleviate the added communication costs from sparse expert algorithms.\\nWhen training normal distributed Transformers it is known in advance what batch of data each ac-\\ncelerator will process. However, dynamic routing algorithms break this property because inputs are\\ndynamically routed to experts, which can often lead to different number of inputs getting sent to each\\nof the experts. Therefore, routing algorithms often encourage load balance over the accelerators to\\nencourage good utilization. Load balance has been accomplished by auxiliary losses (Shazeer et al.,\\n2017) as well as through treating this as a linear assignment problem (Lewis et al., 2021; Clark et al.,\\n2022). More details on advances to load balancing are provided in Section 4.\\nFinally, recent systems advances have further improved both the training and deployment of MoE\\nmodels. Jaszczur et al. (2021) sparsify all the layers (e.g. dense and self-attention) of a Trans-\\nformer model to achieve 37× inference speedups for a special-case of single-example inference\\n(unbatched). Kossmann et al. (2022) relaxes the constraints of static expert batch sizes with the\\nRECOMPILE library. This system dynamically recompiles and optimizes the computational re-\\nsources of Mixture-of-Experts models so tensor sizes are matched to the experts’ computational\\ndemands, not statically-set arrays. Next, in addition to data-, model-, and expert-parallelism, the\\nDeepSpeed-MoE library (Rajbhandari et al., 2022) supports ZeRO partitioning (Rajbhandari et al.,\\n2019) (fully partition tensors and regather as needed) and ZeRO-Ofﬂoad (ofﬂoading to CPU to re-\\nduce GPU memory usage). This system yielded 10× inference improvements (Rajbhandari et al.,\\n2022) and state-of-the-art translation (Kim et al., 2021) – increasing the practicality of these models\\nfor production services.\\n3\\nSCALING PROPERTIES OF SPARSE EXPERT MODELS\\nThe cross-entropy loss of dense neural language models was shown to scale as a power-law (i.e.\\nl(x) = (c/x)α for a variable x) with respect to the model parameter count, amount of data, and\\ncompute budget when not constrained by the other two factors (Kaplan et al., 2020). The power\\nlaw coefﬁcients were later corrected in Hoffmann et al. (2022), which demonstrated that compute-\\noptimal models required a closer balance of data and parameter scaling. In contrast, early research\\nin sparse expert models scaled heuristically – achieving strong empirical results – but without care-\\nful characterization of the scaling laws. Further, several works highlighted discrepancies between\\nupstream (e.g. pre-training) and downstream (e.g. ﬁne-tuning) behavior (Fedus et al., 2021; Artetxe\\net al., 2021), further complicating the understanding and explanation of sparse expert models.\\n2Many routing algorithms (but not all) incur two all2all communication costs in the forward pass and\\nanother two in the backward pass. An example of a routing algorithm using more is BASE layers (Lewis et al.,\\n2021), which requires four all2all in the forward pass and another four in the backward pass.\\n4\\n3.1\\nUPSTREAM SCALING\\nSparse expert models have excelled when trained on large datasets. A common paradigm in natural\\nlanguage processing is to perform upstream training (e.g. pre-training) which is then followed by\\ndownstream training (e.g. ﬁne-tuning) on data distributions of speciﬁc interest. Sparse expert mod-\\nels have consistently yielded high gains over dense counterparts during the upstream phase. Shazeer\\net al. (2017) presented scaling curves with respect to model parameters and the computational budget\\non the 1-Billion-Word Language-Modeling Benchmark (Chelba et al., 2013), achieving signiﬁcant\\ngains over dense versions. Lepikhin et al. (2020) presented translation improvements as a func-\\ntion of model scale, and obtained a 13.5 BLEU score gain on their largest 600B parameter sparse\\nmodel. Switch Transformers (Fedus et al., 2021) measured 4-7× speed-ups in wall-time using the\\nsame compute resources over T5 models. The work also studied the cross entropy loss scaling as a\\nfunction of parameter count, but observed the gains diminished with 256+ experts.\\nFurthering our understanding, Artetxe et al. (2021) distinguished upstream scaling behavior of MoE\\nmodels on in-domain and out-of-domain data and found signiﬁcantly better scaling for in-domain\\nlanguage modeling compared to dense models, corroborating the difﬁculties of transfer from Fedus\\net al. (2021).\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\nExpert Count\\n1.6 × 100\\n1.65 × 100\\n1.7 × 100\\n1.75 × 100\\n1.8 × 100\\nValidation Loss\\nSwitch\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\nExpert Count\\n2 × 100\\n2.05 × 100\\n2.1 × 100\\n2.15 × 100\\n2.2 × 100\\n2.25 × 100\\nValidation Loss\\nHash\\nRL-R\\nS-Base\\nFigure 3: Sparse scaling plots with expert count. The cross-entropy scaling plots as a function\\nof the number of experts are shown from Fedus et al. (2021) (left) and the three sparse variants\\nfrom Clark et al. (2022), S-Base, RL-R, Hash (right). The top left-most point in both plots is an\\napproximately compute-matched dense model. As the expert count increases, the models become\\nincreasingly sparse and yield lower validation losses.\\nAfter these early empirical successes, Clark et al. (2022) conducted the ﬁrst large-scale effort to\\nmathematically characterize the scaling properties of sparse expert models. This work considered\\nthree classes of sparse models and derived a notion of effective parameter count (EPC). The EPC\\nestimates the dense-parameter equivalent for a sparse expert models, based on the FLOPs and the\\nnumber of experts. It was derived by conjecturing that sparse expert models followed a bilinear loss\\nand it was shown empirically that the cross entropy loss scales as a power law in this variable. Figure\\n3 presents the cross entropy scaling of Switch Transformers on the left and the three sparse variants\\nof Clark et al. (2022) on the right.\\nOne key property of the scaling curves was that the gain of sparse expert models decreased with\\nscale, which when extrapolated, implied that there would be no further beneﬁt of sparsity beyond\\n900B parameters of FLOPs. This result, however, was dependent on the number of tokens used for\\ntraining and all models used only 130B tokens. But in light of the recent scaling results from Hoff-\\nmann et al. (2022) which recommends more tokens to train compute-optimal models (Chinchilla\\nwas a 70B parameter model trained on 1.4T tokens), future work might revisit this analysis.\\n3.2\\nDOWNSTREAM SCALING\\nHowever, the reliable upstream scaling did not immediately yield consistent gains on downstream\\ntasks. In one work highlighting the challenge of transfer, Fedus et al. (2021) observed 4× pre-\\ntraining improvements with a low-compute, high-parameter encoder-decoder Transformer (1.6T\\n5\\nparameters with 2048 experts per sparse layer), but it ﬁne-tuned poorly on reasoning-heavy tasks\\nsuch as SuperGLUE (Wang et al., 2019) compared with dense models. This ﬁnding hinted at further\\nnecessary research as well as a potential needed balance between computation and parameters. How-\\never, strong empirical results soon followed in few-shot inference, ﬁne-tuning, and other modalities.\\nDu et al. (2021) presented the scaling of sparse GLaM models ranging from 1B-64B FLOPs using 64\\nexperts per sparse layer. GLaM achieved state-of-the-art results, outperforming the 175B parameter\\nGPT-3 (Brown et al., 2020) model in zero and one-shot performance, while using 49% fewer FLOPs\\nper token at inference and 65% lower power (left plot in Figure 4). In another example of sparse\\nmodels performing well on few-shot inference, the BIG-Bench (Srivastava et al., 2022) collaboration\\nmeasured a 2× improvement of sparse over dense models on the 161 contributed JSON tasks (right\\nplot in Figure 4).\\n100\\n101\\n102\\nGFlops Per Token Prediction\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAccuracy (EM)\\n0.1B\\n1.7B\\n8B\\n137B\\n0.1B/64E\\n1.7B/64E\\n8B/64E\\n64B/64E\\nDense\\nSparse\\n107\\n108\\n109\\n1010\\nEffective Parameter Count\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nNormalized Pref. Metrics\\nBIG-G (0-shot)\\nBIG-G (1-shot)\\nBIG-G (2-shot)\\nBIG-G Sparse (0)\\nBIG-G Sparse (1)\\nBIG-G Sparse (2)\\nFigure 4: Sparse scaling for few-shot inference. Left: Du et al. (2021) measures the few-shot\\ninference performance on TriviaQA, demonstrating consistent gains of sparse MoE models over\\ndense models up to 137B parameters. Each label, such as 8B/64E, says how many parameters\\nper input are used (8B) and how many experts (64E). Right: BigBench (Srivastava et al., 2022)\\nstudied the few-shot scaling properties on a larger set of 161 contributed JSON tasks to conﬁrm\\nimprovements of sparse expert models over their FLOP-matched dense counterparts.\\nFinally, Srivastava et al. (2022) studied the calibration of sparse models on the multiple choice\\nBIG-Bench tasks. Calibration measures the degree to which the probability of a prediction matches\\nthe probability of being correct. This work measured calibration by the Expected Calibration Error\\n(Naeini et al., 2015) which is the absolute deviation between the predicted probability and average\\naccuracy, after binning examples by their predicted probability. While the calibration improves\\nfor both larger dense and sparse models (Figure 5), the sparse models were found to match the\\ncalibration of a dense model using 10× more FLOPs.\\n3.3\\nSCALING THE NUMBER, SIZE AND FREQUENCY OF EXPERT LAYERS\\nSeveral important hyperparameters, beyond those in a dense Transformer, govern the scale of sparse\\nexpert models including, 1) the expert count, 2) the size of each expert, and 3) the frequency of expert\\nlayers. The decisions can have signiﬁcant implications to the upstream and downstream scaling.\\nMany earlier works scaled to thousands of relatively small experts per layer, which has produced\\nexcellent pre-training and translation quality (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al.,\\n2021). However, the quality of sparse models is disproportionately reduced under domain shift\\n(Artetxe et al., 2021) or when ﬁne-tuning on different task distributions (Fedus et al., 2021). The\\nstate-of-the-art sparse models for few-shot inference (GLaM (Du et al., 2021)) and for ﬁne-tuning\\n(ST-MoE (Zoph et al., 2022)) use only up to 64 larger experts – a better balance of computation and\\nparameters. As a result of the increased expert dimensions, these models require speciﬁc system-\\nlevel sharding strategies over accelerators to run efﬁciently (Du et al., 2021; Rajbhandari et al.,\\n2022).\\nNext, we recap the current conventions around the frequency of expert layers. Usually, sparse models\\nare constructed by beginning with a dense model and either inserting or substituting sparse expert\\n6\\n107\\n108\\n109\\n1010\\n1011\\nEffective Parameter Count\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nExpected Calibration Error\\nCalibration on Multiple-Choice Tasks (ECE)\\nBIG-G (0-shot)\\nBIG-G (1-shot)\\nBIG-G (2-shot)\\nBIG-G (3-shot)\\nBIG-G Sparse (0-shot)\\nBIG-G Sparse (1-shot)\\nBIG-G Sparse (2-shot)\\nBIG-G Sparse (3-shot)\\nFigure 5: Sparse model calibration. The Expected Calibration Error improves with scale for both\\ndense and spare models. However, sparse models exhibit signiﬁcantly better calibration and roughly\\nmatch the calibration of a 10× larger dense model. Figure is reproduced from Srivastava et al.\\n(2022).\\nlayers at either a ﬁxed interval or heuristically. As an example, Rajbhandari et al. (2022) put more\\nsparse layers near the ﬁnal layers of the network. In the Transformer, the most common approach\\nis to replace every other Feed-Forward Network (FFN) layer (Lepikhin et al., 2020; Du et al., 2021;\\nArtetxe et al., 2021; Rajbhandari et al., 2022), that is, substitute with a frequency of 0.5. However,\\nother frequencies have been used, including every-fourth-layer (0.25) in Zoph et al. (2022) and\\nevery-layer (e.g. 1.0) in Fedus et al. (2021). Finally, a frequency of 0.5-1.0 is recommended by\\nClark et al. (2022).\\nUltimately, the answer to the question of optimal hyperparameters depends on the application and\\nthe hardware system speciﬁcations. Prior work demonstrated strong pre-training and translation\\nresults with a high number of experts (Shazeer et al., 2017; Lepikhin et al., 2020), whereas, the best\\nperforming models under transfer have used fewer, larger experts (Du et al., 2021; Zoph et al., 2022;\\nMustafa et al., 2022). Further, these decisions are highly hardware-dependent. Due to the added\\nall2all communication costs to implement routing, networks with slower interconnect speeds\\nmay ﬁnd fewer expert layers is optimal on a time-basis to a certain quality. A simulation of the\\ncompute, memory, and communication properties of a distributed system would signiﬁcantly aid\\npractitioners to more quickly determine optimal settings, without costly trail-and-error launches.\\nWe note that this analysis and trade-offs are for experts-as-a-layer approach (Eigen et al., 2013).\\nIn contrast, the Branch-Train-Merge (BTM) approach to experts (Li et al., 2022) is “embarrass-\\ningly parallel“ in that each expert is a fully formed language model, trainable independently and\\nasynchronously, without the expensive communication costs. Therefore, this approach and others\\nfollowing-suit have completely different scaling characteristics with the number of experts.\\n4\\nROUTING ALGORITHMS\\nThe routing algorithm, a key feature to all sparse expert architectures, determines where to send\\nexamples. This area has been studied extensively, including counter-intuitive methods that use\\nﬁxed, non-learned routing patterns (Roller et al., 2021). Typically the naive routing decision is\\nnon-differentiable because it makes a discrete decision of which experts to select. The problem of\\nexpert selection can be recast as a Bandit problem and several works have used reinforcement learn-\\ning to learn the selection (Bengio et al., 2016; Rosenbaum et al., 2017; 2019; Clark et al., 2022).\\nShazeer et al. (2017) proposed a differentiable heuristic that side-stepped reinforcement learning\\nchallenges. Rather than routing the example to the chosen expert and proceeding, the output of\\nthe expert computation is weighted by the probability of choosing it (Equation 2). This produces a\\ngradient to the router since the probability of choosing the expert is differentiable. In contrast to a\\n7\\nBandit approach, where only one expert might be chosen, Shazeer et al. (2017) conjectured that it\\nwas necessary to route to the top-k experts with k > 1. The intuition was two or more experts on\\nthe same example allowed the network to compare and to optimize the relative performance. Lep-\\nikhin et al. (2020) later adapted the same routing algorithm to the Transformer architecture, yielding\\nstate-of-the-art machine translation results. However, Fedus et al. (2021) demonstrated that top-1\\nrouting can achieve competitive results, corroborated by later work (Clark et al., 2022).\\n4.1\\nROUTING TAXONOMY\\nOne way to understand many routing algorithms is to analyze the matrix of routing scores (i.e. the\\nrouter scores from Figure 2). As a demonstrative example, we use a natural language sparse expert\\nmodel. Figure 6 shows the un-normalized router scores computed for three tokens (columns) routed\\nacross ﬁve experts (rows). Each value is produced by the dot product of the token embedding and\\nthe expert embedding (from the router weights). Once the scores are computed, there are a variety of\\n3.13\\n0.51\\n2.25\\n-1.32\\n-2.81\\n0.14\\n-0.25\\n2.61\\n1.97\\n-0.68\\n0.74\\n1.58\\n0.02\\n0.1\\n-0.41\\nExperts\\nE1\\nE2\\nE3\\nE4\\nE5\\nT1\\nT2\\nT3\\n3.13\\n0.51\\n2.25\\n-1.32\\n-2.81\\n0.14\\n-0.25\\n2.61\\n1.97\\n-0.68\\n0.74\\n1.58\\n0.02\\n0.1\\n-0.41\\nExperts\\nTokens\\nE1\\nE2\\nE3\\nE4\\nE5\\nT1\\nT2\\nT3\\n3.13\\n0.51\\n2.25\\n-1.32\\n-2.81\\n0.14\\n-0.25\\n2.61\\n1.97\\n-0.68\\n0.74\\n1.58\\n0.02\\n0.1\\n-0.41\\nExperts\\nTokens\\nE1\\nE2\\nE3\\nE4\\nE5\\nT1\\nT2\\nT3\\nTokens\\nChoose Top-K\\nChoose Top-K\\nGlobally\\nDecide Expert\\nAssignment\\nFigure 6: Three common classes of routing algorithms. We illustrate three methods with an\\nExperts × Tokens activation matrix obtained through the process explained in Figure 2. Left:\\n“Choose Top-k” along the Experts axis includes the standard top-k routing algorithm (Shazeer\\net al., 2017; Lepikhin et al., 2020). Center: “Choose Top-k” along the Tokens axis are rout-\\ning algorithms such as Zhou et al. (2022). Right: “Globally Decide Expert Assignment” routing\\nalgorithms such as BASE layer (Lewis et al., 2021; Clark et al., 2022).\\nways to determine which experts should get which tokens. We highlight three common categories:\\n1) each token chooses the top-k experts, 2) each expert chooses the top-k tokens, and 3) globally\\ndetermine what tokens should go to each expert (and not use a greedy approach). This taxonomy also\\nfurther suggests yet to be explored routing algorithms. One example is an algorithm that beneﬁts by\\nlooking both horizontally and vertically at the router scores, but without incurring the cost of looking\\nglobally. A token could ﬁrst choose what experts it wants to go to, then based on this information,\\neach expert could choose what tokens it wanted.\\nEach token chooses the top-k experts.\\nThis class of routing algorithms have each token choose\\nthe top-k experts to be sent to. This is the original routing top-2 formulation proposed in Shazeer\\net al. (2017) and used in Lepikhin et al. (2020) that achieves state-of-the-art machine translation\\nresults. Fedus et al. (2021); Rajbhandari et al. (2022) used top-1 routing with success. Clark et al.\\n(2022) proposed a reinforcement learning routing algorithm that used top-1 routing. However, in-\\nstead of scaling the output of the expert computation by the router probability, they use REINFORCE\\n(Williams, 1992) with the reward being the negative cross entropy of the predicted token. Figure 7\\ndepicts the top-1, top-2 and reinforcement learning routing algorithms.\\nYang et al. (2021) introduced an extension of top-1 routing by using expert prototyping to split\\nexperts into different groups and then applied k top-1 routing procedures. Nie et al. (2021) begins\\nrouting as a soft gating function where all experts are trained (e.g. a dense model) and anneals\\ndown to the standard top-1 routing algorithm. This approach (DTS-Gate) improves over Switch\\nTransformer (Fedus et al., 2021) on OpenWebText pre-training. Dua et al. (2021) proposes a similar\\napproach of ﬁrst training a dense model where each inputs goes to every expert and then adapting\\nit to be sparse. Hazimeh et al. (2021) proposes DSelect-k, which is a smooth version of the top-k\\nrouting algorithm that improves over standard top-k routing. Rajbhandari et al. (2022) designs PR-\\n8\\nMoE which uses top-2 routing, but each token is sent to a shared dense layer and a single expert of\\nits choosing (instead of two experts).\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nRouter\\nx1\\nx2\\ny1\\ny2\\np = 0.65\\np = 0.8\\n\"The\"\\n\"Dog\"\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nRouter\\nx1\\nx2\\ny1\\ny2\\np = 0.65\\np = 0.8\\n\"The\"\\n\"Dog\"\\np = 0.15\\np = 0.3\\nTop-1 Routing\\nTop-2 Routing\\nHash\\nFunction\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nx1\\nx2\\ny1\\ny2\\n\"The\"\\n\"Dog\"\\nHash Routing\\nHash\\nFunction\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\n\"The\"\\n\"Dog\"\\nExpert Chooses Tokens\\nRouter\\nRouter\\nRouter\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\n\"The\"\\n\"Dog\"\\nBASE Routing\\nSolve Linear\\nAssignment\\nAdd + Normalize\\nx1\\nx2\\ny1\\ny2\\n\"The\"\\n\"Dog\"\\nReinforcement Learning\\nRouter\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nFFN 1\\nFFN 2\\nFFN 4\\nFFN 3\\nRouter\\nLoss += -log(\\xa0 \\xa0) * R\\nLoss += -log(\\xa0 \\xa0) * R\\nFigure 7: Visualization of six different routing algorithms. Each diagram is of a Transformer\\nsparse expert model with four experts (feed-forward networks) routing two tokens: “The” and\\n“Dog”.\\nRiquelme et al. (2021) introduces an improvement for top-k routing named Batch Prioritized Rout-\\ning (BPR) for ViT image classiﬁcation models. MoE models use ﬁxed batch sizes per expert, which\\ncan cause tokens to overﬂow if there is not enough capacity. If a token overﬂows then no computa-\\ntion will be applied to that token at that given expert. In top-k routing, priority of which tokens to\\nnot drop at an overﬂowed expert is given to the tokens sent earlier in a sentence/batch. BPR instead\\nprioritizes inputs that have higher routing scores. This is relevant for ViT models as there is no\\nautoregressive nature to the inputs, so all inputs can see each other. In language there is typically a\\nleft-to-right ordering of the inputs, which could in theory allow the model to cheat during training.\\nZoph et al. (2022) found BPR routing to be helpful for MoE language models. Kim et al. (2021)\\nproposes randomizing the prioritization of the tokens in the sequences to make sure the routing is\\nnot biased towards early tokens in the sentence.\\nStatic routing.\\nMost routing algorithms dynamically learn the routing decisions while training,\\nbut this can be statically determined before training begins. Dynamic routing algorithms typically\\noperate on the internal input representations within the network, so the routing decisions take into\\n9\\naccount the current token and previous inputs to the model (usually through the self-attention layer).\\nMost routing algorithms are dynamic, but a notable example of a static routing algorithm is is Hash\\nLayers from Roller et al. (2021). This work shows random ﬁxed routing by hashing the input token\\nled to competitive performance with learned routing. Load balancing is achieved by choosing hash\\nfunctions before training that balances batches of tokens. A depiction of Hash Layers can be found\\nin Figure 7.\\nEach expert chooses the top-k tokens.\\nInstead of each token choosing what experts to be sent\\nto, Zhou et al. (2022) ﬂips this and has each expert choose what tokens it wants routed to it. This\\nalleviates the need for auxiliary load balancing losses to be added during training or for linear as-\\nsignment algorithms. Now each expert will always have the same amount of tokens, although some\\ntokens might not get sent to any expert or some tokens might get sent to all of them. Empirically\\nthis algorithm performs well and has an adaptive computation interpretation where the model can\\nimplicitly apply more computation to certain tokens.\\nGlobally determine what tokens should go to each expert.\\nBASE layers (Lewis et al., 2021)\\ntreats token routing as a linear assignment problem. It aims to route a ﬁxed number of tokens\\nto each expert and maximize the scores from the routing matrix. Since the tokens per processor\\nare highly correlated as they come from the same sentences, tokens are randomly shufﬂed around\\nbefore locally solving the linear assignment problem on each device. This shufﬂing introduces two\\nadditional communication primitives (all2all) in both the forward and backward pass. Clark\\net al. (2022) proposes their own variant of BASE layer (S-BASE) that uses an optimal transport\\nformulation.\\nOther routing algorithms.\\nSome routing algorithms do not neatly fall into the above three cate-\\ngories. Zuo et al. (2021) introduced THOR, an algorithm which randomly selects two experts for\\neach input during training and inference and found improvements of 2 BLEU points over standard\\nMoE models. Gururangan et al. (2021) proposes DEMix, which explicitly has different experts\\nfor different pre-training domains (e.g. law, medical, etc.). Experts can then be selected by doing\\ndomain matching on the inputs. Fan et al. (2021) uses explicit language-speciﬁc sublayers where\\ninput tokens can be deterministically routed based on their language. This avoids needing dynamic\\nrouting algorithms. Ma et al. (2018) introduces a multi-gate routing algorithm where each task gets\\nit own unique gating function.\\n4.2\\nLOAD BALANCING\\nMost routing algorithms handle load balancing by adding an auxiliary loss during training to encour-\\nage equal amounts of tokens getting sent to the different experts (Shazeer et al., 2017). Some routing\\nalgorithms handle load balancing through their design: BASE Layers (Lewis et al., 2021) solves a\\nlinear assignment problem that enforces an equal number of tokens going to each expert as part of\\nthe problem statement. S-BASE from Clark et al. (2022) follows a similar protocol, but solves the\\nassignment problem using optimal transport. Nie et al. (2021) starts by training a Mixture-of-Expert\\nmodel where all tokens get sent to each expert, but over time adapts the network to do top-1 rout-\\ning. This algorithm doesn’t need load balancing as the network naturally learns to the specialize the\\nexpert representations over training.\\n5\\nSPARSE EXPERT MODELS ACROSS DOMAINS\\nSparse expert and MoE models were introduced and popularized in natural language processing\\n(NLP). This domain was a natural ﬁt for large models which beneﬁt from the easy availability of\\ntrillions of tokens and the strong self-supervised algorithms of next word prediction and masked\\nlanguage modeling. However, the impact of these models is quickly spreading to other domains\\nincluding computer vision, speech recognition and multi-modal applications. The spread of tech-\\nniques from NLP to other domains has been accelerated because the Transformer has been rapidly\\nadopted in other domains and modalities. Some examples are image classiﬁcation (Dosovitskiy\\net al., 2020), object detection (Carion et al., 2020), recommendation systems (Chen et al., 2019),\\nspeech recognition (Dong et al., 2018; Nakatani, 2019; Gulati et al., 2020).\\n10\\nAcross various domains, the sparse architectures and algorithms stay roughly the same, but what\\nis routed to the experts is different. Table 1 shows the different sparse layer inputs for a variety of\\ndifferent domains.\\nDomain\\nInput Representation\\nNLP\\nWord, subword, or sentence\\nVision\\nImage patch\\nSpeech\\nSpectrogram\\nMultimodal\\nWord or image patch\\nTable 1: Inputs to sparse models in different domains. The input is used to determine what expert\\nto route to and is what the MoE layer will apply compute to.\\n5.1\\nNATURAL LANGUAGE PROCESSING\\nInitially Shazeer et al. (2017) introduced the Mixture-of-Expert layer for LSTM language modeling\\nand machine translation. The layers were inserted between the standard layers in the LSTM model.\\nFollow up works are now based around Transformers, and the expert layers typically replace the\\ndense layers.\\nLepikhin et al. (2020) ﬁrst introduced the MoE layer into Transformers and studied it in the context\\nof machine translation. They achieved state-of-the-art translation results across 100 different lan-\\nguages when scaling up to 2048 experts per expert layer. Fedus et al. (2021) later created a sparse\\n1.6T parameter language model that achieved state-of-the-art pre-training quality. They also studied\\nusing sparse layers to produce the q/k/v activations in the Self-Attention layers, but found this tech-\\nnique to be more unstable. Lee-Thorp and Ainslie (2022) introduces the Fast Sparse Mixer, which is\\nan encoder only model that achieves 89% training and 98% inference speedups over BERT (Devlin\\net al., 2018).\\nRecently, there has been a ﬂurry of MoE research on a variety of different topics in the NLP do-\\nmain. As an example, prior MoE architectures in the NLP domain acted on the word or byte-pair\\nlevel. Kudugunta et al. (2021) instead had an MoE architecture route at the task or sentence level,\\nwhich allows for more efﬁcient inference and serving. This was studied in the context of machine\\ntranslation where sentences would be routed based on what language they were translating into.\\nNew results have been able to push state-of-the-art on few-shot inference and ﬁne-tuning bench-\\nmarks. Du et al. (2021) trained a MoE decoder-only language model and achieved state-of-the-art\\nfew-shot results, while requiring only 1/3 the compute needed to train GPT-3. Zoph et al. (2022)\\nintroduced ST-MoE, a sparse encoder-decoder model that achieves state-of-the-art on a large set\\nof reasoning and generation tasks including SuperGLUE, ARC Easy/Challenge, XSum, CNN-GM,\\nWeb-QA, ANLI, and Winogrande. ST-MoE outperforms PaLM-540B (Chowdhery et al., 2022)\\nwhen ﬁne-tuning on SuperGLUE, while using roughly 20× less pre-training FLOPs and 40× less\\ninference FLOPs.\\n5.2\\nCOMPUTER VISION\\nDue to the universality of Transformers (e.g. ViT (Dosovitskiy et al., 2020)), applying improvements\\nto the MoE architecture across domains has been fruitful. Riquelme et al. (2021) created a vision\\nMoE model by adding MoE layers into the ViT architecture. Their model, V-MoE, was applied to\\nimage classiﬁcation and was able to use just half the amount of inference compute while matching\\nthe performance of prior state-of-the-art architectures. Lou et al. (2021) introduces a sparse MoE\\nMLP model for image classiﬁcation based on the MLP-Mixer architecture (Tolstikhin et al., 2021).\\nTheir MoE variant achieved better image classiﬁcation performance on ImageNet and CIFAR com-\\npared to its dense counterpart.\\nWu et al. (2022) improved the efﬁciency of training MoE models through their proposed Resid-\\nual Mixture-of-Expert layer. This architecture achieves 30% reduced training cost and comparable\\nquality to standard MoE models on both segmentation and object detection. Hwang et al. (2022)\\nimplements an efﬁcient framework and adaptive parallelism strategy for MoE layers. To benchmark\\n11\\ntheir system, they add MoE layers to the Swin Tranformer V2 architecture (Liu et al., 2022) for\\nimage classiﬁcation and object detection. Their MoE variant achieves 1.5x-2x speedups in both\\ntraining and inference over the previous MoE implementation. Aljundi et al. (2017) uses expert\\nmodels in a continual learning setup where they add new experts over time and demonstrate im-\\nprovements on image classiﬁcation and video prediction. Caccia et al. (2021) dynamically increases\\nthe expert count over the course of training for image classiﬁcation models on CIFAR and MNIST.\\nRamachandran and Le (2018) studies how depth and architectural diversity impacts sparse expert\\nmodel performance and achieves gains on image recognition. Kirsch et al. (2018) develops an end-\\nto-end algorithm for doing conditional computation based on the input data that achieves strong\\nimage classiﬁcation performance.\\n5.3\\nSPEECH RECOGNITION\\nSpeechMoE (You et al., 2021) uses MoE Transformer models for speech recognition and achieves\\nstrong character error rate improvements across four datasets. They use novel auxiliary losses to\\npromote sparsity and introduce a new routing architecture. SpeechMoE2 (You et al., 2022) further\\nimproves on the SpeechMoE’s results by making a new routing algorithm that adds in new aux-\\niliary information for making routing decisions. Kumatani et al. (2021) yields improvements for\\nmulti-lingual speech recognition by using MoE layers in two different types of Transformer speech\\narchitectures: seqeuence-to-sequence and transducers.\\n5.4\\nMULTIMODAL AND MULTI-TASK\\nMustafa et al. (2022) does multimodal learning by training a MoE model (LIMoE) that takes as\\ninput both images and text and learns using a contrastive loss similar to CLIP (Radford et al., 2021).\\nThe MoE layer can route both the image patches and the word tokens to the available experts.\\nThe model outperforms CLIP when using a comparable training strategy, and when scaled further\\nmatches state-of-the-art methods.\\n6\\nWHEN TO USE A SPARSE VERSUS DENSE MODEL\\nA common question is if you are given a ﬁxed compute or FLOP budget (e.g. 100 GPUs for 20\\nhours), what type of model should you train to achieve the best performance? Many prior works\\nshow that sparsity is better than a dense model for this type of setup (Shazeer et al., 2017; Lepikhin\\net al., 2020; Fedus et al., 2021; Du et al., 2021; Artetxe et al., 2021; Lewis et al., 2021). Given all\\nthe strong state-of-the-art results using sparse models, why should you ever not use a sparse model\\nover a dense model?\\nSparse models are a generalization of a dense model; a sparse model with a single expert is roughly\\na dense model. Fundamentally, sparse models allow to vastly increase the number of parameters in\\na model by increasing the number of experts, while keeping the FLOPs per example approximately\\nconstant. This can be good or bad depending on the setup and how the model is going to be used\\nlater.\\nAt a high level, sparsity is good when you have many accelerators (e.g. GPU/TPU) to host all\\nthe additional parameters that comes when using sparsity. Typically models are trained using data-\\nparallelism where different machines will get different slices of the training/inference data. The\\nmachines used for operating on the different slices of data can now be used to host many more\\nmodel parameters. Therefore, sparse models are good when training with data parallelism and/or\\nhave high throughput while serving: training/serving on many machines which can host all of the\\nparameters.\\nUsing sparsity requires careful consideration for how the model will be used in downstream usage\\ntoo. If there are lots of machines to pre-train a model, but a lot less for ﬁne-tuning or serving, then\\nthe amount of sparsity (e.g. the number of experts) should be tailored to ﬁt the amount of memory\\navailable in the downstream use cases. This is often a practical design consideration used in the\\nliterature.\\nOn a per parameter basis, sparse models will always look comparatively worse to dense models.\\nAssuming that all parameters are kept in the accelerators memory, this is a similar requirement to\\n12\\nseeking the best model that can ﬁt onto a certain hardware size (e.g. 4 GPUs), where again a sparse\\nmodel will be a worse option than a dense one. As mentioned above, sparse models are a great ﬁt\\nwhen you have the ability to either be training or serving on many machines in parallel in order to\\nhost the additional model parameters from the experts.\\nAll hope is not lost for sparse models in memory restrictive settings though. Fedus et al. (2021)\\nshows that sparse models work well with as few as two experts, which requires limited additional\\nmemory. New research also allows for overcoming GPU/TPU memory shortages by dynamically\\nswapping model memory between the CPU and GPU (see Section 2.2 for more details). Other\\napproaches for reducing the memory footprint of sparse models are discussed in Section 7.3.\\n7\\nSPARSE MODEL TRAINING IMPROVEMENTS\\nSparse models often have different dynamics than dense models and beneﬁt from different training\\nand ﬁne-tuning methodologies.\\n7.1\\nINSTABILITY\\nSparse models have frequently been reported to be more unstable, meaning the loss diverges and\\nincreases (Lepikhin et al., 2020; Fedus et al., 2021; Zoph et al., 2022; Mustafa et al., 2022). Insta-\\nbilities also appear more often at larger model scales. Lepikhin et al. (2020) encountered training\\ninstability using bfloat16 activations with a 1 trillion parameter model. Fedus et al. (2021) en-\\ncountered instabilities in their highest-compute Switch-XXL model. Zoph et al. (2022) encountered\\ninstabilities in their largest models, especially in the multi-lingual setting. Mustafa et al. (2022)\\nobserved increased instability when doing multi-modal training on both images and text.\\nMuch research has been done to improve the training dynamics of sparse models. Lepikhin et al.\\n(2020) noted that the largest model instabilities can be ﬁxed by training the models using higher\\nprecision (float32), but comes at the cost of more memory usage and slower training. Fedus\\net al. (2021) recommended using a lower weight initialization scale and casting only a speciﬁc\\nsubset of the routing network to higher precision for better model stability/training. Du et al. (2021)\\nskips batches of data that have any NaNs or Infs in the gradients and also restarts the model from\\nan earlier checkpoint when any training divergences occur. Artetxe et al. (2021) propose a smarter\\ninitialization of the expert layer to account for the reduced batch size of the expert weights. Since\\neach expert will have a batch size of B\\nE\\n3 the authors propose scaling the gradients of the expert layer\\nby\\n1\\n√\\nE . Zoph et al. (2022) introduced the router z-loss to improve both the model instability and also\\nquality. This auxiliary loss aims to reduce ﬂoating point roundoff errors by encouraging the logits\\ngoing into the router function to remain small over the course of training. Mustafa et al. (2022)\\nextensively studied many techniques to ﬁx model instability, and used a combination of different\\napproaches including the router z-loss and two novel entropy losses.\\n7.2\\nTRANSFER TO NEW DISTRIBUTIONS\\nSeveral research papers, especially at larger scales, note that MoE models transferred to new do-\\nmains (as in ﬁne-tuning) lags their dense counterparts. Fedus et al. (2021); Narang et al. (2021)\\ncompared the pre-training perplexity versus ﬁne-tuning performance for dense and sparse models.\\nThey noticed for a given pre-training perplexity, sparse models were ﬁne-tuning worse on reasoning\\ntasks, but better on knowledge heavy tasks. In addition to worse out-of-domain language model-\\ning performance, Artetxe et al. (2021) observed worse ﬁne-tuning compared to dense models on\\nmultiple tasks including HellaSwag, PIQA and Winogrande.\\nSeveral different ways have been proposed to help address the ﬁne-tuning issues. One is to scale\\nmodels by having more FLOPs compared to more sparsity (e.g. fewer experts, but make them\\nlarger). Fedus et al. (2021) trained a 1.6T parameter model with 2048 experts, but it only had as\\nmany FLOPs as a 2B dense model. Conversely, the model with the best ﬁne-tuning performance\\nhad only 128 experts, but the amount of FLOPs as a 11B dense model. Trading off less sparsity\\n3B is the tokens in the batch and E is the number of experts. This assumes top-1 routing, but similar\\nanalysis holds for the other routing variants.\\n13\\nfor more FLOPs when scaling a model is a simple way to ensure better ﬁne-tuning performance.\\nZoph et al. (2022) noticed that the optimal ﬁne-tuning hyperparameters (e.g. learning rate and batch\\nsize) can be dramatically different for dense and sparse models. Using the best hyperparameters for\\ndense models on sparse models can mask any of the sparsity pre-training improvements – therefore,\\nan independent hyperparameter study is beneﬁcial.\\n7.3\\nINFERENCE\\nBy design, sparse expert models have many more parameters than their dense counterparts. While\\nthe computation done is still relatively low, the memory footprint can be a burden. Therefore, some\\nresearch has focused on reducing the number of parameters needed at inference-time to ease serving\\nrequirements. Kudugunta et al. (2021) routes at the task level instead of the word or token level for\\nmachine translation. This allows for more efﬁcient inference because the subset of weights only for\\nthe needed tasks are required. Kim et al. (2021) prunes away experts at inference to reduce the mem-\\nory footprint of the model. Two different methods are used for pruning: randomly selecting a subset\\nof the experts and choosing the experts with the highest utilization at inference time. Fedus et al.\\n(2021) distill large sparse models into smaller dense models for language modeling and ﬁne-tuning.\\nRajbhandari et al. (2022) studies distillation of sparse models for language modeling by reducing\\nthe depth in the expert layers of the network. Rajbhandari et al. (2022) implements an optimized\\nversion of MoE into the DeepSpeed framework that results in 7.3× faster inference latency than\\nexisting frameworks.\\n8\\nINTERPRETABILITY\\nSparse expert models more naturally lend themselves to interpretability studies because each input\\nis processed by an identiﬁable, discrete subset of the model weights (i.e. the chosen experts). There-\\nfore, instead of the daunting task of interpreting possibly trillions of ﬂoating point numbers, one can\\ninstead read off a small discrete set of integers corresponding to which expert the input was sent.\\nShazeer et al. (2017) conducted preliminary studies into expert specialization for the encoder of their\\n2048 MoE layer on the WMT ’14 EnFr machine translation task. They identiﬁed three experts, one\\nwith a specialization of words around innovation, the second which processed the article “a”, and a\\nthird which was routed synonyms of speed.\\nLater, more extensive analyses were conducted by Lewis et al. (2021) on Transformer-based archi-\\ntectures. Lewis et al. (2021) conducted a study where they tracked the most frequent prior input\\ntoken when the expert was selected. This revealed specialization in quantities, numbers, posses-\\nsives, subword fragments, and clusters of related verbs, nouns and adjectives, with selected results\\npresented in Table 2.\\nExpert\\nTop-5 preceding tokens\\n5\\nyear, years, billion, millions, tonnes\\n9\\nelectronic, local, public, national, outdoor\\n34\\nto, will, should it, may\\n42\\ntwo, 50, 1, 80, 000\\n62\\nwork, started, involved, working, launched\\n72\\nis, was, be, been, were\\n74\\ngoing, go, come, back, return\\n101\\nB, T, W, H, k\\nTable 2:\\nExpert specialization based on preceding context in BASE Layers. We reproduce a\\nportion of table of Lewis et al. (2021), presenting the most frequent preceding top-ﬁve tokens for the\\nselected experts. This example shows experts specializing in punctuation, conjunctions & articles,\\nverbs, visual descriptions, proper names, counting & numbers.\\nZoph et al. (2022) trained an encoder-decoder Transformer and ﬁnds similar patterns in the encoder,\\nincluding experts that specialize in a shallow way, such as over articles (e.g. “a”, “the”). Table 3\\nreproduces a portion of the observed specializations of Zoph et al. (2022). Those studies further\\n14\\nfound expert specialization in punctuation, numbers, proper names, verbs, colors and special mask\\ntokens used for the pre-training objective.\\nExpert specialization\\nExpert position\\nRouted tokens\\nSentinel tokens\\nLayer 1\\nbeen <extra id 4><extra id 7>ﬂoral to\\n<extra id 10><extra id 12><extra id 15>\\n<extra id 17><extra id 18><extra id 19>...\\nPunctuation\\nLayer 2\\n, , , , , , , , , - , , , , , ). )\\nLayer 6\\n, , , , , : . : , & , & & ? & - , , ? , , , . <extra id 27>\\nConjunctions and articles\\nLayer 3\\nThe the the the the the the the the The the the\\nLayer 6\\na and and and and and and and or and a and .\\nVerbs\\nLayer 1\\ndied falling identiﬁed fell closed left posted lost felt\\nleft said read miss place struggling falling signed died\\nVisual descriptions\\nLayer 0\\nher over her know dark upper dark outer\\ncolor, spatial position\\ncenter upper blue inner yellow raw mama\\nbright bright over open your dark blue\\nProper names\\nLayer 1\\nA Mart Gr Mart Kent Med Cor Tri Ca Mart\\nR Mart Lorraine Colin Ken Sam Ken Gr Angel A\\nCounting and numbers\\nLayer 1\\nafter 37 19. 6. 27 I I Seven 25 4, 54 I two dead we\\nwritten and numerical forms\\nSome 2012 who we few lower each\\nTable 3:\\nEncoder expert specialization in ST-MoE. We reproduce a table of Zoph et al. (2022)\\ndemonstrating expert specialization in punctuation, conjunctions & articles, verbs, visual descrip-\\ntions, proper names, counting & numbers.\\nBut a deeper analysis of the full encoder-decoder ST-MoE architecture found clearer evidence of\\nspecialization in the encoder, rather than the decoder. This warrants further study into the value and\\npositioning of expert layers. Lack of evident specialization may either signal a difﬁcult to discern\\npatterns or no useful patterns.\\nInterpretability of sparse expert models has not only been limited to text. One example is LIMoE\\n(Mustafa et al., 2022), a multi-modal model that was observed to learn experts that specialize in\\ntextual and visual data, including patches of textures, plants, eyes, and words (Figure 8). As in the\\ntext based models, the complexity of the specialization varies signiﬁcantly. For instance, text-based\\nexperts were found to span simple objectives like processing the article “a” up to more complicated\\nconcepts like past-tense verbs. Similarly, in multi-modal models, the sophistication of expert spe-\\ncialization varies to concepts as simple as a basic textures up to high-level objects such as wheels or\\ndoor handles.\\nFinally, we highlight one signiﬁcant limitation of these interpretability approaches. These consider\\nthe tokens or patches arriving to each expert in a narrow way. Speciﬁcally, the initial embedding of\\na word or of a patch incorporates contextual information from surrounding data (Transformers do\\nthis through self-attention or encoder-decoder attention). Therefore, more nuanced specialization\\nmay be missed by these heuristic techniques. More careful and thorough interpretabilty work will\\nbe needed to better understand sparse expert models. The release of spare expert model checkpoints\\nby Artetxe et al. (2021) and Fedus et al. (2021) allows broader groups to analyze and explain these\\ndynamics.\\n9\\nFUTURE DIRECTIONS AND CONCLUSIONS\\nEven though sparse expert models and Mixture-of-Expertss date back to at least the early nineties\\n– many questions remain. We conclude our review with a conjecture of promising areas of future\\nwork, speciﬁcally highlighting the intersection with two recent developments (adaptive computation\\nand retrieval methods) and our parting thoughts.\\n15\\nFigure 8: Visual expert specialization in LIMoE. We reproduce a ﬁgure from Mustafa et al. (2022)\\nwhich ﬁnds expert specialize in patches of textures (solid and striped), natural objects (plants, hands,\\neyes), and man-made objects (wheels, door handles, words).\\nAdaptive Computation.\\nAdaptive computation is the idea that different inputs to a machine\\nlearning system may use differing amounts of computation (i.e. the amount or the type of compute\\nis adapted on-the-ﬂy). Sparse models build on the mirrored idea: each input uses the same amount of\\ncomputation, but potentially with different parameters. However, these techniques are not mutually\\nexclusive; some routing algorithms (Section 4) allow for adaptive computation by sending a token\\nto a variable number of experts (Riquelme et al., 2021; Zhou et al., 2022). Still future models may\\nbeneﬁt by combining other adaptive computation techniques – as an example, in addition to choosing\\nwhich expert, a network might choose the number of layers to use, as well (Schuster et al., 2022).\\nHeterogeneous expert layers also are a natural ﬁt for adaptive computation. Most sparse models\\nuses experts of the same type and size for simplicity and efﬁciency on modern hardware. But by\\nallowing experts to differ in size (e.g. in depth or width), the routing decision will then result in\\ndiffering amounts of computation. New software systems, such as Pathways (Dean, 2021), will help\\nfacilitate efﬁcient implementations of these heterogeneous architectures and algorithms on modern\\nhardware.\\nRetrieval Methods.\\nRetrieval mechanisms effectively expand the capacity of models by allowing\\nthem to dynamically access information beyond the current context or what is stored in the param-\\neters (Khandelwal et al., 2019; Guu et al., 2020; Borgeaud et al., 2022). Sparse expert models and\\nretrieval models have an overlapping goal: increase the capacity of the model to better store, re-\\ntrieve, and apply knowledge. Sparse expert models do this parametrically (i.e. experts contain more\\nlearnable parameters), while retrieval based systems embed information that can be dynamically re-\\ntrieved non-parametrically (i.e. nearest neighbor lookup over a corpus). Studying the trade-offs and\\ncombining both approaches is likely to prove a useful future direction.\\nConclusions.\\nSparsity reduces the training and inference costs, resulting in massive models with\\na better accuracy than their dense counterparts. But many open questions remain. For instance, we\\nstill poorly understand how the optimal number and size of experts depends on the task (e.g. should\\none use a few large experts or many small experts for translation?). As many works have pointed\\nout, achieving strong out-of-domain generalization is less straight-forward and better explanations\\nare needed. Further, most sparse expert models have relatively low architectural diversity where\\nsparse layers are interspersed at regular intervals. Future models may beneﬁt from less standardized\\nstructure and heterogeneous expert architectures. Additionally, the appropriate granularity of spar-\\nsity still must be determined: most works have focused on experts replacing components, such as\\nfeed-forward network layers, but beneﬁts of more fully modular, independent experts were discov-\\nered (Gururangan et al., 2021; Li et al., 2022). The ﬁeld is still uncovering properties of sparse ex-\\npert models, including much improved calibration (Srivastava et al., 2022); others remain unknown\\nincluding their dynamics under asynchronous training (Recht et al., 2011) or their memorization\\nabilities (Carlini et al., 2020). In short, these models pose a myriad of challenging mathematical,\\n16\\nengineering, and research problems, but the solutions so far have yielded signiﬁcant gains and we\\nbelieve more improvements lie ahead.\\nACKNOWLEDGEMENTS\\nWe’d like to thank the BIG-Bench core authors, Mike Lewis, Aidan Clark, Diego de Las Casas, Nan\\nDu, and Carlos Riquelme for permission to reproduce ﬁgures and tables here. We would also like\\nto thank Daniel S. Park, Nan Du, Jason Wei, James Lee-Thorp, and Yanqi Zhou for feedback and\\ncomments on our drafts.\\n17\\nREFERENCES\\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with\\na network of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 3366–3375, 2017.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui\\nChen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\\nJeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efﬁcient large\\nscale language modeling with mixtures of experts, 2021.\\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\\nin neural networks for faster models, 2016.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\\nImproving language models by retrieving from trillions of tokens. In International Conference on\\nMachine Learning, pages 2206–2240. PMLR, 2022.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\nLucas Caccia, Jing Xu, Myle Ott, Marc’Aurelio Ranzato, and Ludovic Denoyer. On anytime learn-\\ning at macroscale. arXiv preprint arXiv:2106.09563, 2021.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In European conference on\\ncomputer vision, pages 213–229. Springer, 2020.\\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.\\nExtracting training data from large language models, 2020.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\\narXiv preprint arXiv:1312.3005, 2013.\\nQiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. Behavior sequence transformer for\\ne-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on\\nDeep Learning Practice for High-Dimensional Sparse Data, pages 1–4, 2019.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,\\nBogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed scaling laws for\\nrouted language models. arXiv preprint arXiv:2202.01169, 2022.\\nJeff Dean. Introducing pathways: A next-generation ai architecture. Google AI Blog, 2021.\\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\\nperformance. arXiv preprint arXiv:1907.04840, 2019.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\nLinhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence\\nmodel for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 5884–5888. IEEE, 2018.\\n18\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\\nimage is worth 16x16 words: Transformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020.\\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,\\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy\\nMeier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,\\nand Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts, 2021.\\nDheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan. Tricks\\nfor training sparse translation models. arXiv preprint arXiv:2110.08246, 2021.\\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\\nMaking all tickets winners. In International Conference on Machine Learning, pages 2943–2952.\\nPMLR, 2020.\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Man-\\ndeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric\\nmultilingual machine translation. Journal of Machine Learning Research, 22(107):1–48, 2021.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\\npreprint arXiv:1902.09574, 2019.\\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\\nWang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer\\nfor speech recognition. arXiv preprint arXiv:2005.08100, 2020.\\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. Demix\\nlayers: Disentangling domains for modular language modeling, 2021.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\\nlanguage model pre-training. In International Conference on Machine Learning, pages 3929–\\n3938. PMLR, 2020.\\nAaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg\\nGanger, and Phil Gibbons. Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\\nRahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture\\nof experts with applications to multi-task learning, 2021.\\nSepp Hochreiter and J¨\\nurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\\n1735–1780, 1997.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong\\nLee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient training of giant neural\\nnetworks using pipeline parallelism. In Advances in neural information processing systems, pages\\n103–112, 2019.\\n19\\nChangho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas,\\nJithin Jose, Prabhat Ram, et al.\\nTutel: Adaptive mixture-of-experts at scale.\\narXiv preprint\\narXiv:2206.03382, 2022.\\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\\nlocal experts. Neural computation, 3(1):79–87, 1991.\\nSebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski,\\nHenryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. Advances in\\nNeural Information Processing Systems, 34:9895–9907, 2021.\\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\\nNeural computation, 6(2):181–214, 1994.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361, 2020.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\\nthrough memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,\\n2019.\\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,\\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁ-\\ncient moe training for multitask multilingual models, 2021.\\nLouis Kirsch, Julius Kunze, and David Barber. Modular networks: Learning to decompose neural\\ncomputation. Advances in neural information processing systems, 31, 2018.\\nFerdinand Kossmann, Zhihao Jia, and Alex Aiken. Optimizing mixture of experts using dynamic\\nrecompilations. arXiv preprint arXiv:2205.01848, 2022.\\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efﬁcient inference.\\narXiv preprint arXiv:2110.03742, 2021.\\nKenichi Kumatani, Robert Gmyr, Felipe Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric\\nSun, and Yu Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for\\nspeech recognition. arXiv preprint arXiv:2112.05820, 2021.\\nYann LeCun, L´\\neon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\\nJames Lee-Thorp and Joshua Ainslie. Sparse mixers: Combining moe and mixing to build a more\\nefﬁcient bert. arXiv preprint arXiv:2205.12399, 2022.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\\nSimplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.\\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke\\nZettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models,\\n2022. URL https://arxiv.org/abs/2208.03306.\\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng\\nZhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009–12019,\\n2022.\\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Sparse-mlp: A fully-mlp architecture\\nwith conditional computation. arXiv preprint arXiv:2109.02008, 2021.\\n20\\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relation-\\nships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1930–1939,\\n2018.\\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multi-\\nmodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint\\narXiv:2206.02770, 2022.\\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated prob-\\nabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence,\\n2015.\\nTomohiro Nakatani. Improving transformer-based end-to-end speech recognition with connectionist\\ntemporal classiﬁcation and language model integration. In Proc. Interspeech, 2019.\\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations\\ntransfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.\\nXiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao\\nYang, Zhi Yang, and Bin Cui.\\nDense-to-sparse gate for mixture-of-experts.\\narXiv preprint\\narXiv:2112.14397, 2021.\\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv\\npreprint arXiv:2104.10350, 2021.\\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´\\ne Susano Pinto, Sylvain\\nGelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. arXiv\\npreprint arXiv:2009.13239, 2020.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding by generative pre-training, 2018.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International Conference on Machine Learning,\\npages 8748–8763. PMLR, 2021.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yu-\\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\\npher, 2021.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimization\\ntowards training a trillion parameter models. arXiv preprint arXiv:1910.02054, 2019.\\n21\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-\\nmar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts\\ninference and training to power next-generation ai scale. arXiv preprint arXiv:2201.05596, 2022.\\nPrajit Ramachandran and Quoc V Le.\\nDiversity and depth in per-example routing models.\\nIn\\nInternational Conference on Learning Representations, 2018.\\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach\\nto parallelizing stochastic gradient descent. Advances in neural information processing systems,\\n24, 2011.\\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´\\ne Su-\\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\\narXiv preprint arXiv:2106.05974, 2021.\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bu-\\nlian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan\\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\\nup models and data with t5x and seqio.\\narXiv preprint arXiv:2203.17189, 2022.\\nURL\\nhttps://arxiv.org/abs/2203.17189.\\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse\\nmodels. arXiv preprint arXiv:2106.04426, 2021.\\nClemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of\\nnon-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017.\\nClemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and the\\nchallenges of modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019.\\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, and\\nDonald Metzler. Conﬁdent adaptive language modeling. arXiv preprint arXiv:2207.07061, 2022.\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538, 2017.\\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\\nlearning for supercomputers.\\nIn Advances in Neural Information Processing Systems, pages\\n10414–10423, 2018.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`\\na Garriga-Alonso, et al. Beyond the\\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\\narXiv:2206.04615, 2022.\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. ACM\\nComputing Surveys (CSUR), 2020.\\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\\nterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An\\nall-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:24261–\\n24272, 2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems, pages 5998–6008, 2017.\\n22\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman.\\nSuperglue: A stickier benchmark for general-purpose language\\nunderstanding systems. In Advances in Neural Information Processing Systems, pages 3266–\\n3280, 2019.\\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\\nlearning. Machine learning, 8(3):229–256, 1992.\\nLemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu Yuan. Residual\\nmixture of experts. arXiv preprint arXiv:2204.09636, 2022.\\nCanwen Xu and Julian McAuley. A survey on dynamic neural networks for natural language pro-\\ncessing. arXiv preprint arXiv:2202.07101, 2022.\\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\\nmang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. M6-t:\\nExploring sparse expert models and beyond, 2021.\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with\\ndynamic routing mixture of experts. arXiv preprint arXiv:2105.03036, 2021.\\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe2: Mixture-of-experts model with im-\\nproved routing. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 7217–7221. IEEE, 2022.\\nSeniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of experts. IEEE\\ntransactions on neural networks and learning systems, 23(8):1177–1193, 2012.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng\\nChen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. arXiv preprint\\narXiv:2202.09368, 2022.\\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and\\nWilliam Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906,\\n2022.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and\\nJianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2021.\\n23\\n', 'source_name': 'A Review of Sparse Expert Models in Deep Learning', 'source_url': 'https://arxiv.org/abs/2209.01667'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "OpenMoE_NOTES.pdf #80\n",
      "{'content': 'OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models \\nMain Idea: an open-source project, OpenMoE analyzes decoder-only MoE LLMs from 650M to \\n34B total parameters and trained on up to 1T tokens (the largest version – 34B – was only trained \\non 200B tokens, 1T training tokens were used in the 8B version). The findings and \\nrecommendations of these experiments are shared in the paper. \\nThe 34B version (largest one trained) has 6B active parameters per input and 32 experts per MoE \\nlayer. 5 intermediate checkpoints for the 8B model (every 200B training tokens) were released, \\nand a Chat version of this 8B model was also trained (instruction-tuned). \\n \\nDesign \\n- \\nInspired by the facts that including code data in the pre-training dataset boosts \\nperformance and that code is always precise (contrary to text), which leads the authors \\nto think that LLMs would more easily understand it, leading to better training, code data \\nis aggressively sampled during pre-training (excessively/to a fault). \\n- \\nGenerally, follows the ST-MoE architecture and routing design. The reason for this is ST-\\nMoE’s focus on training stability, a characteristic OpenMoE aims to achieve. \\n- \\nTop-2 routing used during the entire training process. \\n- \\nAn MoE layer is inserted every 6 Transformer blocks, so most Transformer blocks do not \\nhave an MoE layer. \\n- \\nUse UL2 method for the training objective (mix of span corruption and prefix language \\nmodeling). \\n- \\nSFT for instruction-tuning is done on a dataset of 58k conversations, each with 1.8 turns \\non average, to analyze alignment (although this is not a big focus of this work). \\n \\nAnalysis \\n- \\nMoE experts did not seem to specialize at the domain or at the task levels, but at the \\ntoken level. \\no This is intuitive and rather obvious since the routing is done at the token-level. \\n- \\nContext-independent specialization \\no MoE routing is done based on token ID and independent of the context around \\nthat token. This means that the routing is not really done based on semantics \\n(context) but on syntax (the token being routed). \\n- \\nExperts cluster tokens together, that is, they seem to specialize on a specific cluster of the \\ntoken input space (the raw token’s embeddings without regard to context). Similar tokens \\nare routed to the same expert. \\n- \\nThe token routing is learned at very early stages of training and remains fixed throughout \\nthe rest of training. \\n- \\nDrop-Towards-the-End \\no Due to this fixed routing characteristic, something like instruction-tuning can lead \\nto issues. This is because instruction-tuning data is out-of-domain, presenting a \\ndistribution shift from the pre-training data. Since the routing is learned from the \\npre-training data and is fixed, the distribution shift from instruction-tuning data \\nwill lead to overloaded experts, subsequently leading to token dropping in later \\nrounds of the conversation (assuming multi-turn chat). \\n \\nTakeaways/Recommendations \\n- \\nThe amount of code present in the pre-training data of over 50% was too aggressive \\n(around 30% is recommended instead) and hurt the performance of the model in text \\ntasks. \\n- \\nThe finding that MoE routing is fixed and established at early stages of training indicates \\nthat the router can be frozen after a warmup stage. \\n- \\nThe Context-Independent Specialization of experts indicates that the FFN (expert) \\ncomputation can be done independently from the attention layer, thus an approach that \\nwould compute the expert FFN and the attention layers in parallel would make sense, \\nbringing a speedup in training and inference. \\no Future research proposition. \\n- \\nTo alleviate the Drop-Towards-the-End issue, mixing instruction-tuning data into the pre-\\ntraining data mix while the routing is being learned (the warmup stage) can be effective. \\nThis would allow the router to learn the instruction-tuning data distribution, so the token \\ndropping issue experienced in later rounds of multi-chat conversation would be \\nsomewhat mitigated. \\n \\nMy takeaways: \\n- \\n5 checkpoints for the 8B OpenMoE model were released. This could potentially add to the \\nrouting analysis project I have planned. \\n- \\nThe conclusion that experts specialize on a specific cluster of the token input space seems \\nto be inconsistent with the Hash Layers paper comparison of cluster-based hashing vs the \\nopposite. \\n- \\nThe conclusion that token routing is fixed at very early stages of training seems to be \\ninconsistent with the analysis done in the StableMoE paper. \\n \\n', 'source_name': 'OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/OpenMoE_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Learning_Factored_Representations.pdf #81\n",
      "{'content': \"Learning Factored Representations in a\\nDeep Mixture of Experts\\nDavid Eigen 1,2\\nMarc’Aurelio Ranzato 1 ∗\\nIlya Sutskever 1\\n1 Google, Inc.\\n2 Dept. of Computer Science, Courant Institute, NYU\\ndeigen@cs.nyu.edu\\nranzato@fb.com\\nilyasu@google.com\\nAbstract\\nMixtures of Experts combine the outputs of several “expert” networks, each of\\nwhich specializes in a different part of the input space. This is achieved by train-\\ning a “gating” network that maps each input to a distribution over the experts. Such\\nmodels show promise for building larger networks that are still cheap to compute\\nat test time, and more parallelizable at training time. In this this work, we ex-\\ntend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with\\nmultiple sets of gating and experts. This exponentially increases the number of\\neffective experts by associating each input with a combination of experts at each\\nlayer, yet maintains a modest model size. On a randomly translated version of the\\nMNIST dataset, we ﬁnd that the Deep Mixture of Experts automatically learns to\\ndevelop location-dependent (“where”) experts at the ﬁrst layer, and class-speciﬁc\\n(“what”) experts at the second layer. In addition, we see that the different combi-\\nnations are in use when the model is applied to a dataset of speech monophones.\\nThese demonstrate effective use of all expert combinations.\\n1\\nIntroduction\\nDeep networks have achieved very good performance in a variety of tasks, e.g. [10, 5, 3]. However,\\na fundamental limitation of these architectures is that the entire network must be executed for all\\ninputs. This computational burden imposes limits network size. One way to scale these networks up\\nwhile keeping the computational cost low is to increase the overall number of parameters and hidden\\nunits, but use only a small portion of the network for each given input. Then, learn a computationally\\ncheap mapping function from input to the appropriate portions of the network.\\nThe Mixture of Experts model [7] is a continuous version of this: A learned gating network mixes\\nthe outputs of N “expert” networks to produce a ﬁnal output. While this model does not itself\\nachieve the computational beneﬁts outlined above, it shows promise as a stepping stone towards\\nnetworks that can realize this goal.\\nIn this work, we extend the Mixture of Experts to use a different gating network at each layer in\\na multilayer network, forming a Deep Mixture of Experts (DMoE). This increases the number of\\neffective experts by introducing an exponential number of paths through different combinations of\\nexperts at each layer. By associating each input with one such combination, our model uses different\\nsubsets of its units for different inputs. Thus it can be both large and efﬁcient at the same time.\\nWe demonstrate the effectiveness of this approach by evaluating it on two datasets. Using a jittered\\nMNIST dataset, we show that the DMoE learns to factor different aspects of the data representation\\nat each layer (speciﬁcally, location and class), making effective use of all paths. We also ﬁnd that all\\ncombinations are used when applying our model to a dataset of speech monophones.\\n∗Marc’Aurelio Ranzato currently works at the Facebook AI Group.\\n1\\narXiv:1312.4314v3  [cs.LG]  9 Mar 2014\\n2\\nRelated Work\\nA standard Mixture of Experts (MoE) [7] learns a set of expert networks fi along with a gating\\nnetwork g. Each fi maps the input x to C outputs (one for each class c = 1, . . . , C), while g(x) is a\\ndistribution over experts i = 1, . . . , N that sums to 1. The ﬁnal output is then given by Eqn. 1\\nFMoE(x)\\n=\\nN\\nX\\ni=1\\ngi(x)softmax(fi(x))\\n(1)\\n=\\nN\\nX\\ni=1\\np(ei|x)p(c|ei, x) = p(c|x)\\n(2)\\nThis can also be seen as a probability model, where the ﬁnal probability over classes is marginalized\\nover the selection of expert: setting p(ei|x) = gi(x) and p(c|ei, x) = softmax(fi(x)), we have\\nEqn. 2.\\nA product of experts (PoE) [6] is similar, but instead combines log probabilities to form a product:\\nFPoE(x)\\n∝\\nN\\nY\\ni=1\\nsoftmax(fi(x)) =\\nN\\nY\\ni=1\\npi(c|x)\\n(3)\\nAlso closely related to our work is the Hierarchical Mixture of Experts [9], which learns a hierarchy\\nof gating networks in a tree structure. Each expert network’s output corresponds to a leaf in the tree;\\nthe outputs are then mixed according to the gating weights at each node.\\nOur model differs from each of these three models because it dynamically assembles a suitable\\nexpert combination for each input. This is an instance of the concept of conditional computation\\nput forward by Bengio [1] and examined in a single-layer stochastic setting by Bengio, Leonard and\\nCourville [2]. By conditioning our gating and expert networks on the output of the previous layer,\\nour model can express an exponentially large number of effective experts.\\n3\\nApproach\\nTo extend MoE to a DMoE, we introduce two sets of experts with gating networks (g1, f 1\\ni ) and\\n(g2, f 2\\nj ), along with a ﬁnal linear layer f 3 (see Fig. 1). The ﬁnal output is produced by composing\\nthe mixtures at each layer:\\nz1\\n=\\nN\\nX\\ni=1\\ng1\\ni (x)f 1\\ni (x)\\nz2\\n=\\nM\\nX\\nj=1\\ng2\\nj (z1)f 2\\nj (z1)\\nF(x) = z3\\n=\\nsoftmax(f 3(z2))\\nWe set each f l\\ni to a single linear map with rectiﬁcation, and each gl\\ni to two layers of linear maps with\\nrectiﬁcation (but with few hidden units); f 3 is a single linear layer. See Section 4 for details.\\nWe train the network using stochastic gradient descent (SGD) with an additional constraint on gating\\nassignments (described below). SGD by itself results in a degenerate local minimum: The experts at\\neach layer that perform best for the ﬁrst few examples end up overpowering the remaining experts.\\nThis happens because the ﬁrst examples increase the gating weights of these experts, which in turn\\ncauses them to be selected with high gating weights more frequently. This causes them to train more,\\nand their gating weights to increase again, ad inﬁnitum.\\nTo combat this, we place a constraint on the relative gating assignments to each expert during train-\\ning. Let Gl\\ni(t) = Pt\\nt′=1 gl\\ni(xt′) be the running total assignment to expert i of layer l at step t, and\\nlet ¯\\nGl(t) = 1\\nN\\nPN\\ni=1 Gl\\ni(t) be their mean (here, xt′ is the training example at step t′). Then for each\\nexpert i, we set gl\\ni(xt) = 0 if Gl\\ni(t) −¯\\nGl(t) > m for a margin threshold m, and renormalize the\\n2\\nx\\t\\n\\r\\nf1\\n1(x)\\t\\n\\r\\nf2\\n1(x)\\t\\n\\r\\nfN\\n1(x)\\t\\n\\r\\ng1(x)\\t\\n\\r\\nz1\\t\\n\\r\\n. . .\\t\\n\\r\\nx\\t\\n\\r\\nf1\\n1(x)\\t\\n\\r\\nf2\\n1(x)\\t\\n\\r\\nfN\\n1(x)\\t\\n\\r\\ng1(x)\\t\\n\\r\\nz1\\t\\n\\r\\n. . .\\t\\n\\r\\nf1\\n2(x)\\t\\n\\r\\nf2\\n2(x)\\t\\n\\r\\nfM\\n2(x)\\t\\n\\r\\ng2(x)\\t\\n\\r\\nz2\\t\\n\\r\\n. . .\\t\\n\\r\\nz3\\t\\n\\r\\n(a)\\n(b)\\nFigure 1: (a) Mixture of Experts; (b) Deep Mixture of Experts with two layers.\\ndistribution gl(xt) to sum to 1 over experts i. This prevents experts from being overused initially,\\nresulting in balanced assignments. After training with the constraint in place, we lift it and further\\ntrain in a second ﬁne-tuning phase.\\n4\\nExperiments\\n4.1\\nJittered MNIST\\nWe trained and tested our model on MNIST with random uniform translations of ±4 pixels, resulting\\nin grayscale images of size 36 × 36. As explained above, the model was trained to classify digits\\ninto ten classes.\\nFor this task, we set all f 1\\ni and f 2\\nj to one-layer linear models with rectiﬁcation, f 1\\ni (x) =\\nmax(0, W 1\\ni x + b1\\ni ), and similarly for f 2\\nj . We set f 3 to a linear layer, f 3(z2) = W 3z2 + b3.\\nWe varied the number of output hidden units of f 1\\ni and f 2\\nj between 20 and 100. The ﬁnal output\\nfrom f 3 has 10 units (one for each class).\\nThe gating networks g1 and g2 are each composed of two linear+rectiﬁcation layers with either 50\\nor 20 hidden units, and 4 output units (one for each expert), i.e. g1(x) = softmax(B · max(0, Ax +\\na) + b), and similarly for g2.\\nWe evaluate the effect of using a mixture at the second layer by comparing against using only a single\\nﬁxed expert at the second layer, or concatenating the output of all experts. Note that for a mixture\\nwith h hidden units, the corresponding concatenated model has N · h hidden units. Thus we expect\\nthe concatenated model to perform better than the mixture, and the mixture to perform better than\\nthe single network. It is best for the mixture to be as close as possible to the concatenated-experts\\nbound. In each case, we keep the ﬁrst layer architecture the same (a mixture).\\nWe also compare the two-layer model against a one-layer model in which the hidden layer z1 is\\nmapped to the ﬁnal output through linear layer and softmax. Finally, we compare against a fully-\\nconnected deep network with the same total number of parameters. This was constructed using the\\nsame number of second-layer units z2, but expanding the number ﬁrst layer units z1 such that the\\ntotal number of parameters is the same as the DMoE (including its gating network parameters).\\n3\\n4.2\\nMonophone Speech\\nIn addition, we ran our model on a dataset of monophone speech samples. This dataset is a random\\nsubset of approximately one million samples from a larger proprietary database of several hundred\\nhours of US English data collected using Voice Search, Voice Typing and read data [8]. For our\\nexperiments, each sample was limited to 11 frames spaced 10ms apart, and had 40 frequency bins.\\nEach input was fed to the network as a 440-dimensional vector. There were 40 possible output\\nphoneme classes.\\nWe trained a model with 4 experts at the ﬁrst layer and 16 at the second layer. Both layers had\\n128 hidden units. The gating networks were each two layers, with 64 units in the hidden layer. As\\nbefore, we evaluate the effect of using a mixture at the second layer by comparing against using only\\na single expert at the second layer, or concatenating the output of all experts.\\n5\\nResults\\n5.1\\nJittered MNIST\\nTable 1 shows the error on the training and test sets for each model size (the test set is the MNIST test\\nset with a single random translation per image). In most cases, the deeply stacked experts performs\\nbetween the single and concatenated experts baselines on the training set, as expected. However, the\\ndeep models often suffer from overﬁtting: the mixture’s error on the test set is worse than that of the\\nsingle expert for two of the four model sizes. Encouragingly, the DMoE performs almost as well as\\na fully-connected network (DNN) with the same number of parameters, even though this network\\nimposes fewer constraints on its structure.\\nIn Fig. 2, we show the mean assignment to each expert (i.e. the mean gating output), both by input\\ntranslation and by class. The ﬁrst layer assigns experts according to translation, while assignment is\\nuniform by class. Conversely, the second layer assigns experts by class, but is uniform according to\\ntranslation. This shows that the two layers of experts are indeed being used in complementary ways,\\nso that all combinations of experts are effective. The ﬁrst layer experts become selective to where\\nthe digit appears, regardless of its membership class, while the second layer experts are selective to\\nwhat the digit class is, irrespective of the digit’s location.\\nFinally, Fig. 3 shows the nine test examples with highest gating value for each expert combination.\\nFirst-layer assignments run over the rows, while the second-layer runs over columns. Note the\\ntranslation of each digit varies by rows but is constant over columns, while the opposite is true for\\nthe class of the digit. Furthermore, easily confused classes tend to be grouped together, e.g. 3 and 5.\\nTest Set Error: Jittered MNIST\\nModel\\nGate Hids\\nSingle Expert\\nDMoE\\nConcat Layer2\\nDNN\\n4 × 100 −4 × 100\\n50 −50\\n1.33\\n1.42\\n1.30\\n1.30\\n4 × 100 −4 × 20\\n50 −50\\n1.58\\n1.50\\n1.30\\n1.41\\n4 × 100 −4 × 20\\n50 −20\\n1.41\\n1.39\\n1.30\\n1.40\\n4 × 50 −4 × 20\\n20 −20\\n1.63\\n1.77\\n1.50\\n1.67\\n4 × 100 (one layer)\\n50\\n2.86\\n1.72\\n1.69\\n–\\nTraining Set Error: Jittered MNIST\\nModel\\nGate Hids\\nSingle Expert\\nDMoE\\nConcat Layer2\\nDNN\\n4 × 100 −4 × 100\\n50 −50\\n0.85\\n0.91\\n0.77\\n0.60\\n4 × 100 −4 × 20\\n50 −50\\n1.05\\n0.96\\n0.85\\n0.90\\n4 × 100 −4 × 20\\n50 −20\\n1.04\\n0.98\\n0.87\\n0.87\\n4 × 50 −4 × 20\\n20 −20\\n1.60\\n1.41\\n1.33\\n1.32\\n4 × 100 (one layer)\\n50\\n2.99\\n1.78\\n1.59\\n–\\nTable 1:\\nComparison of DMoE for MNIST with random translations, against baselines (i) using\\nonly one second layer expert, (ii) concatenating all second layer experts, and (iii) a DNN with same\\ntotal number of parameters. For both (i) and (ii), experts in the ﬁrst layer are mixed to form z1.\\nModels are annotated with “# experts × # hidden units” for each layer.\\n4\\nJittered MNIST: Two-Layer Deep Model\\nby Translation\\nby Class\\nLayer 1\\nOD\\\\HU\\x14\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x15\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x14\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x15\\x03DVVLJQPHQWV\\x1d\\nLayer 2\\nMRLQW\\x03LQLW\\x03UDQGRP\\nJDWH\\x03WUDLQHG\\nEDVHOLQH\\x03\\x14EORFN\\x03OD\\\\HU\\x15 EDVHOLQH\\x03WDUJHW\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x14\\x13\\x13\\x03EDODQFHG\\nU\\x1c\\x1c\\x19\\nU\\x1c\\x1c\\x1a\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x14\\x13\\x13\\x0c\\nU\\x1c\\x1c\\x1b\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x17\\x13\\x13\\x0c\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x16\\x1a\\nU\\x14\\x13\\x16\\x1c\\nU\\x14\\x13\\x17\\x13\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x19\\x17\\nU\\x14\\x13\\x19\\x18\\nU\\x14\\x13\\x19\\x19\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1a\\x1b\\nU\\x14\\x13\\x1a\\x1c\\nU\\x14\\x13\\x1b\\x13\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x19\\x1a\\nU\\x14\\x13\\x19\\x1b\\nU\\x14\\x13\\x19\\x1c\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x14\\nU\\x14\\x13\\x1b\\x15\\nU\\x14\\x13\\x1b\\x16\\n\\x17[\\x18\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x1a\\x13\\nU\\x14\\x13\\x1a\\x14\\nU\\x14\\x13\\x1a\\x15\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x17\\nU\\x14\\x13\\x1b\\x18\\nU\\x14\\x13\\x1b\\x19\\n\\x17[\\x15\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDO\\nU\\x14\\x13\\x1a\\x16\\nU\\x14\\x13\\x1a\\x17\\nU\\x14\\x13\\x1a\\x18\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x1a\\nU\\x14\\x13\\x1b\\x1b\\nU\\x14\\x13\\x1b\\x1c\\n\\x17[\\x15\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDO\\nU\\x14\\x13\\x1a\\x19\\nU\\x14\\x13\\x1a\\x17\\nU\\x14\\x13\\x1a\\x1a\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1c\\x13\\nU\\x14\\x13\\x1b\\x1b\\nU\\x14\\x13\\x1c\\x14\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDO U\\x14\\x14\\x17\\x14\\nU\\x14\\x14\\x17\\x15\\nU\\x14\\x14\\x17\\x16\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x14\\x19[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDO U\\x14\\x14\\x17\\x17\\nU\\x14\\x14\\x17\\x15\\nU\\x14\\x14\\x17\\x18\\nILUVW\\x03WUDLQ\\x03SDVV\\x0f\\x03ERWK\\x03OD\\\\HUV\\x03EDODQFHG\\x03\\x0bEHIRUH\\x03ILQHWXQLQJ\\x0c\\x1d\\nMRLQW\\x03LQLW\\x03UDQGRP\\nJDWH\\x03WUDLQHG\\nEDVHOLQH\\x03\\x14EORFN\\x03OD\\\\HU\\x15 EDVHOLQH\\x03WDUJHW\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x14\\x13\\x13\\x03EDODQFHG\\nU\\x1c\\x1c\\x19\\nU\\x1c\\x1c\\x1a\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x14\\x13\\x13\\x0c\\nU\\x1c\\x1c\\x1b\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x17\\x13\\x13\\x0c\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x16\\x1a\\nU\\x14\\x13\\x16\\x1c\\nU\\x14\\x13\\x17\\x13\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x19\\x17\\nU\\x14\\x13\\x19\\x18\\nU\\x14\\x13\\x19\\x19\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1a\\x1b\\nU\\x14\\x13\\x1a\\x1c\\nU\\x14\\x13\\x1b\\x13\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x19\\x1a\\nU\\x14\\x13\\x19\\x1b\\nU\\x14\\x13\\x19\\x1c\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x14\\nU\\x14\\x13\\x1b\\x15\\nU\\x14\\x13\\x1b\\x16\\n\\x17[\\x18\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHG\\nU\\x14\\x13\\x1a\\x13\\nU\\x14\\x13\\x1a\\x14\\nU\\x14\\x13\\x1a\\x15\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x17\\nU\\x14\\x13\\x1b\\x18\\nU\\x14\\x13\\x1b\\x19\\n\\x17[\\x15\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDO\\nU\\x14\\x13\\x1a\\x16\\nU\\x14\\x13\\x1a\\x17\\nU\\x14\\x13\\x1a\\x18\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1b\\x1a\\nU\\x14\\x13\\x1b\\x1b\\nU\\x14\\x13\\x1b\\x1c\\n\\x17[\\x15\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDO\\nU\\x14\\x13\\x1a\\x19\\nU\\x14\\x13\\x1a\\x17\\nU\\x14\\x13\\x1a\\x1a\\n³\\x03ILQHWXQH\\nU\\x14\\x13\\x1c\\x13\\nU\\x14\\x13\\x1b\\x1b\\nU\\x14\\x13\\x1c\\x14\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDO U\\x14\\x14\\x17\\x14\\nU\\x14\\x14\\x17\\x15\\nU\\x14\\x14\\x17\\x16\\n\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x14\\x19[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDO U\\x14\\x14\\x17\\x17\\nU\\x14\\x14\\x17\\x15\\nU\\x14\\x14\\x17\\x18\\nILUVW\\x03WUDLQ\\x03SDVV\\x0f\\x03ERWK\\x03OD\\\\HUV\\x03EDODQFHG\\x03\\x0bEHIRUH\\x03ILQHWXQLQJ\\x0c\\x1d\\n1-Layer\\nMoE\\nwithout\\njitters\\n—\\nUDQGRP\\x03DVVLJQPHQWV\\x03HVVHQWLDOO\\\\\\x03GRLQJ\\x03PRGHO\\x03DYHUDJLQJ\\x03EHWZHHQ\\x03\\x18\\x03\\x14\\x10OD\\\\HU\\x03UHOX\\x03QHWZRUNV\\nVHDUFKBORVV\\x03DVVLJQPHQWV\\x03ORRN\\x03QRW\\x03VR\\x03EDG\\x03\\x10\\x10\\x03EXW\\x03WKH\\\\\\x03DSSHDU\\x03WR\\x03ILW\\x03WKH\\x03WUDLQLQJ\\x03VHW\\x03WRR\\x03PXF\\nVHDUFKBORVV\\x03DVVLJQPHQWV\\x11\\x03\\x03WUDLQ\\x1e\\x03HYDOBWHVW\\x0f\\x03HYDOBWUDLQ\\nFigure 2: Mean gating output for the ﬁrst and second layers, both by translation and by class. Color\\nindicates gating weight. The distributions by translation show the mean gating assignment to each of\\nthe four experts for each of the 9 × 9 possible translations. The distributions by class show the mean\\ngating assignment to each of the four experts (rows) for each of the ten classes (columns). Note\\nthe ﬁrst layer produces assignments exclusively by translation, while the second assigns experts by\\nclass. For comparison, we show assignments by class of a standard MoE trained on MNIST without\\njitters, using 5 experts × 20 hidden units.\\n5.2\\nMonophone Speech\\nTable 2 shows the error on the training and test sets. As was the case for MNIST, the mixture’s error\\non the training set falls between the two baselines. In this case, however, test set performance is\\nabout the same for both baselines as well as the mixture.\\nFig. 4 shows the 16 test examples with highest gating value for each expert combination (we show\\nonly 4 experts at the second layer due to space considerations). As before, ﬁrst-layer assignments\\nrun over the rows, while the second-layer runs over columns. While not as interpretable as for\\nMNIST, each expert combination appears to handle a distinct portion of the input. This is further\\nbolstered by Fig. 5, where we plot the average number of assignments to each expert combination.\\nHere, the choice of second-layer expert depends little on the choice of ﬁrst-layer expert.\\nTest Set Phone Error: Monophone Speech\\nModel\\nGate Hids\\nSingle Expert\\nMixed Experts\\nConcat Layer2\\n4 × 128 −16 × 128\\n64 −64\\n0.55\\n0.55\\n0.56\\n4 × 128 (one layer)\\n64\\n0.58\\n0.55\\n0.55\\nTraining Set Phone Error: Monophone Speech\\nModel\\nGate Hids\\nSingle Expert\\nMixed Experts\\nConcat Layer2\\n4 × 128 −16 × 128\\n64 −64\\n0.47\\n0.42\\n0.40\\n4 × 128 (one layer)\\n64\\n0.56\\n0.50\\n0.50\\nTable 2:\\nComparison of DMoE for monophone speech data. Here as well, we compare against\\nbaselines using only one second layer expert, or concatenating all second layer experts.\\n5\\nII\\x03QHWV\\x0f\\x03VLQJOH\\x03EORFN\\x1d\\nVLQJOH\\x03OD\\\\HU\\x1d\\nFigure 3: The nine test examples with highest gating value for each combination of experts, for the jittered\\nmnist dataset. First-layer experts are in rows, while second-layer are in columns.\\n6\\nConclusion\\nThe Deep Mixture of Experts model we examine is a promising step towards developing large,\\nsparse models that compute only a subset of themselves for any given input. We see precisely the\\ngating assignments required to make effective use of all expert combinations: for jittered MNIST,\\na factorization into translation and class, and distinctive use of each combination for monophone\\nspeech data. However, we still use a continuous mixture of the experts’ outputs rather than restricting\\nto the top few — such an extension is necessary to fulﬁll our goal of using only a small part of the\\nmodel for each input. A method that accomplishes this for a single layer has been described by\\nCollobert et al. [4], which could possibly be adapted to our multilayer case; we hope to address this\\nin future work.\\nAcknowledgements\\nThe authors would like to thank Matthiew Zeiler for his contributions on enforcing balancing con-\\nstraints during training.\\n6\\n-RLQW\\x03$VVLJQPHQW\\x03([DPSOHV\\n/D\\\\HU\\x03\\x14\\n/D\\\\HU\\x03\\x15\\n\\x17\\x03H[SHUWV\\nERWK\\x03OD\\\\HUV\\nFigure 4: The 16 test examples with highest gating value for each combination of experts for the monophone\\nspeech data. First-layer experts are in rows, while second-layer are in columns. Each sample is represented by\\nits 40 frequency values (vertical axis) and 11 consecutive frames (horizontal axis). For this ﬁgure, we use four\\nexperts in each layer.\\nMonophone Speech: Conditional Assignments\\n([SHUWV\\x03$VVLJQPHQWV\\n/D\\\\HU\\x03\\x14\\n3HU\\x03'DWDSRLQW\\n/D\\\\HU\\x03\\x15\\n%\\\\\\x03/DEHO\\n-RLQW\\x1d\\x03ZHOO\\x03PL[HG\\nFRORU\\x03VFDOH\\n>\\x03\\x13\\x11\\x13\\x03\\x11\\x11\\x03\\x13\\x11\\x14\\x15\\x18\\x03@\\nFRORU\\x03VFDOH\\n>\\x03\\x13\\x11\\x13\\x03\\x11\\x11\\x03\\x14\\x11\\x13\\x03@\\nFigure 5:\\nJoint assignment counts for the monophone speech dataset. Here we plot the average\\nproduct of ﬁrst and second layer gating weights for each expert combination. We normalize each\\nrow, to produce a conditional distribution: This shows the average gating assignments in the second\\nlayer given a ﬁrst layer assignment. Note the joint assignments are well mixed: Choice of second\\nlayer expert is not very dependent on the choice of ﬁrst layer expert. Colors range from dark blue\\n(0) to dark red (0.125).\\n7\\nReferences\\n[1] Y. Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013.\\n2\\n[2] Y. Bengio, N. L´\\neonard, and A. C. Courville. Estimating or propagating gradients through\\nstochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 2\\n[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high\\nperformance convolutional neural networks for image classiﬁcation. In IJCAI, 2011. 1\\n[4] R. Collobert, Y. Bengio, and S. Bengio. Scaling large learning problems with hard parallel\\nmixtures. International Journal on Pattern Recognition and Artiﬁcial Intelligence (IJPRAI),\\n17(3):349–365, 2003. 6\\n[5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net-\\nworks. In ICASSP, 2013. 1\\n[6] G. E. Hinton. Products of experts. ICANN, 1:1–6, 1999. 2\\n[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\\nNeural Computation, 3:1–12, 1991. 1, 2\\n[8] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke. Application of pretrained deep neural\\nnetworks to large vocabulary speech recognition. Interspeech, 2012. 4\\n[9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural\\nComputation, 6:181–214, 1994. 2\\n[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation with deep convolutional\\nneural networks. In NIPS, 2012. 1\\n8\\n\", 'source_name': 'Learning Factorized Representations in a Deep Mixture-of-Experts', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf'}\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the contents of the files\n",
    "for i, (file, content) in enumerate(file_contents.items()):\n",
    "    if file[-4:] == \".pdf\":\n",
    "        print(f\"{file} #{i+1}\")\n",
    "        print(content)  # Print the first 500 characters for brevity\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text into chunks of specified size\n",
    "def chunk_text(text, chunk_size=1000, overlap=250):\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk file contents\n",
    "chunked_contents = {\n",
    "    key: {\n",
    "        'chunks': [],\n",
    "        'source_name': '',\n",
    "        'source_url': ''\n",
    "    }\n",
    "    for key in file_contents.keys()\n",
    "}\n",
    "for file, content in file_contents.items():\n",
    "    chunks = chunk_text(content['content'])\n",
    "    chunked_contents[file]['chunks'] = chunks\n",
    "    chunked_contents[file]['source_name'] = file_contents[file]['source_name']\n",
    "    chunked_contents[file]['source_url'] = file_contents[file]['source_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Scaling_Laws_NOTES.pdf\n"
     ]
    }
   ],
   "source": [
    "for i, (file, content) in enumerate(chunked_contents.items()):\n",
    "    print(file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_contents[\"Unified_Scaling_Laws_NOTES.pdf\"]['chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': ['Unified Scaling Laws For Routed Language Models \\nMain idea: this paper investigates the scaling behaviors of routing networks, more \\nspecifically in the axis of parameter count (in terms of total number of parameters) and \\ncomputational requirements (total number of active parameters).  \\nRouting: \\nIt experiments with 3 different routing techniques: \\n- \\nAn approach based on BASE (linear programming). \\no This represents a more traditional learned algorithm for routing. BASE in \\nspecific approaches routing as a linear programming problem, which \\nnaturally distributes tokens evenly through experts (no load balancing \\nissues). The algorithm experimented with has slight modifications to BASE \\nto be more efficient in accelerated hardware (they call it Sinkhorn-BASE). \\n- \\nA non-parametric approach (hash layer). \\no HASH layers approaches routing as a fixed function of the input, meaning \\nit does not have learnable parameters. \\n- \\nA Reinforcement Learning approach. \\nResults: \\n- \\nAlthough routing',\n",
       "  'l it Sinkhorn-BASE). \\n- \\nA non-parametric approach (hash layer). \\no HASH layers approaches routing as a fixed function of the input, meaning \\nit does not have learnable parameters. \\n- \\nA Reinforcement Learning approach. \\nResults: \\n- \\nAlthough routing (sparse) performs better than no routing (dense) on all sizes \\nexperimented with (up to 1.3B active parameters, up to 512 experts – biggest \\nmodel has around 200B parameters), the sparse gains over dense are diminishing \\nwith scale (BASE is more robust than other routing techniques). \\n- \\nScaling the number of experts when the number of active? parameters is fixed \\nimproves the validation loss during pre-training. \\n- \\nEffective Parameter Count (EPC) is created to compare the performance of dense \\nagainst sparse models based on an equation that considers the total number of \\nparameters and the active parameters of a model. \\nMain takeaways (as listed in the paper): \\n- \\nRouting improves performance across all model sizes and routing strategies',\n",
       "  ' \\nagainst sparse models based on an equation that considers the total number of \\nparameters and the active parameters of a model. \\nMain takeaways (as listed in the paper): \\n- \\nRouting improves performance across all model sizes and routing strategies \\n(compared to dense aka no routing). \\n- \\nRL routing is more effective than expected, although BASE is the best performer. \\n- \\nPerformance can be described by scaling the number of experts and dense model \\nsize. \\n- \\nDevelopment of an effective parameter count mapping for performance vs scaling. \\nRecommendations: \\n- \\nUse routing when training any model with N (parameter count of base model) <= \\n1.3B. \\n- \\nSinkhorn-BASE is a good default routing algorithm. \\n- \\nAlthough more experts lead to improved performance, it is recommended to use \\nbetween 64 and 128 experts due to diminishing returns above that range. \\n- \\nIt is recommended to use k=1 experts. \\n \\n \\nMy takeaways: \\n- \\nShows that learned routing (represented through BASE) is the best routing',\n",
       "  ' performance, it is recommended to use \\nbetween 64 and 128 experts due to diminishing returns above that range. \\n- \\nIt is recommended to use k=1 experts. \\n \\n \\nMy takeaways: \\n- \\nShows that learned routing (represented through BASE) is the best routing \\nstrategy.  \\no Non-parametric routing can be used in cases where there might not be \\nenough data to train specific experts (for example, on task/domain-level \\nMoE like DEMix where we are not certain if the training load for each expert \\nwill be similar, which will lead to load balancing issues that cannot be solved \\nthrough traditional auxiliary loss or adding noise – this might not happen at \\ntoken-level routing) \\no RL routing performs worse than BASE but looks to not be too far off \\n- \\nTo describe performance, the number of experts and dense model size (number \\nof active parameters for each forward pass) are the most relevant features. This is \\nlogic as the number of experts represents the horizontal scale of the model while \\nthe dense m',\n",
       "  'cribe performance, the number of experts and dense model size (number \\nof active parameters for each forward pass) are the most relevant features. This is \\nlogic as the number of experts represents the horizontal scale of the model while \\nthe dense model size represents the vertical scale of the model. (dense model size \\ncorresponds to vertical scaling, does number of experts as mentioned here \\ncorrespond to an increase in the number of experts with the same total parameter \\ncount or is this accounting for an increase in the total parameter count coming from \\nthe added experts). \\n- \\nSparse models seem to be the most useful at small scales, with diminishing returns \\nover dense with an increase in the scale of active parameters, but this can be \\nprevented to a certain extent by choosing a robust routing strategy. \\n- \\nThe result arrived at that scaling the number of experts when the number of active \\nparameters is fixed is logical as this scales the model horizontally. However, my \\nintuit',\n",
       "  'be \\nprevented to a certain extent by choosing a robust routing strategy. \\n- \\nThe result arrived at that scaling the number of experts when the number of active \\nparameters is fixed is logical as this scales the model horizontally. However, my \\nintuition in this is that scaling the number of experts might make things difficult for \\nfine-tuning (more data will be needed to update all experts while not overfitting on \\nothers). Therefore, a balance is needed. (authors recommend between 64 and 128 \\nexperts due to diminishing returns in increasing the number of experts). -> how \\ndoes fine-tuning performance change with differing number of experts and in \\nrespect to more training data to use for fine-tuning (explore how scaling the number \\nof experts while keeping the active parameter count constant impacts fine-tuning \\nperformance)? \\n- \\nThe EPC equation seems to be useful for practitioners looking to train a MoE model \\nfrom scratch. This would help with design choices in number of active par',\n",
       "  'rts while keeping the active parameter count constant impacts fine-tuning \\nperformance)? \\n- \\nThe EPC equation seems to be useful for practitioners looking to train a MoE model \\nfrom scratch. This would help with design choices in number of active parameters \\nand number of total parameters. \\n- \\nInteresting how the authors recommend MoE in scenarios of training smallish \\nmodels (up to 1.3B). I believe that this is because that was the bigger dense model \\nstudied, so it is not saying that dense models perform better when scaled above \\n1.3B, but just that a bigger dense model was not used in the experiments. It is \\nimportant to note that the experiments showed diminishing returns for routing \\nmodels -> did any other papers dive into this question? \\n- \\nIt is also interesting to note that the authors concluded that k=1 experts is the ideal \\nnumber for k. \\n \\n',\n",
       "  'on? \\n- \\nIt is also interesting to note that the authors concluded that k=1 experts is the ideal \\nnumber for k. \\n \\n'],\n",
       " 'source_name': 'Unified Scaling Laws for Routes Language Models NOTES',\n",
       " 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Unified_Scaling_Laws_NOTES.pdf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_contents[\"Unified_Scaling_Laws_NOTES.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Scaling Laws For Routed Language Models \n",
      "Main idea: this paper investigates the scaling behaviors of routing networks, more \n",
      "specifically in the axis of parameter count (in terms of total number of parameters) and \n",
      "computational requirements (total number of active parameters).  \n",
      "Routing: \n",
      "It experiments with 3 different routing techniques: \n",
      "- \n",
      "An approach based on BASE (linear programming). \n",
      "o This represents a more traditional learned algorithm for routing. BASE in \n",
      "specific approaches routing as a linear programming problem, which \n",
      "naturally distributes tokens evenly through experts (no load balancing \n",
      "issues). The algorithm experimented with has slight modifications to BASE \n",
      "to be more efficient in accelerated hardware (they call it Sinkhorn-BASE). \n",
      "- \n",
      "A non-parametric approach (hash layer). \n",
      "o HASH layers approaches routing as a fixed function of the input, meaning \n",
      "it does not have learnable parameters. \n",
      "- \n",
      "A Reinforcement Learning approach. \n",
      "Results: \n",
      "- \n",
      "Although routing\n"
     ]
    }
   ],
   "source": [
    "print(chunked_contents[\"Unified_Scaling_Laws_NOTES.pdf\"]['chunks'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone + SQLite Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermeosorio/Library/Caches/pypoetry/virtualenvs/moe-rag-chatbot-uQH9tqUR-py3.12/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_INDEX_HOST = os.getenv('PINECONE_INDEX_HOST')\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(host=PINECONE_INDEX_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n"
     ]
    }
   ],
   "source": [
    "total_chunks = 0\n",
    "for file in chunked_contents:\n",
    "    for i, chunk in enumerate(chunked_contents[file]['chunks']):\n",
    "        continue\n",
    "    total_chunks += i\n",
    "print(total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Scaling_Laws_NOTES.pdf\n",
      "Switch_Transformers.pdf\n",
      "GLaM.pdf\n",
      "GShard.pdf\n",
      "Unified_Scaling_Laws.pdf\n",
      "ST_MoE.pdf\n",
      "GLaM_NOTES.pdf\n",
      "ST_MoE_NOTES.pdf\n",
      "Switch_Transformers_NOTES.pdf\n",
      "GShard_NOTES.pdf\n",
      "Efficient_Large_Scale_LM.pdf\n",
      "Efficient_Large_Scale_LM_NOTES.pdf\n",
      "BTM.pdf\n",
      "Benefits_of_ELMs.pdf\n",
      "Expert_Gate_NOTES.pdf\n",
      "BeyondDistillation_Task_Level_MoE.pdf\n",
      "DEMix_NOTES.pdf\n",
      "BTM_NOTES.pdf\n",
      "DEMix.pdf\n",
      "cBTM_NOTES.pdf\n",
      "BeyondDistillation_Task_Level_MoE_NOTES.pdf\n",
      "Expert_Gate.pdf\n",
      "cBTM.pdf\n",
      "Benefits_of_ELMs_NOTES.pdf\n",
      "MoE_Mamba.pdf\n",
      "BlackMamba.pdf\n",
      "MoE_meets_instruction_tuning.pdf\n",
      "MoE_Mamba_NOTES.pdf\n",
      "Sparse_Upcycling.pdf\n",
      "Soft_Merging_of_Experts.pdf\n",
      "Sparse_Upcycling_NOTES.pdf\n",
      "MoE_meets_instruction_tuning_NOTES.pdf\n",
      "Soft_Merging_of_Experts_NOTES.pdf\n",
      "EvoMoE.pdf\n",
      "BlackMamba_NOTES.pdf\n",
      "EvoMoE_NOTES.pdf\n",
      "PE_SparsityCrafting_NOTES.pdf\n",
      "MegaBlocks.pdf\n",
      "QMoE.pdf\n",
      "QMoE_NOTES.pdf\n",
      "FastInferenceMoE.pdf\n",
      "ExtremelyPE_MoE_for_InstructionTuning.pdf\n",
      "MegaBlocks_NOTES.pdf\n",
      "PE_SparsityCrafting.pdf\n",
      "FastInferenceMoE_NOTES.pdf\n",
      "PE_MoE_for_LMs.pdf\n",
      "PE_MoE_for_LMs_NOTES.pdf\n",
      "FFFs_NOTES.pdf\n",
      "FFF.pdf\n",
      "FFF_to_language.pdf\n",
      "DSelect_k_NOTES.pdf\n",
      "Hash_Layers_NOTES.pdf\n",
      "Soft_MoE_NOTES.pdf\n",
      "Expert_Choice_NOTES.pdf\n",
      "Expert_Choice.pdf\n",
      "Soft_MoE.pdf\n",
      "BASE_layers.pdf\n",
      "MixtureOfTokens.pdf\n",
      "DeepSeekMoE.pdf\n",
      "StableMoE.pdf\n",
      "DeepSeekMoE_NOTES.pdf\n",
      "MixtureOfTokens_NOTES.pdf\n",
      "StableMoE_NOTES.pdf\n",
      "BASE_layers_NOTES.pdf\n",
      "DSelect_k.pdf\n",
      "Hash_Layers.pdf\n",
      "Mistral.pdf\n",
      "Mixtral.pdf\n",
      "Mixtral_NOTES.pdf\n",
      "Towards_Understanding_MoE_NOTES.pdf\n",
      "Towards_Understanding_MoE.pdf\n",
      "Mixture of Experts Explained_HF_NOTES.pdf\n",
      "Sparsely_Gated_MoE_NOTES.pdf\n",
      "Sparsely_Gated_MoE.pdf\n",
      "Original_MoE.pdf\n",
      "Learning_Factored_Representations_NOTES.pdf\n",
      "Mixture of Experts Explained_HF.pdf\n",
      "OpenMoE.pdf\n",
      "MoESurvey.pdf\n",
      "OpenMoE_NOTES.pdf\n",
      "Learning_Factored_Representations.pdf\n"
     ]
    }
   ],
   "source": [
    "for k in chunked_contents.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database (it will create the database file if it doesn't exist)\n",
    "conn = sqlite3.connect('../chunks.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table to store chunks\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS chunks (\n",
    "    chunk_id TEXT PRIMARY KEY,\n",
    "    content TEXT,\n",
    "    source_name TEXT,\n",
    "    source_url TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert chunk data into the database\n",
    "def insert_chunk(chunk_id, content, source_name, source_url):\n",
    "    conn = sqlite3.connect('chunks.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "    INSERT INTO chunks (chunk_id, content, source_name, source_url) VALUES (?, ?, ?, ?)\n",
    "    ''', (chunk_id, content, source_name, source_url))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, contents in chunked_contents.items():\n",
    "    source_name = contents['source_name']\n",
    "    source_url = contents['source_url']\n",
    "    for i, chunk in enumerate(contents['chunks']):\n",
    "        chunk_id = f\"{file}_chunk_{i}\"\n",
    "        # SQLite3 insert\n",
    "        insert_chunk(chunk_id, chunk, source_name, source_url)\n",
    "        # Pinecone insert\n",
    "        metadata = {\"file_name\": file, \"source_name\": source_name, \"source_url\": source_url}\n",
    "        embed = get_embedding(chunk)\n",
    "        upsert_response = index.upsert(\n",
    "            vectors=[\n",
    "                (chunk_id, embed, metadata),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunks': ['MOE PAPER REVIEWS\\nEarly Days of MoE\\n\\nLearning Factored Representations in a Deep Mixture-of-Experts\\n\\nMain Idea:\\nTo apply stacked layers of mixture-of-experts, so to have multiple sets of (gating, experts). This allows multiple combinations of experts to be called while keeping a modest model size.\\nThe problem they are trying to solve for is that deep neural networks are expensive to compute at inference time since all the neurons are used.\\nThe solution proposed is to implement stacked MoE layers, where multiple expert combinations are possible, and the gating mechanism ensures only useful neurons for that input are used (experts on the specific input space). This gives better computational efficiency at inference, allowing for a model that is both large and efficient.\\n\\nApproach:\\nThe input is first passed through the first MoE layer (represented by z1):\\nwhere  and represent the gating probability and expert output for expert i at layer 1, respectively.\\nboth the gating mechanism and the ', 'is both large and efficient.\\n\\nApproach:\\nThe input is first passed through the first MoE layer (represented by z1):\\nwhere  and represent the gating probability and expert output for expert i at layer 1, respectively.\\nboth the gating mechanism and the expert function use a non-linearity (ReLU)\\nThe outputs of the first layer (z1) are then passed as an input to the next MoE layer z2, which replaces x with z1.\\nz2 is then passed through a final layer (f3) and a softmax is applied (in the context of classification)\\n\\nThe network is trained with SGD with a caveat to help balance the training through the experts:\\nThe mean of all experts’ total assignment is compared to each expert’s running total assignment. If an expert is found to have a running total assignment significantly higher than the mean, its training is paused temporarily to allow for the training of other experts.\\nThis strategy is found to mostly be useful in early stages of training, where the experts have not yet specialized signi', 'tal assignment significantly higher than the mean, its training is paused temporarily to allow for the training of other experts.\\nThis strategy is found to mostly be useful in early stages of training, where the experts have not yet specialized significantly on a part of the input space. After some training, the experts are expected to have some specialization, and thus this constraint can be lifted.\\nThis paper makes use of conditional computation, although the details about this are not shown in-depth.\\nResults:\\nThe stacked MoE layer showed promising results, as it came close to fully dense networks in terms of performance while having significant inference pros due to conditional computation.\\nExperiments in specific tasks also showed that different experts indeed did specialize in different clusters of the data.\\nMy takeaways:\\nThis paper is revolutionary in terms of the idea presented in terms of stacking MoE layers in a deep neural network and trying to find a way to balance the load ', 'different experts indeed did specialize in different clusters of the data.\\nMy takeaways:\\nThis paper is revolutionary in terms of the idea presented in terms of stacking MoE layers in a deep neural network and trying to find a way to balance the load between experts.\\nIntroduces the idea that MoE can have improved performance when stacked, paving the way for adding this as a modular component that can be added to other architectures.\\nThis strategy is still not sparse (top-k), but it opens the field to the idea that a top-k strategy is possible as a future line of research.\\n\\nOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\\n\\nMain Idea:\\nTo propose a way to improve model capacity, training time and model quality through a conditional computation approach that alternates between dense LSTM and MoE blocks.\\nApproach:\\nIntroduces a new neural network component (a new block/layer) which consists of:\\nn experts, each a feed-forward neural network\\na trainable gating net', 'ough a conditional computation approach that alternates between dense LSTM and MoE blocks.\\nApproach:\\nIntroduces a new neural network component (a new block/layer) which consists of:\\nn experts, each a feed-forward neural network\\na trainable gating network, which selects a sparse combination of these experts to process each input token given.\\nThe gating network presented is an improvement over the standard approach, which trains a weight matrix to give score to an input x and pass that to a softmax (gating output . The gating mechanism proposed is called noisy top-k routing, which adds noise and sparsity:\\nGaussian noise is added before taking the softmax to help with load balancing between experts during training.\\n\\nSparsity is added by taking only the top k scores given by the gating mechanism.\\n\\nIf not in the top k, H(x) becomes -inf so it is not considered in the final output.\\nTo balance expert utilization, an auxiliary term to the loss is added, which works by being computed at a batch', ' only the top k scores given by the gating mechanism.\\n\\nIf not in the top k, H(x) becomes -inf so it is not considered in the final output.\\nTo balance expert utilization, an auxiliary term to the loss is added, which works by being computed at a batch level.\\nFor each expert and the training batch X, take the expert’s importance in the batch:\\n\\nImportance(X)e = sum of all the expert’s G(x) for the batch\\nThe term Limportance is added to the loss (which will be computed at the batch level) to encourage all experts to have equal importance:\\n\\n is a hand-tuned scaling factor and V is the coefficient of variation squared.\\nThe final network consists of alternating LSTM blocks with these new MoE blocks.\\nMy takeaways:\\nThis approach means that for the first time MoE was used as a network component and not as the network itself, providing a method to integrate it with dense layers.\\nIntroduced top-k routing.\\nExperiments showed that experts tend to become specialized on syntax and semantics, which is ', 'irst time MoE was used as a network component and not as the network itself, providing a method to integrate it with dense layers.\\nIntroduced top-k routing.\\nExperiments showed that experts tend to become specialized on syntax and semantics, which is an important follow-up to the findings of the “Learning Factored Representations…” paper which hinted that different experts specialize in different clusters of the data.\\nThis paper also provides advancements in load balancing, crafting an auxiliary loss term for load balancing that seems much more effective than the previous method of pausing the training of highly utilized experts.\\n\\n\\n\\n\\n\\nUnderstanding MoE\\n\\nMoE articles\\nThe original MoE had 3 components:\\nExperts, specialized models which are either regressors or classifiers.\\nManager (router), gate mechanism (like a softmax, for example) which decides in which area(s) of the input space each expert is trustworthy.\\nProbabilistic model, which combines the expert and the manager. It joins the e', 'her regressors or classifiers.\\nManager (router), gate mechanism (like a softmax, for example) which decides in which area(s) of the input space each expert is trustworthy.\\nProbabilistic model, which combines the expert and the manager. It joins the experts’ Gaussian distributions (outputs) together based on the probability given by the manager. Y = summation of pi (probability given to expert I by the manager) * yi (output of the expert), for all experts.\\nThis forms a fully differentiable dense ensemble of all experts with no inference speedup, as no expert computation is discarded.\\nLarge dense neural networks are not efficient scaling in terms of training costs. Conditional computation models (sparse models) can provide advantages, but have their downfalls, such as the computational limitations of training such models (GPUs and TPUs are optimized for large matrix-matrix multiplication).\\n“Sparsely-Gated MoE Layer” tries to propose a solution to MoE’s computation issues. When training a', 'e their downfalls, such as the computational limitations of training such models (GPUs and TPUs are optimized for large matrix-matrix multiplication).\\n“Sparsely-Gated MoE Layer” tries to propose a solution to MoE’s computation issues. When training an MoE model the deep learning way, the input is passed through the router the same way as the original MoE method, however, the router only sends the input signal through to the top-k selected experts (a discrete choice, not fully differentiable), and uses the scores given by the router as weights of each expert’s output on the final output. The final output is then a combination of the top-k experts’ outputs weighted by their respective router score.\\nThis deep learning approach has numerous potential problems:\\nIf one expert gets ahead and generalizes well fast, the router might send most of the data to this expert, overfitting and undertraining others while not specializing on anything. Therefore, training between experts needs to be somew', 'ential problems:\\nIf one expert gets ahead and generalizes well fast, the router might send most of the data to this expert, overfitting and undertraining others while not specializing on anything. Therefore, training between experts needs to be somewhat uniform.\\nCommon approaches to fix this are adding random noise to the router’s probabilities (scores given to experts) in order to create some randomness in the selection of experts’ process, especially in early stages of training (although we don’t want this to be fully random, since it will prevent specialization) to ensure that worse performing experts are still randomly picked for updates; adding a penalty term for uneven router choice to the loss function so the router has motivation to distribute its picks in a more uniform manner. This means the loss would look like: loss = cross-entropy loss + auxiliary loss, where auxiliary loss represents the penalty term for uneven distribution.\\nThis sparse approach is promising in some ways ', ' distribute its picks in a more uniform manner. This means the loss would look like: loss = cross-entropy loss + auxiliary loss, where auxiliary loss represents the penalty term for uneven distribution.\\nThis sparse approach is promising in some ways as it provides computational efficiency for inference (only the selected expert weights are a part of the computation). So given 8 experts of 100M parameters each and a dense model of 800M parameters, a forward pass on the MoE model using k=2 would only trigger 2*100M=200M parameters, while the dense model would always activate all 800M parameters (in reality, shared parameters should be accounted as well in MoE, but this is not mentioned here for simplicity). In theory, the quality of these 2 models should be roughly the same since they both have the same number of total parameters available (800M).\\nOn another hand, due to the need to balance loads through the router function, MoE can be a bit slower to train. That is, the random noise and', 'odels should be roughly the same since they both have the same number of total parameters available (800M).\\nOn another hand, due to the need to balance loads through the router function, MoE can be a bit slower to train. That is, the random noise and auxiliary loss to help with router uniformity between experts can slow down training due to data being sent and updated on suboptimal places. Due to its parameter efficiency, MoE has the potential to provide significant speed ups on training steps, but due to challenges such as load balancing and communication costs incurred by MoE, the cost of each step tends to be larger, so each training step takes longer. Therefore, when comparing training speed-ups between sparse and dense models, it is important to consider both training steps and training time.\\n\\nTowards Understanding MoE\\n\\nAn MoE layer contains many experts that share the same network architecture and are trained by the same algorithm, with a gating/routing function that routes indiv', 'portant to consider both training steps and training time.\\n\\nTowards Understanding MoE\\n\\nAn MoE layer contains many experts that share the same network architecture and are trained by the same algorithm, with a gating/routing function that routes individual inputs to a few experts among all the candidates.\\nThe number of experts used for an input can be a hyperparameter choice called top-k (usually 1 or 2). The computation (inference) cost will only be the computation cost for the top-k expert(s) used.\\nIn practice, all experts are initialized with the same weight distribution, optimization configuration, and the router is configured to distribute the data evenly between experts (traditionally through random noise and/or an auxiliary load balancing loss). This makes it unclear how this leads to specialization of each expert, instead of collapsing into a single model.\\nKey findings:\\nMoE with linear experts cannot be trained to find a good classifier efficiently. An MoE with non-linear expert', 'cing loss). This makes it unclear how this leads to specialization of each expert, instead of collapsing into a single model.\\nKey findings:\\nMoE with linear experts cannot be trained to find a good classifier efficiently. An MoE with non-linear experts trained with gradient descent from random initialization can accomplish this. The gating mechanism, however, can be linear, since it only needs to differentiate between input clusters.\\nThe study shows that adding random noise to the router’s choice in soft routing (before the discrete choice) helps distribute the data across experts.\\nFor nonlinear MoE with non-linear expert functions, experts will diverge at the end of the exploration stage. At the end of the exploration stage, an expert will achieve low error in a specific cluster, but high error on the other clusters.\\nThere is a potential load unbalancing issue when training MoE, with the probability of each input being routed to the same few experts being high. This is a self-fulfillin', 'achieve low error in a specific cluster, but high error on the other clusters.\\nThere is a potential load unbalancing issue when training MoE, with the probability of each input being routed to the same few experts being high. This is a self-fulfilling prophecy, as it will lead to more training of these few experts, resulting in a bigger imbalance. Normalized gradient descent can help with this issue, as well as adding a penalty term to the loss function (auxiliary load balancing loss) or random noise to the router.\\nThe advantage of MoE over dense models in terms of performance depends on the task and the cluster structure of the data.\\nMy takeaway(s):\\nIn MoE, the router specializes in dividing the input space into n parts/clusters (where n is the number of experts). Each expert then becomes a specialist on a specific cluster of the input space (as divided by the router). \\nThe router’s task can be performed linearly, as it only needs to learn how to divide the input space into clusters, ', 's the number of experts). Each expert then becomes a specialist on a specific cluster of the input space (as divided by the router). \\nThe router’s task can be performed linearly, as it only needs to learn how to divide the input space into clusters, while the expert’s task is more challenging, benefitting from non-linearities.\\nIt is important to employ load balancing strategies to ensure that this clustering is done correctly, especially at early stages of training when the clusters are not yet clear. If this is not done, it can lead to generalization (some experts being assigned to large areas of the input space while others are assigned to too small areas).\\nThe advantages of MoE will, therefore, depend on the input space of the data – if the data can be clustered into “specialization” areas, MoE will perform better, otherwise if the task benefits from a generalized knowledge of the input space, a dense model will outperform MoE.\\n\\n\\n\\n\\n\\nScaling MoE & Other\\nGShard: Scaling Giant Models w', 'the data can be clustered into “specialization” areas, MoE will perform better, otherwise if the task benefits from a generalized knowledge of the input space, a dense model will outperform MoE.\\n\\n\\n\\n\\n\\nScaling MoE & Other\\nGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\\nMain Idea: \\nGShard looks to make improvements on different challenges of training MoE models, particularly related to scaling. A 600B MoE model (2048E, 36L) is successfully trained, while the authors fail to reach a stable 1T parameter model (2048E, 60L) due to issues with training stability caused by reduced precision (bfloat32 to bfloat16). The improvements made were in the following topics:\\nComputation costs when scaling.\\nEase of programming when scaling.\\nEfficient scaling implementation on parallel devices.\\nGShard modifies the traditional Transformer architecture by alternating between a self-attention and a MoE layer with top-2 routing. To scale, the model is stretched vertically (incr', 'when scaling.\\nEfficient scaling implementation on parallel devices.\\nGShard modifies the traditional Transformer architecture by alternating between a self-attention and a MoE layer with top-2 routing. To scale, the model is stretched vertically (increase in number of layers in each expert) and/or horizontally (increase in the number of experts per MoE layer).\\nFor dealing with load balancing:\\nA hyperparameter for a maximum threshold for the number of tokens to be sent to each expert per batch is set (expert capacity, set to N/E – N=# of tokens in the batch; E=# of experts).\\nExtra tokens (that couldn’t ‘t make it due to the expert capacity being reached) are overflown/discarded.\\nTraining tokens are distributed evenly into G groups to take advantage of parallelism. Expert capacity is evaluated in a group basis – local group dispatching.\\nExperts are divided into groups that are optimized for communication (communication between experts in the same group is faster than between experts in ot', 'antage of parallelism. Expert capacity is evaluated in a group basis – local group dispatching.\\nExperts are divided into groups that are optimized for communication (communication between experts in the same group is faster than between experts in other groups). Local communication (which is optimized) are used more between experts instead of global communication.\\nAddition of a load balancing term to the loss function based on the mean number of token assignment to all experts compared to the token assignment for each expert (calculated at the group level).\\nRandom routing is employed to help with the expert capacity constraint. Top-2 routing requires a capacity factor of 2. To help with this, some tokens which have a low gating weight for the 2nd-best expert are not propagated through this expert (becoming top-1 routing). These 2nd-best experts are dropped randomly in proportion to the gating weight they were assigned (if assigned a score of 0.2, it would have a higher chance of being ', 'he 2nd-best expert are not propagated through this expert (becoming top-1 routing). These 2nd-best experts are dropped randomly in proportion to the gating weight they were assigned (if assigned a score of 0.2, it would have a higher chance of being dropped than if it was assigned a score of 0.3).\\nResults:\\nScaling the number of layers (vertical scaling) leads to consistent gains.\\nIncreasing the number of experts used has diminishing returns.\\nIncreasing the number of experts helps with high-resource tasks (which have more data), while dense models adjust better to low-resource tasks (low amount of data).\\nIn terms of training efficiency:\\nScaling with conditional computation is more practical and efficient than with dense models.\\nDeeper models are more sample efficient (converge faster with fewer examples). That is, increasing the number of layers in a model leads to an almost proportional speed up in terms of training steps to reach a certain loss (a 3x increase in number of layers would', ' are more sample efficient (converge faster with fewer examples). That is, increasing the number of layers in a model leads to an almost proportional speed up in terms of training steps to reach a certain loss (a 3x increase in number of layers would lead to ~3x speed up in training steps to reach a certain loss).\\nAs mentioned previously, scaling the number of experts per-layer has diminishing returns.\\nMy takeaways:\\nGShard is the first attempt of massively scaling MoE in a Transformer architecture. It does so by optimizing the technical implementation of MoE for communication costs and parallelism.\\nHighlights:\\nThe techniques used for balancing dropping tokens by adjusting the expert capacity (needs to be higher for a higher k) as well as randomly dropping the 2nd-best expert are interesting.\\nThe local group dispatching technique to minimize communication overhead costs also seems interesting and deserves a deeper look/understanding.\\nBased on my analysis of this paper, I was left with a', 'ndomly dropping the 2nd-best expert are interesting.\\nThe local group dispatching technique to minimize communication overhead costs also seems interesting and deserves a deeper look/understanding.\\nBased on my analysis of this paper, I was left with a few questions/thoughts:\\nAre MoE layers robust to dropped tokens? As each expert is assigned to a specific input space, my first thought is that if the experts are of modest size and enough experts are employed per layer (making the specialized input space for each expert smaller), the MoE architecture should be robust to dropping tokens.\\nAre different experiments employed at future research works regarding MoE performing better at high-resource tasks and dense models performing better at low-resource tasks? This seems to mean that MoE’s gains over dense models come at the expense of a bigger amount of data being needed.\\nThe fact that increasing the number of experts per-layer leads to diminishing returns makes sense –> as each expert speci', 'source tasks? This seems to mean that MoE’s gains over dense models come at the expense of a bigger amount of data being needed.\\nThe fact that increasing the number of experts per-layer leads to diminishing returns makes sense –> as each expert specializes in a certain area of the input space, increasing the number of experts will decrease the size of that area, allowing the experts to specialize further. However, at some point the experts will become redundant (too many experts for a small input space area/cluster), leading to diminishing returns due to these redundant experts having the same specialization.\\nBased on this logic, it should be possible to balance the expert size with the number of experts in each layer. The logic is that decreasing the expert size would lead to each expert only being able to handle smaller input spaces/clusters of the data, so more experts would be needed.\\nThe fact that deeper models are more sample efficient hints that the scaling laws for MoE should b', 'reasing the expert size would lead to each expert only being able to handle smaller input spaces/clusters of the data, so more experts would be needed.\\nThe fact that deeper models are more sample efficient hints that the scaling laws for MoE should be like those of dense models. This makes sense as adding more layers is adding computation to the model.\\n\\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\\nMain Idea: the idea for this paper is based on the “Scaling Laws for Neural Language Models”, which states that larger models are more sample-efficient, and thus advises that the optimal allocation of a fixed compute budget should prioritize increasing the number of model parameters while decreasing the number of training steps. This created the motivation to scale MoE models, which allow for an increase in parameter count while keeping FLOPs constant. The main issues to be addressed related to scaling MoEs are:\\nComplexity\\nCommunication costs\\nTr', 'the number of training steps. This created the motivation to scale MoE models, which allow for an increase in parameter count while keeping FLOPs constant. The main issues to be addressed related to scaling MoEs are:\\nComplexity\\nCommunication costs\\nTraining instability\\n\\nTop-1 Routing\\nSparsely-Gated MoE had claimed that top-k routing had to have k > 1 to have non-trivial gradients to the routing function (the routers were thought to not train properly if they didn’t have at least two experts to compare results with). Switch challenges this idea and successfully uses top-1 routing. This introduces advantages such as reduced computation, reduced batch size (top-2 routing requires an expert capacity factor of 2, which is not needed in top-1 routing) and reduced communication costs.\\n\\nExpert Capacity\\nEach expert has an expert capacity, which is the computation it can perform on each batch. Due to the dynamic nature of routing (load balancing in a batch is not guaranteed), this expert capacity', 'ing) and reduced communication costs.\\n\\nExpert Capacity\\nEach expert has an expert capacity, which is the computation it can perform on each batch. Due to the dynamic nature of routing (load balancing in a batch is not guaranteed), this expert capacity can lead to memory overflow issues (where the overflown tokens in a batch are skipped). This can be managed by setting a capacity factor to the experts (keep some buffer to each expert’s machine).\\nExpert capacity = (tokens per batch/number of experts) * capacity factor\\nAlthough this helps with memory overflow and the issue of skipped tokens, it results in increased computation and memory costs.\\n\\nLoad Balancing Loss\\nFor the auxiliary loss, Switch introduces a differentiable load balancing loss that considers both the fraction of tokens assigned to each expert and the probability given to each expert by the router (sum of the probabilities given to each expert when it was selected).\\nAuxiliary loss = alpha * N * summation over all experts (fi', 'hat considers both the fraction of tokens assigned to each expert and the probability given to each expert by the router (sum of the probabilities given to each expert when it was selected).\\nAuxiliary loss = alpha * N * summation over all experts (fi * pi), where N is the number of experts, fi is the fraction of tokens in the batch dispatched to expert I and pi is the fraction of the router probability allocated to expert i.\\nThis loss ensures load balancing by leveraging the fact that the product fi * pi is minimized under a uniform distribution, where both fi and pi are equal or close to 1/N for each expert, corresponding to a balanced load. The sum of fi and pi is constrained to 1 across all experts, highlighting the zero-sum nature of resource distribution. The non-linear impact of the product fi * pi in the loss function means that the sum of these products across experts is minimized when the load (dispatched tokens) and router probabilities are evenly distributed. This minimizati', 'source distribution. The non-linear impact of the product fi * pi in the loss function means that the sum of these products across experts is minimized when the load (dispatched tokens) and router probabilities are evenly distributed. This minimization drives the model toward a uniform distribution, promoting load balance by ensuring that no single expert is disproportionally favored in terms of load or router’s allocation.\\nThis loss is a complement to the cross-entropy loss -> total loss = cross-entropy + auxiliary loss. \\nT5 (dense) vs MoE (top-2 routing) vs Switch (top-1 routing)\\nModels were trained on a masked language modeling objective with 15% token dropout (for MoE and Switch).\\nThe same computation per token is applied (equal FLOPs) for each model. However, MoE has more active parameters since it uses top-2 routing.\\nSwitch outperforms T5 and MoE in terms of speed-quality (fixed amount of computation and wall-clock time)\\nSwitch has a lower computational footprint -> increasing it', 'for each model. However, MoE has more active parameters since it uses top-2 routing.\\nSwitch outperforms T5 and MoE in terms of speed-quality (fixed amount of computation and wall-clock time)\\nSwitch has a lower computational footprint -> increasing its size to match the speed of MoE leads to outperforming MoE and T5 on a per step basis (since MoE is slower than Switch due to higher number of active parameters)\\nSwitch performs better at lower capacity factors (1, 1.25)\\n\\nTraining and Fine-Tuning Techniques\\nInstability in MoE comes mainly from the hard-switching routing strategy. This makes it challenging to train in lower precision. To combat this, a few tricks are used:\\nSelective precision with large sparse models\\nSelective casting to float32. More specifically, the router input is casted to float32 within the body of the router function (local computations) but back to float16 at the end of the routing function when the results are dispatched for the selection of the router computation ', '. More specifically, the router input is casted to float32 within the body of the router function (local computations) but back to float16 at the end of the routing function when the results are dispatched for the selection of the router computation (between devices). This optimizes the router stability while keeping the communication costs low.\\nSmaller parameter initialization for stability\\nSimple initialization changes (especially reducing the normal initialization scale of a Transformer by 10) drastically helps with stability.\\nA popular initialization strategy is used -> weights randomly initialized from a distribution with mean of 0 and st dev of sq root(s/n), where s is a scale hyper-parameter and n is the number of input units in the weight tensor.\\nRegularizing large sparse models\\nSince MoE models have much more parameters than regular dense Transformers, they can be more prone to overfitting when fine-tuned in small downstream tasks.\\nSwitch proposes increasing dropout in expert ', 'weight tensor.\\nRegularizing large sparse models\\nSince MoE models have much more parameters than regular dense Transformers, they can be more prone to overfitting when fine-tuned in small downstream tasks.\\nSwitch proposes increasing dropout in expert layers while keeping a smaller dropout rate in other layers. This is shown to lead to improvements in fine-tuning.\\n\\nScaling properties\\nWhen keeping the FLOPs per token fixed, having more total parameters (increase in number of experts) speeds up training (although at a cost in memory) in a per-step basis (training is more sample-efficient).\\nMoE models have higher communication costs than dense models. So even though Switch is more efficient in a per-step basis, this can fail to hold in a time basis.\\nWith a fixed training duration and computational budget, Switch achieves a 7x speedup in training compared to T5 (Switch achieves the same loss 7x faster).\\nSwitch shows improvements in both per-step and time basis during pre-training over T5 eve', 'sis.\\nWith a fixed training duration and computational budget, Switch achieves a 7x speedup in training compared to T5 (Switch achieves the same loss 7x faster).\\nSwitch shows improvements in both per-step and time basis during pre-training over T5 even when compared to T5-Large (3.5x increase in FLOPs).\\n\\nFine-tuning\\nWith an increase in dropout rate (0.4 vs 0.1), Switch was shown to have improved fine-tuning results over T5-Base and T5-Large in a FLOP-matched basis in NLP tasks, including reasoning and knowledge-heavy tasks.\\n\\nDistillation\\nWhen distilling a large sparse model into a small dense model, it is found that reducing the model to 1/20th of its original parameter count still retains 30% of the Switch gains over T5. This is a sign that not all gains are due to increased parameter count, indicating that some part of the gains can be due to other reasons related to the MoE capturing parameters more efficiently.\\n\\nParallelism (Data, Model, Experts)\\nData parallelism – data is shared ov', ' not all gains are due to increased parameter count, indicating that some part of the gains can be due to other reasons related to the MoE capturing parameters more efficiently.\\n\\nParallelism (Data, Model, Experts)\\nData parallelism – data is shared over all cores available, while keeping a copy of the model in each core (model is replicated over each core). Each core (model) only needs to communicate at the end of each batch to perform an update on the model’s parameters.\\nModel parallelism – model is distributed over all cores, while passing all tokens through each core. This method leads to high communication costs between cores since each token needs to be passed from core to core to produce a label.\\nModel and data parallelism – model is split through m cores and data is split through n cores (mix of pure model parallelism and pure data parallelism.\\nExpert and data parallelism – the model is distributed by having one expert in each core while sharding the data over all cores. This sha', 'plit through m cores and data is split through n cores (mix of pure model parallelism and pure data parallelism.\\nExpert and data parallelism – the model is distributed by having one expert in each core while sharding the data over all cores. This sharding is done by the routing function, assuming the auxiliary loss will help with load balancing to prevent the token overflow issue.\\nExpert, model and data parallelism – more complex method where each expert is distributed through multiple cores (in case a single expert does not fit in a single core, which can happen if we want to increase the number of FLOPs – this leads to a decreased batch size because more memory is needed for the experts and the communication costs between cores, leading to less memory available for the data). This needs to consider the communication costs between the routing function distributing the data and the model/expert sharding.\\n\\nIncreasing the number of experts does not seem to lead to instability in training', 'o less memory available for the data). This needs to consider the communication costs between the routing function distributing the data and the model/expert sharding.\\n\\nIncreasing the number of experts does not seem to lead to instability in training (as seen in training the 1.6T model). What caused instability is increasing the number of FLOPs (increasing the size of each expert).\\n\\nMy takeaways:\\nThe claim made on Sparsely-Gated MoE that k > 1 is needed for top-k routing initially seems to make sense. This would help the gradient to differentiate between good and bad experts for that input. For example, with k = 2, the router can compare the gradients that come from each expert, and therefore learn which expert was more useful to the final output. With k = 1, this property is not present.\\nTop-1 routing, even if it works, would not benefit from overlaps in the clusters that each expert specializes in. Coupled with reduced computation (less parameters used during inference), it seems tha', 'output. With k = 1, this property is not present.\\nTop-1 routing, even if it works, would not benefit from overlaps in the clusters that each expert specializes in. Coupled with reduced computation (less parameters used during inference), it seems that this would lead to efficiency gains but with a loss in performance.\\nIt is true that increasing the model parameters makes the model more prone to overfitting, especially when there is not enough data available (the more data, the less the risk of overfitting), which is more likely during fine-tuning. Increasing regularization (dropout in this case) is logical when it comes to helping with that. Remember that dropout will randomly drop training samples, allowing the model to go through the data more times.\\nSwitch is shown to perform significantly better than dense models in pre-training (in both a per-step and per-time basis).\\nIncreasing regularization during fine-tuning shows promise for MoE models. However, MoE architectures are not as w', ' more times.\\nSwitch is shown to perform significantly better than dense models in pre-training (in both a per-step and per-time basis).\\nIncreasing regularization during fine-tuning shows promise for MoE models. However, MoE architectures are not as well suited as dense models for fine-tuning due a higher amount of data being needed to prevent overfitting.\\nResults from Switch show that it outperformed T5 in fine-tuning tasks such as GLUE and SQuAD. However, these seem like tasks that have enough data to prevent the issue of overfitting in Switch. It would be interesting to see how this holds when fine-tuning on tasks with less data available.\\nDistillation results from Switch show great promise, as it hints that models can be pre-trained in a MoE architecture and then distilled while still performing better than just pre-training on a dense architecture.\\nWhen training an MoE model, it would make sense to use expert parallelism in the scenario where a single expert fits into a core, and t', ' MoE architecture and then distilled while still performing better than just pre-training on a dense architecture.\\nWhen training an MoE model, it would make sense to use expert parallelism in the scenario where a single expert fits into a core, and to use expert, model and data parallelism in the case of a single expert not fitting into a core.\\nThe observation that increasing the size of each expert (and not the number of experts) is what causes instability is interesting as it shows that this perhaps leads to experts that are too complex for the clusters they are assigned to (although this should be true for an increase in the number of experts – more experts = smaller clusters for each expert). From intuition, it seems that a balance between number of experts and expert size is needed.\\n\\n\\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts\\nMain Idea: with the improvement of language models mainly coming from scaling the number of parameters in a dense setting, training ', 'een number of experts and expert size is needed.\\n\\n\\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts\\nMain Idea: with the improvement of language models mainly coming from scaling the number of parameters in a dense setting, training these models requires more and more compute and resources. GLaM looks to explore sparse language models (MoE) to reach comparable or superior performance to dense models while decreasing training costs. During evaluation, GLaM focuses on zero-shot and few-shot learning capabilities. The importance of data quality during pre-training is also analyzed.\\nThe largest GLaM model has:\\n1.2T total number of parameters.\\n96.6B active parameters.\\nFor comparison, GPT-3 is a 175B parameter dense model.\\n64 experts per MoE layer.\\nGLaM seems to have been the first study to use a decoder-only MoE on a model of comparable size to GPT-3. Switch, for example, had only around 1B active parameters (compared to 96.6B of GLaM) per input and was an encoder-decoder m', 'erts per MoE layer.\\nGLaM seems to have been the first study to use a decoder-only MoE on a model of comparable size to GPT-3. Switch, for example, had only around 1B active parameters (compared to 96.6B of GLaM) per input and was an encoder-decoder model.\\nThe training dataset used to train GLaM was highly filtered to ensure that low-quality content was not prominent (although a small collection of low-quality training data is kept to prevent systematic biases).\\n\\nArchitecture\\nAlternate between FF (dense) and MoE (sparse) layers.\\nRegular top-2 routing, with the output being a weighted average based on the scores given by the routing.\\nAuxiliary load balancing loss.\\n\\nEvaluation Setting\\nMainly focuses on zero-shot, one-shot and few-shot performances of the models being evaluated.\\nThis is different to Switch, which focuses on fine-tuning performance.\\nThis is consistent with new capabilities shown by scaling LMs as shown by GPT-3.\\n\\nResults\\nGLaM (64B/64E) (96.6B active parameters) has consiste', 'nces of the models being evaluated.\\nThis is different to Switch, which focuses on fine-tuning performance.\\nThis is consistent with new capabilities shown by scaling LMs as shown by GPT-3.\\n\\nResults\\nGLaM (64B/64E) (96.6B active parameters) has consistent gains in zero, one and few-shot performances over GPT-3, while requiring roughly only half of the compute FLOPs at inference (96.6B vs 175B).\\nImproved performance (over GPT-3) on the challenging TriviaQA domain indicates that the additional capacity of GLaM plays a crucial role in its performance gains.\\nOn GPT-3’s paper, GPT-3 was shown to consistently improve on this task (TriviaQA) given an increase in parameters, which was attributed to its ability to retain more knowledge with an increase in parameters.\\nUsing a small model (GLaM (1.7B/64E)), it was shown that the quality of the pretraining data plays a crucial role in determining the quality of the model.\\nThe impact of data quality was bigger in NLG tasks compared to NLU tasks.\\nMoE m', ' in parameters.\\nUsing a small model (GLaM (1.7B/64E)), it was shown that the quality of the pretraining data plays a crucial role in determining the quality of the model.\\nThe impact of data quality was bigger in NLG tasks compared to NLU tasks.\\nMoE models can be scaled in two ways:\\nIncreasing the number of experts\\nKeeps the number of active parameters (and thus the compute FLOPs at inference) constant.\\nIncreasing the number of experts generally resulted in better performance up to 64 experts (there was a decline in performance in further increases after 64).\\nIncreasing the size of experts\\nLeads to an increase in inference costs.\\nResults in improved performance.\\nGLaM MoE models perform consistently better than GLaM dense models for similar effective FLOPs per token.\\nMoE models perform similarly to dense at smaller scales but obtain an advantage when scaling the model.\\nIn terms of data efficiency, GLaM MoE models require significantly less data than dense models of comparable FLOPs.\\nWhen', 'ffective FLOPs per token.\\nMoE models perform similarly to dense at smaller scales but obtain an advantage when scaling the model.\\nIn terms of data efficiency, GLaM MoE models require significantly less data than dense models of comparable FLOPs.\\nWhen the same amount of data is used for training, MoE models perform much better, and the difference in performance becomes larger when training up to 630B tokens, so this advantage increases with scale.\\nIn terms of computational efficiency and energy consumption, sparse models take much less computational resources to achieve the same performance.\\nGLaM (64B/64E) has around 1/3 of training costs of GPT-3, while also halving the inference cost and using 1/6th of the energy costs.\\nThese gains can be attributed to the MoE architecture’s superior training efficiency.\\n\\nMy takeaways:\\nIn terms of architecture, GLaM does not seem to provide any significant advancements in MoE. The main quality of this research was to analyze how the MoE architecture w', 'attributed to the MoE architecture’s superior training efficiency.\\n\\nMy takeaways:\\nIn terms of architecture, GLaM does not seem to provide any significant advancements in MoE. The main quality of this research was to analyze how the MoE architecture would perform at a large scale (especially of number of active parameters) in a decoder-only model for NLG.\\n\\nST-MoE: Designing Stable and Transferable Sparse Expert Models\\nMain Idea(s): this paper provides a thorough study on MoEs. It tackles the biggest challenges presented to MoE models at the time of its release, with those being instabilities in training and poor fine-tuning performance. Its main goal is therefore to improve the practicality and reliability of sparse models.\\nTrains a 269B sparse encoder-decoder model.\\nIntroduces router z-loss to resolve instability issues.\\n\\nStabilizing Training of Sparse Models\\nTransformer models today are normally trained by using float32 to compute gradients and float16 to compute the forward and backw', 'rse encoder-decoder model.\\nIntroduces router z-loss to resolve instability issues.\\n\\nStabilizing Training of Sparse Models\\nTransformer models today are normally trained by using float32 to compute gradients and float16 to compute the forward and backward pass. Sparse models contain several exponential functions (like softmax), which can lead to large values flowing through the network. Float16 does not handle large numbers well, as the larger the number, the larger its resulting rounding error. It is proposed that this abundance of exponential functions in MoE is what causes training instability. Router z-loss is a trick to penalize large values from flowing through the network, thus improving stability.\\nRouter z-loss is a function that stabilizes the training of MoE models without degradation in model quality by penalizing large values from flowing through the network.\\nStability is referred to as constant/smooth decrease in the training loss.\\nRouter z-loss is a complement to the overal', 'ilizes the training of MoE models without degradation in model quality by penalizing large values from flowing through the network.\\nStability is referred to as constant/smooth decrease in the training loss.\\nRouter z-loss is a complement to the overall loss function, as cross-entropy and auxiliary load-balancing loss are also used.\\nSo total loss = cross-entropy + auxiliary load-balancing loss + router z-loss.\\nFine-Tuning Sparse Models\\nModel characteristics:\\nDense and sparse models both pre-trained on 500B tokens.\\nBoth roughly match T5 (encoder-decoder), which has 770M parameters.\\nSparse model has 32 experts and a sparse layer every 4 layers.\\nTrain capacity factor = 1.25, 2.0 at eval time.\\nFine-tuned on 2 SuperGLUE tasks, one with 100,000 training examples and the other with 250 to analyze overfitting of sparse models during fine-tuning.\\nSparse models are thought of to be more prone to overfitting during fine-tuning, especially when there is little data to work with (the more data, the l', 'ining examples and the other with 250 to analyze overfitting of sparse models during fine-tuning.\\nSparse models are thought of to be more prone to overfitting during fine-tuning, especially when there is little data to work with (the more data, the lesser the risk of overfitting). This is observed on the smaller task (250 training examples), where the sparse model performs better against the training set but worse in the evaluation set (classic overfitting). This does not happen in the larger task (which has 100k training examples), where the sparse model performs better than the dense one on both the training and evaluation sets. This leads us to the conclusion that sparse models have fine-tuning advantages if enough data is available to prevent the model from overfitting. Increasing regularization did not seem to have much effect at the small-scale fine-tuning, showing that small amounts of data are hard to overcome in this scenario.\\nTo explore fine-tuning MoEs further, the authors e', 'revent the model from overfitting. Increasing regularization did not seem to have much effect at the small-scale fine-tuning, showing that small amounts of data are hard to overcome in this scenario.\\nTo explore fine-tuning MoEs further, the authors experiment with exclusively updating a few layers while keeping the remaining layers frozen. They test this for different combinations.\\nMost combinations yield similar results, except one -> only updating sparse layers, which resulted in degraded performance.\\nThis indicates that the overfitting comes from sparse layers (although updating all parameters leads to better performance than updating all non-MoE parameters only).\\nAnother explanation for this can be the frequency of sparse layers being too sparse (only 1 sparse layer for every 3 dense layers), so the number of parameters being updated is not large enough.\\nAnother fine-tuning aspect analyzed was the batch size and learning rate of dense vs sparse models. Experiments showed that they ', 'oo sparse (only 1 sparse layer for every 3 dense layers), so the number of parameters being updated is not large enough.\\nAnother fine-tuning aspect analyzed was the batch size and learning rate of dense vs sparse models. Experiments showed that they do not respond the same way to changes in these training decisions.\\nSparse models benefit from smaller batch sizes and larger learning rates, while the opposite is observed for dense models.\\nThe range of batch sizes used was 65K to 1M, and the range of learning rates used was 1e-4 to 1e-3.\\nThis is consistent with the overfitting hypothesis proposed for MoE models, as smaller batch sizes have less accurate gradient updates (the higher the number of inputs used for an update, the more accurate the update). This reduced accuracy can be thought of as added noise, which serves as regularization, helping with overfitting.\\nLoad balancing is seen as a key challenge for effectively training and fine-tuning sparse models to optimize for modern hardwa', ' update). This reduced accuracy can be thought of as added noise, which serves as regularization, helping with overfitting.\\nLoad balancing is seen as a key challenge for effectively training and fine-tuning sparse models to optimize for modern hardware and prevent token dropping (expert overflow). Experiments conducted on this topic, however, contradict this (for fine-tuning):\\nThe percentage of tokens dropped (up to 15%) did not seem to have a significant impact in fine-tuning. So, token dropping in fine-tuning does not seem like a problem.\\nHigh capacity factors used during fine-tuning do not seem to have an impact on model quality.\\nThe addition of an auxiliary load balancing loss seems to have very little impact on fine-tuning.\\nIn terms of the number of experts to choose, works like Switch Transformers show that a large number of experts (up to 512) can improve model quality if designed correctly (although at a diminishing rate). However, to ensure hardware efficiency, an important co', ' the number of experts to choose, works like Switch Transformers show that a large number of experts (up to 512) can improve model quality if designed correctly (although at a diminishing rate). However, to ensure hardware efficiency, an important constraint is necessary: each GPU/TPU core available should have a maximum of 1 expert to minimize memory transfer costs. The main reason for this can be attributed to modern hardware not being optimized for loading parameters to memory. If more than 1 expert is present in a core, whenever the other expert gets called, all experts in that core need to be loaded, leading to inefficiencies. \\nDeciding on CF (capacity factor) and k in top-k routing should depend on memory and computational resources, as it leads to performance boosts but at the expense of increased costs.\\n\\nResults\\nThe ST-MoE 32B (32B active parameters, 369B total parameters) becomes the new SOTA in the SuperGLUE tasks (it was trained on all the tasks concurrently).\\nA 4.1B sparse ', 'as it leads to performance boosts but at the expense of increased costs.\\n\\nResults\\nThe ST-MoE 32B (32B active parameters, 369B total parameters) becomes the new SOTA in the SuperGLUE tasks (it was trained on all the tasks concurrently).\\nA 4.1B sparse model designed to match the FLOPs of T5-L (800M active parameters) shows improved performance on all fine-tuning tasks except the two with fewer training examples (around 250 each) -> more signs of MoEs being prone to overfitting.\\nFinally, an observation was made that upon analysis, the encoder layers generally show specialization in areas such as punctuation, verbs, numbers, names, etc. While decoder expert layers do not show specialization.\\n\\nMy takeaways:\\nRouter z-loss seems to be a helpful loss function in terms of stabilizing pre-training of MoE models in a mixed-precision environment.\\nAs shown by the experiments with fine-tuning MoEs, the performance issues seem to come from the scenario where there is a lack of data to use for fine-tu', 'ss function in terms of stabilizing pre-training of MoE models in a mixed-precision environment.\\nAs shown by the experiments with fine-tuning MoEs, the performance issues seem to come from the scenario where there is a lack of data to use for fine-tuning. If there is enough data available, fine-tuning MoEs obtains better performance than fine-tuning dense models.\\nThis can perhaps be explained to the distribution shift fine-tuning data brings in comparison to pre-training data. Naturally, a few experts will be more suited to the fine-tuning data, and thus will be used more than others, leading to overfitting of the more commonly used experts and underfitting of others.\\nAlthough the fact that only updating sparse parameters during fine-tuning leads to worse performance, it would be worth exploring further if this can be caused by the lack of MoE layers present (only one for every 3 dense layers).\\nIt is also interesting how MoEs benefit from smaller batch sizes, which add a regularizing e', ' leads to worse performance, it would be worth exploring further if this can be caused by the lack of MoE layers present (only one for every 3 dense layers).\\nIt is also interesting how MoEs benefit from smaller batch sizes, which add a regularizing effect (adding strength to the claim that MoEs are prone to overfitting).\\nExperiments done for this paper show that load balancing is not an issue for fine-tuning, which makes sense since the router should already be fully trained. This seems to indicate that routers can be frozen during fine-tuning.\\nIt is explained how, although having many experts may lead to performance boosts, a balance needs to be achieved depending on the number of GPU/TPU cores available. Loading more than 1 expert per core leads to inefficiencies.\\nDeciding on CF (capacity factor) and k in top-k routing should depend on memory and computational resources, as it leads to performance boosts but at the expense of increased costs.\\n\\n\\n\\nUnified Scaling Laws For Routed Langua', 'e leads to inefficiencies.\\nDeciding on CF (capacity factor) and k in top-k routing should depend on memory and computational resources, as it leads to performance boosts but at the expense of increased costs.\\n\\n\\n\\nUnified Scaling Laws For Routed Language Models\\nMain idea: this paper investigates the scaling behaviors of routing networks, more specifically in the axis of parameter count (in terms of total number of parameters) and computational requirements (total number of active parameters). \\nRouting:\\nIt experiments with 3 different routing techniques:\\nAn approach based on BASE (linear programming).\\nThis represents a more traditional learned algorithm for routing. BASE in specific approaches routing as a linear programming problem, which naturally distributes tokens evenly through experts (no load balancing issues). The algorithm experimented with has slight modifications to BASE to be more efficient in accelerated hardware (they call it Sinkhorn-BASE).\\nA non-parametric approach (hash l', 'urally distributes tokens evenly through experts (no load balancing issues). The algorithm experimented with has slight modifications to BASE to be more efficient in accelerated hardware (they call it Sinkhorn-BASE).\\nA non-parametric approach (hash layer).\\nHASH layers approaches routing as a fixed function of the input, meaning it does not have learnable parameters.\\nA Reinforcement Learning approach.\\nResults:\\nAlthough routing (sparse) performs better than no routing (dense) on all sizes experimented with (up to 1.3B active parameters, up to 512 experts – biggest model has around 200B parameters), the sparse gains over dense are diminishing with scale (BASE is more robust than other routing techniques).\\nScaling the number of experts when the number of active? parameters is fixed improves the validation loss during pre-training.\\nEffective Parameter Count (EPC) is created to compare the performance of dense against sparse models based on an equation that considers the total number of para', ' number of active? parameters is fixed improves the validation loss during pre-training.\\nEffective Parameter Count (EPC) is created to compare the performance of dense against sparse models based on an equation that considers the total number of parameters and the active parameters of a model.\\nMain takeaways (as listed in the paper):\\nRouting improves performance across all model sizes and routing strategies (compared to dense aka no routing).\\nRL routing is more effective than expected, although BASE is the best performer.\\nPerformance can be described by scaling the number of experts and dense model size.\\nDevelopment of an effective parameter count mapping for performance vs scaling.\\nRecommendations:\\nUse routing when training any model with N (parameter count of base model) <= 1.3B.\\nSinkhorn-BASE is a good default routing algorithm.\\nAlthough more experts lead to improved performance, it is recommended to use between 64 and 128 experts due to diminishing returns above that range.\\nIt is r', 'N (parameter count of base model) <= 1.3B.\\nSinkhorn-BASE is a good default routing algorithm.\\nAlthough more experts lead to improved performance, it is recommended to use between 64 and 128 experts due to diminishing returns above that range.\\nIt is recommended to use k=1 experts.\\n\\n\\nMy takeaways:\\nShows that learned routing (represented through BASE) is the best routing strategy. \\nNon-parametric routing can be used in cases where there might not be enough data to train specific experts (for example, on task/domain-level MoE like DEMix where we are not certain if the training load for each expert will be similar, which will lead to load balancing issues that cannot be solved through traditional auxiliary loss or adding noise – this might not happen at token-level routing)\\nRL routing performs worse than BASE but looks to not be too far off\\nTo describe performance, the number of experts and dense model size (number of active parameters for each forward pass) are the most relevant features. ', 'appen at token-level routing)\\nRL routing performs worse than BASE but looks to not be too far off\\nTo describe performance, the number of experts and dense model size (number of active parameters for each forward pass) are the most relevant features. This is logic as the number of experts represents the horizontal scale of the model while the dense model size represents the vertical scale of the model. (dense model size corresponds to vertical scaling, does number of experts as mentioned here correspond to an increase in the number of experts with the same total parameter count or is this accounting for an increase in the total parameter count coming from the added experts).\\nSparse models seem to be the most useful at small scales, with diminishing returns over dense with an increase in the scale of active parameters, but this can be prevented to a certain extent by choosing a robust routing strategy.\\nThe result arrived at that scaling the number of experts when the number of active par', 'nishing returns over dense with an increase in the scale of active parameters, but this can be prevented to a certain extent by choosing a robust routing strategy.\\nThe result arrived at that scaling the number of experts when the number of active parameters is fixed is logical as this scales the model horizontally. However, my intuition in this is that scaling the number of experts might make things difficult for fine-tuning (more data will be needed to update all experts while not overfitting on others). Therefore, a balance is needed. (authors recommend between 64 and 128 experts due to diminishing returns in increasing the number of experts). -> how does fine-tuning performance change with differing number of experts and in respect to more training data to use for fine-tuning (explore how scaling the number of experts while keeping the active parameter count constant impacts fine-tuning performance)?\\nThe EPC equation seems to be useful for practitioners looking to train a MoE model ', 're training data to use for fine-tuning (explore how scaling the number of experts while keeping the active parameter count constant impacts fine-tuning performance)?\\nThe EPC equation seems to be useful for practitioners looking to train a MoE model from scratch. This would help with design choices in number of active parameters and number of total parameters.\\nInteresting how the authors recommend MoE in scenarios of training smallish models (up to 1.3B). I believe that this is because that was the bigger dense model studied, so it is not saying that dense models perform better when scaled above 1.3B, but just that a bigger dense model was not used in the experiments. It is important to note that the experiments showed diminishing returns for routing models -> did any other papers dive into this question?\\nIt is also interesting to note that the authors concluded that k=1 experts is the ideal number for k.\\nEfficient Large Scale Language Modeling with Mixtures of Experts\\nMain Idea: this ', 'or routing models -> did any other papers dive into this question?\\nIt is also interesting to note that the authors concluded that k=1 experts is the ideal number for k.\\nEfficient Large Scale Language Modeling with Mixtures of Experts\\nMain Idea: this paper has the goal of comparing how the traditional MoE architecture from “Sparsely-Gated MoE”, using top-2 routing, scales in relation to dense models. \\nModel sizes trained for this experiment range from (in total number of parameters):\\n125M to 13B (in a dense setting).\\n15B to 1.1T (in a MoE setting).\\nThe maximum number of experts used was 512, and the capacity factor used for MoE models was 2 (to support top-2 routing).\\nDense and sparse models were compared on a FLOPs-matching basis (models with the same FLOPs are comparable). The dense baseline used was GPT-3.\\nEvaluations done:\\nPerplexity (from next-token predictions).\\nPerformance on downstream tasks (benchmarks, zero-shot, few-shot).\\nMoE speedup factor – how much more efficient MoEs are', 'th the same FLOPs are comparable). The dense baseline used was GPT-3.\\nEvaluations done:\\nPerplexity (from next-token predictions).\\nPerformance on downstream tasks (benchmarks, zero-shot, few-shot).\\nMoE speedup factor – how much more efficient MoEs are at achieving a specific performance level relative to dense models (how many training FLOPs are needed to reach a certain performance goal).\\nResults:\\nMoE outperforms dense in all evaluation datasets, although at a different scale depending on the dataset’s domain and model size.\\nMoEs are the most efficient (highest speedup factor in in-domain tasks), reaching an 8x to 16x speedup (8x-16x less compute needed for the same performance)\\nThis speedup decreases to a 2x-4x speedup in out-of-domain tasks.\\nThe speedup advantages of MoE decrease at scale, especially in in-domain tasks.\\nThe closer the data used for evaluation is to the training corpus, the larger the speedup obtained by MoE.\\nOn downstream zero-shot task evaluation, MoE also outperfor', 'ks.\\nThe speedup advantages of MoE decrease at scale, especially in in-domain tasks.\\nThe closer the data used for evaluation is to the training corpus, the larger the speedup obtained by MoE.\\nOn downstream zero-shot task evaluation, MoE also outperforms the dense model (which performs on par with GPT-3), but this gain is, again, diminishing at scale.\\nIn a few-shot setting, MoE still outperforms dense, but the MoE improvements over zero-shot are smaller than dense. This indicates that although MoE still outperforms dense in a few-shot setting, dense models benefit more from few-shot examples.\\nIn terms of fine-tuning, dense models (as expected) always incur substantial gains. Although this is true in some cases for MoE, fine-tuning MoE models on some domains/datasets leads to worse performance. More research is needed to determine why. Perhaps this comes from fine-tuning MoEs not being deeply explored yet, with an alternative approach needed to obtain good results (the same setting as den', 'on some domains/datasets leads to worse performance. More research is needed to determine why. Perhaps this comes from fine-tuning MoEs not being deeply explored yet, with an alternative approach needed to obtain good results (the same setting as dense was used for fine-tuning after all).\\n\\nMy takeaways:\\nThe results from this paper’s experiments show that the traditional MoE architecture does indeed provide speedups over a dense setting. The results from the speedup provided by MoE are bigger the closer the evaluation domains are from the training domains. This seems to indicate that the biggest gains from MoE come from memorization. Generalization gains provided by MoE over dense are not as apparent, although there still are gains (MoE still provides a speedup when evaluated in out-of-domain tasks).\\nThe diminishing gains from MoE at scale are more apparent in out-of-domain tasks, as they stay relatively constant when training domains (or close to) are used for evaluation.\\nIt is interes', 'l provides a speedup when evaluated in out-of-domain tasks).\\nThe diminishing gains from MoE at scale are more apparent in out-of-domain tasks, as they stay relatively constant when training domains (or close to) are used for evaluation.\\nIt is interesting to note that few-shot has a bigger effect on dense performance than on MoE performance (dense benefits more), although MoE outperforms dense in this scenario.\\nA previous work, ST-MoE, concludes that sparse models benefit from smaller batch sizes and larger learning rates during fine-tuning, while the opposite is observed for dense models. ST-MoE also concludes that MoEs are significantly more prone to overfitting during fine-tuning compared to dense. The fine-tuning results from this paper can be replicated and analyzed with these two aspects in mind as future research.\\n\\nMegaBlocks: Efficient Sparse Training with Mixture-of-Experts\\nMain Idea(s): MegaBlocks aims to improve the challenges of load imbalance and token dropping in MoE archi', 'can be replicated and analyzed with these two aspects in mind as future research.\\n\\nMegaBlocks: Efficient Sparse Training with Mixture-of-Experts\\nMain Idea(s): MegaBlocks aims to improve the challenges of load imbalance and token dropping in MoE architecture using block sparse matrices. The idea is to present a router that dynamically handles the token allocation to experts. While in a regular MoE architecture each expert is assigned to a single GPU in a fixed allocation system (each expert gets the same amount of compute), having to drop tokens in the case of overflow to a specific expert/GPU, while at the same time padding tokens to compensate for idle computational resources in experts which were not assigned enough tokens in a batch, MegaBlocks makes this allocation dynamically from the start, so the computational resources assigned to an expert is variable, being adjusted on a per-batch basis based on the tokens assigned to the expert on that specific batch.\\nOBS: Tutel, a previous ', 'aBlocks makes this allocation dynamically from the start, so the computational resources assigned to an expert is variable, being adjusted on a per-batch basis based on the tokens assigned to the expert on that specific batch.\\nOBS: Tutel, a previous work, used a similar strategy, by implementing a dynamic CF (capacity factor) for each expert, but this leads to computational inefficiencies.\\nMegaBlocks is possible by making use of block-sparse matrix multiplication as opposed to batched matrix multiplication. This approach maps efficiently to hardware accelerators and allows for variable expert size and allocation.\\nMegaBlocks leads to training speedups, which is logical since it makes optimum use of computational resources at each update.\\n\\nMy takeaways:\\nMegaBlocks is an approach for maximizing computing efficiency when training MoE models. It dynamically adjusts how much compute to be given to each expert at every batch, preventing token dropping and idle resources. Although this is inte', ' takeaways:\\nMegaBlocks is an approach for maximizing computing efficiency when training MoE models. It dynamically adjusts how much compute to be given to each expert at every batch, preventing token dropping and idle resources. Although this is interesting, per the experiments of ST-MoE, this seems to only be useful at pre-training, as load balancing does not seem to affect fine-tuning much.\\n\\n\\n\\nSparse Upcycling: Training Mixture-of-Experts From Dense Checkpoints\\nMain Idea: the paper aims to provide an efficient way to train an MoE model from a dense checkpoint (a pre-trained dense transformer) to minimize training costs, that is, provide an MoE training strategy that is cheaper than training from scratch.\\nThe paper shows that training a MoE from a dense checkpoint outperforms continued dense training.\\nExpert-choice routing (with CF of 2) is generally used for the encoder and top-k (with k=2) is used for the decoder.\\nThe T5 encoder-decoder model is used as the dense checkpoint.\\nEach ex', 'E from a dense checkpoint outperforms continued dense training.\\nExpert-choice routing (with CF of 2) is generally used for the encoder and top-k (with k=2) is used for the decoder.\\nThe T5 encoder-decoder model is used as the dense checkpoint.\\nEach expert’s weights are initialized as the exact MLP of the dense checkpoint, and the router needs to be trained from scratch.\\nThe layer-norm, attention, embedding and output layers are copied to the new model from the dense checkpoint.\\nResults:\\nWhen continuing pre-training, the larger the training continues after the checkpoint, the bigger the advantage obtained by the upcycle model vs a dense model.\\nThe continued pre-training is referred to as sparse upcycling.\\nWhen sparse upcycling for language, there are two comparisons made:\\nUpcycle vs dense – upcycle performs better, with continued dense pre-training giving inconsistent results.\\nUpcycle vs MoE – upcycle generally performs better for small computational budgets. When enough computational bu', 'here are two comparisons made:\\nUpcycle vs dense – upcycle performs better, with continued dense pre-training giving inconsistent results.\\nUpcycle vs MoE – upcycle generally performs better for small computational budgets. When enough computational budget is given (>100% of the initial pre-trained dense computational budget), MoE can catch up and perform better than upcycled models.\\nSparse upcycling is also shown to perform better than warm starting (“dense upcycling”).\\nMy takeaways:\\nIt sounds like the approach studied takes T5 (encoder-decoder model) and stretches its feedforward layers horizontally (in other words, transforms them in MoE layers). All other layers remain static – assuming the sparse upcycling is only done on the new MoE layers and routing mechanism, while other layers remain frozen during this process. \\nThe main takeaway of this paper is that it indicates that with enough training computing budget, it is more efficient to train an MoE model than a dense one, and when n', 'ers and routing mechanism, while other layers remain frozen during this process. \\nThe main takeaway of this paper is that it indicates that with enough training computing budget, it is more efficient to train an MoE model than a dense one, and when not much training computing budget is given, the best-performing approach is to train a sparse upcycled model from a dense checkpoint.\\n\\nMixture-of-Experts Meets Instruction Tuning: a Winning Combination for Large Language Models\\nMain Idea: this study aims to measure the impact of instruction-tuning in MoE models compared to its impact in dense models.\\nInstruction-tuning is related to fine-tuning as fine-tuning is training a pre-trained model on a specific task, while instruction-tuning consists of training a language model in a supervised manner to perform well in a dialogue setting. This means for the model to perform well on the task of predicting p(answer | question) instead of the pre-training objective of predicting p(word | context).\\nT', 'f training a language model in a supervised manner to perform well in a dialogue setting. This means for the model to perform well on the task of predicting p(answer | question) instead of the pre-training objective of predicting p(word | context).\\nThree different scenarios were evaluated:\\nDirect finetuning on individual tasks (no instruction tuning).\\nInstruction tuning followed by in-context learning (no direct fine-tuning)\\nInstruction tuning followed by further finetuning on individual tasks.\\nThe conclusion of this paper was that MoE models outperform dense models of equivalent computational capacity on direct finetuning, but significantly outperform dense models on instruction tuning scenarios. Let’s understand how they reached this conclusion.\\n\\nSetup\\nTwo dense models were considered: T5 and PaLM.\\nFour MoE architectures were considered:\\nSwitch Transformers\\nGShard\\nExpert-Choice\\nST-MoE\\nAll instruction tuning was done using the FLAN dataset.\\n\\nResults\\nA base MoE architecture outperforms', 'lusion.\\n\\nSetup\\nTwo dense models were considered: T5 and PaLM.\\nFour MoE architectures were considered:\\nSwitch Transformers\\nGShard\\nExpert-Choice\\nST-MoE\\nAll instruction tuning was done using the FLAN dataset.\\n\\nResults\\nA base MoE architecture outperforms a dense architecture (T5) after instruction-tuning across all scales.\\nScaling the number of experts helps when fine-tuning on challenging tasks but saturates when fine-tuning on easier tasks (more experts is not always better as it might confuse the gating algorithm).\\nAs expected, increasing k in top-k routing improves performance at an increase in the inference cost.\\nOverperformance of MoE compared to dense models when instruction-tuning only exacerbates with scale (the bigger the models, the bigger the performance gain of MoE over dense).\\nExpert-choice outperforms GShard (token-choice) in an instruction-tuning scenario, however, this difference is bridged by incorporating advanced auxiliary loss (router z-loss) and pre-training strategy ', 'bigger the performance gain of MoE over dense).\\nExpert-choice outperforms GShard (token-choice) in an instruction-tuning scenario, however, this difference is bridged by incorporating advanced auxiliary loss (router z-loss) and pre-training strategy as employed in ST-MoE (also token-choice).\\nEven though FLAN-PaLM62B (dense instruction-tuned model) has 3x the number of FLOPs per token than FLAN-ST32B (largest MoE instruction-tuned model trained for this work) at inference, FLAN-ST32B significantly outperforms FLAN-PaLM (57.6 vs 63.6 average score).\\nDifferent auxiliary losses gave different results:\\nZ-loss worked better than balance-loss in FLAN-ST\\nBalance-loss worked better than z-loss in FLAN-EC\\nFreezing certain parts of the MoE layers during fine-tuning was evaluated to investigate how to prevent overfitting in MoE fine-tuning:\\nFreezing the gate led to small improvements.\\nFreezing any other areas resulted in worse performance.\\nMy takeaways:\\nFirst thought is that instruction-tuning sho', 'ng fine-tuning was evaluated to investigate how to prevent overfitting in MoE fine-tuning:\\nFreezing the gate led to small improvements.\\nFreezing any other areas resulted in worse performance.\\nMy takeaways:\\nFirst thought is that instruction-tuning should work better in dense models than in MoE models based on the difficulties in obtaining good fine-tuning performance with MoE. This may not hold since the instruction-tuning process can be thought of a very specific type of fine-tuning.\\nThis is shown to be false, as MoE significantly outperforms dense models when it comes to instruction-tuning. This is even more interesting when showed that this advantage of MoE over dense in the task of instruction-tuning only increases with scale.\\nMoE results after instruction-tuning are quite promising. For some reason, MoE captures the instruction-tuning task much more efficiently than dense models.\\nMore experts do not guarantee better performance with fine-tuning. In fact, on easier tasks, more exper', 's after instruction-tuning are quite promising. For some reason, MoE captures the instruction-tuning task much more efficiently than dense models.\\nMore experts do not guarantee better performance with fine-tuning. In fact, on easier tasks, more experts result in worse fine-tuning performance.\\nWhat was the size of the datasets used for fine-tuning? Perhaps easier tasks are more prone to overfitting, explaining the underperformance of fine-tuning MoE on easier datasets. If this was the case, these tasks would require more regularization -> how much regularization to use might depend on the difficulty of the task.\\nThis makes sense to the overall MoE theory as easier tasks have less complex data distributions The less complex data distribution will lead to less of the experts being called consistently, causing them to overfit. In a complex task, the data distribution will result in a more distributed load balancing due to more semantic/syntax patterns being in place, thus using more expert', 'will lead to less of the experts being called consistently, causing them to overfit. In a complex task, the data distribution will result in a more distributed load balancing due to more semantic/syntax patterns being in place, thus using more experts, preventing overfitting.\\nThere might be router issues leading to this difficulty in fine-tuning on easier tasks as well.\\nExpert-choice seems to be better than regular token-choice routing. However, ST-MoE, which has improvements over traditional token-choice routing, surpasses expert-choice.\\nWhy did Mixtral decide to not use Expert-Choice and seems to use a routing strategy that resembles GShard more, even though it underperforms both Expert Choice and ST-MoE’s routing strategies? Maybe they started training before this paper came out? (investigate if Mixtral’s routing strategy resembles more GShard than ST-MoE).\\nZ-loss is better for token-choice, but balance-loss is better for Expert-Choice?\\nThe routing learned during pre-training is tho', 'tarted training before this paper came out? (investigate if Mixtral’s routing strategy resembles more GShard than ST-MoE).\\nZ-loss is better for token-choice, but balance-loss is better for Expert-Choice?\\nThe routing learned during pre-training is thought to already have a good estimate of data distributions at a semantic and syntactic level, therefore more specialization is not needed during fine-tuning. The idea is that the semantics and syntax at fine-tuning domains are not new, what changes is their distribution. Therefore, the routing algorithm does not to be updated -> gating/routing should be kept frozen during fine-tuning (this is not the first research work to come to this conclusion).\\nMoE models are prone to overfitting, so often underperform dense models on single-task fine-tuning. MoE works better when scaling the number of tasks, that is, fine-tuning on more than just one domain. However, instruction-tuning seems to bring a reversal to this trend, with FLAN-MoE performing b', 'nderperform dense models on single-task fine-tuning. MoE works better when scaling the number of tasks, that is, fine-tuning on more than just one domain. However, instruction-tuning seems to bring a reversal to this trend, with FLAN-MoE performing better than FLAN-T5 in single task fine-tuning. \\nPerhaps a reason for this is how FLAN does not have a single task per-say, it instead has data from many different domains with the common aspect being the structure how it is presented (in a dialogue format).\\n\\nTask/Domain-Level MoE\\nBeyond Distillation: Task-level Mixture-of-Experts for Efficient Inference\\nMain idea: the goal of this work is to find an alternative method to distillation to store MoE models. It experimented with token-level, task-level and sentence-level routing. MoE solves the issue of training efficiency when compared to dense models (since only a subset of the network is activated at a time) (tradeoff of a few more communication costs due to experts’ communication and routin', 'vel and sentence-level routing. MoE solves the issue of training efficiency when compared to dense models (since only a subset of the network is activated at a time) (tradeoff of a few more communication costs due to experts’ communication and routing but less parameters needing to be updated per forward pass compared to a dense model of the same size in terms of total parameters) but still leaves room for improvement in inference efficiency due to the requirement of storing the model across many devices, adding to communication costs and idle resources for calling small batches (since in small batches, most machines will not be used since the respective expert is not needed). This paper’s main goal is to improve inference efficiency for sparse MoE models. Distillation is a possible solution but tends to lead to loss in quality. The task used for experiments was a multilingual machine translation task.\\n\\nApproach\\nTrained a routing strategy to leverage global task-level information to ro', 'arse MoE models. Distillation is a possible solution but tends to lead to loss in quality. The task used for experiments was a multilingual machine translation task.\\n\\nApproach\\nTrained a routing strategy to leverage global task-level information to route all tokens corresponding to a particular task collectively to the same set of experts.\\nDecode different tasks separately and only load the subset of experts associated with the corresponding task during inference.\\nTask-level routing strategy showed gains over a dense model trained from scratch and a distilled model (student) trained from learning through a token-level MoE teacher model.\\nComparable quality to token-MoE model (not distilled) while achieving significant inference gains (1.9x peak throughput and 6.3% of the decoder size).\\nTop-2 routing mechanism used.\\n\\nRouting Strategies Experimented With\\nToken-level.\\nTraditional MoE where each token is routed independently.\\nSentence-level.\\nRoute tokens by sentence, determined by the expert', 'ak throughput and 6.3% of the decoder size).\\nTop-2 routing mechanism used.\\n\\nRouting Strategies Experimented With\\nToken-level.\\nTraditional MoE where each token is routed independently.\\nSentence-level.\\nRoute tokens by sentence, determined by the expert with the highest average token weight in the sentence.\\nFirst thought is that this won’t work well due to the average token weight per expert is used (this is proven to be correct by experiments done later in the paper). A better sentence-level approach could be to use sentence embeddings, which would also only need to call the router once per sentence.\\nTask-level.\\nRoute tokens based on a task. In the multilingual translation task, this can be determined by either the target language or the language pair.\\n\\nInference Implications\\nThe token-level and sentence-level approach makes inference costly. To help with the challenge of needing to have all experts ready and loaded to the server at inference, these approaches can have experts be dynamic', 'uage pair.\\n\\nInference Implications\\nThe token-level and sentence-level approach makes inference costly. To help with the challenge of needing to have all experts ready and loaded to the server at inference, these approaches can have experts be dynamically loaded based on the routing decision or model parallelism can be employed (the server often needs to load all experts). Both incur high communication costs.\\nThis needs to be done for every then, hence the high cost.\\nTask-level routing only need to pre-load the top-k experts for the given input sequence. This is done by determining which task most resembles the input sequence and using the top-k experts for that task only for all tokens.\\nLoading experts only needs to be done once for each input sequence.\\n\\nResults\\nSentence-level MoE did not perform well.\\nThe best encoder-decoder model used had a token-MoE in the encoder and a task-MoE in the decoder.\\nThe best decoder-only model was the task-MoE decoder.\\nStatically determining the task th', 'put sequence.\\n\\nResults\\nSentence-level MoE did not perform well.\\nThe best encoder-decoder model used had a token-MoE in the encoder and a task-MoE in the decoder.\\nThe best decoder-only model was the task-MoE decoder.\\nStatically determining the task through a deterministic approach did not work very well (experts are deterministically allocated to tasks).\\nTask-level MoE has higher throughput (tokens/sec), uses less decoder parameters and has less communication overhead (or none) compared to token-level MoE.\\nTask-level MoE performs better than models distilled from token-level MoE.\\nAdditionally, analysis of the routing decisions shows that at a task level, the experts called in the encoder do not change much, but experts in the decoder seem to naturally specialize in tasks, giving a possible explanation why the decoder-only task MoE performed well.\\n\\nMy takeaways:\\nIn MoE, there is a tradeoff in training costs compared to dense models. MoE provides less communication costs overall:\\nThere is', ' naturally specialize in tasks, giving a possible explanation why the decoder-only task MoE performed well.\\n\\nMy takeaways:\\nIn MoE, there is a tradeoff in training costs compared to dense models. MoE provides less communication costs overall:\\nThere is a partial increase in communication costs due to the communication that needs to be done between activated experts and between these activated experts and the router.\\nOverall, however, MoE is more efficient at training due to only a subset of parameters needing to be updated per forward pass (on the MoE layers, where the bulk of parameters are located). This allows MoE to scale the total number of parameters in an easier way.\\nThe inspiration for the approach used comes from trying to decrease the cost of storing experts during inference. \\nThis is a necessary step as all experts need to be ready to be called during inference, which leads to idle resources (no experts being used for some batches but needing to be stored and ready).\\nDistillat', 'he cost of storing experts during inference. \\nThis is a necessary step as all experts need to be ready to be called during inference, which leads to idle resources (no experts being used for some batches but needing to be stored and ready).\\nDistillation is (was) the most common approach for this, but distilling experts tends to lead to significant loss in quality.\\nDistillation consists of training a small dense model (student) from a large MoE expert (teacher).\\nThe gains obtained from inference efficiency do not come from calling less parameters at inference (number of active parameters), but from the number of experts being loaded (number of total parameters available).\\nThe idea seems to be to predict the most relevant experts that will be needed on a task level, so only those need to be loaded and ready during inference.\\nThe meat of this approach is to correctly predict the experts needed. If this prediction is correct, the model will have good quality, otherwise it won’t.\\nThe approa', ' needed on a task level, so only those need to be loaded and ready during inference.\\nThe meat of this approach is to correctly predict the experts needed. If this prediction is correct, the model will have good quality, otherwise it won’t.\\nThe approach seemed to work since the quality of the resulting model was comparable to token-MoE.\\nThis ends up reducing the latency costs since the experts used only need to be loaded once per input sequence, and not for every token.\\nThe task-level approach seems to be useful in some scenarios but not possible in others. For example, if an out-of-domain task is shown at testing that is different than the training tasks, my intuition tells me that the router won’t be able to select the most relevant experts very well (and the experts won’t be prepared for this situation), thus leading to the model underperforming a token-level approach, which I believe would be more robust to these situations.\\nThis approach sounds interesting in a scenario where there', 's very well (and the experts won’t be prepared for this situation), thus leading to the model underperforming a token-level approach, which I believe would be more robust to these situations.\\nThis approach sounds interesting in a scenario where there are predefined tasks that we want the model to perform well on, and it does not necessarily need to perform so well on out-of-domain tasks.\\nThis should be considered when choosing between the task-level MoE and a distilled student model (the student model, in theory, would perform better in terms of generalization – not as good in a few tasks, but good in everything -, while task-level MoE would probably perform better in specific tasks scenarios – especially good at a few tasks (depends on training)).\\n\\nExpert Gate: Lifelong Learning with a Network of Experts\\nMain idea: this paper experiments with a novel approach for using MoE in a multi-task setting. More specifically, it focuses on the gating mechanism used. Expert Gate also focuses on ', 'ining)).\\n\\nExpert Gate: Lifelong Learning with a Network of Experts\\nMain idea: this paper experiments with a novel approach for using MoE in a multi-task setting. More specifically, it focuses on the gating mechanism used. Expert Gate also focuses on scalability, as it is a lifelong learning approach (can be scaled with time). A lifelong learning approach means that:\\nModels are trained sequentially.\\nNo need to store the data used for training, only the models.\\nExpert Gate is trained on image classification and video prediction problems, but could technically also be used in an NLP/LLM setting (but was not experimented with)\\n\\nAdvantages of Expert Gate\\nThe meat of this method is in the autoencoder gating mechanism used. This mechanism solves problems as:\\nData storage, since the models can be trained sequentially, so keeping all training data is not necessary.\\nLater the paper will show that storing training data used previously is not necessary.\\nCatastrophic forgetting, which is an issue o', 'roblems as:\\nData storage, since the models can be trained sequentially, so keeping all training data is not necessary.\\nLater the paper will show that storing training data used previously is not necessary.\\nCatastrophic forgetting, which is an issue other models suffer with. For example, continuously training and fine-tuning the same model on new tasks will lead to this issue.\\nTask biases when fine-tuning which can lead to suboptimal local minima.\\nIf a model is trained on a task and fine-tuned on a widely different task, it can lead to suboptimal results due to the biases inferred in the initial task being different. \\nMemory efficiency, as only one expert needs to be loaded into memory at a time.\\nTask relatedness, which can be measured by the autoencoder’s results and then be used to figure out how to initialize the expert’s parameters for a new class and either to use fine-tuning or learning-without-forgetting (LwF) for training the new expert.\\nLwF vs Fine-tuning\\nWhen two tasks are suf', 'e autoencoder’s results and then be used to figure out how to initialize the expert’s parameters for a new class and either to use fine-tuning or learning-without-forgetting (LwF) for training the new expert.\\nLwF vs Fine-tuning\\nWhen two tasks are sufficiently related (above a certain task relatedness in threshold), it is beneficial to train a new expert with LwF based on an old task, otherwise the best approach is to fine-tune the expert for the similar (existing) task on the training data from the new task.\\nFine-tuning\\nBased on an existing model, simply continue training using a new dataset\\nThe result of this fine-tuning on an existing expert will be a brand-new expert, while the existing expert that it was based on will remain unchanged.\\nSo, this process starts with 1 expert and ends up with 2 experts.\\nLwF\\nTechnique used to prevent catastrophic forgetting when training an existing model on new data. LwF uses soft targets (outputs of the old model) to help retain knowledge from old ta', 'So, this process starts with 1 expert and ends up with 2 experts.\\nLwF\\nTechnique used to prevent catastrophic forgetting when training an existing model on new data. LwF uses soft targets (outputs of the old model) to help retain knowledge from old tasks.\\nAs with fine-tuning, this results in 2 experts.\\n\\nAutoencoder Mechanism – Expert Gate Inner Working\\nGoals:\\nTo select an expert based on input data.\\nTo measure task relatedness to figure out optimal parameters to initialize an expert (based on most related task) and training strategy (fine-tuning or LwF – LwF in the case of task relatedness being above a certain threshold).\\nThe Inner Workings of the Autoencoder\\nIt follows a regular encoder-decoder architecture.\\nEncoder , maps the input x to a code h(x).\\nDecoder , maps the encoder’s code (h(x)) to a reconstruction of the input.\\nThe autoencoder simply uses an encoder-decoder architecture to deconstruct the input (done by the encoder) and reconstruct it (done by the decoder).\\nThe loss funct', ' code h(x).\\nDecoder , maps the encoder’s code (h(x)) to a reconstruction of the input.\\nThe autoencoder simply uses an encoder-decoder architecture to deconstruct the input (done by the encoder) and reconstruct it (done by the decoder).\\nThe loss function  is simply the reconstruction error.\\nThe encoder learns, through a hidden layer, a lower dimensional representation (undercomplete autoencoder) or a higher dimensional representation (overcomplete autoencoder) of the input data.\\nThe lower dimensional subspace learned by one of the undercomplete autoencoders will be maximally sensitive to variations observed in the task data but insensitive to changes orthogonal to the manifold (it represents only the variations that are needed to reconstruct relevant samples)\\nThe autoencoder of a domain/task should be better at reconstructing the data of the task it was trained on better than the other autoencoders.\\nThe reconstruction error for each autoencoder then allows the input to be routed to the ', ' relevant samples)\\nThe autoencoder of a domain/task should be better at reconstructing the data of the task it was trained on better than the other autoencoders.\\nThe reconstruction error for each autoencoder then allows the input to be routed to the expert of the task of the autoencoder with the lowest reconstruction error for that input (or multiple, in the case of multiple very good autoencoders for that input).\\nThe reconstruction error then acts like a score (all reconstruction errors are passed through a SoftMax to determine a normalized score).\\nThe task relatedness between two tasks is also measured through the autoencoder’s reconstruction error through the following formula:\\n\\n = new task.  = old task.\\n is the relatedness between task k and task a.\\n is the reconstruction error of the autoencoder for task a in the data for task k.\\n is the reconstruction error of the autoencoder for task k on its own data.\\nHow can the reconstruction error of the autoencoder for task k on its own dat', 'k and task a.\\n is the reconstruction error of the autoencoder for task a in the data for task k.\\n is the reconstruction error of the autoencoder for task k on its own data.\\nHow can the reconstruction error of the autoencoder for task k on its own data be computed before the expert (and thus its autoencoder) is trained, since its initialization method relies on this task relatedness computation? This seems redundant.\\n\\nExperiments Results\\nExpert Gate was compared with and outperformed (on image classification):\\nSingle fine-tuned model (sequentially fine-tuned on each task).\\nOne would think that this would result in severe catastrophic forgetting.\\nSingle LwF model (sequentially trained on each task).\\nOne would think that you can’t train the same model with LwF forever on many different tasks without running into catastrophic forgetting issues.\\nExpert Gate performed on-par with:\\nJoint training (assumes all is always available for re-training).\\nMultiple fine-tuned models (fine-tuned on each', 'me model with LwF forever on many different tasks without running into catastrophic forgetting issues.\\nExpert Gate performed on-par with:\\nJoint training (assumes all is always available for re-training).\\nMultiple fine-tuned models (fine-tuned on each task separately)\\nThis assumes an oracle gate, that is, a gate that knows perfectly how to route each input to the corresponding expert.\\nMultiple LwF models (trained on each task separately).\\nAlso assumes an oracle gate.\\nExpert Gate vs Discriminative Classifier (neural net trained on all the data available for gating decisions – a routing mechanism).\\nWithout ever having simultaneous access to the data of different tasks, Expert Gate based on autoencoders manages to assign test samples to the relevant tasks equally accurately as a discriminative classifier (which assumes all training data is available).\\nTask relatedness analysis\\nExpert Gate succeeds in predicting when a task could help another in the LwF framework and when it cannot (LwF vs ', 'evant tasks equally accurately as a discriminative classifier (which assumes all training data is available).\\nTask relatedness analysis\\nExpert Gate succeeds in predicting when a task could help another in the LwF framework and when it cannot (LwF vs fine-tuning decision).\\n\\n\\nMy takeaways:\\nThis is an interesting point to take note of when thinking of a problem related to fine-tuning, especially when fine-tuning MoE.\\nTask biases when fine-tuning which can lead to suboptimal local minima.\\nIf a model is trained on a task and fine-tuned on a widely different task, it can lead to suboptimal results due to the biases inferred in the initial task being different (think that the pre-training distribution shift can lead to local minima that is optimal for that distribution, but distribution of new tasks can be different and gain from other local minima that are unreachable due to the pre-training local minima – imagine the gradient descent valley)\\nExpert Gate seems like DEMix. \\nExpert Gate focuse', ' for that distribution, but distribution of new tasks can be different and gain from other local minima that are unreachable due to the pre-training local minima – imagine the gradient descent valley)\\nExpert Gate seems like DEMix. \\nExpert Gate focuses on the LwF or fine-tuning decision when being presented a new task, DEMix focuses more on the modularity of each expert.\\nExpert Gate focuses on task-level experts while DEMix focuses on domain-level experts.\\nExpert Gate experiments on computer vision tasks while DEMix focuses on NLP tasks.\\nBoth LwF and fine-tuning lead to the existing expert that was further trained with LwF or fine-tuning remaining unchanged while also creating a new expert. So, 2 experts are a result of this process (one old, one new).\\nBoth the routing to determine the similarity of an input with the tasks reflected in the existing experts and the task relatedness are determined by an autoencoder mechanism which is independent for each expert (it is trained as the exper', ', one new).\\nBoth the routing to determine the similarity of an input with the tasks reflected in the existing experts and the task relatedness are determined by an autoencoder mechanism which is independent for each expert (it is trained as the expert is trained).\\nThe LwF method seems to be fine-tuning with a twist – instead of only fine-tuning with hard targets from the new data, fine-tune is done by considering the new data and soft targets given by the existing expert.\\nRun through the methodology:\\nThis method is a task-level MoE – it has the advantage of only routing the input sequence once. Since this is done at the beginning of inference, the selected task experts can be pre-loaded to memory and the routing does not need to be performed again, saving on memory costs of loading different experts for every new token. \\nEach task expert consists of the expert itself and an autoencoder, which is used for two things:\\nDetermine the similarity of an input sequence to the task (how well do', 'd again, saving on memory costs of loading different experts for every new token. \\nEach task expert consists of the expert itself and an autoencoder, which is used for two things:\\nDetermine the similarity of an input sequence to the task (how well does the task expert fit into the input sequence).\\nDetermine the task relatedness between different tasks to help training of new experts.\\nTraining new experts can be done in one of two ways:\\nLwF, which uses soft targets of the existing/old model to train a new model based on the new task’s data.\\nFine-tuning, which fine-tunes an existing/old expert with new data, resulting in a new expert.\\nExpert Gate also has the advantage of not all data needing to be stored on the same place at once for training. Since training can be done sequentially, training data can be used and sequentially discarded, saving on storage costs.\\nThe autoencoder is simply a function that deconstructs and attempts to reconstruct the input. The logic is that the closer the ', 'g. Since training can be done sequentially, training data can be used and sequentially discarded, saving on storage costs.\\nThe autoencoder is simply a function that deconstructs and attempts to reconstruct the input. The logic is that the closer the input is to the training data used to train that task’s expert, the better the autoencoder will be at reconstructing the input.\\nIn computing the relatedness between two tasks, how can the reconstruction error of the autoencoder for task k on its own data be computed before the expert (and thus its autoencoder) is trained, since its initialization method relies on this task relatedness computation? This seems redundant.\\n\\n\\nDEMix Layers: Disentangling Domains for Modular Language Modeling\\nMain Idea: DEMix, which stands for domain-expert mixture, is a type of architecture that encourages domain specialization. It looks to train multiple feedforward networks that are each specialized in a specific domain, and similarly to MoE, pick one to run du', ': DEMix, which stands for domain-expert mixture, is a type of architecture that encourages domain specialization. It looks to train multiple feedforward networks that are each specialized in a specific domain, and similarly to MoE, pick one to run during inference, depending on the input space. DEMix layers are modular, meaning they can be mixed, added, removed or used to initialize other layers after initial training. DEMix aims to achieve domain specialization in the sparse layer, while retaining generalization knowledge with shared parameters.\\n\\nMotivation\\nDense training consists of updating all parameters to minimize loss on all the data. This means that it assumes that the model will be able to learn/fit different domains equally. In practice, domains are skewed to domains that are more prevalent in the training data, so models have a hard time generalizing to other domains. Fine-tuning these large dense models can also be expensive and lead to catastrophic forgetting – worsening p', 'actice, domains are skewed to domains that are more prevalent in the training data, so models have a hard time generalizing to other domains. Fine-tuning these large dense models can also be expensive and lead to catastrophic forgetting – worsening performance on pre-training domains not represented in the fine-tuning data – since all weights need to be updated. Finally, managing unwanted behavior in dense models is also a challenge.\\nTo help with these issues, a DEMix (modular) architecture is proposed. That is, an LLM with different components that can be modified during inference.\\n\\nSome Characteristics/Highlights of DEMix\\nDEMix is a substitute for an FF layer in the Transformers architecture (every FF layer is replaced by a DEMix layer) and can be conditioned on the input text in cases where the domain is previously known, as well as when the input domain is not known.\\nThe router used for DEMix is parameter-free and thus not learned, depending on the natural segmentation of the data.', 'nd can be conditioned on the input text in cases where the domain is previously known, as well as when the input domain is not known.\\nThe router used for DEMix is parameter-free and thus not learned, depending on the natural segmentation of the data.\\nParameter-free probabilistic approach to dynamically estimate a weighted mixture of domains during inference, which is used for novel domains (when it is not clear/known in advance where the input is from, or it is from a brand-new domain).\\nMixing (like using top-k > 1) experts is shown to improve performance in novel domains as well as training domains during test time (probably due to overlap between domains that the shared parameters are not enough to capture).\\nThe modularity of DEMix offers flexibility by enabling the removal or addition of new domains at inference, thus allowing the ability of choosing what is forgotten. Catastrophic forgetting is also not an issue since a new domain expert can be initialized or an existing one can be', ' flexibility by enabling the removal or addition of new domains at inference, thus allowing the ability of choosing what is forgotten. Catastrophic forgetting is also not an issue since a new domain expert can be initialized or an existing one can be further specialized without modifying the model’s behavior on other domains.\\n\\nData\\n8 training domains\\n8 testing domains\\nUsed to test robustness of mixing experts to data distribution shifts not seen during training.\\n\\nDEMix vs Traditional MoE\\nWhile in traditional MoE the routing function is learned through training at a token-level, DEMix routing is done at the document (sequence) level and only needs to be performed once per input (all tokens in an input sequence are routed the same way).\\nToken-level routing has been shown to specialize experts in token-level areas, such as semantics and syntax. Document-level routing should enable experts to specialize in specific tasks/domains.\\nBecause of this characteristic in specializing in domains co', '-level routing has been shown to specialize experts in token-level areas, such as semantics and syntax. Document-level routing should enable experts to specialize in specific tasks/domains.\\nBecause of this characteristic in specializing in domains compared to semantics, the experts are more flexible in terms of addiction and subtraction to the network and provide an ease of interpretation that traditional MoEs don’t have (they are more of a black box).\\n\\nTraining\\nDuring training, each domain expert is assigned to a single GPU (similarly to how it is done in traditional MoE).\\nEach mini batch sends k domain examples to each expert (a balanced load is easy to achieve since we know each input’s domain for training).\\nDistributed data parallelism is used (expert is replicated through the number of GPUs available for that specific expert, since there were more GPUs available than experts)\\nThis is efficient because only globally shared parameters are synced through all GPUs, while domain expert', 'is used (expert is replicated through the number of GPUs available for that specific expert, since there were more GPUs available than experts)\\nThis is efficient because only globally shared parameters are synced through all GPUs, while domain expert parameters are only synced between the GPUs assigned to that expert.\\nReduced communication costs due to a decrease in alltoall computations.\\n\\nEvaluation\\nIn-domain performance\\n4 variations used:\\nDENSE – regular dense model with no conditioning on domain.\\nDENSE (balanced) – dense model with equal amount of data used for each domain.\\n+DOMAIN-TOKEN – variation of DENSE (balanced) with a prepended token on every input sequence to indicate its domain.\\nThe motivation behind this is to add info about the domain of the input to the context to try to create a dense oracle gate.\\nDEMix – DEMix architecture with known domain for each input.\\nUses top-1 routing for in-domain experts based on the already known domain of the input.\\nAdding domain info (DENS', 'he domain of the input to the context to try to create a dense oracle gate.\\nDEMix – DEMix architecture with known domain for each input.\\nUses top-1 routing for in-domain experts based on the already known domain of the input.\\nAdding domain info (DENSE (balanced) and +DOMAIN-TOKEN) is shown to help the dense baseline.\\nThe smaller the model, the more helpful this is.\\nHeterogeneous domains (diverse domains like WEBTEXT and REDDIT) have more overlap with other training domains, and thus don’t really benefit from DEMix vs a dense baseline.\\nUnknown domain performance – mixing experts at inference time.\\nRouting approach\\nIn practice, the domain of an input is not always known. In this case, it makes more sense to use a soft choice for routing (top-2 routing), as it was proposed for cases where the domain was known.\\nTo not increase training costs with a learned routing approach (more communication costs), a probabilistic routing score based on Bayes’ Rule was used (this is parameter-free).\\nProb', '2 routing), as it was proposed for cases where the domain was known.\\nTo not increase training costs with a learned routing approach (more communication costs), a probabilistic routing score based on Bayes’ Rule was used (this is parameter-free).\\nProbabilistic Routing Score:\\nThe main part of this is calculating the domain posterior – the probability that the input is from a certain domain d.\\nThis approach is very inefficient (the input needs to go through each expert, so the routing is useless in practice) and is improved in future work.\\nThey propose 3 variations on the posterior calculation:\\nUniform - each domain is estimated to be equally likely.\\nUpdating - weighted moving average of the posteriors from the previous timesteps.\\nCached – fixed prior estimated from the test data (100 test sequences used)\\nThe estimates of posteriors for both the training and the novel domains is shown to be sparse, justifying the top-1 and top-2 routing selections (so not all experts need to be used, aka ', 'ed prior estimated from the test data (100 test sequences used)\\nThe estimates of posteriors for both the training and the novel domains is shown to be sparse, justifying the top-1 and top-2 routing selections (so not all experts need to be used, aka sparsity is justified).\\nEnsembling DEMix experts (mixing) using the cached approach performs better than all models analyzed.\\nCompared to DENSE, this is beneficial at smaller scales, while the dense models can catch up as the parameter count increases.\\nPerhaps more data is needed when increasing the DEMix parameters?\\nEnsembling/mixing is also shown to lead to improvements on training domains, especially more heterogeneous ones (more diverse domains).\\n\\nDEMix-DAPT\\nDEMix-DAPT consists of adopting existing experts to new domains.\\nPreviously, experiments were made to evaluate the performance of DEMix in novel domains (domains not seen during training). DEMix-DAPT is different in the sense that it applies new domain data to existing domain expert', 'xisting experts to new domains.\\nPreviously, experiments were made to evaluate the performance of DEMix in novel domains (domains not seen during training). DEMix-DAPT is different in the sense that it applies new domain data to existing domain experts to create a new expert.\\nThe new expert is initialized with the parameters of the closest existing domain expert. So, the new expert is a fine-tuned version of an existing domain expert.\\nHow close each domain expert is from each other is calculated from the router’s domain posterior.\\nIn DEMix-DAPT, only the expert parameters are trained. Shared parameters are kept frozen.\\nFor inference, the cached posterior approach is taken.\\nResults (DEMix-DAPT)\\nDEMix-DAPT is compared to Dense-DAPT, which is a dense version of adapting to a new domain.\\nAs expected, it is shown that Dense-DAPT suffers from the issue of catastrophic forgetting. This is apparent when seeing how training a Dense-DAPT model in a novel domain leads to degraded performance on (o', ' dense version of adapting to a new domain.\\nAs expected, it is shown that Dense-DAPT suffers from the issue of catastrophic forgetting. This is apparent when seeing how training a Dense-DAPT model in a novel domain leads to degraded performance on (original) training domains.\\nAs expected, adding experts through DEMix-DAPT significantly improves performance on those novel domains.\\n\\nIn this paper, it was also shown how removing an expert from an unwanted domain (for example, due to hate speech or leaking of private data), leads to similar performance on that domain compared to DEMix models not trained on that domain. This shows that expert domains can be removed from DEMix, if desirable. This also shows that most domain specialization comes from the DEMix layers.\\n\\n\\nBranch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models\\nMain Idea: This paper serves as a continuation to DEMix, focusing more on the aspect of employing techniques to train these modular models more eff', 'rom the DEMix layers.\\n\\n\\nBranch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models\\nMain Idea: This paper serves as a continuation to DEMix, focusing more on the aspect of employing techniques to train these modular models more efficiently. Due to the modularity of DEMix, Branch-Train-Merge (BTM) shows that it is possible to train these domain experts independently, saving on multi-node synchronization costs commonly required in the training of LLMs. BTM also explores scaling up the number of experts to 64 (DEMix only trained up to 16).\\nBTM trains an ELMForest (Expert Language Models for Efficient Sparse Training), which are embarrassingly parallel, that is, different parts of the model are independently trained on different subsets of the data, with no need for multi-node training or inference.\\nEach ELM is specialized in a different domain with no shared parameters (contrary to DEMix).\\nELMs can be added or removed to the model at any time, or parameter-averaged to c', 'ent subsets of the data, with no need for multi-node training or inference.\\nEach ELM is specialized in a different domain with no shared parameters (contrary to DEMix).\\nELMs can be added or removed to the model at any time, or parameter-averaged to collapse back into a single LM.\\n\\nBranch-Train-Merge Algorithm\\nThe BTM algorithm consists of repeatedly expanding the ELMForest (combination of experts) by adding experts in an embarrassingly parallel manner. There are two possible scenarios: when we are first building the forest (creating the first expert) and when we already have at least one expert created, which makes the process of initializing other experts easier.\\nThe addition of a new expert is done by:\\nBranch – initializing a new LM with an average of the parameters of the most relevant of the currently existing experts.\\nTrain – train this recently initialized expert on new domain data.\\nMerge – merge the trained expert into the ELMForest.\\nThe first step (branch) needs to be done in a', 'an average of the parameters of the most relevant of the currently existing experts.\\nTrain – train this recently initialized expert on new domain data.\\nMerge – merge the trained expert into the ELMForest.\\nThe first step (branch) needs to be done in a different manner when training the first expert since there are no experts to initialize this expert to. The training of the initial expert is done by training on heterogeneous (diverse) data.\\nThis approach is shown to outperform dense and DEMix when used as an ensemble or when parameter-averaging the weights of the experts. This shows that there are inherent gains from training using the BTM approach.\\nOverall, BTM shows an efficient way of scaling LLMs without having to train extremely large models. Instead, an ensemble of domain experts, or even a parameter-average, outperforms the dense version. (the models were compared based on GPU training time; the parameter-averaged model is also compute-matched to dense).\\nIn this work, the domains', 'odels. Instead, an ensemble of domain experts, or even a parameter-average, outperforms the dense version. (the models were compared based on GPU training time; the parameter-averaged model is also compute-matched to dense).\\nIn this work, the domains are defined by provenance (source). This is suboptimal and improved in later work.\\nLike DEMix, BTM has the advantages of fully adding and removing experts, if desired. Since each expert is trained on their own specific data split and there are no parameters shared, this means that removing an expert will lead to complete removal of that domain from the model. The only caveat is if other domain experts were initialized from an undesired domain. In this scenario, simply removing the undesired domain may not be sufficient.\\n\\nEnsembling and Averaging ELMs\\nEnsembling and averaging ELMs keeps the inference cost constant regardless of the number of experts added.\\nEnsembling leads to higher inference costs (due to multiple expert results needed), h', 'ain may not be sufficient.\\n\\nEnsembling and Averaging ELMs\\nEnsembling and averaging ELMs keeps the inference cost constant regardless of the number of experts added.\\nEnsembling leads to higher inference costs (due to multiple expert results needed), however, results show that top-k routing should be possible.\\nThe expert routing (for top-k) or score for parameter-averaging are done through the same domain posterior method from DEMix (with a cached prior, more specifically).\\n\\nBTM Approach (in more detail)\\nBTM can be done asynchronously, that is, multiple new ELMs can be trained in parallel. This can be thought of as having multiple BTM training rounds, each initializing its new experts based on the existing experts at the beginning of the training round.\\nStep 0\\nThe first ELM needs to be initialized differently, since there are no existing ELMs yet to obtain parameters to initialize an expert from.\\nFor this, an initial ELM is trained on heterogeneous (diverse) data.\\nOnce this initial ELM i', 'ning round.\\nStep 0\\nThe first ELM needs to be initialized differently, since there are no existing ELMs yet to obtain parameters to initialize an expert from.\\nFor this, an initial ELM is trained on heterogeneous (diverse) data.\\nOnce this initial ELM is trained, its parameters can be used to initialize the weights of the first batch of the ELMs.\\nBranch\\nRefers to adding a new ELM (Expert Language Model).\\nIdea is to initialize the new ELM to be a parameter-average of the current ELMForest (all existing domain experts).\\nThe best approach for initialization was to perform a weighted average of existing ELM parameters based on their domain posterior or the new domain data (finding the closest domains to the new domain and only use the parameters of the most relevant experts for this new domain).\\nTrain\\nAfter initializing the weights of the new ELM (branching), the ELM is trained independently on its domain data.\\nMerge\\nOnce the new ELM is fully trained on its domain data, it can be added to the', 'f the most relevant experts for this new domain).\\nTrain\\nAfter initializing the weights of the new ELM (branching), the ELM is trained independently on its domain data.\\nMerge\\nOnce the new ELM is fully trained on its domain data, it can be added to the ELMForest.\\nIt would make sense that the more ELMs exist, the less time new ELMs need to be trained for, since more ELMs means more specialized ELMs, and that the data distribution of the new domain will probably be closer to the distribution of existing domains (since there are more domains to pick from).\\n\\nInitial Results\\nSetup\\nELMForest trained on 8 domains, one trained at step 0 and the remaining 7 were trained in parallel from the initial domain (only one BTM cycle done).\\nModels compared at a compute-matched basis at training.\\n3 models used:\\nDense Transformer - where the data from each domain is balanced.\\nDEMix – domain specialized layer (domain-level MoE).\\nELMForest – full domain models (ELMs).\\nELMForest provides the best performance o', 'a compute-matched basis at training.\\n3 models used:\\nDense Transformer - where the data from each domain is balanced.\\nDEMix – domain specialized layer (domain-level MoE).\\nELMForest – full domain models (ELMs).\\nELMForest provides the best performance on all sizes (up to 1.3B dense), and these hold with scale.\\nHowever, a full ELMForest ensemble has an increased inference cost.\\nELMForest provides speedups during training (more updates per second).\\nThis is justified by the reduction in cross-GPU communication for parameter synchronization (no alltoall operations needed).\\nTo match inference costs with dense, the ELMForest weights can be averaged. This is experimented through 3 strategies:\\nUniform – each ELM is given the same weight.\\nArgmax – use only the ELM that is closer to the target data, equivalent to top-k with k=1.\\nPosterior – weighted average between all domains based on the domain posterior score.\\nUniform performs worse than all other strategies, even dense.\\nArgmax performs better t', 'only the ELM that is closer to the target data, equivalent to top-k with k=1.\\nPosterior – weighted average between all domains based on the domain posterior score.\\nUniform performs worse than all other strategies, even dense.\\nArgmax performs better than dense in training domains, but worse in evaluation domains.\\nThis is expected since evaluation domains (out-of-domain performance) benefit more from using shared knowledge/parameters.\\nPosterior performs better than all strategies (including dense) except for the smallest model (dense is the best in that scenario).\\nWith enough training, Posterior top-k can outperform dense at the 125M scale.\\nEven though Posterior parameter-averaging is promising due to improved performance over dense at the same training and inference cost, a full ensemble still provides the best results.\\nThe significantly reduced inference cost from Posterior parameter-averaging makes this much more practical.\\n\\nFurther Analysis\\nAblations are made to compare the tradition', 'me training and inference cost, a full ensemble still provides the best results.\\nThe significantly reduced inference cost from Posterior parameter-averaging makes this much more practical.\\n\\nFurther Analysis\\nAblations are made to compare the traditional BTM model with:\\nA random ensemble - same setup but each ELM is trained on a random data split, not on a specific domain. This results in an ensemble of general experts instead of an ensemble of specialized experts.\\nAn ELMForest where all ELMs are randomly initialized. This should take away the effect of optimizing the initialization of new experts.\\nThese 2 variations led to worse performances, so the ELMForest performance is not simply the result of ensembling parameters.\\nAblations were done to decide on how much compute should be given to the seed training (step 0) – these ablations explain and fix the underperformance of ELMForest compared to dense at the 125M scale:\\nIn the initial setup used (8 training domains), the optimal amount of', 'to decide on how much compute should be given to the seed training (step 0) – these ablations explain and fix the underperformance of ELMForest compared to dense at the 125M scale:\\nIn the initial setup used (8 training domains), the optimal amount of deed training, in relation to the total training budget, was from 40%-60%.\\nFor the parameter-averaging approach, the ideal is 60%-70%, and randomly initialized ELMs (0% seed training) do not work well at all (they perform very poorly) in this setup.\\nAlthough not optimal, reducing seed training down to 10% of the total budget results in gains over dense and randomly initialized ELMs.\\nThis shows that ELMForest performance is robust to a wide range of seed LM training compute allocations.\\nMore seed training is especially useful for evaluation domains (out-of-domain performance).\\nFurther ablations were done using different datasets for seed training (using a 50% compute allocation to seed training).\\nThe more heterogeneous (diverse) the seed da', 'd training is especially useful for evaluation domains (out-of-domain performance).\\nFurther ablations were done using different datasets for seed training (using a 50% compute allocation to seed training).\\nThe more heterogeneous (diverse) the seed data is, the better.\\nHowever, performance is robust to the choice of seed training corpus.\\nEven using only JavaScript code for seed training led to better performance than dense.\\nRemoval of unwanted ELM domains is also robust to the seed training corpus.\\nPerformance on removed domains degrades significantly when such domain is removed.\\n\\nScaling the ELMForest to 64 Domains\\n64 domains used for training and 16 for evaluation (80 total).\\n4 BTM cycles are done, 16 training domains for each cycle/batch.\\nThe dense Transformer used for comparison:\\n1.3B parameter model.\\nTrained for 6144 total GPU hours (using 128 GPUs).\\nThe 64-domain ELMForest:\\nUses seed training of 75%.\\n4 GPUs per ELM (4x16 = 64 GPUs used concurrently).\\nFor BTM cycles/batches 2 and 3', '\\nThe dense Transformer used for comparison:\\n1.3B parameter model.\\nTrained for 6144 total GPU hours (using 128 GPUs).\\nThe 64-domain ELMForest:\\nUses seed training of 75%.\\n4 GPUs per ELM (4x16 = 64 GPUs used concurrently).\\nFor BTM cycles/batches 2 and 3, 40 GPU hours were used for each domain, and for batch 4 20 GPU hours per domain were used.\\nThe total training cost of training this 64-domain ELMForest was 2565 hours, significantly lower than training the dense model.\\nUsing only 40% of the dense Transformer’s computational budget for training, the ELM full ensemble (not parameter-averaged) performs comparably to the Transformer (although at an increased inference cost).\\nELMForest is especially better for training domains since the parameters for each training domain is not updated and thus not forgotten.\\nAnalysis shows that sparsity in the ELMForest posterior, suggesting a top-k approach can be taken to reduce inference costs:\\nTop-8 routing performs similarly to a full ensemble. \\nThis me', 'for each training domain is not updated and thus not forgotten.\\nAnalysis shows that sparsity in the ELMForest posterior, suggesting a top-k approach can be taken to reduce inference costs:\\nTop-8 routing performs similarly to a full ensemble. \\nThis means that the ensemble can be reduced to 8 experts chosen at inference without a loss in quality.\\nTop-1 routing still performs better than the dense Transformer if the Transformer was given the same amount of training.\\nParameter-averaging performs significantly better than top-1, and almost as well as top-2 (top-2 has double the inference costs).\\nAnywhere from averaging the parameters to condense them into a single LM or using top-2 to top-8 routing seems optimal, depending on the compute available.\\n\\nMy takeaways:\\nFuture research can include:\\nHow to improve the weights taken for parameter averaging of the ELMForest?\\nThere is a hot area of research, so different techniques exist.\\nBest practices for scaling and coordinating the training of ELM', 'le.\\n\\nMy takeaways:\\nFuture research can include:\\nHow to improve the weights taken for parameter averaging of the ELMForest?\\nThere is a hot area of research, so different techniques exist.\\nBest practices for scaling and coordinating the training of ELMForests.\\nCombining ELMForests with adapters to scale into smaller domains.\\nUnsupervised domain assignment.\\n\\nA small sampling of training data is required for calculating the domain posterior. Unsupervised assignment would get rid of this.\\nTopic of the next research paper that gives continuation from the research work by the University of Washington – “Scaling Expert Language Models with Unsupervised Domain Discovery”.\\nRecipes for leveraging ELMForests for user safety.\\n\\n\\nScaling Expert Language Models with Unsupervised Domain Discovery\\nMain Idea: this research work picks up where BTM left off, adding a caveat to the framework – instead of classifying domains based on provenance (source), this is done in an unsupervised manner, assigning doma', 'odels with Unsupervised Domain Discovery\\nMain Idea: this research work picks up where BTM left off, adding a caveat to the framework – instead of classifying domains based on provenance (source), this is done in an unsupervised manner, assigning domain data based on clusters. The new framework is named c-BTM (cluster Branch-Train-Merge), and it still holds the embarrassingly parallel characteristic of the original BTM.\\n\\nPros of Unsupervised vs Provenance-based Domain Classification\\nNot all datasets are able to be grouped based on provenance (like internet crawls).\\nGroups created by provenance cannot be easily merged or divided, so one ELM is needed for each group. This is not flexible in terms of adjusting the size and number of ELMs.\\nInstead of a domain posterior routing approach, which comes with many disadvantages, routing in c-BTM is done based on the distance of a document’s vector to a cluster’s center, a simpler and more effective approach for routing.\\n\\nc-BTM vs MoE\\nc-BTM routes', 'ad of a domain posterior routing approach, which comes with many disadvantages, routing in c-BTM is done based on the distance of a document’s vector to a cluster’s center, a simpler and more effective approach for routing.\\n\\nc-BTM vs MoE\\nc-BTM routes sequences instead of tokens. This allows for different specialization in domains/tasks instead of specializing in semantics/syntax because of the routing being done at a sentence/document level, not at a token level.\\nc-BTM uses offline balanced clustering (size of the clusters can be adjusted to achieve load balancing) compared to online load balancing.\\nc-BTM has no shared parameters, which leads to savings in communication costs and results in a highly efficient framework for training domain experts.\\nc-BTM has more interpretable experts.\\nOBS: c-BTM is directly compared to sparse upcycling, which mirrors how c-BTM initializes experts but instead of training ELMs, sparse upcycling substitutes the feedforward networks in the dense checkpoint', 'xperts.\\nc-BTM has more interpretable experts.\\nOBS: c-BTM is directly compared to sparse upcycling, which mirrors how c-BTM initializes experts but instead of training ELMs, sparse upcycling substitutes the feedforward networks in the dense checkpoints by MoE layers.\\n\\nc-BTM Algorithm\\nOBS: this paper only focuses on using 1 iteration/cycle for c-BTM, meaning training all ELMs from the seed ELM (no cycles trained based on existing specialized ELMs.\\nStep 0: Cluster\\nK-means clustering, with enforced balanced clusters (during training, not inference), is used during training.\\nTf-idf is used since it was the best performing embedding approach.\\nStep 1: Branch (from seed LM)\\nThe seed LM is trained on a diverse corpus – experiments can be found at the BTM paper.\\nDone the same way as in BTM.\\nStep 2: Train\\nDone the same way as in BTM.\\nStep 3: Merge\\nDone the same way as in BTM.\\n\\nInference\\nSparse ensemble of outputs of existing ELMs - router chooses top-k ELMs and a weighted average of those ELMs is', 'e BTM paper.\\nDone the same way as in BTM.\\nStep 2: Train\\nDone the same way as in BTM.\\nStep 3: Merge\\nDone the same way as in BTM.\\n\\nInference\\nSparse ensemble of outputs of existing ELMs - router chooses top-k ELMs and a weighted average of those ELMs is used.\\nThe router is fixed at inference and does not need to be updated after training.\\n\\nExperimental Setup\\nOPT is used as the seed LM (the dense checkpoint).\\nBoth the 1.3B and the 6.7B versions of OPT were experimented with.\\nK between 2 and 128 for k-means clustering was experimented with to evaluate the optimal number of ELMs.\\nDropout of 0.1 is used for all layers except the embedding layer.\\nUsing only the second half of a document from the embedding-based routing is shown to not result in a performance drop while leading to faster inference.\\nModels are compared against each other in a compute basis. Using total training parameters is said to be misleading for sparse models, so it is not used.\\nTotal GPU-time is used to evaluate training e', 'erformance drop while leading to faster inference.\\nModels are compared against each other in a compute basis. Using total training parameters is said to be misleading for sparse models, so it is not used.\\nTotal GPU-time is used to evaluate training efficiency.\\nGPU-time needed for inference and latency for end-users are used to evaluate inference efficiency.\\n\\nResults\\nControlling for total training tokens:\\nUsing a single cluster (dense) is always the worst setup.\\nIncreasing cluster count in c-BTM improves language modeling performance for a fixed compute budget (up to 16 clusters experimented with).\\nThe advantage of c-BTM only increases with an increase in the number of total training tokens.\\nThere is a range of optimum number of training clusters and this increases with an increase in total training tokens.\\nIt is cheaper to train on more clusters in parallel, so there could be a tradeoff there.\\nThis is consistent to scaling up the size of each ELM.\\nComparing training time:\\nDue to c-BTM ', 'sters and this increases with an increase in total training tokens.\\nIt is cheaper to train on more clusters in parallel, so there could be a tradeoff there.\\nThis is consistent to scaling up the size of each ELM.\\nComparing training time:\\nDue to c-BTM models with higher cluster counts using fewer GPUs per expert under a fixed budget and having no communication costs between experts, c-BTM models with more clusters can be exposed to more data for the same amount of time as dense models.\\nThe more clusters, the faster the training updates.\\nTraining on more clusters, due to the small number of GPUs per ELM and the fact that no communication is needed between ELMs, results in a much more robust training setup to GPU failure.\\nModels trained with more clusters have faster updates as we increase the total compute.\\nOpposite is true for MoE due to communication costs between experts.\\nControlling for inference costs via parameter count:\\nThe largest training budget used was 168B tokens.\\nc-BTM with t', 're clusters have faster updates as we increase the total compute.\\nOpposite is true for MoE due to communication costs between experts.\\nControlling for inference costs via parameter count:\\nThe largest training budget used was 168B tokens.\\nc-BTM with top-1 routing (same inference cost as dense) outperforms dense.\\nThe more clusters, the better the top-1 routing performance.\\nTop-2 to top-4 routing (of c-BTM) pretty much matches the performance of a full c-BTM ensemble (better parameter efficiency than regular c-BTM, which was top-2 to top-8).\\nTop-2 to top-4 routing sometimes even perform better than a full ensemble.\\nThese conclusions hold true even when scaling the ELM count to large values (128).\\nComparing to a larger dense model:\\n6.7B parameter dense model vs 1.3B parameter ELM c-BTM with 16 clusters and top-4 routing (5.2B inference cost) (1.3B latency cost – since the parameters of the ELMs can be run in embarrassingly parallel fashion).\\nc-BTM has a 3.5x speedup in training (using 168B', 'er dense model vs 1.3B parameter ELM c-BTM with 16 clusters and top-4 routing (5.2B inference cost) (1.3B latency cost – since the parameters of the ELMs can be run in embarrassingly parallel fashion).\\nc-BTM has a 3.5x speedup in training (using 168B training tokens).\\nDownstream Task Results (few-shot results)\\n6 classification tasks experimented with; 8-shot evaluations used. c-BTM trained on the C4 dataset.\\n16-cluster c-BTM always outperforms its 1-cluster version (1.3B ELM size).\\nComparing to MoE (sparse upcycling aka MoE from a dense checkpoint)\\nSparse upcycling was shown to be unstable with a high number of experts (64 and 128). With 32 experts, it was shown to have stable training and perform better than the higher expert-count stable runs.\\nBased on this, the sparse upcycling model used here was a 32-expert MoE with top-2 routing.\\nMoE has more expensive training due to top-2 routing.\\nShared parameters need to communicate with each other, resulting in slower training.\\nSparse upcycl', 'runs.\\nBased on this, the sparse upcycling model used here was a 32-expert MoE with top-2 routing.\\nMoE has more expensive training due to top-2 routing.\\nShared parameters need to communicate with each other, resulting in slower training.\\nSparse upcycling performs better at small compute budgets but performs much worse at larger budgets, even performing worse than dense models.\\nAuthors speculate this could be due to distribution shifts after pretraining, which might increase the instability of sparse upcycling.\\n\\nFinal Analysis\\nClustering is important as random clustering underperforms it significantly.\\nThe load balancing constraint in k-means is shown to be useful.\\nThis becomes more important with an increase in the number of clusters.\\nUsing the tf-idf approach, an analysis of important terms in clusters point to cluster specialization. Further analysis also shows that ELMs successfully specialize in their own cluster.\\nIt is shown that metadata may not correspond to the most optimal segm', 'the tf-idf approach, an analysis of important terms in clusters point to cluster specialization. Further analysis also shows that ELMs successfully specialize in their own cluster.\\nIt is shown that metadata may not correspond to the most optimal segmentation of the corpus (although it is more interpretable).\\nSince c-BTM performs better than regular BTM, with the only significant change being how the segmentation of data is done.\\nFuture research may investigate improving the technique to merge model weights (as this is a hot area of research).\\n\\nMy takeaways:\\nRegarding the relatively low dropout used for the training of ELMs, I believe that these are more robust to overfitting than traditional MoEs due to ELMs being full networks, and thus having more parameters than a single expert FF. \\nThe fact that k-means has a constraint to ensure the loads are balanced between ELMs at training time might also help with overfitting.\\nOn a similar note, ELMs seem to benefit from larger batch sizes. Th', 'ving more parameters than a single expert FF. \\nThe fact that k-means has a constraint to ensure the loads are balanced between ELMs at training time might also help with overfitting.\\nOn a similar note, ELMs seem to benefit from larger batch sizes. This is also a sign that ELMs would be more robust to overfitting since the opposite is true for MoEs.\\nLarger batch sizes = more accurate updates (less noise) = less regularization effect.\\n\\n\\nExploring the Benefits of Training Expert Language Models over Instruction Tuning\\nMain Idea: this paper looks to explore the author’s discovery that training an expert LM fine-tuned on a single task can outperform a multi-task (MT) LM trained on hundreds of tasks (more specifically regarding multi-task performance). This goes against other findings that claim that scaling the number of tasks in MT-LMs is key to making them have stronger performance. Referring to LMs fine-tuned on a single task means a system of multiple Expert Language Models (ELMs), each', 'ance). This goes against other findings that claim that scaling the number of tasks in MT-LMs is key to making them have stronger performance. Referring to LMs fine-tuned on a single task means a system of multiple Expert Language Models (ELMs), each fine-tuned on a single task, not a single LM trained on a single task.\\nOBS: Instruction-tuning -> fine-tuning LMs with instructions (prompts).\\n\\nELM Framework\\nTraining experts – two types of experts are trained:\\nPrompt Experts\\nTrained via PEFT through an adapter (an adapter layer is trained on top of the pre-trained LLM, with the pre-trained LLM’s weights kept frozen).\\nTrained to perform well on a single prompt specific to the task.\\nDataset Experts\\nTrained via regular fine-tuning of the pre-trained LLM’s weights on a single task (all weights are updated).\\nIdea is to train an expert that will perform well to different prompts, so it can merge with other experts.\\nRouting mechanism – Retrieval-of-Experts\\nConsists of constructing an Expert Libr', 'ned LLM’s weights on a single task (all weights are updated).\\nIdea is to train an expert that will perform well to different prompts, so it can merge with other experts.\\nRouting mechanism – Retrieval-of-Experts\\nConsists of constructing an Expert Library and training a dense retriever.\\nEach row in the Expert Library corresponds to an expert and consists of keys of S random training instances of that expert and a corresponding expert id.\\nS used was 100.\\nThe dense retriever is a Sentence Transformer, and it also assumes that Q examples of the target task are available. It takes the embeddings of the input task and chooses the most relevant expert(s) based on each expert’s similarity to this input task (based on the training instances stored for each expert and the target task instances).\\nQ used was 32.\\nMerging of experts\\nThe merging of Dataset Experts is also explored, retrieving more than one expert for an unseen task.\\nMerging does not make sense with Prompt Experts, since they were trai', 'r each expert and the target task instances).\\nQ used was 32.\\nMerging of experts\\nThe merging of Dataset Experts is also explored, retrieving more than one expert for an unseen task.\\nMerging does not make sense with Prompt Experts, since they were trained to perform well on a single prompt, therefore they would not be performant at this setting (at merging).\\nThe merged LM ends up being created at the parameter level. It is a weighted-average (parameter-average) of the selected experts.\\nSince the parameters are merged, the inference cost will be the same as the inference cost of the single MT-LM trained on hundreds of tasks.\\n\\nExperimental Setup\\n296 Prompt Experts, 36 Dataset Experts (on around 8 prompts each) trained.\\n50,000 samples used for training each classification task. 10,000 for each generative task.\\nOn top of the pre-trained T5 model.\\n5 epochs used for training with a constant learning rate of 1e-4.\\nRouge-L score used for evaluating generative tasks.\\nResults – Prompt Experts\\nA si', 'raining each classification task. 10,000 for each generative task.\\nOn top of the pre-trained T5 model.\\n5 epochs used for training with a constant learning rate of 1e-4.\\nRouge-L score used for evaluating generative tasks.\\nResults – Prompt Experts\\nA single Prompt Expert significantly outperforms its dense MT-LM baseline (trained on hundreds of tasks).\\nThe single Prompt Expert that achieved this was trained on CosmosQA.\\nPerhaps this means that the dataset being diverse is more important than the number of tasks trained?\\nThe Retrieval-of-Experts (ROE) method with an oracle gate significantly outperforms all other models, including T0-11B (the base LLM used for the adapters was T5-3B) and GPT-3.\\nThis shows that improving the retrieval method is a promising area of future research.\\nA simple ROE approach outperforms T0-3B (the MT-LM baseline) on classification tasks, but not on generative tasks.\\nA better ROE method reverses this.\\nUsing more diverse data (in quantity) seems to help seems to he', 'a promising area of future research.\\nA simple ROE approach outperforms T0-3B (the MT-LM baseline) on classification tasks, but not on generative tasks.\\nA better ROE method reverses this.\\nUsing more diverse data (in quantity) seems to help seems to help generative tasks (perhaps due to the higher complexity in text generation compared to classification?).\\nResults – Dataset Experts\\nThere was negative task transfer when merging the adapter experts (Prompt Experts).\\nMerging Prompt Experts results in worse performance – does not work.\\nMerging the fully fine-tuned experts (Dataset Experts) resulted in positive task transfer.\\nMerging resulted in improved performance (merged capabilities > individual capabilities).\\nThe 3 datasets that show the best performance on unseen tasks (when training on a single task) are all commonsense reasoning datasets (for both merging and not merging).\\nPoints to models trained on commonsense reasoning having higher generalization abilities to unseen tasks – common', ' performance on unseen tasks (when training on a single task) are all commonsense reasoning datasets (for both merging and not merging).\\nPoints to models trained on commonsense reasoning having higher generalization abilities to unseen tasks – commonsense reasoning data is higher quality data.\\nRetrieval of the correct expert(s) seems important as the best expert on unseen generative tasks performed poorly on unseen classification tasks.\\n\\nBenefits of Expert LMs over MT-LMs\\nELMs are less susceptible to negative task transfer on seen tasks (the tasks used for training).\\nELMs have continual learning abilities on new tasks without needing access to all the data at the same time.\\nELMs allow for merging experts on compositional instructions (merging of task prompts).\\nLimitations of ELMs over MT-LMs\\nThe method explored assumes a batch of the target task is available for RoE, which is not always a realistic assumption.\\nMT-LMs bigger than 11B parameters, which might not suffer from negative task', 'ng of task prompts).\\nLimitations of ELMs over MT-LMs\\nThe method explored assumes a batch of the target task is available for RoE, which is not always a realistic assumption.\\nMT-LMs bigger than 11B parameters, which might not suffer from negative task transfer due to increased capacity, were not analyzed.\\nFor some tasks, merging experts on compositional instructions may not be so simple.\\n\\nMy takeaways:\\nA system of ELMs outperforming a single LM in a multi-task setting seems to show that the benefits of specialization outweigh the benefits of shared knowledge between tasks.\\nAn ELM system also allows for choosing an expert trained on a task that resembles the target data – ensemble of closely-related experts sounds, in theory, better than a single LM fine-tuned on multiple tasks (that could be both relevant and irrelevant to the target task).\\nMore exploration is needed in the Retrieval-of-Experts (routing mechanism used) to alleviate the constraint of having training and target instances ', 'ngle LM fine-tuned on multiple tasks (that could be both relevant and irrelevant to the target task).\\nMore exploration is needed in the Retrieval-of-Experts (routing mechanism used) to alleviate the constraint of having training and target instances stored, as well as to appropriate it to scenarios where we do not have examples of the target task available since this task would be unknown.\\n\\n\\nAlternative Approaches\\nBASE Layers: Simplifying Training of Large, Sparse Models\\nMain Idea: introduces a new routing approach that approaches the problem as a linear assignment. This ensures load balancing without the need for auxiliary losses or adjusting CF. BASE also shows that a single expert/MoE layer can be effective.\\nMakes use of top-1 routing like Switch.\\nThe linear assignment problem is designed to maximize token-expert affinities and has the constraint of balanced loads.\\nBASE Algorithm\\nCompute token-expert score for all experts.\\nSolve the linear assignment problem.\\nGoal - Maximize token-e', 'ke Switch.\\nThe linear assignment problem is designed to maximize token-expert affinities and has the constraint of balanced loads.\\nBASE Algorithm\\nCompute token-expert score for all experts.\\nSolve the linear assignment problem.\\nGoal - Maximize token-expert affinity.\\nConstraint – ensure balanced loads to experts at a batch-level.\\nRoute tokens to experts.\\nCompute the expert scores as a weighted sum based on the routing weights.\\nTop-2 routing is used at training.\\nReturn the output to the original worker.\\nThis approach is only used during training, as during test time the strategy of top-1 routing without load balancing is taken.\\n\\nResults\\nHaving a single BASE layer in the network can be effective.\\nExpert layers are robust to changes in the expert-shared parameters ratio and the position(s) of the layer in the network.\\nExploration of which inputs are assigned to each expert shows the same specialization patterns of other works: experts specialize on simple input patterns related to semantics', 't-shared parameters ratio and the position(s) of the layer in the network.\\nExploration of which inputs are assigned to each expert shows the same specialization patterns of other works: experts specialize on simple input patterns related to semantics and syntax.\\n\\nDSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning\\nMain Idea: sparse gates commonly used for MoE, like top-k, are not smooth (continuously differentiable), which can lead to performance issues in gradient-based methods. Dselect-k presents a fully differentiable sparse gate for MoE.\\nDifferentiability – a function that is differentiable is a function which has a defined derivative at every point. This is a requirement for gradient-based methods.\\nContinuous Differentiability – a function is continuously differentiable if it is fully differentiable AND the corresponding derivative is continuous. Continuous meaning smooth, with no abrupt changes and bumps. Continuous differentiabi', 'ased methods.\\nContinuous Differentiability – a function is continuously differentiable if it is fully differentiable AND the corresponding derivative is continuous. Continuous meaning smooth, with no abrupt changes and bumps. Continuous differentiability is not a requirement but optimizes performance of gradient-based methods.\\nTop-k routing is not continuously differentiable due to the router’s hard selection of experts. This hard routing leads to it being possible for small changes in the input score to have large changes in the expert weights, which is not ideal.\\nDselect-k achieves continuous differentiability through smoothing techniques.\\n\\nMy takeaways:\\nAlthough Dselect-k in theory should perform better than top-k, this technique has not been applied much in practice. This can be attributed to the increased computational complexity it brings, as well as to the simplicity and proven practical use of top-k.\\nSome other recently proposed continuous differentiability methods for MoE rout', ' been applied much in practice. This can be attributed to the increased computational complexity it brings, as well as to the simplicity and proven practical use of top-k.\\nSome other recently proposed continuous differentiability methods for MoE routing, check soft MoE (optimized for vision) and Mixture-of-Tokens (optimized for text generation).\\n\\nHash Layers for Large Sparse Models\\nMain Idea: this paper experiments with using a hash router in an MoE architecture and compares it to other methods like top-k (Switch) and BASE. \\n\\nHashing\\nHashing refers to using a function that converts the input into a fixed size output that is unique for each input. \\nHashing does not need to be learned and there is no need for a load balancing loss.\\nIn a setting where the goal is to scale a model as much as possible (MoE), it is not realistic to try to optimize every hyperparameter and modeling decision (extensive tuning at this scale is too expensive). Hashing does not require this.\\nHashing is also a fix', 'ng where the goal is to scale a model as much as possible (MoE), it is not realistic to try to optimize every hyperparameter and modeling decision (extensive tuning at this scale is too expensive). Hashing does not require this.\\nHashing is also a fixed mapping function, meaning it does not suffer from the issue from routing fluctuation/inconsistency during training (this was explored in the StableMoE paper).\\nSome hashing functions used:\\nClustered hashes – hash training inputs based on k-means clustering.\\nDispersed hashes – assume the opposite of clustered hashes, that similar inputs need a more fine-grained distinction and should be assigned to different experts (closer inputs should be routed to different experts).\\nRandom hashing.\\nBalanced assignment hashing.\\nOracle future hash – obtains a hash to route token t based on token t+1 (the next token).\\nThis paper also experiments with what they call MultiHashLayer, which consists of using different hashing strategies in the same network, a', ' assignment hashing.\\nOracle future hash – obtains a hash to route token t based on token t+1 (the next token).\\nThis paper also experiments with what they call MultiHashLayer, which consists of using different hashing strategies in the same network, as to not rely on a single hashing strategy.\\n\\nModels used in Experiments\\nBaseline is a 222M parameter dense Transformer.\\nWider dense Transformer of 755M parameters.\\nDeeper dense Transformer of 755M parameters.\\nTo compare to BASE, a 4.5B total parameter architecture with balanced assignment hashing is used.\\n\\nResults\\nWhen using a single MoE layer in a Transformer architecture (all other FFs remain the same), balanced Hash slightly outperforms Switch (using 64 experts in a model of 751M total parameters).\\nDeep dense models of the same size as MoE (in terms of total parameters) outperform MoE, showing good dense models make better use of each parameter. At the same inference speed (active parameters), MoE performs better.\\nIncreasing the number o', 'ters).\\nDeep dense models of the same size as MoE (in terms of total parameters) outperform MoE, showing good dense models make better use of each parameter. At the same inference speed (active parameters), MoE performs better.\\nIncreasing the number of experts (from 64 to 128) leads to a better increase in Hash over Switch.\\nThis indicates that the more experts there are in a layer, the less important it is to learn to route.\\nAs with BASE layers, adding a single Hash layer to a Transformer is shown to work better at later layers of the network.\\nIncreasing the number of sparse layers (in a setting where dense FFs and MoE layers are alternated) (5 sparse layers with 16 experts each) leads to better Switch performance over Hash.\\nIncreasing the number of experts per layer might change this.\\nFine-tuning trends are consistent with pre-training trends.\\nThe only part that can be frozen without hurting performance are the sparse layers.\\n\\nAnalysis/Results of Hashing Strategies\\nRandom and balanced ', 'umber of experts per layer might change this.\\nFine-tuning trends are consistent with pre-training trends.\\nThe only part that can be frozen without hurting performance are the sparse layers.\\n\\nAnalysis/Results of Hashing Strategies\\nRandom and balanced hashing have similar performance (but balanced hashing has training advantages over distributed training schemes).\\nRandom hashing outperforms clustered hashes.\\nProves the hypothesis that if tokens are like each other, a more fine-grained distinction is needed, and the tokens need to be routed differently.\\nDispersed hashing (opposite of clustered hashing) performs slightly better than random hashing.\\nLearned routing (like BASE or Switch) generally provide clustered expert modules, which could be a disadvantage based on the results obtained during this research.\\nBigram and previous token hashing perform worse than just relying on the current token.\\nThis indicates that using the previous token to help with routing is harmful.\\nIncreasing the di', 'a disadvantage based on the results obtained during this research.\\nBigram and previous token hashing perform worse than just relying on the current token.\\nThis indicates that using the previous token to help with routing is harmful.\\nIncreasing the dictionary size used for tokenization (thus increasing the number of possible hashes) leads to a decrease in performance against Switch.\\nThis indicates that Hash might be better suited for scenarios where the dictionary size is small (so there are less possible hashes), while Switch is better suited to large dictionary size scenarios.\\nOracle future token hashing essentially solves the task.\\nThis is expected since the hashing is performed on the target token (the answer).\\nIncreasing the diversity of hashing strategies (MultiHashLayer) seems to help.\\nA learned routing based on the current token (and not on the hidden state, as Switch routing works) leads to small improvements.\\nThis is a mix between the hashing strategy and Switch.\\nWhen comparin', 'f hashing strategies (MultiHashLayer) seems to help.\\nA learned routing based on the current token (and not on the hidden state, as Switch routing works) leads to small improvements.\\nThis is a mix between the hashing strategy and Switch.\\nWhen comparing Hash vs BASE, Hash outperforms BASE in every training step. BASE also shows instabilities at late stages of training, while Hash’s performance consistently improves (due to fixed routing).\\n\\nConclusion\\nHash shows that there are lots of room for improvement in learned routing strategies. Hash should be used as a baseline for improving learned strategies in future work.\\n\\nMy takeaways:\\nWhy is it that random routing outperforms clustered hashes and dispersed hashing performs even better? Shouldn’t clustered hashing make more sense since we want experts to specialize in specific clusters of the input space? These results seem to indicate the opposite.\\n\\nMixture-of-Experts with Expert Choice Routing\\nMain Idea: the main goal of this paper is to ta', 'clustered hashing make more sense since we want experts to specialize in specific clusters of the input space? These results seem to indicate the opposite.\\n\\nMixture-of-Experts with Expert Choice Routing\\nMain Idea: the main goal of this paper is to tackle the limitations in the MoE architecture caused by imperfect load balancing, which leads to under-training some experts and over-training others, as well as dropping tokens, through a novel approach called Expert Choice (EC).\\n\\nHow Traditional MoE (Token-Choice) Works\\nPass inputs into a gating mechanism which selects the most relevant k expert(s), in a process that relies on each individual token selecting the most relevant expert (token-choice). This leads to training inefficiency as the tokens are unevenly distributed. \\nTo help with this, an auxiliary loss is commonly used and added to the loss function, but this still leads to some imbalance. \\nThe issue of expert capacity is also prevalent, since for efficient computation, usually eac', 'ens are unevenly distributed. \\nTo help with this, an auxiliary loss is commonly used and added to the loss function, but this still leads to some imbalance. \\nThe issue of expert capacity is also prevalent, since for efficient computation, usually each expert has a fixed block size to work with in terms of token assignment. A capacity factor can be increased to minimize dropped tokens, but this leads to more memory inefficiency.\\nIn token-choice, each input is also assigned a fixed compute, regardless of its complexity and/or task.\\nExpert Choice (EC)\\nAt a high-level, EC routing has the expert models picking the most relevant input tokens instead of the other way around.\\n\\n, where n is the number of tokens in a batch, c is the capacity factor hyperparameter and e is the number of experts. (c can also be thought of as the average number of experts assigned to each token). Expert capacity is the maximum number of tokens that can be assigned to each expert at a batch-level.\\n, where x is the i', 'yperparameter and e is the number of experts. (c can also be thought of as the average number of experts assigned to each token). Expert capacity is the maximum number of tokens that can be assigned to each expert at a batch-level.\\n, where x is the input token representation and  is expert’s g embedding.\\nG = matrix with weights given to each expert.\\nI = index matrix where I[I, j] specifies the j-th selected token of the i-th expert.\\n\\n\\nThe gating input to each expert is then determined by \\nW1, W2 = parameters of the experts.\\n\\nOBS: EC routing has no constraints for the number of experts assigned to each token. \\nOBS2: The capacity factor in EC is the equivalent to top-k in token-choice -> it is the average experts assigned to each token.\\n\\nResults\\nEC with a capacity factor of 2 should be computationally equivalent to top-2 routing. EC-CF2 has training convergence 2x faster than GShard top-2 routing.\\nScaling the number of experts during pre-training, given the same expert size, leads to bet', 'lts\\nEC with a capacity factor of 2 should be computationally equivalent to top-2 routing. EC-CF2 has training convergence 2x faster than GShard top-2 routing.\\nScaling the number of experts during pre-training, given the same expert size, leads to better results, as expected (more total parameters = more specialized model = better quality).\\nEC-CF2 performs better than Switch top-1 and GShard top-2 in all settings, but given a fixed expert size of 100M, increasing the number of experts seems to lead to worse fine-tuning results (opposite to pre-training results).\\nCapping the number of experts to be assigned to each token leads to worse fine-tuning results. This shows that allowing variable number of experts per token is indeed helpful.\\nEC learns to allocate a variable number of experts per token.\\n\\nMy takeaways:\\nUnderstanding the routing mechanism as an unsupervised clustering method\\nAt the early stages of training a model with MoE layers, the routing mechanism (assuming it is a token-cho', 'rns to allocate a variable number of experts per token.\\n\\nMy takeaways:\\nUnderstanding the routing mechanism as an unsupervised clustering method\\nAt the early stages of training a model with MoE layers, the routing mechanism (assuming it is a token-choice method and that it is learned) is random, that is, it does not have information regarding of the area it will specialize in on the embedding input space. Without load balancing, the risk is of a specific expert being disproportionately chosen at these early stages, and thus taking up a large area of the input space for itself. \\nIn other words, as an expert is picked by the routing mechanism on inputs of a specific cluster that it performs well on in relation to other experts, it will gain abilities that can be generalized to other clusters that other experts still do not have, due to the lack of tokens being assigned to them. This will create a feedback loop that results in a single expert taking up more and more input space, due, again', 'ilities that can be generalized to other clusters that other experts still do not have, due to the lack of tokens being assigned to them. This will create a feedback loop that results in a single expert taking up more and more input space, due, again, to the generalization abilities that it picks up along the way, which will end up averaging a single dense model, since the tendency is for this over-generalized expert to take up the entire input space area for itself.\\nThe addition of an auxiliary load balancing loss is added to prevent this. To visualize this, we can think of an expert trying to grow its input space area but quickly reverting to a smaller area because of penalization effects. \\nAlthough this is helpful, there is still the risk of non-perfect clusters being assigned to each expert, especially at a batch level, which leads to other issues like token dropping.\\nIn Expert Choice, this auxiliary load balancing loss is not needed, as the experts themselves will pick the tokens ', 'k of non-perfect clusters being assigned to each expert, especially at a batch level, which leads to other issues like token dropping.\\nIn Expert Choice, this auxiliary load balancing loss is not needed, as the experts themselves will pick the tokens that are more relevant to them at a batch level (and not the other way around). If an expert has already reached full capacity, the 2nd expert that wanted that token the most will be chosen, etc.\\nI can imagine this leading to other problems. For example, some batches will contain tons of tokens that are part of the cluster of a specific expert, but the expert won’t be able to choose them because it has reached full capacity. In a token-choice scenario, this might lead to token dropping, which has the negative consequence of certain tokens not being used for inference (loss of information). In EC, this is not felt, but a new consequence may arise: tokens from the cluster of an expert will be given to another expert. Due to this impacting the', 's the negative consequence of certain tokens not being used for inference (loss of information). In EC, this is not felt, but a new consequence may arise: tokens from the cluster of an expert will be given to another expert. Due to this impacting the next update, it can lead to nearby experts fighting for the input space of other nearby experts. Although this can be suboptimal at a batch level, training for many batches might neglect this effect (?).\\nThought: Training an MoE model using token-choice and the strategy of MegaBlocks seems to be the ideal way to train a MoE model. This would get rid of the token dropping of token-choice, and not suffer from the negative consequence created by EC. The only assumption we’d have to make is that the load balancing loss and random noise penalties are a reliable way to find optimal token-expert assignments, given that token dropping is not an issue.\\nFuture work idea – visualize the gating mechanism process and how it routes training inputs based', 'e load balancing loss and random noise penalties are a reliable way to find optimal token-expert assignments, given that token dropping is not an issue.\\nFuture work idea – visualize the gating mechanism process and how it routes training inputs based on the clusters of each input embedding. Perhaps this can be done by using the checkpoints of the OpenMoE model (12 checkpoints available at HF I believe)?\\nBy not enforcing a constraint on the number of experts that can choose each token, EC creates a way for experts to determine how much compute will be used for each input. The idea is that the experts will learn complex and trivial inputs, maybe the intuition for this can be that complex inputs are in more complex/gray areas of the input space. With complex inputs, more experts will choose the token, leading to more computation being assigned to it. With trivial inputs that do not affect the output, no expert will choose the token, leading to no compute being applied to the token (token ', 'e. With complex inputs, more experts will choose the token, leading to more computation being assigned to it. With trivial inputs that do not affect the output, no expert will choose the token, leading to no compute being applied to the token (token is dropped).\\nThe difference between this token-dropping and token-choice’s token dropping is that this token dropping is learned, and not forced by lack of expert capacity.\\nThis is shown to be helpful to fine-tuning performance.\\nThe result of increasing the number of experts helping in pre-training but hurting fine-tuning performance matches the findings of previous papers already discussed here.\\n\\nFast Feedforward Networks + Exponentially Faster Language Modeling\\nMain Idea: the goal of this work is to introduce a new MoE architecture to improve inference time (up to 6x faster than other MoEs). They also claim that FFF (Fast FeedForward) has better training properties due to noiseless conditional execution (no randomness in the gating functi', ' is to introduce a new MoE architecture to improve inference time (up to 6x faster than other MoEs). They also claim that FFF (Fast FeedForward) has better training properties due to noiseless conditional execution (no randomness in the gating function).\\nTraditional MoEs scale down inference time but remain linear in the width of the feedforward layer (increase in expert parameters). These models also rely on noisy gating for load balancing, which complicates training.\\nFFF, on the other hand, uses a binary tree-like structure to improve on these challenges.\\n\\nMethod\\nFFF uses nodes to aid the routing mechanism and leaves for the experts.\\nThe input representation goes through a first node (which is a common MLP layer). The node’s output is then passed to a sigmoid to give a probability p. This probability p is used to route the input representation into the next node (left or right branch, as in a binary tree). This process is repeated until a leaf node (expert) is reached)\\nThe number of ', 'n passed to a sigmoid to give a probability p. This probability p is used to route the input representation into the next node (left or right branch, as in a binary tree). This process is repeated until a leaf node (expert) is reached)\\nThe number of nodes the input goes through (in case of hard routing) corresponds to the depth d of the network.\\nMoE chooses an expert width e (size of expert) and trains n separate expert blocks by the partially randomized output of a gating network of width g = [w/e]. The target is then predicted based on the mixture of the k best scoring experts.\\nMoE cost of inference is k*e neurons plus the gating overhead g (g tends to be small).\\nFFFs of depth d learn a tree partition R1, … , R2^d of the input space determined by their nodes, and 2^d small leaf feedforward networks (experts) of width l.\\nFFFs uses a soft routing approach to training, meaning that backpropagation is done by considering the soft routing probabilities p, so training a FFF is more costly ', 'mined by their nodes, and 2^d small leaf feedforward networks (experts) of width l.\\nFFFs uses a soft routing approach to training, meaning that backpropagation is done by considering the soft routing probabilities p, so training a FFF is more costly than even a feedforward network. However, a hard routing approach at inference (routing only happens through the most relevant nodes) ensures an inference gain over MoE.\\nThis soft to hard routing transition is referred to as hardening.\\nThe FFF routing is more efficient than regular MoE routing.\\nIn MoE, a gating network for each expert is needed to calculate the suitability of the specific expert to the input.\\nIn FFF, the input is passed through d nodes. Since each node halves the number of experts (leaves) to be considered in future routings for the same input, and because the left/right decision is simpler and thus requires less parameters than a normal MoE gating function, FFF provides a logarithmic routing improvement over MoE in terms o', 'ts (leaves) to be considered in future routings for the same input, and because the left/right decision is simpler and thus requires less parameters than a normal MoE gating function, FFF provides a logarithmic routing improvement over MoE in terms of computational overhead at inference. This is especially significant when scaling the number of experts.\\nThe strategy of soft routing during training comes with the idea that as the leaves specialize, the nodes will be more confident in the routing, leading to probabilities closer to 1 (to the correct path). This process is referred to as hardening. If hardening does not occur at the expected rate, the hard routing required for inference might not work as well. In those cases, a hardening loss is used.\\nLocalized overfitting can occur with a high number of leaves, with each leaf being responsible for a very small part of the input space. To diminish this, one can add random child transpositions (flip the p scores given by a node to its chil', 'is used.\\nLocalized overfitting can occur with a high number of leaves, with each leaf being responsible for a very small part of the input space. To diminish this, one can add random child transpositions (flip the p scores given by a node to its child nodes randomly), which ensures the gradients are more diversely distributed, and exposing different nodes and leaves to areas of the input space they otherwise wouldn’t see.\\nHardening can also lead to a shrinking batch problem, mitigated by using larger batch sizes, gradient accumulation and smaller learning rates.\\n\\nFFFs Applied to NLP\\nA variant of BERT, deemed UltraFastBERT, is developed, where the feedforward layers are replaced with FFFs.\\nFFFs provide a forward pass speedup over regular FFs of O(log^2n) compared to O(n), a logarithmic improvement (where n is neurons). This improvement comes from FFF’s balanced binary tree structure, which only executes one branch of the tree conditionally on the input.\\nUltraFastBERT has 4095 neurons (l', ' of O(log^2n) compared to O(n), a logarithmic improvement (where n is neurons). This improvement comes from FFF’s balanced binary tree structure, which only executes one branch of the tree conditionally on the input.\\nUltraFastBERT has 4095 neurons (leaves + nodes) and is compared to a 3072-neuron BERT.\\nUltraFastBERT only uses 1/341 of its neurons for inference while BERT uses all its 3072 neurons.\\nThis leads to a 78x speedup (not a 341x speedup, as would be expected) due to hardware optimization for matrix multiplication favoring FFs.\\nResults\\nUltraFastBERT performs on-par with BERT on fine-tuning in downstream tasks, with a 78x inference speedup.\\nUltraFastBERT shows that only a fraction of parameters of feedforward networks needs to be applied at inference.\\nThe concept of FFFs can technically be applied to decoder-only models as well.\\n\\nMy takeaways:\\nThe efficiency gains are a result of instead of passing the input to a routing mechanism which considers all experts, having the router on', 'ied at inference.\\nThe concept of FFFs can technically be applied to decoder-only models as well.\\n\\nMy takeaways:\\nThe efficiency gains are a result of instead of passing the input to a routing mechanism which considers all experts, having the router only decide between two experts (sending the input to a specific side of the binary tree.\\nWhile traditional routing expects the router to choose the specific part of the input space of each expert, this binary tree approach has the router dividing the input space in half at every decision, eventually leading to the desired input space.\\nFFF routing seems to be theoretically less expensive, but not allow parallelization (each node decision needs to be performed sequentially), so gains might not be as significant as expected.\\n\\nFrom Sparse to Soft Mixtures of Experts\\nMain Idea: Soft MoE presents a strategy to combat the issue of a traditional MoE of not having the property of continuous differentiability. By making a discrete choice (hard routing', 's significant as expected.\\n\\nFrom Sparse to Soft Mixtures of Experts\\nMain Idea: Soft MoE presents a strategy to combat the issue of a traditional MoE of not having the property of continuous differentiability. By making a discrete choice (hard routing) to obtain sparsity, traditional MoE introduces training instabilities, as small changes in the input may lead to large changes in the model’s output, since this small change may end up changing the expert(s) chosen. The soft MoE architecture is compatible with certain tasks such as image classification in Vision or machine translation in Language, but it is not compatible with Natural Language Generation (NLG). An equivalent approach that could be compatible with language generation was proposed as “Mixture-of-Tokens” (MoT) (in a different paper), but this MoT architecture seemed to also bring significant challenges that remain unsolved. The continuous differentiability property of Soft MoE is, therefore, only able to be applied to a limi', 'ed as “Mixture-of-Tokens” (MoT) (in a different paper), but this MoT architecture seemed to also bring significant challenges that remain unsolved. The continuous differentiability property of Soft MoE is, therefore, only able to be applied to a limited set of tasks.\\n\\nTraditional MoE Routing\\nIn traditional MoE, each input is routed to its corresponding expert in a hard manner (SoftMax is converted to 1 or 0) and the available slots are then occupied by a single token at a time (each slot gets 1 token). This means the experts will be updated solely based on that token.\\nOBS: slot refers to each inference run supported by the expert until it reaches its maximum capacity.\\n\\nSoft MoE Algorithm\\nEach slot pi of each expert has learnable parameters.\\nThe input tokens X are passed through each slot and a SoftMax is applied at the column level.\\nThis means that the input slots to be passed to each expert are simply a weighted linear combination of all the input tokens with the respective slot’s lea', '\\nThe input tokens X are passed through each slot and a SoftMax is applied at the column level.\\nThis means that the input slots to be passed to each expert are simply a weighted linear combination of all the input tokens with the respective slot’s learnable parameters.\\nWe then obtain the output slots by passing each input slot to a corresponding expert.\\nThe output slots are then merged through some combine weights, which are the inputs passed through the slot’s learnable parameters but now softmaxed at the row level (per token).\\nAs explained in the paper’s figure:\\n\\nSoft MoE first computes scores or logits for every pair of input token and slot, based on some learnable per-slot parameters.\\nThese logits are then normalized per slot (columns) \\nSo now we have, for each slot, a weight to give to each input token, which sum up to one per slot.\\nAnd every slot computes a linear combination of all the input tokens based on these weights.\\nThe tokens’ weights/embeddings are adjusted based on the w', 'So now we have, for each slot, a weight to give to each input token, which sum up to one per slot.\\nAnd every slot computes a linear combination of all the input tokens based on these weights.\\nThe tokens’ weights/embeddings are adjusted based on the weights/importance assigned to them per slot.\\nEach expert (an MLP) then processes its slots.\\nNow we have the experts’ outputs.\\nFinally, the same original logits are then normalized per token (by row) and used to combine all the slot outputs, for every token.\\nTo get the final output for each token, we then obtain the softmaxed weights now normalized per token (instead of the slot’s weights sum up to 1, each token’s weights sum up to 1) and combine the expert’s outputs with those weights accordingly.\\nIntuition for softmaxes\\nBy slot (column)\\nLeads to scores being given for each token by the slot, used to measure the importance which should be given to each token for a specific slot (how much should the slot consider each token).\\nBy token (row)\\n', 'y.\\nIntuition for softmaxes\\nBy slot (column)\\nLeads to scores being given for each token by the slot, used to measure the importance which should be given to each token for a specific slot (how much should the slot consider each token).\\nBy token (row)\\nLeads to scores being given for each slot (by the token), used to measure the importance which should be given to each slot for a specific token (to help determine the final output for each token) (how much should the token consider each slot).\\nProperties of Soft MoEs\\nUsually to get past the token-expert assignment problem, MoE architectures resort to hard assignment methods such as top-k token-choice or expert-choice. These measures are discrete in nature, and thus non-differentiable. Soft MoE, on the other hand, is fully differentiable and continuous.\\nSoft MoE does not suffer from token dropping or expert imbalance.\\nSoft MoEs adjust better to hardware accelerators than “hard” MoE methods, mainly due to avoiding top-k/sorting routing opera', ' on the other hand, is fully differentiable and continuous.\\nSoft MoE does not suffer from token dropping or expert imbalance.\\nSoft MoEs adjust better to hardware accelerators than “hard” MoE methods, mainly due to avoiding top-k/sorting routing operations (these are not well suited for hardware accelerators). Therefore, Soft MoEs are fast.\\nSoft MoEs are neither sparse (since every token is a weighted average for all input tokens) nor dense (since every expert only processes a subset of the slots, and not all input tokens).\\nTraditional MoE models are not so predictable at the sequence-level since inputting a single sequence may force the router to use every expert to balance the load and thus minimize the loss. This can lead to too generalist experts. Traditional MoEs are more predictable at the batch-level (more tokens) since a small number of tokens can fight for the same expert at the sequence level, but this risk is smaller at the batch level. Since in soft MoEs all tokens are group', 't experts. Traditional MoEs are more predictable at the batch-level (more tokens) since a small number of tokens can fight for the same expert at the sequence level, but this risk is smaller at the batch level. Since in soft MoEs all tokens are grouped together and every expert handles tokens from every input, this risk is not present – leading to more deterministic/predictable and faster inference.\\nOBS: the number of slots in a soft MoE is a hyper-parameter (must be equal or greater than the number of experts).\\nLimitation in NLG:\\nSoft MoE was only experimented with in an image classification scenario. Translating this method to an NLG setting is not so straightforward.\\nThis is because soft MoE uses all input tokens to compute all output tokens at once. In NLG, each input token is generated at a time/separately (one-by-one) and is used as a part of the context to predict the next token. It is possible to use causal masking techniques to only take one token at a time, but this can lead ', 'kens at once. In NLG, each input token is generated at a time/separately (one-by-one) and is used as a part of the context to predict the next token. It is possible to use causal masking techniques to only take one token at a time, but this can lead to a bias in training (correlation between token position and a slot).\\nThe sequential nature of token generation thus complicates the application of the Soft MoE architecture to language generation tasks.\\nMore research is needed to translate Soft MoE into an NLG setting.\\nMemory Consumption\\nSoft MoE works best when each expert is assigned to one slot only. Therefore, many experts need to be trained and stored, which comes with big costs in terms of memory.\\nExperiments (Image Classification only)\\nSoft MoE is compared to other MoE methods – token-choice and expert-choice – and a dense setting and outperforms all of them in all hyperparameter scenarios.\\nWith cheaper training and inference costs, Soft MoE outperforms Vision Transformers at a lar', 'Soft MoE is compared to other MoE methods – token-choice and expert-choice – and a dense setting and outperforms all of them in all hyperparameter scenarios.\\nWith cheaper training and inference costs, Soft MoE outperforms Vision Transformers at a large scale for a given compute budget in both pre-training and fine-tuning.\\nSoft MoE scales the number of experts well (more experts = better). Additionally, scaling the number of experts in Soft MoE doesn’t really change training time, while this can have a tremendous negative effect in training time with token-choice and expert-choice.\\n\\nMy takeaways:\\nSoft MoE seems to instead of routing each token individually, to route all tokens to each expert. This means that the expert will choose how much importance to give to each input token. The weighted average of the experts is then summed up based on the weights given to each token (normalized per token) to get the final output for each token.\\n\\nMixture-of-Tokens: Efficient LLMs Through Cross-Exam', 'mportance to give to each input token. The weighted average of the experts is then summed up based on the weights given to each token (normalized per token) to get the final output for each token.\\n\\nMixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation\\nMain Idea: traditional sparse MoE has some drawbacks - the router decision is discrete, making it not fully differentiable for training, which can cause training instabilities; the load balancing between experts is also not guaranteed, which leads to the need to apply methods such as using an auxiliary loss or adding random noise to training inputs, which do not guarantee solving this challenge. MoT tries to improve on the traditional MoE architecture, providing a fully differentiable strategy which automatically results in load balancing.\\nOBS: MoT is compatible with both masked and causal LLM training and inference (fill missing tokens and autoregressive language modeling).\\n\\nIssues with MoE\\nThe router is discrete, which cau', 'iable strategy which automatically results in load balancing.\\nOBS: MoT is compatible with both masked and causal LLM training and inference (fill missing tokens and autoregressive language modeling).\\n\\nIssues with MoE\\nThe router is discrete, which causes training instabilities since small changes in the input may cause big changes in the gradient (if the small change in the input results in a different router selection). This makes the training process not fully differentiable. Using a weighted average of the selected experts to form the outputs seems to help with this but is not an optimal solution.\\nThere is no guarantee that the MoE will distribute loads evenly among experts. A capacity factor (CF) can be set for minimizing token dropping, but this does not help with load balancing and increases memory requirements. This prompts the use of an auxiliary loss, which is, again, not ideal.\\nMost studies done with MoE are not compatible with autoregressive decoding (take soft MoE for exampl', ' but this does not help with load balancing and increases memory requirements. This prompts the use of an auxiliary loss, which is, again, not ideal.\\nMost studies done with MoE are not compatible with autoregressive decoding (take soft MoE for example).\\n\\nMoT Algorithm\\nThe first step is to pass the input tokens (all of them) through a router/controller (linear layer) and apply a SoftMax to get the token importance scores for each expert.\\n\\nWhere  is a matrix with each input token as a row and each expert as a column. \\nEach column sums up to 1, so each expert has its own designated router.\\nThen, the tokens are mixed by their importance weights, forming a mix of tokens for each expert.\\nSo, the token mix passed to expert i is -> \\nAfter having the mix of tokens for each expert, the next step is to pass the expert’s mix of tokens through its respective FFN.\\nTo obtain the final output for a specific expert, we need to scale the expert output based on the importance for each token in its mix:\\nF', 'e mix of tokens for each expert, the next step is to pass the expert’s mix of tokens through its respective FFN.\\nTo obtain the final output for a specific expert, we need to scale the expert output based on the importance for each token in its mix:\\nFinal_output (for token t and a given expert) = expert output * imp_weight for token t\\nThe final output for a given token t is then the sum of all the final outputs of each expert for that token t.\\nWhen doing this process for decoding, having to recompute each token multiple times seems inefficient, so a strategy to group tokens needs to be employed.\\nThe authors group tokens according to their position in a sequence (1st tokens grouped together, 2nd tokens grouped together, etc.). This way, for a given batch, each sequence can be computed in parallel, token-by-token.\\n\\nExperiments\\nThe authors compare a GPT-like model to a MoT model (Transformer architecture with all feed-forward layers replaced with MoT layers).\\nThe MoT model shows promising ', ' given batch, each sequence can be computed in parallel, token-by-token.\\n\\nExperiments\\nThe authors compare a GPT-like model to a MoT model (Transformer architecture with all feed-forward layers replaced with MoT layers).\\nThe MoT model shows promising pre-training results, achieving the vanilla Transformer’s final loss in 1/4th of the training steps and 1/3 of the training time.\\n\\nMy takeaways:\\nIntuition about the MoT algorithm:\\nCalculating the importance vector of the input tokens is done at the expert level. This means that the input tokens are passed through the router for expert n, which will give the importance weights of each token for that expert. This is calculating how the mix of tokens which is passed to each expert will be weighted.\\nMoT sounds like Expert Choice Routing in terms of the expert choosing the importance to give to each token (in Expert Choice, however, the method used to determine if the token will be sent to an expert is given by the affinity or importance weight ', '\\nMoT sounds like Expert Choice Routing in terms of the expert choosing the importance to give to each token (in Expert Choice, however, the method used to determine if the token will be sent to an expert is given by the affinity or importance weight given by the expert, while in MoT every token is considered by every expert).\\nTo get a final output, each token looks at the output of each expert, and considers how much importance to give to each expert’s output based on the importance the expert gave it.\\nAlthough it is possible to do natural language generation with this approach, it seems to be very inefficient since generation of tokens cannot be done in parallel for all input tokens in the same sequence, while this approach takes all input tokens in the same sequence in consideration during inference and performs a forward pass in all experts.\\nHighly impractical in its given form for language generation. The design presented only works at a batch-level.\\nThese limitations create the ne', ' tokens in the same sequence in consideration during inference and performs a forward pass in all experts.\\nHighly impractical in its given form for language generation. The design presented only works at a batch-level.\\nThese limitations create the need for future research to make this approach practical.\\nFor now, this is only a training strategy, but does not work for inference.\\n\\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\\nMain Idea: the goal of this work is to compare vanilla MoE (Transformer MoE) with vanilla Mamba and MoE-Mamba to explore if these architectures are compatible with each other. The main highlight of this paper is that MoE-Mamba outperforms both Transformer-MoE and vanilla-Mamba, reaching the same performance of vanilla-Mamba with 2.2x less training steps, while preserving Mamba inference gains over the Transformer. This shows that MoE results in performance gains when combined with the Mamba architecture, similarly to when applied to Tran', 'ame performance of vanilla-Mamba with 2.2x less training steps, while preserving Mamba inference gains over the Transformer. This shows that MoE results in performance gains when combined with the Mamba architecture, similarly to when applied to Transformers. In theory, this should result in easier scaling for Mamba, with even more inference gains due to the sparsity of MoE.\\n\\nMamba\\nMamba is an SSM (State-Space Model) architecture (SSM meaning that it is based on hidden states that update and drop/forget irrelevant information) like RNNs, GRUs and LSTMs. \\nMamba is an improvement over previous SSM architectures because it is optimized for GPUs and can make use of parallelism. \\nMamba is an improvement over Transformers because the characteristic of dropping irrelevant info of SSM architectures allows for a much lesser complexity as the input size increases. In theory, this should result in increased quality and reduced inference costs for Mamba compared to Transformers when scaling the co', 'ic of dropping irrelevant info of SSM architectures allows for a much lesser complexity as the input size increases. In theory, this should result in increased quality and reduced inference costs for Mamba compared to Transformers when scaling the context length.\\nTransformers’ complexity increases quadratically with an increase in input size (O(n^2)). Mamba does not impose this constraint.\\n\\nMoE-Mamba Architecture\\nMoE-Mamba makes use of a similar architecture to Switch Transformer.\\nToken-choice routing (top-k) with k=1 (one expert used per token)\\nEvery other Mamba layer is replaced with an FF MoE (each block alternates between dense (Mamba) and sparse (Mamba MoE) layers).\\nThe active parameters of the models experimented with were ~26M per token.\\nThe total number of parameters of the biggest MoE-Mamba model used was 416M parameters (32 experts).\\nMoE-Mamba scales well with an increase in the number of experts (expert size was constant, so increasing the number of experts means increasing ', 'ken.\\nThe total number of parameters of the biggest MoE-Mamba model used was 416M parameters (32 experts).\\nMoE-Mamba scales well with an increase in the number of experts (expert size was constant, so increasing the number of experts means increasing the number of total parameters while keeping the number of active parameters constant). The largest number of experts experimented with was 32.\\nMoE-Mamba needed at least 8 experts to improve over vanilla Mamba.\\n\\nMy takeaways:\\nMamba’s main advantage over Transformers seems to be of the handling of large context lengths due to SSM architectures inherently having the ability to drop irrelevant info from token to token. This is not true for the attention process in the Transformer architecture, which has an exponential increase in complexity with an increase in the context length.\\nThe main questions about Mamba’s legitimacy today are:\\nHow will Mamba scale in terms of increasing parameter size and data?\\nWill Mamba work given huge context lengths', 'h has an exponential increase in complexity with an increase in the context length.\\nThe main questions about Mamba’s legitimacy today are:\\nHow will Mamba scale in terms of increasing parameter size and data?\\nWill Mamba work given huge context lengths (tens/hundreds of thousands of tokens)?\\nMore research on the Mamba architecture is needed on my end.\\nMore research on Mamba-MoE needs to be done at increased parameter scales. A 416M parameter model with 26M active parameters per token is too small. Thus, the results of this paper should be seen as a mere indication and be taken with a grain of salt.\\n\\nBlackMamba: Mixture of Experts for State-Space Models\\nMain Idea: this work looks to combine the Mamba with the MoE architecture. Each of these architectures have unique advantages: Mamba has linear time and memory complexity to increases in context length (is robust to long-range context), while MoE has the advantage of allowing for scaling model’s parameters while keeping inference costs fix', 'chitectures have unique advantages: Mamba has linear time and memory complexity to increases in context length (is robust to long-range context), while MoE has the advantage of allowing for scaling model’s parameters while keeping inference costs fixed at the expense of a larger memory footprint. BlackMamba (Mamba + MoE fusion) is then expected to have the long-range context robustness of Mamba while having the inference efficiency of MoE.\\nThe models experimented with are larger than the previous work done (Mamba-MoE) but could be scaled much more (the models are, in [ACTIVE PARAMETERS/TOTAL PARAMETERS], 340M/1.5B and 630M/2.8B)\\n\\nExpected Advantages (Synergies) of BlackMamba vs Dense Transformer\\nFrom Mamba\\nLinear computational complexity with respect to input sequence length for both training and inference.\\nAutoregressive generation in constant time and memory.\\nFrom MoE\\nInference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close', 'th respect to input sequence length for both training and inference.\\nAutoregressive generation in constant time and memory.\\nFrom MoE\\nInference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close to an equivalent dense model in terms of total parameters.\\n\\nMoE Details\\nMoE top-k routing is used.\\nMoE is compared/evaluated based on:\\n(Forward pass or active parameters) / total parameters ratio\\nSimilarly to Mixtral8x7B, a relatively small number of experts is used in BlackMamba (even though scaling laws show promise in having many experts) to balance the inference FLOPs and memory cost of MoE (more experts = more memory costs).\\n\\nArchitecture\\nBlackMamba consists of replacing a few layers in the Transformer architecture:\\nThe MLP/FF layers are replaced by sparse MoE layers.\\nThe Attention layers are replaced by Mamba layers.\\nBlackMamba was trained on 300B tokens. This is consistent with the scaling done in this paper compared to the previous', 'e Transformer architecture:\\nThe MLP/FF layers are replaced by sparse MoE layers.\\nThe Attention layers are replaced by Mamba layers.\\nBlackMamba was trained on 300B tokens. This is consistent with the scaling done in this paper compared to the previous work trying to combine these architectures (MoE-Mamba was trained on 10B tokens and had significantly smaller model size).\\n340M/1.5B and 630M/2.8B sized models trained (active parameters/total parameters).\\n8 experts used per MoE layer.\\nFound a slight advantage in using sequential versus parallel blocks, so prioritized a sequential setup.\\nThis is equivalent to depth vs width.\\nUsed top-1 routing with the Sinkhorn algorithm to ensure load balancing between experts.\\nSinkhorn was the same algorithm used in BASE routing. It makes routing more efficient in accelerated hardware (GPUs).\\nA novel version of Sinkhorn was developed, which has faster convergence.\\nUsed the Megatron-LM framework for distributed training.\\nTrained using bf16 precision.\\n\\nRes', 'used in BASE routing. It makes routing more efficient in accelerated hardware (GPUs).\\nA novel version of Sinkhorn was developed, which has faster convergence.\\nUsed the Megatron-LM framework for distributed training.\\nTrained using bf16 precision.\\n\\nResults\\nFor the same number of active parameters (equal at inference) and the same amount of training FLOPs (equal amount of training), BlackMamba performed significantly better than the Transformer, Transformer-MoE and Mamba equivalents.\\nAs expected, BlackMamba also showed significant latency improvements over the other architectures. These latency improvements increase with an increase in context length.\\nThis indicates that the synergy between Mamba and MoE works.\\nIn terms of expert balance, most layers show this happens successfully. However, later layers show a clear transition towards expert imbalance.\\nPerhaps this is due to numerical instabilities that show as we get deeper into the network?\\nThis pattern of instability in later MoE layer', ' layers show this happens successfully. However, later layers show a clear transition towards expert imbalance.\\nPerhaps this is due to numerical instabilities that show as we get deeper into the network?\\nThis pattern of instability in later MoE layers was also shown in the “Faster-MoE” paper.\\nBlackMamba leaves room for future work in terms of the Mamba + MoE fusion:\\nFew-shot performance.\\nQuantization and PEFT performance.\\nFine-tuning, instruction-tuning and DPO performance.\\nAre the expert’s specialization dynamics in BlackMamba the same as in Transformer MoEs?\\n\\nMy takeaways:\\nThe checkpoints of BlackMamba were released, so perhaps some investigation can be done in terms of exploring the expert’s specialization dynamics in the BlackMamba architecture and compare it to regular Transformer MoEs.\\n\\nStableMoE: Stable Routing Strategy for Mixture of Experts\\nMain Idea: the goal of this paper is to solve the sample efficiency issue of training MoEs. The expert selection for a specific input may ', 'itecture and compare it to regular Transformer MoEs.\\n\\nStableMoE: Stable Routing Strategy for Mixture of Experts\\nMain Idea: the goal of this paper is to solve the sample efficiency issue of training MoEs. The expert selection for a specific input may change during training, causing the weights of experts to be updated that will not be using it in inference – suboptimal training with experts being updated based on an input space that is not attributed to them during inference (routing fluctuation problems).\\n\\nProblem\\nBy observing the routing fluctuation issue when using BASE layers, it was observed that:\\n40.9% of tokens are unstable (inconsistent in routing) after 20% of the training steps.\\nthis number decreases to 29.1% after 50% of training, and to 15.4% after 80% of training.\\nSolution\\nSplit training into 2 parts:\\nStage 1\\nstart by training a router (with a new balance loss introduced – not much different, simply penalizes the loss in the case of expert overloading) and using sigmoid ins', ' and to 15.4% after 80% of training.\\nSolution\\nSplit training into 2 parts:\\nStage 1\\nstart by training a router (with a new balance loss introduced – not much different, simply penalizes the loss in the case of expert overloading) and using sigmoid instead of SoftMax (sigmoid is thought to propagate the signal better) for determining the assigned expert’s weight.\\nDuring stage 1 of training, the router is distilled. This distillation process is accounted for in the training loss:\\nTotal loss = task loss + balance loss + distillation loss.\\nThe components that are important for this distillation are the experts’ centroids and the routing feature of the token t (distilled through a word embedding).\\nAt the end of training stage 1, the parameters for the distilled router (which were being trained synchronously) (these parameters are the word embeddings for the tokens and the experts’ centroids) are frozen and kept frozen for the remainder of training (which consists of stage 2).\\nStage 2\\nIn stag', 'r the distilled router (which were being trained synchronously) (these parameters are the word embeddings for the tokens and the experts’ centroids) are frozen and kept frozen for the remainder of training (which consists of stage 2).\\nStage 2\\nIn stage 2 of training, the router is distilled and stable, so only the task loss is needed. The sigmoid gate is kept so the gating signal is still being trained (I believe this is only for the actual weights given to each expert at inference).  Everything else remains the same.\\n\\nResults\\nThe StableMoE method is compared to a dense Transformer, a Base MoE, a Hash Layer MoE and Switch Transformer at a base and a large setting (454M and 3.22B total parameters, respectively).\\nStableMoE outperforms all others in all settings and shows robustness in scaling both model parameters and number of experts.\\nModels improve perplexity with a higher number of experts (tested up to 64), given the same model size.\\nStacking MoE layers in-between Transformer blocks ', 'rs in all settings and shows robustness in scaling both model parameters and number of experts.\\nModels improve perplexity with a higher number of experts (tested up to 64), given the same model size.\\nStacking MoE layers in-between Transformer blocks was shown to have the best results in comparison to sticking them in other positions.\\n\\nMy takeaways: \\nAt first glance, it seems logical that the routing fluctuation issue presented will result in suboptimal training, so traditional MoEs leave room for improvement in terms of training efficiency, especially in early stages of training.\\nThe part which seems to help the most is the routing distillation. The idea is to learn parameters to learn optimal expert centroids and token embeddings. Once this is learned, the router can be frozen to keep stability during training.\\nThe paper provides evidence that scaling the number of experts with StableMoE leads to improved performance not only in pre-training but also in downstream tasks like multiling', \"s is learned, the router can be frozen to keep stability during training.\\nThe paper provides evidence that scaling the number of experts with StableMoE leads to improved performance not only in pre-training but also in downstream tasks like multilingual machine translation, as evidenced by higher average test BLEU scores compared to other models. This indicates that the advantages of scaling are not confined to pre-training. However, the paper doesn't provide an extensive evaluation on a variety of downstream tasks or fine-tuning with different amounts of data, which would be valuable for comprehensively understanding the scalability and efficiency of the model in varied contexts.\\n\\nEvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate\\nMain Idea: EvoMoE is a proposed end-to-end framework for training MoE models. The focus of EvoMoE is to deal with the issues of immature experts and unstable sparse gates (instabilities related to early stages of training, \", 'k via Dense-To-Sparse Gate\\nMain Idea: EvoMoE is a proposed end-to-end framework for training MoE models. The focus of EvoMoE is to deal with the issues of immature experts and unstable sparse gates (instabilities related to early stages of training, the same issue explored in StableMoE), which come from the traditional MoE framework and are harmful to convergence performance. This issue from traditional MoE is thought to come from training a sparse gate from scratch, with randomly initialized weights for both experts and router – impossible to not have router instabilities with this setup. To solve this, EvoMoE proposes starting training with a single expert, and gradually evolving that into a large and sparse MoE structure.\\nIn sum:\\nEvoMoE allows the model to warm-up before dividing it into experts.\\nThe gate starts as dense and gradually sparsifies, allowing it to better understand how to route inputs to experts before it reaches a high degree of sparsity.\\n\\nMethod\\n2 stages:\\nExpert-Dive', 'allows the model to warm-up before dividing it into experts.\\nThe gate starts as dense and gradually sparsifies, allowing it to better understand how to route inputs to experts before it reaches a high degree of sparsity.\\n\\nMethod\\n2 stages:\\nExpert-Diversify – can be seen as an improved initialization technique.\\nStart by training a single expert (so the early stages of training are the equivalent of training a dense Transformer architecture).\\nAfter T training steps, the single expert is replicated N times to initialize all experts. The initialization of experts from the initial expert can be done in multiple ways: adding random noise to each expert, randomly masking the initial expert’s weights, etc.\\nEvoMoE adopts the random masking strategy for initializing experts from an original warmed-up expert.\\nOnce all experts are initialized, EvoMoE goes into a standard MoE period with a Dense-to-Sparse (DTS) Gate.\\nThe training of the DTS Gate is what the next stage is all about.\\nGate-Sparsify – t', 'or initializing experts from an original warmed-up expert.\\nOnce all experts are initialized, EvoMoE goes into a standard MoE period with a Dense-to-Sparse (DTS) Gate.\\nThe training of the DTS Gate is what the next stage is all about.\\nGate-Sparsify – training the router.\\nThe router starts as a dense gate which routes the input to most experts. The idea is that at early stages of routing, the gate is not so good at its task, so would benefit from more dense routing so it can analyze the relevant experts more thoroughly, gaining more information about which experts work better from each input, instead of just using 1 or 2 experts at a time.\\nAs more training steps are done with the router, the better it becomes, so the sparser it can be. So DTS-Gate gradually becomes sparser.\\nThis stage uses an auxiliary load balancing loss.\\n\\nExperiments\\nBaselines\\nSwitch - top-1 routing.\\nBASE – linear assignment routing.\\nHash Layer – hashing-based routing.\\nDSelectK – differentiable routing achieved through ', 'Gate gradually becomes sparser.\\nThis stage uses an auxiliary load balancing loss.\\n\\nExperiments\\nBaselines\\nSwitch - top-1 routing.\\nBASE – linear assignment routing.\\nHash Layer – hashing-based routing.\\nDSelectK – differentiable routing achieved through smoothing techniques.\\nStableMoE – gate distillation and freezing for routing consistency during training.\\nEvaluated on (all with 355M active parameters)\\nMachine translation - encoder-decoder setup.\\nMasked language modeling - encoder-only setup.\\nLanguage modeling - decoder-only setup.\\nEvery other FFN layer is replaced by an MoE layer (EvoMoE alternates between dense and sparse FFNs.\\nEvoMoE beats other variants on all architectures (encoder, decoder, encoder-decoder) and provides training speedups.\\nBoth the expert-diversify and gate-sparsify stages are shown to be useful, per ablation studies.\\nCompared to GPT-MoE, EvoMoE can provide a 2x training speedup (2x less training samples needed to achieve the same perplexity) as well as a 1.42x speed', '.\\nBoth the expert-diversify and gate-sparsify stages are shown to be useful, per ablation studies.\\nCompared to GPT-MoE, EvoMoE can provide a 2x training speedup (2x less training samples needed to achieve the same perplexity) as well as a 1.42x speedup in terms of FLOPs efficiency (1.42x less training FLOPs needed to achieve the same perplexity).\\nThe sample efficiency and FLOPs efficiency speedups are different because EvoMoE’s routing is dense during some of the gate-sparsify stage, which requires more FLOPs per training sample.\\nWith increasing number of experts per layer, EvoMoE shows consistent improvements.\\nWith increasing number of MoE layers (replacing denser FFNs by MoE layers than in the initial setup), EvoMoE shows better performance while maintaining inference FLOPs (although with a higher memory cost – more total parameters).\\n\\nMy takeaways:\\nResearch Idea – it might not be efficient to enforce load balancing due to some areas of the input space being more common than others (', 'ce while maintaining inference FLOPs (although with a higher memory cost – more total parameters).\\n\\nMy takeaways:\\nResearch Idea – it might not be efficient to enforce load balancing due to some areas of the input space being more common than others (load balancing could cause undesired overlap in clusters at the token-level). Perhaps there could be some synergy between early-stage stability and MegaBlocks (for stable gating + no necessary load balancing at the batch level). Could also explore how custom compute depending on the complexity of the input could be implemented, and how this would perform.\\nOverall, EvoMoE shows promising results. The challenge to this framework is the dense routing stage of training, which incurs high compute costs, but is a part of the trade-off for achieving better routing stability and sample efficiency.\\n\\nSoft Merging of Experts with Adaptive Routing\\nMain Idea: develop a technique called SMEAR (Soft Merging of Experts with Adaptive Routing) – single merge', 'ts, but is a part of the trade-off for achieving better routing stability and sample efficiency.\\n\\nSoft Merging of Experts with Adaptive Routing\\nMain Idea: develop a technique called SMEAR (Soft Merging of Experts with Adaptive Routing) – single merged expert constructed via a weighted average of all the experts’ parameters - to address the non-differentiability issue of discrete routing in MoE, hypothesizing that this lack of differentiability is what causes instabilities and underperformance in MoE.\\nPast research points that stable task/domain-level learned experts are possible (like in the DEMix line of work), but this is harder to achieve at the token-level. A few works showing the challenges of learned MoE at the token-level:\\nHash layer (random routing based on a fixed heuristic) achieves comparable results through a fixed random strategy.\\nSwitch and the Scaling Laws paper find that increasing the active parameters and the number of experts provides a predictable performance improv', ' (random routing based on a fixed heuristic) achieves comparable results through a fixed random strategy.\\nSwitch and the Scaling Laws paper find that increasing the active parameters and the number of experts provides a predictable performance improvement, but this is not the same when just scaling the total number of parameters (this shows limited returns).\\nThis can perhaps be explained by suboptimal routing.\\nWith SMEAR, the authors hypothesize that these inefficiencies in MoE are caused by gradient estimation issues. First, they explore if fixed heuristic routing can overperform learned routing, and then compare that to SMEAR (which is fully differentiable).\\n\\nSMEAR\\nIn traditional MoE routing, the router training needs to resort to gradient estimation techniques. The goal of SMEAR is to develop an architecture that enables end-to-end gradient-based training (fully differentiable, no gradient estimation) without an increase in computational costs.\\nEnsemble routing\\nWould allow for an en', 't estimation techniques. The goal of SMEAR is to develop an architecture that enables end-to-end gradient-based training (fully differentiable, no gradient estimation) without an increase in computational costs.\\nEnsemble routing\\nWould allow for an end-to-end gradient-based training but with a significant increase in computational costs.\\nMerging of Experts\\nRecent work has shown that averaging the parameters of models that share a common architecture can often produce an aggregate model that shares the capabilities of the individual models.\\nSMEAR\\nConstructs a single merged expert whose parameters are computed as the weighted average of the experts within a routing block.\\nEach expert’s set of weights is set by the corresponding routing probability generated by the router.\\nInstead of only taking the top-k experts selected by the router, which is the discrete step in the strategy, SMEAR weighs each expert’s parameters according to the weight given by the router and merges them into a single', 'lity generated by the router.\\nInstead of only taking the top-k experts selected by the router, which is the discrete step in the strategy, SMEAR weighs each expert’s parameters according to the weight given by the router and merges them into a single expert.\\nAllows updating each expert in each forward pass in a fully differentiable manner.\\nAlmost equivalent (slightly higher due to the cost of merging) cost of top-1 routing at inference but more expensive training costs (due to having to backpropagate through each expert after each forward pass).\\n\\nExperimental Setup\\nMain question to be answered is if SMEAR can outperform heuristic routing strategies.\\nUse T5 fine-tuned on GLUE for NLP tasks, while also conducting computer vision experiments based on ResNet.\\nUsed a “tag routing” strategy as one of the baselines, which is a routing strategy based on metadata (oracle routing).\\nAdd experts to existing pre-trained network (models are not trained from scratch and are based off pre-trained dens', 'ased on ResNet.\\nUsed a “tag routing” strategy as one of the baselines, which is a routing strategy based on metadata (oracle routing).\\nAdd experts to existing pre-trained network (models are not trained from scratch and are based off pre-trained dense models).\\nSimilarly to adding adapters for fine-tuning (all pre-trained parameters are kept frozen).\\nRouter is a simple linear classifier.\\nEach layer has 8 experts.\\nNo balance loss was used.\\nResults\\nModels using learned routing strategies learned through gradient estimation (thus not fully differentiable) often underperform heuristic routing strategies.\\nSMEAR outperforms every routing strategy (heuristic or learned) in both NLP and Vision settings, including tag routing (determined by metadata) and a parameter-matched (in terms of total parameters) dense baseline.\\nConsistent with DEMix line of research, which says that a good learned routing strategy should be better than routing determined by metadata.\\nSMEAR performs comparably to a fully', ' and a parameter-matched (in terms of total parameters) dense baseline.\\nConsistent with DEMix line of research, which says that a good learned routing strategy should be better than routing determined by metadata.\\nSMEAR performs comparably to a fully active MoE ensemble (especially in T5-GLUE), which is seen as the upper bound of this approach.\\nIn terms of inference, SMEAR performs comparably to the top-1 routing strategy.\\nDoubling the number of experts (from 8 to 16) in SMEAR led to a slight performance boost in Vision but no notable difference in T5-GLUE.\\nSignificant sparsity observed when visualizing the router’s distribution, suggesting expert specialization.\\n\\nMy takeaways:\\nSMEAR offers a novel training framework that might set a precedent for future MoE models by mitigating the non-differentiability issue common in discrete routing decisions, thereby leading to more stable and efficient learning.\\nThe gradual diversification from a single expert to a full MoE configuration in SMEAR', 'ent for future MoE models by mitigating the non-differentiability issue common in discrete routing decisions, thereby leading to more stable and efficient learning.\\nThe gradual diversification from a single expert to a full MoE configuration in SMEAR could inspire new initialization techniques for complex neural networks, ensuring a smoother transition to specialized expert utilization.\\nGiven SMEAR’s performance improvements and computational efficiency, it would be worthwhile to investigate how it could be adapted to real-world tasks requiring modularity and efficiency, such as personalized recommendation systems or multi-domain language models.\\n\\nParameter-Efficiency\\n\\nParameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models\\nMain Idea: proposes an architecture to make more efficient use of parameters in MoE models by sharing information among experts. Mainly uses matrix product operator (MPO), a tensor decomposition approach from quantum physics to reconstruc', 'uage Models\\nMain Idea: proposes an architecture to make more efficient use of parameters in MoE models by sharing information among experts. Mainly uses matrix product operator (MPO), a tensor decomposition approach from quantum physics to reconstruct the expert layer, then shares parameters from the central tensor (core information) between experts while maintaining specificity through auxiliary tensors (complementary to the central tensor). The intuition behind this approach is to solve MoE’s issue of expert redundancy (different experts learning common knowledge, leading to parameter-inefficiency).\\n\\nApproach – MPOE\\nCore idea is to share the central tensors from the expert layers and enable specificity via expert-specific auxiliary tensors based on the matric decomposition strategy.\\nThe final MoE layer would consist of a shared central tensor (looks the same for each expert) and small auxiliary tensors (unique to each expert).\\nThe central tensor acts like a global parameter – is the ', 's based on the matric decomposition strategy.\\nThe final MoE layer would consist of a shared central tensor (looks the same for each expert) and small auxiliary tensors (unique to each expert).\\nThe central tensor acts like a global parameter – is the same for each expert in a layer.\\nLess total parameters are then needed in total since each expert layer will contain a globally shared tensor for all experts (the central tensor) while retaining expert specificity through auxiliary tensors specific to each expert.\\nIdea is to capture the shared knowledge between experts in the central tensor, and the specialized expert knowledge in the auxiliary tensors.\\nIn theory, MPOE leads to suboptimal optimization since central tensors are always updated. To stabilize the optimization process, a gradient mask strategy is used:\\nThe central tensor is not always updated (determined randomly).\\nEquivalent to a gradient dropout, employed in the central tensor of each MoE layer.\\nMPOE is employed on already pre', ' stabilize the optimization process, a gradient mask strategy is used:\\nThe central tensor is not always updated (determined randomly).\\nEquivalent to a gradient dropout, employed in the central tensor of each MoE layer.\\nMPOE is employed on already pre-trained language models (for the matrix decomposition to make sense, the models need to already have been pre-trained, having knowledge to decompose).\\n\\nExperiments\\nGPT-2 (decoder-only) and T5 (encoder-decoder) are used as base models for MPOE.\\n8 experts per MoE layer are generally used.\\nAdding MPOE to fine-tune pre-trained LMs in downstream tasks leads to better performance than Switch with a 27.2x parameter reduction.\\nMPOE is especially better at low-resource tasks, indicating that MPOE’s parameter-sharing leads to positive task transfers.\\nThe caveat is that MPOE needs an already pre-trained LLM.\\nAdding more experts (and thus having more auxiliary tensors) leads to improved MPOE performance.\\nMPOE can also potentially work well in a multi-', 'meter-sharing leads to positive task transfers.\\nThe caveat is that MPOE needs an already pre-trained LLM.\\nAdding more experts (and thus having more auxiliary tensors) leads to improved MPOE performance.\\nMPOE can also potentially work well in a multi-task setting (with task-level routing).\\n\\nMy takeaways:\\nDeepSeekMoE is a recent model that was also trained with the idea of improving parameter efficiency by sharing weights of experts to capture common knowledge.\\nAlso is like sparse upcycling and parameter-efficient sparsity crafting in the sense that it takes a pre-trained LLM and modifies its architecture to have the advantages of MoE.\\nThis approach is compatible with distillation techniques to further improve inference time.\\n\\nPushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning\\nMain Idea: this work looks to answer the question “can we leverage MoEs for instruction fine-tuning?”, especially in the context of PEFT methods such as LoRA.\\nPEFT -> ', '-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning\\nMain Idea: this work looks to answer the question “can we leverage MoEs for instruction fine-tuning?”, especially in the context of PEFT methods such as LoRA.\\nPEFT -> addresses the challenges associated with updating many parameters by restricting weight updates to a limited number of parameters.\\nTraditional PEFT experimented with were (IA)^3 and LoRA (both add a small number of parameters to the existing model.\\n(IA)^3\\nAdds 3 rescaling vectors Vk, Vv and Vff which rescale the keys and values in the self-attention mechanism, and the feed-forward parameters. During finetuning, only the 3 scaling vectors are updated.\\nLoRA\\nOptimizes low-rank decomposition of dense layers in LLMs. LoRA modifies the original weights of a layer in the LLM by adding a low-rank approximation of the change required for adaptation.\\n –> normal fine-tuning\\n –> LoRA\\nSo, the change Wx to the original weights (Wo) is approximated throug', 'e layers in LLMs. LoRA modifies the original weights of a layer in the LLM by adding a low-rank approximation of the change required for adaptation.\\n –> normal fine-tuning\\n –> LoRA\\nSo, the change Wx to the original weights (Wo) is approximated through low-rank matrices B and A, and the original weights are kept frozen during the fine-tuning process, only requiring updating B and A.\\nThe specific rank to use for these low-rank matrices is a hyperparameter.\\n\\nExtremely Parameter-Efficient MoE\\nLeverages lightweight adapters as experts on top of a pretrained model.\\nRouter used is simply a trainable dense layer that outputs a softmaxed score for each expert based on the input.\\nAdds expert layers only for finetuning, and each expert is a PEFT adapter ((IA)^3 or LoRA). The dense model parameters are kept frozen during fine-tuning, so at this stage, only the expert adapters and router will be trained. The experts learn to adapt the dense Transformer layers during fine-tuning.\\nSince LoRA and (IA)', 'er ((IA)^3 or LoRA). The dense model parameters are kept frozen during fine-tuning, so at this stage, only the expert adapters and router will be trained. The experts learn to adapt the dense Transformer layers during fine-tuning.\\nSince LoRA and (IA)^3 adapters are linear functions, it is possible to apply soft merging of experts (as in Soft MoE).\\nVery efficient in terms of training (fine-tuning) and inference.\\n\\nExperiments\\nBaselines used for comparison:\\nFully fine-tuned dense model.\\nStandard PEFT methods ((IA)^3 and LoRA with rank=4).\\nAblations:\\nUsing sentence embeddings for the router (all tokens in the same sentence activate the same expert) vs token embeddings (experts are activated based on individual tokens).\\nToken routing is better than sentence routing at all levels.\\nSoft routing (output is a weighted mixture of all experts) vs discrete routing (top-k experts with k=1 or k=2 with use of an auxiliary load balancing loss).\\nSoft merging shows significantly better performance in a ', 'han sentence routing at all levels.\\nSoft routing (output is a weighted mixture of all experts) vs discrete routing (top-k experts with k=1 or k=2 with use of an auxiliary load balancing loss).\\nSoft merging shows significantly better performance in a PEFT MoE setting than top-k routing.\\nPerhaps due to its continuous differentiability characteristics?\\nNot compatible with NLG.\\nResults\\nHow does PEFT MoE compare to traditional PEFT models?\\nBase model used was T5-3B.\\nPEFT MoE provides a significant performance boost.\\nPerforms on-par with full-finetuning, with the largest PEFT MoEs even surpassing it.\\nThese effects are shown to be true with scale.\\nGiven the same parameter budget, MoV (based on (IA)^3) outperforms MoLoRA (based on LoRA) at large model sizes, but the opposite is true at small model sizes.\\nIncreasing the number of experts in MoV and MoLoRA generally improves performance on all model sizes experimented with (700M, 3B and 11B), however, this should be taken with a grain of salt wh', 'del sizes, but the opposite is true at small model sizes.\\nIncreasing the number of experts in MoV and MoLoRA generally improves performance on all model sizes experimented with (700M, 3B and 11B), however, this should be taken with a grain of salt when the number of experts is high (>30), since increasing the number of experts in that case can lead to worse performance.\\nEvaluation of the routing for the last experts’ layer shows that experts are activated at different magnitudes based on the input task for both seen and unseen tasks (during training). This shows that different experts learn different skills (or that different tasks have different data distributions in terms of tokens?).\\nThe larger the batch size used in training, the more likely the MoE is to converge to a single expert (the larger the batch size, the less stable the training).\\nBased on PEFT MoEs degrading performance after 5k steps to lie close to the dense PEFT models.\\nA smaller learning rate stabilizes training and ', 'ikely the MoE is to converge to a single expert (the larger the batch size, the less stable the training).\\nBased on PEFT MoEs degrading performance after 5k steps to lie close to the dense PEFT models.\\nA smaller learning rate stabilizes training and leads to improved performance (3e-4 was used, and the range 3e-3 to 6e-4 was tested).\\n\\nMy takeaways:\\nImportant to note that this is a method to help in the process of fine-tuning, and it is not compatible with pre-training.\\nAlso seems like sparse upcycling and parameter-efficient sparse crafting.\\nSoft MoE is compatible with the approach used in this research because NLG tasks are not explored, only text-to-text tasks and encoder-decoder models are explored.\\n\\n\\nParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nMain Idea: introduces parameter-efficient sparsity crafting (PESC), a technique consisting of transforming a dense LLM into an MoE architecture for added model capacity and ma', 'g from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\\nMain Idea: introduces parameter-efficient sparsity crafting (PESC), a technique consisting of transforming a dense LLM into an MoE architecture for added model capacity and making use of adapters to differentiate experts without altering their original weights. This work focuses specifically on instruction-tuning.\\nThe motivation for this work comes mainly from leveraging the idea of sparse upcycling (converting dense into MoE) to improve LLMs’ performance on instruction-tuning, since “Mixture-of-experts meets instruction-tuning: A winning combination for large language models” showed how the MoE architecture is highly effective for instruction-tuning tasks.\\n\\nMethod:\\nSparse upcycling:\\nFrom a dense pre-trained LLM, transform the FFN layers in each Transformer block into a mixture-of-experts by replicating (copying) the FFN layer n times (n being the number of experts per layer).\\nThe other layers of the Transformer', 'Sparse upcycling:\\nFrom a dense pre-trained LLM, transform the FFN layers in each Transformer block into a mixture-of-experts by replicating (copying) the FFN layer n times (n being the number of experts per layer).\\nThe other layers of the Transformer block (embedding, attention, normalization) remain the same.\\nContinue pre-training on this sparse architecture.\\nPESC:\\nGenerally the same as sparse upcycling with a few caveats.\\nThe dense to sparse transformation is similar, but instead of replicating the actual FFN layer, PESC initializes an adapter to represent each expert, while the FFN remains the same for each expert.\\nPESC does not continue normal pre-training as sparse upcycling, it only performs instruction-tuning.\\nPESC does not update all experts’ parameters/weights, but only each expert’s adapter instead for parameter-efficiency.\\nThis means that we don’t need n copies of the FFN parameters, but instead the equivalent of n copies of the adapter.\\nFor constructing the adapter, PESC us', 'l experts’ parameters/weights, but only each expert’s adapter instead for parameter-efficiency.\\nThis means that we don’t need n copies of the FFN parameters, but instead the equivalent of n copies of the adapter.\\nFor constructing the adapter, PESC uses QLoRA.\\nTop-2 routing and auxiliary load balancing loss were used.\\nParameter Efficiency Gains\\nWhile in sparse upcycling we are trying to optimize Fi (Theta(o)), where this represents the objective function in respect to all experts’ parameters, in PESC we are optimizing expert adapters to approximate Fi(Theta(o)) through ~Fi(Theta(o), w(o)), where w(o) represents the adapters’ weights.\\nThis provides more efficiency in:\\nTraining costs, since w(o) is significantly smaller than Theta(o).\\nMemory costs, since instead of replicating a full FFN layer for each expert, we are replicating an adapter for each expert, which is significantly smaller.\\nOriginal FFN weights are shared between experts, so only one copy per MoE layer is needed.\\n\\nExperiment', 'osts, since instead of replicating a full FFN layer for each expert, we are replicating an adapter for each expert, which is significantly smaller.\\nOriginal FFN weights are shared between experts, so only one copy per MoE layer is needed.\\n\\nExperiments\\nThe largest PESC model trained was Camelidae-8x34B-pro (38B total parameters, ~34B activated parameters).\\nStrong performance of Camelidae-8x34B-pro on benchmarks analyzed when compared to other SOTA chat models (Mixtral-8x7B-Instruct, GPT 3.5, Llama-2-70B-Chat).\\nEspecially strong in knowledge and reasoning, math and coding.\\nComparable overall performance to GPT 3.5.\\nDense vs sparse variations\\nSignificant advantage of Camelidae-8x7B over Llama2-7B-Chat and Vicuna-7B, especially in more complex areas (coding and math).\\nAdvantages are only amplified in the 10-20B range with Camelidae-8x13B.\\nStrong performance continues in the 30-50B range, with Cameliade-8x34B-pro outperforming the leading sparse model Mixtral-8x7B-Instruct (47B total parame', 'areas (coding and math).\\nAdvantages are only amplified in the 10-20B range with Camelidae-8x13B.\\nStrong performance continues in the 30-50B range, with Cameliade-8x34B-pro outperforming the leading sparse model Mixtral-8x7B-Instruct (47B total parameters, 13B active parameters).\\nPESC effectively mitigates the knowledge forgetting issue observed in the instruction-tuning process of Camelidae’s dense counterpart Camel.\\nIncreasing the number of experts in the MoE layers significantly improves the model’s performance.\\nExperimented with relatively low number of experts per MoE layer, from 4 to 16.\\nIncreasing the number of experts in this approach seems way less costly than with a regular MoE, since we would need to add m more adapters and not m more FFNs.\\n\\nMy takeaways:\\nThe sparsity crafting idea seems to pretty much be parameter-efficient sparse upcycling applied to instruction-tuning.\\nSounds possible to practically apply this to TinyLlama-1B? Or another model in the 1-3B range.\\n\\n\\nQMoE: Pr', 'more FFNs.\\n\\nMy takeaways:\\nThe sparsity crafting idea seems to pretty much be parameter-efficient sparse upcycling applied to instruction-tuning.\\nSounds possible to practically apply this to TinyLlama-1B? Or another model in the 1-3B range.\\n\\n\\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\\nMain Idea: this paper presents a technique to quantize/compress large MoE models. More specifically, it compresses Switch Transformer (1.6T parameter model, 3.2 TBs of memory needed) 20x, to only 160 GB (0.8 bits per parameter compared to 16 without any compression). This quantization technique at the Switch scale can be done in less than a day on a single GPU and results in a minor accuracy loss.\\n\\nMoE: faster inference with the tradeoff of higher memory cost.\\nUsually, post-training compression techniques only reduce parameters to around 3 or 4 bits. This would not be enough to make MoE practical – giving inspiration to QMoE.\\n\\nMoE Quantization\\nIt might suffice only quantizing MoE la', ' of higher memory cost.\\nUsually, post-training compression techniques only reduce parameters to around 3 or 4 bits. This would not be enough to make MoE practical – giving inspiration to QMoE.\\n\\nMoE Quantization\\nIt might suffice only quantizing MoE layers (not FFs).\\nLarge dense models are more resistant to quantization, so MoE models can be a good target for it (due to increase in scale seeming to relate to better teachers).\\nMoE training might be highly resistant to noise.\\nMain Challenges:\\nMemory\\nThe quantization process requires data. In the case of MoEs, the data needed is much larger than with dense models, due to the potential large number of experts. This means that it is even more important to have data that represents different parts of the distribution, so all experts are represented.\\nGPU utilization\\nLarge-scale quantization had previously been applied to dense models, which consists of applying it to single massive individual layers, which is fast and efficient on GPUs. This ca', 'of the distribution, so all experts are represented.\\nGPU utilization\\nLarge-scale quantization had previously been applied to dense models, which consists of applying it to single massive individual layers, which is fast and efficient on GPUs. This can be challenging for MoEs as instead of single massive layers there can potentially be many experts.\\n\\nQMoE Method\\nFor dense part of the model:\\nFetch one sample X, containing a few hundreds of tokens, from CPU to GPU.\\nPass it through the corresponding dense layers to obtain the result Y.\\nCalculate and store expert assignments for tokens in Y.\\nSend Y back to CPU and overwrite X in B (large buffer).\\nFor sparse part of the model (expert FFs):\\nFetch all individual tokens in B that have been assigned to expert E, denoted by Xe, from CPU to GPU.\\nUse these tokens to produce compressed expert E’ (for example, with GPTQ).\\nRun Xe through E’ to get Ye’.\\nSend Ye’ back to CPU and overwrite Xe in B.\\nIn sum:\\nThe dense part consists of passing a set of samp', 'to expert E, denoted by Xe, from CPU to GPU.\\nUse these tokens to produce compressed expert E’ (for example, with GPTQ).\\nRun Xe through E’ to get Ye’.\\nSend Ye’ back to CPU and overwrite Xe in B.\\nIn sum:\\nThe dense part consists of passing a set of samples X from CPU to GPU, performing a forward pass through only the dense layers, calculating the expert assignments of each input in X to know which experts were used and then storing the outputs of the forward pass and expert assignments in Y, passing it back to the CPU (in QMoE, the dense parts of the model are left uncompressed).\\nThe sparse part consists of performing a loop through each expert. For each expert, from buffer B, all the tokens in X assigned to that expert are taken, forming Xe. A quantized version of the expert is also formed (through a technique like GPTQ), resulting in E’. Then, Xe is passed through E’, with the outputs forming Ye’. Finally, the results Ye’ are stored in the buffer B, replacing the input tokens Xe.\\nThe me', 'A quantized version of the expert is also formed (through a technique like GPTQ), resulting in E’. Then, Xe is passed through E’, with the outputs forming Ye’. Finally, the results Ye’ are stored in the buffer B, replacing the input tokens Xe.\\nThe method described provides the main compression gains from QMoE but is not sufficient to achieve the goal of 1 bit per parameter established. To achieve this, the authors adopt GPTQ optimizations for the MoE case, GPU decoding optimizations, and more.\\n\\nResults\\nMoEs are shown to be highly robust to quantization as vanilla rounding with ternary precision does not lead to a model collapse.\\nUsing data-dependent quantization in MoE (method explained) allows 2-bit and ternary quantization with minimal accuracy loss.\\n\\nMy takeaways:\\nDue to my lack of expertise in quantization methods like GPTQ, I did not find it relevant to go into this topic more in-depth.\\n\\n\\nFast-Inference of Mixture-of-Experts Language Models with Offloading\\nMain Idea: this paper in', 'curacy loss.\\n\\nMy takeaways:\\nDue to my lack of expertise in quantization methods like GPTQ, I did not find it relevant to go into this topic more in-depth.\\n\\n\\nFast-Inference of Mixture-of-Experts Language Models with Offloading\\nMain Idea: this paper investigates a strategy to run large MoE in consumer hardware with limited accelerator memory. It succeeds in running Mixtral 8x7B in a free-tier Google Colab instance.\\n\\nMethod for MoE Generative Inference\\nEncoding the input prompt.\\nDone in parallel (layer-by-layer).\\nGenerate tokens conditioned on the input prompt.\\nDone sequentially (token-by-token and layer-by-layer).\\nIn other words, step 1 is easy to optimize since we can simply pass all tokens in parallel layer-by-layer. During token generation this is not possible since we need to pass one token at a time, making the offloading challenging to optimize for.\\n\\nImprovements from this approach\\nCaching experts\\nTo exploit the fact that previous work shows that activated experts tend to be active', ' this is not possible since we need to pass one token at a time, making the offloading challenging to optimize for.\\n\\nImprovements from this approach\\nCaching experts\\nTo exploit the fact that previous work shows that activated experts tend to be active for more than one token at a time (common for them to stay active for 2-4 tokens in a row), the experts activated from the previous token can simply be stored in a GPU cache.\\nPrefetching\\nWith dense models, offloading is simple due to the fixed order of layers to load. This is not true in MoE, so future layers cannot be pre-loaded since they are usually selected based on the previous layer’s output. To help with this, a speculative loading technique is developed based on the heuristic that the previous layer’s hidden state can be a good proxy for the next hidden state (since these hidden states are only updated and not recomputed from scratch). This allows us to predict the next layer’s experts before knowing its hidden state (in case of wr', 'revious layer’s hidden state can be a good proxy for the next hidden state (since these hidden states are only updated and not recomputed from scratch). This allows us to predict the next layer’s experts before knowing its hidden state (in case of wrong guesses, the gains are lost since we must load the experts while no computations are being done).\\nIn terms of quantization, HQQ (data-free) is used for convenience, however, other techniques such as GPTQ could also work. (QMoE was experimented with on Mixtral 8x7B, but loss in quality was too significant due to the 1-bit quantization).\\nFound that ideally experts can be quantized to 3 or even 2 bits and that attention layers should be kept at a larger bit width (16 or minimum 4 bits).\\nFor expert offloading, a cache of 2 experts per layer is used with 12 GB GPUs and of 4 for 16 GB GPUs. Additionally, 1 or 2 experts per layer are loaded speculatively as soon as the previous layer’s hidden states are available.\\n\\nResults (on Mixtral 8x7B)\\nOn', 'ert offloading, a cache of 2 experts per layer is used with 12 GB GPUs and of 4 for 16 GB GPUs. Additionally, 1 or 2 experts per layer are loaded speculatively as soon as the previous layer’s hidden states are available.\\n\\nResults (on Mixtral 8x7B)\\nOn the free Google Colab tier, inference speed is of around 2 tokens per second.\\n In terms of cache hits, the accuracy to guess the next expert goes from around 0.2 with cache size of 1 to around 0.6-0.7 for cache size of 4.\\nFor speculative loading the results are even better and show that active experts can be estimated even when 10 layers ahead (that is, using the hidden state of the 10th hidden layer behind it).\\n\\nMy takeaways:\\nAble to use this method to experiment with Mixtral in Google Colab.\\n\\n\\nRecent MoE Models\\nMixtral of Experts (+Mistral 7B)\\nMain Idea: Mixtral is a recent MoE model that is based on the Mistral architecture (Mistral is a dense model). The difference between these models is that each Mixtral layer consists of sparse FFNs', '\\n\\nRecent MoE Models\\nMixtral of Experts (+Mistral 7B)\\nMain Idea: Mixtral is a recent MoE model that is based on the Mistral architecture (Mistral is a dense model). The difference between these models is that each Mixtral layer consists of sparse FFNs, when these are dense in Mistral, with each sparse layer containing 8 experts each and being the equivalent of a 7B Mistral model.\\n\\nMistral 7B\\nMistral uses grouped-query-attention (GQA) for accelerated inference speed and reduced memory requirements (allowing larger batch sizes) and sliding window attention (SWA) for handling longer sequences at a lower computational cost. The goal of Mistral is to provide an open-source model that beats other existing open-source models of similar size while improving on inference speed and memory/computational requirements, with a focus on practical use of the model and ease of fine-tuning.\\nSliding Window Attention (SWA)\\nIn regular attention, each token in a sequence attends to every other token, resulti', 'mproving on inference speed and memory/computational requirements, with a focus on practical use of the model and ease of fine-tuning.\\nSliding Window Attention (SWA)\\nIn regular attention, each token in a sequence attends to every other token, resulting in a complexity of O(n^2) with respect to the sequence length. In SWA, the tokens attended are limited by a sliding window, which masks tokens that are farther away from the current token than a pre-defined distance. This changes the complexity to O(n*w), where w is the maximum number of tokens to be attended (maximum window size).\\nSWA reduces computational complexity and memory usage – the longer the sequences the bigger the improvement.\\nSWA, due to the fixed window size, allows for a rolling buffer cache (this increases efficiency).\\nSWA also allows for pre-fill and chunking for more efficient inference.\\nResults\\nMistral is compared to Llama 2 7B/13B, Llama 1 34B and Code-Llama 7B.\\nCompared to Llama 2 7B/13B and Llama 1 34B, Mistral perf', 'g buffer cache (this increases efficiency).\\nSWA also allows for pre-fill and chunking for more efficient inference.\\nResults\\nMistral is compared to Llama 2 7B/13B, Llama 1 34B and Code-Llama 7B.\\nCompared to Llama 2 7B/13B and Llama 1 34B, Mistral performs significantly better in complex reasoning areas (code, math, reasoning) and comes close to Code-Llama 7B in coding tasks.\\nOn knowledge tasks, Mistral also tended to perform better but the gap observed was not as significant as in complex reasoning tasks.\\nInstruction fine-tuning was performed using publicly available data to show the straightforwardness of fine-tuning on Mistral 7B.\\nThis resulted in comparable performance to 13B instruct models.\\n\\nMixtral\\nMixtral uses top-2 token-choice routing.\\nMixtral excels at math, code generation and multilingual benchmarks (consistent with Mistral).\\nA Mixtral-Instruct model (performed SFT and DPO) is also provided and surpasses GPT 3.5-Turbo.\\nThe context length of Mixtral is 32k.\\nThe gating mechani', 'ng.\\nMixtral excels at math, code generation and multilingual benchmarks (consistent with Mistral).\\nA Mixtral-Instruct model (performed SFT and DPO) is also provided and surpasses GPT 3.5-Turbo.\\nThe context length of Mixtral is 32k.\\nThe gating mechanism of Mixtral takes the SoftMax of the top-2 expert scores and weights the expert’s outputs based on these weights. \\nThe final output is then a weighted average of the sum of the two selected experts’ outputs.\\nMixtral seems to be robust to long-range contexts.\\nPerhaps due to Mistral’s SWA?\\nExperiments showed that up to a context length of 30k tokens, information can accurately be retrieved, and the perplexity of Mixtral decreases with an increase in context length.\\nThe name Mixtral 8-7B might induce the thought of the architecture having 56B total parameters (8*7), but it consists of around 47B parameters due to shared parameters between experts across the embedding, attention and normalization layers (7B is the full size of each expert if ', 'duce the thought of the architecture having 56B total parameters (8*7), but it consists of around 47B parameters due to shared parameters between experts across the embedding, attention and normalization layers (7B is the full size of each expert if converted to a dense model). Likewise, the inference cost is not the equivalent of running 14B parameters (7*2), but around 13B parameters due to these shared parameters.\\nIn terms of routing analysis, it was shown that experts seem to be selected based on syntax rather than on specific domains – experts specialize in semantics and syntax, not on tasks. This is logical due to the token-choice routing. If routing is done on a token granularity, the experts are expected to specialize on token-level areas. With domain or task-routing (done at a sequence level), experts can be expected to specialize in domain/task-level areas.\\n\\nMy takeaways:\\nThe goal of Mistral 7B is to provide an open-source model with an optimal performance and efficiency bala', ' areas. With domain or task-routing (done at a sequence level), experts can be expected to specialize in domain/task-level areas.\\n\\nMy takeaways:\\nThe goal of Mistral 7B is to provide an open-source model with an optimal performance and efficiency balance.\\nPerformance meaning quality, efficiency meaning inference speed and computational requirements.\\nSliding Window Attention seems to sacrifice the context length capacity in return of higher inference speed. The assumption taken for this not to hurt performance seems to be that the more you move away from a token, the lower the odds of it having meaningful dependencies to the current token.\\nLarge context lengths are possible under SWA, but each individual token will not use the full context length for inference if the input is larger than the maximum window size.\\nPerhaps the idea for Mixtral came after analyzing Mistral’s results? Since Mistral performs significantly better on reasoning tasks but the improvement in knowledge tasks is not ', 'ngth for inference if the input is larger than the maximum window size.\\nPerhaps the idea for Mixtral came after analyzing Mistral’s results? Since Mistral performs significantly better on reasoning tasks but the improvement in knowledge tasks is not so big, it would make sense to try to apply a MoE architecture to this model, with the idea being to retain the reasoning abilities while improving knowledge abilities. This makes sense because other studies seem to show that MoE, due to additional model capacity added, tend to perform very well on knowledge tasks (weakness of Mistral) but the performance on reasoning and fine-tuning tasks (strength of Mistral) leaves room for improvement (although MoE was shown to benefit from instruction-tuning in a more significant way than dense models).\\n\\nDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\\nMain Idea: traditional top-k MoE experts acquire non-overlapping and focused knowledge, creating challenges for', 'g in a more significant way than dense models).\\n\\nDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\\nMain Idea: traditional top-k MoE experts acquire non-overlapping and focused knowledge, creating challenges for expert specialization. DeepSeekMoE plans on architectural changes to enforce expert specialization through expert segmentation and isolating experts as shared ones to capture common/overlapping knowledge.\\n3 versions of DeepSeekMoE are trained (in total # of parameters):\\n2B\\n16B\\nCan run on a single GPU with 40GB of memory.\\nExperimented with SFT to create an instruction-tuned chat model.\\n145B\\nLargest model trained.\\n\\n2 Potential Issues of Traditional (top-k) MoE:\\nKnowledge hybridity\\nCurrent MoE models have a low number of experts (8 or 16). This division assigns each expert to a diverse part of the data, so the parameters are not used so efficiently (there is more sparsity in the data possible than reflected by the number of experts).\\nKnowledg', 'els have a low number of experts (8 or 16). This division assigns each expert to a diverse part of the data, so the parameters are not used so efficiently (there is more sparsity in the data possible than reflected by the number of experts).\\nKnowledge redundancy\\nExperts may benefit from common knowledge, but since they are isolated, some experts might end up learning the same information, causing redundancy in their parameters.\\nSolutions Proposed by DeepSeekMoE\\nFine-grained Expert Segmentation\\nSegment experts into a finer grain by splitting the FFN hidden dimension. More experts are also activated (increase the number of experts while maintaining the number of total and active parameters).\\nMore flexibility on which parameters of the experts to use – introduce sparsity within each expert – while keeping computational costs constant.\\nShared expert isolation\\nIsolate certain experts to serve as shared experts, which are always activated. The goal is for these experts to retain the common k', ' to use – introduce sparsity within each expert – while keeping computational costs constant.\\nShared expert isolation\\nIsolate certain experts to serve as shared experts, which are always activated. The goal is for these experts to retain the common knowledge between experts, avoiding parameter redundancy.\\nLeads to parameter-efficiency + increased specialization.\\n\\nArchitecture\\nAs mentioned above, DeepSeekMoE incorporates two new strategies on top of the generic MoE architecture:\\nFine-grained expert segmentation\\nNot just simply adding more experts but keeping the number of active and the number of total parameters the same while doing so.\\nA small number of experts combined with a low number of activated experts per input makes experts learn a diverse amount of knowledge when what we want is specialization.\\nTo solve this, DeepSeekMoE divides the expert’s weights (more specifically, the FFN hidden dimension) into m segments, creating another level of experts. This allows for a scaling of m', ' diverse amount of knowledge when what we want is specialization.\\nTo solve this, DeepSeekMoE divides the expert’s weights (more specifically, the FFN hidden dimension) into m segments, creating another level of experts. This allows for a scaling of m in the number of experts (if m is 8, the total number of experts will be scaled by 8, for example).\\nmN possible expert combinations vs N possible combinations.\\nThis allows for a more flexible combination of experts, since the router will not only pick specific experts, but specific segments within experts.\\nThis allows for a greater number of experts to be activated without increasing computational costs.\\nShared expert isolation\\nExperts in conventional MoE are isolated. This means that if experts have overlap in knowledge in the data fed to them, this will be learned independently, so repeated parameters will exist for the same information, bringing parameter inefficiency.\\nDeepSeekMoE has shared experts – experts that are always activated –', 's have overlap in knowledge in the data fed to them, this will be learned independently, so repeated parameters will exist for the same information, bringing parameter inefficiency.\\nDeepSeekMoE has shared experts – experts that are always activated – which have the goal of capturing this common knowledge so there is no parameter redundancy.\\nThe number of shared experts is Ks. To keep computational costs, the number of routed experts will then decrease to mN-Ks and the nonzero gates (segment activations) will be mK-Ks.\\nBalance loss\\nAn expert-level and a device-level balance loss are used, with more emphasis/weight on the device-level loss.\\n\\nExperiments (2B parameter model)\\nSubstitute all FFNs by an MoE layer.\\n9 Transformer blocks with hidden dimension of 1280.\\nRandom initialization.\\n16 experts with 4 segments each (64 total expert segments), with 1 shared segment.\\nComputation equivalent of top-k with k=2.\\n2B parameter model, 0.3B active parameters.\\nTraining of 100B tokens with 2k batch ', ' dimension of 1280.\\nRandom initialization.\\n16 experts with 4 segments each (64 total expert segments), with 1 shared segment.\\nComputation equivalent of top-k with k=2.\\n2B parameter model, 0.3B active parameters.\\nTraining of 100B tokens with 2k batch size.\\nNo dropout due to abundance of data used.\\nBaselines:\\nDense – equivalent to top-1 routing (~0.2B active parameters)\\nSwitch – equivalent to top-1 routing (~0.2B active parameters)\\nHash Layer – equivalent to top-1 routing (~0.2B active parameters)\\nGShard\\nResults\\nSwitch and Hash Layer perform better than Dense (with same number of active parameters but more total parameters).\\nGShard performs slightly better than Switch (with more active parameters).\\nDeepSeekMoE performs significantly better than GShard, with the same number of active and total parameters.\\nDeepSeekMoE closely aligns with the upper bound of MoE models (dense with same number of total parameters) (at least on the 2B total parameters scale when training with 100B tokens).\\nDee', 'an GShard, with the same number of active and total parameters.\\nDeepSeekMoE closely aligns with the upper bound of MoE models (dense with same number of total parameters) (at least on the 2B total parameters scale when training with 100B tokens).\\nDeepSeekMoE 2B performs comparably to GShard 2.9B (1.5x the expert size) (the advantages increase when scaling to 13.3B and 19.8B, respectively).\\nDeepSeekMoE 2B achieves comparable performance to Dense with FFNs scaled by 16 (same number of total parameters, 16 is number of experts per layer used).\\nAblation studies reassure the positive effects brought by fine-grained expert segmentation and shared expert isolation.\\nAdditionally, the number of shared experts (1,2 and 4 tested with 64 total experts) did not seem to make much difference. A ratio of 1:3 (shared/total activated experts) is used when scaling the architecture.\\nExpert specialization\\nDeepSeekMoE was more sensitive to disabling the top-k experts, showing that there is less common knowl', ' did not seem to make much difference. A ratio of 1:3 (shared/total activated experts) is used when scaling the architecture.\\nExpert specialization\\nDeepSeekMoE was more sensitive to disabling the top-k experts, showing that there is less common knowledge between experts, thus less redundancy.\\nShared experts are irreplaceable in DeepSeekMoE, that is, substituting a shared expert by a not-shared expert results in a significant drop in performance.\\nDeepSeekMoE can acquire knowledge more accurately and efficiently. Even using only 4 active experts (equivalent to top-1 routing), DeepSeekMoE performs similarly to GShard.\\nWhen using this setting of 4 active experts at training time, and not only at inference time, DeepSeekMoE outperforms GShard even with half of the number of active expert parameters.\\nDeepSeekMoE 16B\\nScaling up of the architecture to a model with 16B total parameters, trained on 2T tokens (same number of training tokens as Llama2-7B).\\n28 Transformer blocks, all FFNs are subst', 'en with half of the number of active expert parameters.\\nDeepSeekMoE 16B\\nScaling up of the architecture to a model with 16B total parameters, trained on 2T tokens (same number of training tokens as Llama2-7B).\\n28 Transformer blocks, all FFNs are substituted by an MoE layer except for the first one (because the first layer takes longer to converge if the FFN is substituted by an MoE layer).\\nEach MoE layer has 2 shared and 64 routed experts. Each FFN is divided into 4 experts.\\n8 experts per layer activated per input (2 shared, 6 routed), corresponding to 2.8B active parameters.\\nSimilar training setting to DeepSeekMoE 2B.\\nCompared to DeepSeek 7B (its dense counterpart):\\nDeepSeekMoE 16B, with around 40% of active computation at inference, performs comparably to DeepSeek 7B.\\nDeepSeekMoE 16B performs especially well in language modeling tasks.\\nThis indicates that scaling up the total FFN parameters helps with memorization.\\nDeepSeekMoE 16B does not perform well in multiple-choice questions.\\nA ', 'ms comparably to DeepSeek 7B.\\nDeepSeekMoE 16B performs especially well in language modeling tasks.\\nThis indicates that scaling up the total FFN parameters helps with memorization.\\nDeepSeekMoE 16B does not perform well in multiple-choice questions.\\nA possible explanation for this can be due to the attention parameters. The number of attention parameters are thought of as being crucial for MC tasks, and the MoE version has around 5x less attention parameters than its dense counterpart (0.5B vs 2.5B).\\nCompared to Llama2-7B:\\nDeepSeekMoE 16B, with about 40% of Llama2-7B activations at inference, outperforms it at most baselines (MC tasks like MMLU are the exceptions).\\nDeepSeekMoE 16B is stronger at math and reasoning tasks (strengths of Llama2-7B) probably due to the distribution of the dataset used for training.\\nDespite being trained on less English text, DeepSeekMoE 16B achieves equal or better performance at English understanding and knowledge-intensive tasks.\\nConsistent with MoE’s advan', 'B) probably due to the distribution of the dataset used for training.\\nDespite being trained on less English text, DeepSeekMoE 16B achieves equal or better performance at English understanding and knowledge-intensive tasks.\\nConsistent with MoE’s advantage in memorization due to increase total parameter count compared to dense.\\nOn Hugging Face’s Open LLM leaderboard (collection of evaluation tasks), DeepSeekMoE 16B significantly outperforms models of the same size in terms of active parameters and achieves comparable performance to Llama2-7B.\\nChat Alignment for DeepSeekMoE 16B (SFT/Instruction-Tuning)\\n3 models are compared in this section, all trained on the same data:\\nLlama2 SFT 7B – Llama 2 instruction-tuned independently from its chat version, to control for the training data.\\nDeepSeek Chat 7B.\\nDeepSeekMoE 16B Chat – has around 40% of active computations compared to the other models used in this section.\\nResults:\\nThe MoE variant achieves comparable performance to the dense models in l', 'ion, to control for the training data.\\nDeepSeek Chat 7B.\\nDeepSeekMoE 16B Chat – has around 40% of active computations compared to the other models used in this section.\\nResults:\\nThe MoE variant achieves comparable performance to the dense models in language understanding and reasoning, machine reading comprehension, and mathematical and knowledge-intensive tasks.\\nThe MoE variant performs significantly better at code generation.\\nThe gap in multiple-choice questions still exists but is narrowed.\\nScaling DeepSeekMoE to 145B Total Parameters\\nTrained on 245B tokens (will probably be scale dup in the future, so this can be seen more as a baseline).\\n62 Transformer blocks, all FFNs substituted by an MoE layer except the first one.\\n4 shared experts and 128 routed experts per MoE layer.\\nEach expert is 1/8th the size of a standard FFN (different than the ratio used for the smaller 2B and 16B models, which was 1/4th).\\nAt inference, the 4 shared experts and 12 routed experts are activated.\\nAround 2', 'and 128 routed experts per MoE layer.\\nEach expert is 1/8th the size of a standard FFN (different than the ratio used for the smaller 2B and 16B models, which was 1/4th).\\nAt inference, the 4 shared experts and 12 routed experts are activated.\\nAround 22.2B active parameters.\\nResults:\\n3 additional models were trained for comparison, using the same training corpus and hyperparameters:\\nDeepSeek 67B (dense)\\nGShard 137B (GShard architecture trained on the same data)\\nDeepSeekMoE 142B (half-activated)\\nUses half of the activations of DeepSeekMoE 145B – 2 shared experts, 6 routed experts.\\nWith similar number of active and total parameters, the MoE 145B variant significantly outperforms GShard.\\nWith only 28.5% of its active computations, the 145B MoE model reaches comparable performance to DeepSeek 67B.\\nExhibits strong performance in language understanding and knowledge-intensive tasks but struggles in multiple-choice (consistent with the 16B MoE model performance).\\nDespite having only half of the', 'odel reaches comparable performance to DeepSeek 67B.\\nExhibits strong performance in language understanding and knowledge-intensive tasks but struggles in multiple-choice (consistent with the 16B MoE model performance).\\nDespite having only half of the activated parameters, the 142B version is not too far behind from the 145B fully activated version, still matches the performance of DeepSeek 67B (with around 18.2% of its computations at inference) and easily beats GShard 137B.\\n\\nMy takeaways:\\nDeepSeekMoE has its 16B version with 7 checkpoints released to HF. This could add to a potential exploration of how experts in MoE specialize.\\n\\nOpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\\nMain Idea: an open-source project, OpenMoE analyzes decoder-only MoE LLMs from 650M to 34B total parameters and trained on up to 1T tokens (the largest version – 34B – was only trained on 200B tokens, 1T training tokens were used in the 8B version). The findings and recommendations of these e', 'analyzes decoder-only MoE LLMs from 650M to 34B total parameters and trained on up to 1T tokens (the largest version – 34B – was only trained on 200B tokens, 1T training tokens were used in the 8B version). The findings and recommendations of these experiments are shared in the paper.\\nThe 34B version (largest one trained) has 6B active parameters per input and 32 experts per MoE layer. 5 intermediate checkpoints for the 8B model (every 200B training tokens) were released, and a Chat version of this 8B model was also trained (instruction-tuned).\\n\\nDesign\\nInspired by the facts that including code data in the pre-training dataset boosts performance and that code is always precise (contrary to text), which leads the authors to think that LLMs would more easily understand it, leading to better training, code data is aggressively sampled during pre-training (excessively/to a fault).\\nGenerally, follows the ST-MoE architecture and routing design. The reason for this is ST-MoE’s focus on trainin', 'uld more easily understand it, leading to better training, code data is aggressively sampled during pre-training (excessively/to a fault).\\nGenerally, follows the ST-MoE architecture and routing design. The reason for this is ST-MoE’s focus on training stability, a characteristic OpenMoE aims to achieve.\\nTop-2 routing used during the entire training process.\\nAn MoE layer is inserted every 6 Transformer blocks, so most Transformer blocks do not have an MoE layer.\\nUse UL2 method for the training objective (mix of span corruption and prefix language modeling).\\nSFT for instruction-tuning is done on a dataset of 58k conversations, each with 1.8 turns on average, to analyze alignment (although this is not a big focus of this work).\\n\\nAnalysis\\nMoE experts did not seem to specialize at the domain or at the task levels, but at the token level.\\nThis is intuitive and rather obvious since the routing is done at the token-level.\\nContext-independent specialization\\nMoE routing is done based on token ID', 'xperts did not seem to specialize at the domain or at the task levels, but at the token level.\\nThis is intuitive and rather obvious since the routing is done at the token-level.\\nContext-independent specialization\\nMoE routing is done based on token ID and independent of the context around that token. This means that the routing is not really done based on semantics (context) but on syntax (the token being routed).\\nExperts cluster tokens together, that is, they seem to specialize on a specific cluster of the token input space (the raw token’s embeddings without regard to context). Similar tokens are routed to the same expert.\\nThe token routing is learned at very early stages of training and remains fixed throughout the rest of training.\\nDrop-Towards-the-End\\nDue to this fixed routing characteristic, something like instruction-tuning can lead to issues. This is because instruction-tuning data is out-of-domain, presenting a distribution shift from the pre-training data. Since the routing is', 'Towards-the-End\\nDue to this fixed routing characteristic, something like instruction-tuning can lead to issues. This is because instruction-tuning data is out-of-domain, presenting a distribution shift from the pre-training data. Since the routing is learned from the pre-training data and is fixed, the distribution shift from instruction-tuning data will lead to overloaded experts, subsequently leading to token dropping in later rounds of the conversation (assuming multi-turn chat).\\n\\nTakeaways/Recommendations\\nThe amount of code present in the pre-training data of over 50% was too aggressive (around 30% is recommended instead) and hurt the performance of the model in text tasks.\\nThe finding that MoE routing is fixed and established at early stages of training indicates that the router can be frozen after a warmup stage.\\nThe Context-Independent Specialization of experts indicates that the FFN (expert) computation can be done independently from the attention layer, thus an approach that w', 'stages of training indicates that the router can be frozen after a warmup stage.\\nThe Context-Independent Specialization of experts indicates that the FFN (expert) computation can be done independently from the attention layer, thus an approach that would compute the expert FFN and the attention layers in parallel would make sense, bringing a speedup in training and inference.\\nFuture research proposition.\\nTo alleviate the Drop-Towards-the-End issue, mixing instruction-tuning data into the pre-training data mix while the routing is being learned (the warmup stage) can be effective. This would allow the router to learn the instruction-tuning data distribution, so the token dropping issue experienced in later rounds of multi-chat conversation would be somewhat mitigated.\\n\\nMy takeaways:\\n5 checkpoints for the 8B OpenMoE model were released. This could potentially add to the routing analysis project I have planned.\\nThe conclusion that experts specialize on a specific cluster of the token inpu', 'ould be somewhat mitigated.\\n\\nMy takeaways:\\n5 checkpoints for the 8B OpenMoE model were released. This could potentially add to the routing analysis project I have planned.\\nThe conclusion that experts specialize on a specific cluster of the token input space seems to be inconsistent with the Hash Layers paper comparison of cluster-based hashing vs the opposite.\\nThe conclusion that token routing is fixed at very early stages of training seems to be inconsistent with the analysis done in the StableMoE paper.\\n\\n\\n\\n\\nMultimodal MoE\\nMultimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts\\n\\nMoE-Llava: Mixture of Experts for Large Vision-Language Models (+ Visual Instruction Tuning aka Llava)\\n\\nLlava-Phi: Efficient Multi-Modal Assistant with Small Language Model\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Assistant with Small Language Model\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'], 'source_name': 'MoE NOTES', 'source_url': 'https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE%20Notes.docx'}\n"
     ]
    }
   ],
   "source": [
    "for key in chunked_contents.keys():\n",
    "    print(chunked_contents[key])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe-rag-chatbot-uQH9tqUR-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
