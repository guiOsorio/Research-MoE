# Research-MoE

## Understanding MoE
- Learning Factorized Representations in a Deep Mixture-of-Experts
- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
- MoE Architecture
- Towards Understanding MoE
- OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

## Routing & Architecture
- BASE Layers: Simplifying Training of Large, Sparse Models
- DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning
- Hash Layers for Large Sparse Models
- Mixture-of-Experts with Expert Choice Routing
- From Sparse to Sparse: Mixture of Experts Cross-Example Aggregation
- MixMoE: Stable Routing Strategy for Mixture of Experts
- StabLeNet: Expertsâ€™ Stability on MoE

## Deep & Stable MoE
- ScalableMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

## Scaling & Stability
- Switch: Scaling Giant Models with Conditional Computation and Automatic Sharding
- GShard Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
- GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
- ST-MoE: Designing Stable and Transferable Sparse Expert Models
- Unified Scaling Laws For Routed Language Models
- Efficent Large Scale Language Modeling with Mixture of Experts

## Task/Doman-level MoE
- Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference
- Expert Gate: Scaling Large-scale Generative Language Modeling
- DEMix Layers: Domain-Level Mixture of Experts for Efficient Adaptation
- Expert Deviation: Scaling Expert Language Models with Unsupervised Domain Discovery
- Exploring the Benefits of Training Expert Language Models over Instruction Tuning

## MoE Efficiency
- MegaBlocks: Efficient Sparse Training with Mixture-of-Experts
- Fast Feedback: Efficient Networks + Exponentially Faster Language Modeling
- Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models
- Pushing Forward MoE-Experts to the Limit: Extremely Parameter-Efficient MoE for Instruction Tuning
- Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks
- QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models

## Fast-Inference MoE
- Fast-Inference of Mixture-of-Experts Language Models with Offloading

## Hybrid Approaches
- Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints
- EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate
- Soft Merging of Experts with Adaptive Routing
- Bloc-Mamba: Efficient Selective State Space Models with Mixture of Experts
- MaE: MaE: Mixture of Experts for Space-Models
- Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models


# TO BE ADDED IN THE FUTURE

- Routing to the Expert: Efficient Reward-guided Ensemble of Large
Language Models
- Scalable Modular Network: a Framework for Adaptive Learning via Agreement Routing
- ComPEFT: Compression for Communicating Parameter Efficient Updates via
Sparsification and Quantization
- Emergent Mixture-of-Experts: Can Dense Pre-Trained Transformers Benefit From Emergent Modular Structures?
- Merge, Then Compress: Demystify Efficient SMOE With Hints From Its Routing Policy
- MOLE: Mixture of LoRA Experts
- Statistical Perspective of Top-k Sparse Softmax Gating Mixture of Experts
- Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
- Fusing Models With Complementary Expertise
- Divide and Not Forget: Ensemble of Selectively Trained Experts in Continual Learning
- Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

## Multimodal MoE
- Multimodal Mixture of Experts for Single-Modal Language Model
- MoE-Livia: Contrastive Learning with Vision: the Language-Image Mixture of Experts
- Live-Phi: Efficient Multi-Modal Assistant with Small Language Model
