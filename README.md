# Research-MoE

## Understanding MoE
- Learning Factorized Representations in a Deep Mixture-of-Experts
- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
- HuggingFace MoE Article
- Towards Understanding MoE
- OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

## Routing & Architecture
- BASE Layers: Simplifying Training of Large, Sparse Models
- DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning
- Hash Layers for Large Sparse Models
- Mixture-of-Experts with Expert Choice Routing
- From Sparse to Soft Mixture of Experts
- Mixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation
- StableMoE: Stable Routing Strategy for Mixture of Experts
- Mixtral of Experts (+Mistral 7B)
- DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

## Scaling & Stability
- GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
- GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
- ST-MoE: Designing Stable and Transferable Sparse Expert Models
- Unified Scaling Laws for Routed Language Models
- Efficient Large Scale Language Modeling with Mixtures of Experts
- 
## Task/Domain-level MoE
- Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference
- Expert Gate: Lifelong Learning with a Network of Experts
- DEMix Layers: Disentangling Domains for Modular Language Modeling
- Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models
- Scaling Expert Language Models with Unsupervised Domain Discovery
- Exploring the Benefits of Training Expert Language Models over Instruction Tuning

## MoE Efficiency
- MegaBlocks: Efficient Sparse Training with Mixture-of-Experts
- Fast Feedforward Networks + Exponentially Faster Language Modeling
- Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models
- Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
- Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks
- QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models
- Fast-Inference of Mixture-of-Experts Language Models with Offloading

## Hybrid Approaches
- Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints
- EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate
- Soft Merging of Experts with Adaptive Routing
- MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
- BlackMamba: Mixture of Experts for State-Space Models
- Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models





# TO BE ADDED IN THE FUTURE

- Routing to the Expert: Efficient Reward-guided Ensemble of Large
Language Models
- Scalable Modular Network: a Framework for Adaptive Learning via Agreement Routing
- ComPEFT: Compression for Communicating Parameter Efficient Updates via
Sparsification and Quantization
- Emergent Mixture-of-Experts: Can Dense Pre-Trained Transformers Benefit From Emergent Modular Structures?
- Merge, Then Compress: Demystify Efficient SMOE With Hints From Its Routing Policy
- MOLE: Mixture of LoRA Experts
- Statistical Perspective of Top-k Sparse Softmax Gating Mixture of Experts
- Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
- Fusing Models With Complementary Expertise
- Divide and Not Forget: Ensemble of Selectively Trained Experts in Continual Learning
- Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

## Multimodal MoE
- Multimodal Mixture of Experts for Single-Modal Language Model
- MoE-Livia: Contrastive Learning with Vision: the Language-Image Mixture of Experts
- Live-Phi: Efficient Multi-Modal Assistant with Small Language Model
