paper_names = {'MoE Notes.docx': ("MoE NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE%20Notes.docx"),
 'MoE Notes FINAL.docx': ("MoE NOTES FINAL", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE%20Notes%20FINAL.docx"),
 'Unified_Scaling_Laws_NOTES.pdf': ("Unified Scaling Laws for Routes Language Models NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Unified_Scaling_Laws_NOTES.pdf"),
 'Switch_Transformers.pdf': ("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "https://arxiv.org/abs/2101.03961"),
 'ST_MoE.docx': ("ST-MoE: Designing Stable and Transferable Sparse Expert Models", "https://arxiv.org/abs/2202.08906"),
 'GLaM.pdf': ("GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "https://arxiv.org/abs/2112.06905"),
 'GShard.pdf': ("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "https://arxiv.org/abs/2006.16668"),
 'Unified_Scaling_Laws.pdf': ("Unified Scaling Laws for Routes Language Models", "https://arxiv.org/abs/2202.01169"),
 'ST_MoE.pdf': ("ST-MoE: Designing Stable and Transferable Sparse Expert Models", "https://arxiv.org/abs/2202.08906"),
 'GLaM_NOTES.pdf': ("GLaM: Efficient Scaling of Language Models with Mixture-of-Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GLaM_NOTES.pdf"),
 'Efficient_Large_Scale_LM.docx': ("Efficient Large Scale Language Modeling with Mixtures of Experts", "https://arxiv.org/abs/2112.10684"),
 'Unified_Scaling_Laws.docx': ("Unified Scaling Laws for Routed Language Models", "https://arxiv.org/abs/2202.01169"),
 'GShard.docx': ("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "https://arxiv.org/abs/2006.16668"),
 'ST_MoE_NOTES.pdf': ("ST-MoE: Designing Stable and Transferable Sparse Expert Models NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/ST_MoE_NOTES.pdf"),
 'Switch_Transformers.docx': ("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "https://arxiv.org/abs/2101.03961"),
 'GLaM.docx': ("GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "https://arxiv.org/abs/2112.06905"),
 'Switch_Transformers_NOTES.pdf': ("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Switch_Transformers_NOTES.pdf"),
 'GShard_NOTES.pdf': ("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/GShard_NOTES.pdf"),
 'Efficient_Large_Scale_LM.pdf': ("Efficient Large Scale Language Modeling with Mixtures of Experts", "https://arxiv.org/abs/2112.10684"),
 'Efficient_Large_Scale_LM_NOTES.pdf': ("Efficient Large Scale Language Modeling with Mixtures of Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Scaling_and_Stability/Efficient_Large_Scale_LM_NOTES.pdf"),
 'Benefits_of_ELMs_NOTES.docx': ("Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Benefits_of_ELMs_NOTES.pdf"),
 'BTM_NOTES.docx': ("Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BTM_NOTES.pdf"),
 'BTM.pdf': ("Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models", "https://arxiv.org/abs/2208.03306"),
 'Benefits_of_ELMs.pdf': ("Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES", "https://arxiv.org/abs/2302.03202"),
 'Expert_Gate_NOTES.pdf': ("Expert Gate: Lifelong Learning with a Network of Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Expert_Gate_NOTES.pdf"),
 'BeyondDistillation_Task_Level_MoE.pdf': ("Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference", "https://arxiv.org/abs/2110.03742"),
 'cBTM_NOTES.docx': ("Scaling Expert Language Models with Unsupervised Domain Discovery NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/cBTM_NOTES.pdf"),
 'Expert_Gate_NOTES.docx': ("Expert Gate: Lifelong Learning with a Network of Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Expert_Gate_NOTES.pdf"),
 'BeyondDistillation_Task_Level_MoE_NOTES.docx': ("Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BeyondDistillation_Task_Level_MoE_NOTES.pdf"),
 'DEMix_NOTES.pdf': ("DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/DEMix_NOTES.pdf"),
 'BTM_NOTES.pdf': ("Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BTM_NOTES.pdf"),
 'DEMix.pdf': ("DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES", "https://arxiv.org/abs/2108.05036"),
 'cBTM_NOTES.pdf': ("Scaling Expert Language Models with Unsupervised Domain Discovery NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/cBTM_NOTES.pdf"),
 'BeyondDistillation_Task_Level_MoE_NOTES.pdf': ("Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/BeyondDistillation_Task_Level_MoE_NOTES.pdf"),
 'Expert_Gate.pdf': ("Expert Gate: Lifelong Learning with a Network of Experts", "https://arxiv.org/abs/1611.06194"),
 'cBTM.pdf': ("Scaling Expert Language Models with Unsupervised Domain Discovery", "https://arxiv.org/abs/2303.14177"),
 'DEMix_NOTES.docx': ("DEMix Layers: Disentangling Domains for Modular Language Modeling NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/DEMix_NOTES.pdf"),
 'Benefits_of_ELMs_NOTES.pdf': ("Exploring the Benefits of Training Expert Language Models over Instruction Tuning NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/TaskDomain_Level_MoE/Benefits_of_ELMs_NOTES.pdf"),
 'MoE_Mamba.pdf': ("MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts", "https://arxiv.org/abs/2401.04081"),
 'MoE_meets_instruction_tuning_NOTES.docx': ("Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf"),
 'BlackMamba.pdf': ("BlackMamba: Mixture of Experts for State-Space Models", "https://arxiv.org/abs/2402.01771"),
 'Soft_Merging_of_Experts_NOTES.docx': ("Soft Merging of Experts with Adaptive Routing NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Soft_Merging_of_Experts_NOTES.pdf"),
 'MoE_meets_instruction_tuning.pdf': ("Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "https://arxiv.org/abs/2305.14705"),
 'EvoMoE_NOTES.docx': ("EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/EvoMoE_NOTES.docx"),
 'MoE_Mamba_NOTES.pdf': ("MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_Mamba_NOTES.pdf"),
 'Sparse_Upcycling.pdf': ("Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints", "https://arxiv.org/abs/2212.05055"),
 'Soft_Merging_of_Experts.pdf': ("Soft Merging of Experts with Adaptive Routing", "https://arxiv.org/abs/2306.03745"),
 'Sparse_Upcycling_NOTES.pdf': ("Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Sparse_Upcycling_NOTES.pdf"),
 'MoE_meets_instruction_tuning_NOTES.pdf': ("Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf"),
 'Soft_Merging_of_Experts_NOTES.pdf': ("Soft Merging of Experts with Adaptive Routing NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Soft_Merging_of_Experts_NOTES.pdf"),
 'EvoMoE.pdf': ("EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate", "https://arxiv.org/abs/2112.14397"),
 'BlackMamba_NOTES.pdf': ("BlackMamba: Mixture of Experts for State-Space Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/BlackMamba_NOTES.pdf"),
 'MoE_Mamba_NOTES.docx': ("MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_Mamba_NOTES.pdf"),
 'Sparse_Upcycling_NOTES.docx': ("Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/Sparse_Upcycling_NOTES.pdf"),
 'EvoMoE_NOTES.pdf': ("EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate NOTES", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/EvoMoE_NOTES.docx"),
 'BlackMamba_NOTES.docx': ("BlackMamba: Mixture of Experts for State-Space Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/BlackMamba_NOTES.pdf"),
 'PE_SparsityCrafting_NOTES.pdf': ("Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_SparsityCrafting_NOTES.pdf"),
 'MegaBlocks.pdf': ("MegaBlocks: Efficient Sparse Training with Mixture-of-Experts", "https://arxiv.org/abs/2211.15841"),
 'QMoE.pdf': ("QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models", "https://arxiv.org/abs/2310.16795"),
 'QMoE_NOTES.pdf': ("QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/QMoE_NOTES.pdf"),
 'PE_SparsityCrafting_NOTES.docx': ("Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_SparsityCrafting_NOTES.pdf"),
 'PE_MoE_for_LMs_NOTES.docx': ("Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_MoE_for_LMs_NOTES.pdf"),
 'FastInferenceMoE.pdf': ("Fast-Inference of Mixture-of-Experts Language Models with Offloading", "https://arxiv.org/abs/2312.17238"),
 'ExtremelyPE_MoE_for_InstructionTuning_NOTES.docx': ("Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/ExtremelyPE_MoE_for_InstructionTuning_NOTES.docx"),
 'QMoE_NOTES.docx': ("QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/QMoE_NOTES.pdf"),
 'ExtremelyPE_MoE_for_InstructionTuning.pdf': ("Pushing Mixture-of-Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "https://arxiv.org/abs/2309.05444"),
 'FastInferenceMoE_NOTES.docx': ("Fast-Inference of Mixture-of-Experts Language Models with Offloading", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FastInferenceMoE_NOTES.pdf"),
 'MegaBlocks_NOTES.pdf': ("MegaBlocks: Efficient Sparse Training with Mixture-of-Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/MegaBlocks_NOTES.pdf"),
 'PE_SparsityCrafting.pdf': ("Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "https://arxiv.org/abs/2401.02731"),
 'FastInferenceMoE_NOTES.pdf': ("Fast-Inference of Mixture-of-Experts Language Models with Offloading", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FastInferenceMoE_NOTES.pdf"),
 'MegaBlocks_NOTES.docx': ("MegaBlocks: Efficient Sparse Training with Mixture-of-Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/MegaBlocks_NOTES.pdf"),
 'PE_MoE_for_LMs.pdf': ("Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models", "https://arxiv.org/abs/2203.01104"),
 'PE_MoE_for_LMs_NOTES.pdf': ("Parameter-Efficient Mixture-of-Experts Architecture for Pre-Trained Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/PE_MoE_for_LMs_NOTES.pdf"),
 'FFFs_NOTES.docx': ("Fast Feedforward Networks", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FFFs/FFFs_NOTES.pdf"),
 'FFFs_NOTES.pdf': ("Fast Feedforward Networks", "https://github.com/guiOsorio/Research-MoE/blob/master/research/MoE_Efficiency/FFFs/FFFs_NOTES.pdf"),
 'FFF.pdf': ("Fast Feedforward Networks", "https://arxiv.org/abs/2308.14711"),
 'FFF_to_language.pdf': ("Exponentially Faster Language Modeling", "https://arxiv.org/abs/2311.10770"),
 'MixtureOfTokens_NOTES.docx': ("Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf"),
 'BASE_layers_NOTES.docx': ("BASE Layers: Simplifying Training of Large, Sparse Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/BASE_layers_NOTES.pdf"),
 'DSelect_k_NOTES.pdf': ("DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DSelect_k_NOTES.pdf"),
 'StableMoE_NOTES.docx': ("StableMoE: Stable Routing Strategy for Mixture of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/StableMoE_NOTES.pdf"),
 'Hash_Layers_NOTES.pdf': ("Hash Layers for Large Sparse Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Hash_Layers_NOTES.pdf"),
 'Expert_Choice_NOTES.docx': ("Mixture-of-Experts with Expert Choice Routing", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Expert_Choice_NOTES.pdf"),
 'Soft_MoE_NOTES.pdf': ("From Sparse to Soft Mixture of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Soft_MoE_NOTES.pdf"),
 'Soft_MoE_NOTES.docx': ("From Sparse to Soft Mixture of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Soft_MoE_NOTES.pdf"),
 'Expert_Choice_NOTES.pdf': ("Mixture-of-Experts with Expert Choice Routing", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Expert_Choice_NOTES.pdf"),
 'Expert_Choice.pdf': ("Mixture-of-Experts with Expert Choice Routing", "https://arxiv.org/abs/2202.09368"),
 'Soft_MoE.pdf': ("From Sparse to Soft Mixture of Experts", "https://arxiv.org/abs/2308.00951"),
 'BASE_layers.pdf': ("BASE Layers: Simplifying Training of Large, Sparse Models", "https://arxiv.org/abs/2103.16716"),
 'MixtureOfTokens.pdf': ("Mixture-of-Tokens: Efficient LLMs Through Cross-Example Aggregation", "https://arxiv.org/abs/2310.15961"),
 'DeepSeekMoE.pdf': ("DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", "https://arxiv.org/abs/2401.06066"),
 'StableMoE.pdf': ("StableMoE: Stable Routing Strategy for Mixture of Experts", "https://arxiv.org/abs/2204.08396"),
 'DeepSeekMoE_NOTES.pdf': ("DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DeepSeekMoE_NOTES.pdf"),
 'DSelect_k_NOTES.docx': ("DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DSelect_k_NOTES.pdf"),
 'MixtureOfTokens_NOTES.pdf': ("Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Hybrid_Approaches/MoE_meets_instruction_tuning_NOTES.pdf"),
 'Hash_Layers_NOTES.docx': ("Hash Layers for Large Sparse Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Hash_Layers_NOTES.pdf"),
 'StableMoE_NOTES.pdf': ("StableMoE: Stable Routing Strategy for Mixture of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/StableMoE_NOTES.pdf"),
 'BASE_layers_NOTES.pdf': ("BASE Layers: Simplifying Training of Large, Sparse Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/BASE_layers_NOTES.pdf"),
 'DeepSeekMoE_NOTES.docx': ("DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/DeepSeekMoE_NOTES.pdf"),
 'DSelect_k.pdf': ("DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "https://arxiv.org/abs/2106.03760"),
 'Hash_Layers.pdf': ("Hash Layers for Large Sparse Models", "https://arxiv.org/abs/2106.04426"),
 'Mistral.pdf': ("Mistral 7B", "https://arxiv.org/abs/2310.06825"),
 'Mixtral.pdf': ("Mixtral of Experts", "https://arxiv.org/abs/2401.04088"),
 'Mixtral_NOTES.docx': ("Mixtral of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Mixtral/Mixtral_NOTES.pdf"),
 'Mixtral_NOTES.pdf': ("Mixtral of Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Routing_and_Architecture/Mixtral/Mixtral_NOTES.pdf"),
 'Towards_Understanding_MoE_NOTES.pdf': ("Towards Understanding MoE", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Towards_Understanding_MoE_NOTES.pdf"),
 'Towards_Understanding_MoE.pdf': ("Towards Understanding MoE", "https://arxiv.org/abs/2208.02813"),
 'Mixture of Experts Explained_HF_NOTES.docx': ("HuggingFace MoE Article", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Mixture%20of%20Experts%20Explained_HF_NOTES.pdf"),
 'Mixture of Experts Explained_HF_NOTES.pdf': ("HuggingFace MoE Article", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Mixture%20of%20Experts%20Explained_HF_NOTES.pdf"),
 'Sparsely_Gated_MoE_NOTES.pdf': ("Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Sparsely_Gated_MoE_NOTES.pdf"),
 'Sparsely_Gated_MoE.pdf': ("Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "https://arxiv.org/abs/1701.06538"),
 'Towards_Understanding_MoE_NOTES.docx': ("Towards Understanding MoE", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Towards_Understanding_MoE_NOTES.pdf"),
 'Learning_Factored_Representations_NOTES.docx': ("Learning Factorized Representations in a Deep Mixture-of-Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf"),
 'Original_MoE.pdf': ("Adaptive Mixture of Local Experts", "https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf"),
 'Learning_Factored_Representations_NOTES.pdf': ("Learning Factorized Representations in a Deep Mixture-of-Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf"),
 'Mixture of Experts Explained_HF.pdf': ("HuggingFace MoE Article", "https://huggingface.co/blog/moe"),
 'Sparsely_Gated_MoE_NOTES.docx': ("Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Sparsely_Gated_MoE_NOTES.pdf"),
 'OpenMoE.pdf': ("OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models", "https://arxiv.org/abs/2402.01739"),
 'OpenMoE_NOTES.docx': ("OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/OpenMoE_NOTES.pdf"),
 'MoESurvey.pdf': ("A Review of Sparse Expert Models in Deep Learning", "https://arxiv.org/abs/2209.01667"),
 'OpenMoE_NOTES.pdf': ("OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/OpenMoE_NOTES.pdf"),
 'Learning_Factored_Representations.pdf': ("Learning Factorized Representations in a Deep Mixture-of-Experts", "https://github.com/guiOsorio/Research-MoE/blob/master/research/Understanding_MoE/Learning_Factored_Representations_NOTES.pdf"),
 }